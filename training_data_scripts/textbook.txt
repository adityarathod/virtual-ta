ALGORITHMSINTRODUCTION TO
THIRD EDITIONTHOMAS H .  
CHARLES E.       
RONALD L .  
CLIFFORD STEINRIVESTLEISERSONCORMENIntroduction to Algorithms
Third EditionThomas H. Cormen
Charles E. LeisersonRonald L. RivestClifford Stein
Introduction to Algorithms
Third Edition
The MIT Press
Cambridge, Massachusetts London, Englandc/CR2009 Massachusetts Institute of Technology
All rights reserved. No part of this book may be reproduced in any form or by any electronic or mechanical means
(including photocopying, recording, or information storage and retrieval) without permission in writing from the
publisher.
For information about special quantity discounts, please email special
 sales@mitpress.mit.edu.
This book was set in Times Roman and Mathtime Pro 2 by the authors.Printed and bound in the United States of America.Library of Congress Cataloging-in-Publication DataIntroduction to algorithms / Thomas H. Cormen ...[ e ta l . ] . — 3 r de d .
p. cm.
Includes bibliographical references and index.
ISBN 978-0-262-03384-8 (hardcover : alk. paper)—ISBN 978-0-262-53305-8 (pbk. : alk. paper)
1. Computer programming. 2. Computer algorithms. I. Cormen, Thomas H.
QA76.6.I5858 2009
005.1—dc22
2009008593
1 098765432Contents
Preface xiii
I Foundations
Introduction 3
1 The Role of Algorithms in Computing 5
1.1 Algorithms 5
1.2 Algorithms as a technology 11
2 Getting Started 16
2.1 Insertion sort 16
2.2 Analyzing algorithms 23
2.3 Designing algorithms 29
3 Growth of Functions 43
3.1 Asymptotic notation 43
3.2 Standard notations and common functions 53
4 Divide-and-Conquer 65
4.1 The maximum-subarray problem 68
4.2 Strassen’s algorithm for matrix multiplication 75
4.3 The substitution method for solving recurrences 83
4.4 The recursion-tree method for solving recurrences 88
4.5 The master method for solving recurrences 93
? 4.6 Proof of the master theorem 97
5 Probabilistic Analysis and Randomized Algorithms 114
5.1 The hiring problem 114
5.2 Indicator random variables 118
5.3 Randomized algorithms 122
? 5.4 Probabilistic analysis and further uses of indicator random variables
130vi Contents
II Sorting and Order Statistics
Introduction 147
6H e a p s o r t 151
6.1 Heaps 151
6.2 Maintaining the heap property 154
6.3 Building a heap 156
6.4 The heapsort algorithm 159
6.5 Priority queues 162
7 Quicksort 170
7.1 Description of quicksort 170
7.2 Performance of quicksort 174
7.3 A randomized version of quicksort 179
7.4 Analysis of quicksort 180
8 Sorting in Linear Time 191
8.1 Lower bounds for sorting 191
8.2 Counting sort 194
8.3 Radix sort 197
8.4 Bucket sort 200
9 Medians and Order Statistics 213
9.1 Minimum and maximum 214
9.2 Selection in expected linear time 215
9.3 Selection in worst-case linear time 220
III Data Structures
Introduction 229
10 Elementary Data Structures 232
10.1 Stacks and queues 232
10.2 Linked lists 236
10.3 Implementing pointers and objects 241
10.4 Representing rooted trees 246
11 Hash Tables 253
11.1 Direct-address tables 254
11.2 Hash tables 256
11.3 Hash functions 262
11.4 Open addressing 269
? 11.5 Perfect hashing 277Contents vii
12 Binary Search Trees 286
12.1 What is a binary search tree? 286
12.2 Querying a binary search tree 289
12.3 Insertion and deletion 294
? 12.4 Randomly built binary search trees 299
13 Red-Black Trees 308
13.1 Properties of red-black trees 308
13.2 Rotations 312
13.3 Insertion 315
13.4 Deletion 323
14 Augmenting Data Structures 339
14.1 Dynamic order statistics 339
14.2 How to augment a data structure 345
14.3 Interval trees 348
IV Advanced Design and Analysis Techniques
Introduction 357
15 Dynamic Programming 359
15.1 Rod cutting 360
15.2 Matrix-chain multiplication 370
15.3 Elements of dynamic programming 378
15.4 Longest common subsequence 390
15.5 Optimal binary search trees 397
16 Greedy Algorithms 414
16.1 An activity-selection problem 415
16.2 Elements of the greedy strategy 423
16.3 Huffman codes 428
? 16.4 Matroids and greedy methods 437
? 16.5 A task-scheduling problem as a matroid 443
17 Amortized Analysis 451
17.1 Aggregate analysis 452
17.2 The accounting method 456
17.3 The potential method 459
17.4 Dynamic tables 463viii Contents
V Advanced Data Structures
Introduction 481
18 B-Trees 484
18.1 Deﬁnition of B-trees 488
18.2 Basic operations on B-trees 491
18.3 Deleting a key from a B-tree 499
19 Fibonacci Heaps 505
19.1 Structure of Fibonacci heaps 507
19.2 Mergeable-heap operations 510
19.3 Decreasing a key and deleting a node 518
19.4 Bounding the maximum degree 523
20 van Emde Boas Trees 531
20.1 Preliminary approaches 532
20.2 A recursive structure 536
20.3 The van Emde Boas tree 545
21 Data Structures for Disjoint Sets 561
21.1 Disjoint-set operations 561
21.2 Linked-list representation of disjoint sets 564
21.3 Disjoint-set forests 568
? 21.4 Analysis of union by rank with path compression 573
VI Graph Algorithms
Introduction 587
22 Elementary Graph Algorithms 589
22.1 Representations of graphs 589
22.2 Breadth-ﬁrst search 594
22.3 Depth-ﬁrst search 603
22.4 Topological sort 612
22.5 Strongly connected components 615
23 Minimum Spanning Trees 624
23.1 Growing a minimum spanning tree 625
23.2 The algorithms of Kruskal and Prim 631Contents ix
24 Single-Source Shortest Paths 643
24.1 The Bellman-Ford algorithm 651
24.2 Single-source shortest paths in directed acyclic graphs 655
24.3 Dijkstra’s algorithm 658
24.4 Difference constraints and shortest paths 664
24.5 Proofs of shortest-paths properties 671
25 All-Pairs Shortest Paths 684
25.1 Shortest paths and matrix multiplication 686
25.2 The Floyd-Warshall algorithm 693
25.3 Johnson’s algorithm for sparse graphs 700
26 Maximum Flow 708
26.1 Flow networks 709
26.2 The Ford-Fulkerson method 714
26.3 Maximum bipartite matching 732
? 26.4 Push-relabel algorithms 736
? 26.5 The relabel-to-front algorithm 748
VII Selected Topics
Introduction 769
27 Multithreaded Algorithms 772
27.1 The basics of dynamic multithreading 774
27.2 Multithreaded matrix multiplication 792
27.3 Multithreaded merge sort 797
28 Matrix Operations 813
28.1 Solving systems of linear equations 813
28.2 Inverting matrices 827
28.3 Symmetric positive-deﬁnite matrices and least-squares approximation
832
29 Linear Programming 843
29.1 Standard and slack forms 850
29.2 Formulating problems as linear programs 859
29.3 The simplex algorithm 864
29.4 Duality 879
29.5 The initial basic feasible solution 886x Contents
30 Polynomials and the FFT 898
30.1 Representing polynomials 900
30.2 The DFT and FFT 906
30.3 Efﬁcient FFT implementations 915
31 Number-Theoretic Algorithms 926
31.1 Elementary number-theoretic notions 927
31.2 Greatest common divisor 933
31.3 Modular arithmetic 939
31.4 Solving modular linear equations 946
31.5 The Chinese remainder theorem 950
31.6 Powers of an element 954
31.7 The RSA public-key cryptosystem 958
? 31.8 Primality testing 965
? 31.9 Integer factorization 975
32 String Matching 985
32.1 The naive string-matching algorithm 988
32.2 The Rabin-Karp algorithm 990
32.3 String matching with ﬁnite automata 995
? 32.4 The Knuth-Morris-Pratt algorithm 1002
33 Computational Geometry 1014
33.1 Line-segment properties 1015
33.2 Determining whether any pair of segments intersects 1021
33.3 Finding the convex hull 1029
33.4 Finding the closest pair of points 1039
34 NP-Completeness 1048
34.1 Polynomial time 1053
34.2 Polynomial-time veriﬁcation 1061
34.3 NP-completeness and reducibility 1067
34.4 NP-completeness proofs 1078
34.5 NP-complete problems 1086
35 Approximation Algorithms 1106
35.1 The vertex-cover problem 1108
35.2 The traveling-salesman problem 1111
35.3 The set-covering problem 1117
35.4 Randomization and linear programming 1123
35.5 The subset-sum problem 1128Contents xi
VIII Appendix: Mathematical Background
Introduction 1143
A Summations 1145
A.1 Summation formulas and properties 1145
A.2 Bounding summations 1149
B Sets, Etc. 1158
B.1 Sets 1158
B.2 Relations 1163
B.3 Functions 1166
B.4 Graphs 1168
B.5 Trees 1173
C Counting and Probability 1183
C.1 Counting 1183
C.2 Probability 1189
C.3 Discrete random variables 1196
C.4 The geometric and binomial distributions 1201
? C.5 The tails of the binomial distribution 1208
D Matrices 1217
D.1 Matrices and matrix operations 1217
D.2 Basic matrix properties 1222
Bibliography 1231
Index 1251Preface
Before there were computers, there were algorithms. But now that there are com-
puters, there are even more algorithms, and algorithms lie at the heart of computing.
This book provides a comprehensive introduction to the modern study of com-
puter algorithms. It presents many algorithms and covers them in considerabledepth, yet makes their design and analysis accessible to all levels of readers. Wehave tried to keep explanations elementary without sacriﬁcing depth of coverageor mathematical rigor.
Each chapter presents an algorithm, a design technique, an application area, or a
related topic. Algorithms are described in English and in a pseudocode designed to
be readable by anyone who has done a little programming. The book contains 244
ﬁgures—many with multiple parts—illustrating how the algorithms work. Since
we emphasize efﬁciency as a design criterion, we include careful analyses of the
running times of all our algorithms.
The text is intended primarily for use in undergraduate or graduate courses in
algorithms or data structures. Because it discusses engineering issues in algorithm
design, as well as mathematical aspects, it is equally well suited for self-study bytechnical professionals.
In this, the third edition, we have once again updated the entire book. The
changes cover a broad spectrum, including new chapters, revised pseudocode, anda more active writing style.
To the teacher
We have designed this book to be both versatile and complete. You should ﬁnd it
useful for a variety of courses, from an undergraduate course in data structures upthrough a graduate course in algorithms. Because we have provided considerablymore material than can ﬁt in a typical one-term course, you can consider this bookto be a “buffet” or “smorgasbord” from which you can pick and choose the materialthat best supports the course you wish to teach.xiv Preface
You should ﬁnd it easy to organize your course around just the chapters you
need. We have made chapters relatively self-contained, so that you need not worryabout an unexpected and unnecessary dependence of one chapter on another. Eachchapter presents the easier material ﬁrst and the more difﬁcult material later, withsection boundaries marking natural stopping points. In an undergraduate course,you might use only the earlier sections from a chapter; in a graduate course, youmight cover the entire chapter.
We have included 957 exercises and 158 problems. Each section ends with exer-
cises, and each chapter ends with problems. The exercises are generally short ques-
tions that test basic mastery of the material. Some are simple self-check thoughtexercises, whereas others are more substantial and are suitable as assigned home-work. The problems are more elaborate case studies that often introduce new ma-terial; they often consist of several questions that lead the student through the stepsrequired to arrive at a solution.
Departing from our practice in previous editions of this book, we have made
publicly available solutions to some, but by no means all, of the problems and ex-
ercises. Our Web site, http://mitpress.mit.edu/algorithms/, links to these solutions.
You will want to check this site to make sure that it does not contain the solution to
an exercise or problem that you plan to assign. We expect the set of solutions thatwe post to grow slowly over time, so you will need to check it each time you teachthe course.
We have starred ( ?) the sections and exercises that are more suitable for graduate
students than for undergraduates. A starred section is not necessarily more difﬁ-
cult than an unstarred one, but it may require an understanding of more advancedmathematics. Likewise, starred exercises may require an advanced background ormore than average creativity.
To the student
We hope that this textbook provides you with an enjoyable introduction to the
ﬁeld of algorithms. We have attempted to make every algorithm accessible andinteresting. To help you when you encounter unfamiliar or difﬁcult algorithms, wedescribe each one in a step-by-step manner. We also provide careful explanationsof the mathematics needed to understand the analysis of the algorithms. If youalready have some familiarity with a topic, you will ﬁnd the chapters organized sothat you can skim introductory sections and proceed quickly to the more advancedmaterial.
This is a large book, and your class will probably cover only a portion of its
material. We have tried, however, to make this a book that will be useful to younow as a course textbook and also later in your career as a mathematical deskreference or an engineering handbook.Preface xv
What are the prerequisites for reading this book?
/SIYou should have some programming experience. In particular, you should un-
derstand recursive procedures and simple data structures such as arrays andlinked lists.
/SIYou should have some facility with mathematical proofs, and especially proofsby mathematical induction. A few portions of the book rely on some knowledgeof elementary calculus. Beyond that, Parts I and VIII of this book teach you allthe mathematical techniques you will need.
We have heard, loud and clear, the call to supply solutions to problems and
exercises. Our Web site, http://mitpress.mit.edu/algorithms/, links to solutions fora few of the problems and exercises. Feel free to check your solutions against ours.We ask, however, that you do not send your solutions to us.
To the professional
The wide range of topics in this book makes it an excellent handbook on algo-
rithms. Because each chapter is relatively self-contained, you can focus in on thetopics that most interest you.
Most of the algorithms we discuss have great practical utility. We therefore
address implementation concerns and other engineering issues. We often provide
practical alternatives to the few algorithms that are primarily of theoretical interest.
If you wish to implement any of the algorithms, you should ﬁnd the transla-
tion of our pseudocode into your favorite programming language to be a fairlystraightforward task. We have designed the pseudocode to present each algorithmclearly and succinctly. Consequently, we do not address error-handling and othersoftware-engineering issues that require speciﬁc assumptions about your program-ming environment. We attempt to present each algorithm simply and directly with-out allowing the idiosyncrasies of a particular programming language to obscureits essence.
We understand that if you are using this book outside of a course, then you
might be unable to check your solutions to problems and exercises against solutionsprovided by an instructor. Our Web site, http://mitpress.mit.edu/algorithms/, linksto solutions for some of the problems and exercises so that you can check yourwork. Please do not send your solutions to us.
To our colleagues
We have supplied an extensive bibliography and pointers to the current literature.
Each chapter ends with a set of chapter notes that give historical details and ref-erences. The chapter notes do not provide a complete reference to the whole ﬁeldxvi Preface
of algorithms, however. Though it may be hard to believe for a book of this size,
space constraints prevented us from including many interesting algorithms.
Despite myriad requests from students for solutions to problems and exercises,
we have chosen as a matter of policy not to supply references for problems andexercises, to remove the temptation for students to look up a solution rather than toﬁnd it themselves.
Changes for the third edition
What has changed between the second and third editions of this book? The mag-
nitude of the changes is on a par with the changes between the ﬁrst and secondeditions. As we said about the second-edition changes, depending on how youlook at it, the book changed either not much or quite a bit.
A quick look at the table of contents shows that most of the second-edition chap-
ters and sections appear in the third edition. We removed two chapters and onesection, but we have added three new chapters and two new sections apart fromthese new chapters.
We kept the hybrid organization from the ﬁrst two editions. Rather than organiz-
ing chapters by only problem domains or according only to techniques, this book
has elements of both. It contains technique-based chapters on divide-and-conquer,
dynamic programming, greedy algorithms, amortized analysis, NP-Completeness,
and approximation algorithms. But it also has entire parts on sorting, on data
structures for dynamic sets, and on algorithms for graph problems. We ﬁnd thatalthough you need to know how to apply techniques for designing and analyzing al-gorithms, problems seldom announce to you which techniques are most amenableto solving them.
Here is a summary of the most signiﬁcant changes for the third edition:
/SIWe added new chapters on van Emde Boas trees and multithreaded algorithms,and we have broken out material on matrix basics into its own appendix chapter.
/SIWe revised the chapter on recurrences to more broadly cover the divide-and-conquer technique, and its ﬁrst two sections apply divide-and-conquer to solvetwo problems. The second section of this chapter presents Strassen’s algorithmfor matrix multiplication, which we have moved from the chapter on matrixoperations.
/SIWe removed two chapters that were rarely taught: binomial heaps and sortingnetworks. One key idea in the sorting networks chapter, the 0-1 principle, ap-pears in this edition within Problem 8-7 as the 0-1 sorting lemma for compare-exchange algorithms. The treatment of Fibonacci heaps no longer relies onbinomial heaps as a precursor.Preface xvii
/SIWe revised our treatment of dynamic programming and greedy algorithms. Dy-
namic programming now leads off with a more interesting problem, rod cutting,than the assembly-line scheduling problem from the second edition. Further-more, we emphasize memoization a bit more than we did in the second edition,and we introduce the notion of the subproblem graph as a way to understandthe running time of a dynamic-programming algorithm. In our opening exam-ple of greedy algorithms, the activity-selection problem, we get to the greedyalgorithm more directly than we did in the second edition.
/SIThe way we delete a node from binary search trees (which includes red-blacktrees) now guarantees that the node requested for deletion is the node that isactually deleted. In the ﬁrst two editions, in certain cases, some other nodewould be deleted, with its contents moving into the node passed to the deletionprocedure. With our new way to delete nodes, if other components of a programmaintain pointers to nodes in the tree, they will not mistakenly end up with stalepointers to nodes that have been deleted.
/SIThe material on ﬂow networks now bases ﬂows entirely on edges. This ap-proach is more intuitive than the net ﬂow used in the ﬁrst two editions.
/SIWith the material on matrix basics and Strassen’s algorithm moved to otherchapters, the chapter on matrix operations is smaller than in the second edition.
/SIWe have modiﬁed our treatment of the Knuth-Morris-Pratt string-matching al-gorithm.
/SIWe corrected several errors. Most of these errors were posted on our Web siteof second-edition errata, but a few were not.
/SIBased on many requests, we changed the syntax (as it were) of our pseudocode.We now use “D” to indicate assignment and “
==” to test for equality, just as C,
C++, Java, and Python do. Likewise, we have eliminated the keywords doand
then and adopted “ //” as our comment-to-end-of-line symbol. We also now use
dot-notation to indicate object attributes. Our pseudocode remains procedural,rather than object-oriented. In other words, rather than running methods onobjects, we simply call procedures, passing objects as parameters.
/SIWe added 100 new exercises and 28 new problems. We also updated manybibliography entries and added several new ones.
/SIFinally, we went through the entire book and rewrote sentences, paragraphs,and sections to make the writing clearer and more active.xviii Preface
Web site
You can use our Web site, http://mitpress.mit.edu/algorithms/, to obtain supple-
mentary information and to communicate with us. The Web site links to a list ofknown errors, solutions to selected exercises and problems, and (of course) a listexplaining the corny professor jokes, as well as other content that we might add.The Web site also tells you how to report errors or make suggestions.
How we produced this book
Like the second edition, the third edition was produced in L
ATEX2". We used the
Times font with mathematics typeset using the MathTime Pro 2 fonts. We thankMichael Spivak from Publish or Perish, Inc., Lance Carnes from Personal TeX,Inc., and Tim Tregubov from Dartmouth College for technical support. As in theprevious two editions, we compiled the index using Windex, a C program that wewrote, and the bibliography was produced with B
IBTEX. The PDF ﬁles for this
book were created on a MacBook running OS 10.5.
We drew the illustrations for the third edition using MacDraw Pro, with some
of the mathematical expressions in illustrations laid in with the psfrag packagefor L
ATEX2". Unfortunately, MacDraw Pro is legacy software, having not been
marketed for over a decade now. Happily, we still have a couple of Macintoshes
that can run the Classic environment under OS 10.4, and hence they can run Mac-Draw Pro—mostly. Even under the Classic environment, we ﬁnd MacDraw Pro tobe far easier to use than any other drawing software for the types of illustrationsthat accompany computer-science text, and it produces beautiful output.
1Who
knows how long our pre-Intel Macs will continue to run, so if anyone from Apple
is listening: Please create an OS X-compatible version of MacDraw Pro!
Acknowledgments for the third edition
We have been working with the MIT Press for over two decades now, and what a
terriﬁc relationship it has been! We thank Ellen Faran, Bob Prior, Ada Brunstein,and Mary Reilly for their help and support.
We were geographically distributed while producing the third edition, working
in the Dartmouth College Department of Computer Science, the MIT Computer
1We investigated several drawing programs that run under Mac OS X, but all had signiﬁcant short-
comings compared with MacDraw Pro. We brieﬂy attempted to produce the illustrations for thisbook with a different, well known drawing program. We found that it took at least ﬁve times as long
to produce each illustration as it took with MacDraw Pro, and the resulting illustrations did not look
as good. Hence the decision to revert to MacDraw Pro running on older Macintoshes.Preface xix
Science and Artiﬁcial Intelligence Laboratory, and the Columbia University De-
partment of Industrial Engineering and Operations Research. We thank our re-spective universities and colleagues for providing such supportive and stimulatingenvironments.
Julie Sussman, P.P.A., once again bailed us out as the technical copyeditor. Time
and again, we were amazed at the errors that eluded us, but that Julie caught. Shealso helped us improve our presentation in several places. If there is a Hall of Famefor technical copyeditors, Julie is a sure-ﬁre, ﬁrst-ballot inductee. She is nothing
short of phenomenal. Thank you, thank you, thank you, Julie! Priya Natarajan also
found some errors that we were able to correct before this book went to press. Anyerrors that remain (and undoubtedly, some do) are the responsibility of the authors(and probably were inserted after Julie read the material).
The treatment for van Emde Boas trees derives from Erik Demaine’s notes,
which were in turn inﬂuenced by Michael Bender. We also incorporated ideasfrom Javed Aslam, Bradley Kuszmaul, and Hui Zha into this edition.
The chapter on multithreading was based on notes originally written jointly with
Harald Prokop. The material was inﬂuenced by several others working on the Cilk
project at MIT, including Bradley Kuszmaul and Matteo Frigo. The design of the
multithreaded pseudocode took its inspiration from the MIT Cilk extensions to Cand by Cilk Arts’s Cilk++ extensions to C++.
We also thank the many readers of the ﬁrst and second editions who reported
errors or submitted suggestions for how to improve this book. We corrected all the
bona ﬁde errors that were reported, and we incorporated as many suggestions as
we could. We rejoice that the number of such contributors has grown so great thatwe must regret that it has become impractical to list them all.
Finally, we thank our wives—Nicole Cormen, Wendy Leiserson, Gail Rivest,
and Rebecca Ivry—and our children—Ricky, Will, Debby, and Katie Leiserson;Alex and Christopher Rivest; and Molly, Noah, and Benjamin Stein—for their loveand support while we prepared this book. The patience and encouragement of ourfamilies made this project possible. We affectionately dedicate this book to them.
T
HOMAS H. C ORMEN Lebanon, New Hampshire
CHARLES E. L EISERSON Cambridge, Massachusetts
RONALD L. R IVEST Cambridge, Massachusetts
CLIFFORD STEIN New York, New York
February 2009Introduction to Algorithms
Third EditionI FoundationsIntroduction
This part will start you thinking about designing and analyzing algorithms. It is
intended to be a gentle introduction to how we specify algorithms, some of thedesign strategies we will use throughout this book, and many of the fundamentalideas used in algorithm analysis. Later parts of this book will build upon this base.
Chapter 1 provides an overview of algorithms and their place in modern com-
puting systems. This chapter deﬁnes what an algorithm is and lists some examples.It also makes a case that we should consider algorithms as a technology, along-side technologies such as fast hardware, graphical user interfaces, object-orientedsystems, and networks.
In Chapter 2, we see our ﬁrst algorithms, which solve the problem of sorting
a sequence of nnumbers. They are written in a pseudocode which, although not
directly translatable to any conventional programming language, conveys the struc-
ture of the algorithm clearly enough that you should be able to implement it in the
language of your choice. The sorting algorithms we examine are insertion sort,
which uses an incremental approach, and merge sort, which uses a recursive tech-
nique known as “divide-and-conquer.” Although the time each requires increases
with the value of n, the rate of increase differs between the two algorithms. We
determine these running times in Chapter 2, and we develop a useful notation toexpress them.
Chapter 3 precisely deﬁnes this notation, which we call asymptotic notation. It
starts by deﬁning several asymptotic notations, which we use for bounding algo-rithm running times from above and/or below. The rest of Chapter 3 is primarilya presentation of mathematical notation, more to ensure that your use of notationmatches that in this book than to teach you new mathematical concepts.4 Part I Foundations
Chapter 4 delves further into the divide-and-conquer method introduced in
Chapter 2. It provides additional examples of divide-and-conquer algorithms, in-cluding Strassen’s surprising method for multiplying two square matrices. Chap-ter 4 contains methods for solving recurrences, which are useful for describingthe running times of recursive algorithms. One powerful technique is the “mas-ter method,” which we often use to solve recurrences that arise from divide-and-conquer algorithms. Although much of Chapter 4 is devoted to proving the cor-rectness of the master method, you may skip this proof yet still employ the master
method.
Chapter 5 introduces probabilistic analysis and randomized algorithms. We typ-
ically use probabilistic analysis to determine the running time of an algorithm incases in which, due to the presence of an inherent probability distribution, therunning time may differ on different inputs of the same size. In some cases, weassume that the inputs conform to a known probability distribution, so that we areaveraging the running time over all possible inputs. In other cases, the probabilitydistribution comes not from the inputs but from random choices made during thecourse of the algorithm. An algorithm whose behavior is determined not only by itsinput but by the values produced by a random-number generator is a randomizedalgorithm. We can use randomized algorithms to enforce a probability distributionon the inputs—thereby ensuring that no particular input always causes poor perfor-mance—or even to bound the error rate of algorithms that are allowed to produceincorrect results on a limited basis.
Appendices A–D contain other mathematical material that you will ﬁnd helpful
as you read this book. You are likely to have seen much of the material in theappendix chapters before having read this book (although the speciﬁc deﬁnitionsand notational conventions we use may differ in some cases from what you haveseen in the past), and so you should think of the Appendices as reference material.On the other hand, you probably have not already seen most of the material inPart I. All the chapters in Part I and the Appendices are written with a tutorialﬂavor.1 The Role of Algorithms in Computing
What are algorithms? Why is the study of algorithms worthwhile? What is the role
of algorithms relative to other technologies used in computers? In this chapter, wewill answer these questions.
1.1 Algorithms
Informally, an algorithm is any well-deﬁned computational procedure that takes
some value, or set of values, as input and produces some value, or set of values, as
output . An algorithm is thus a sequence of computational steps that transform the
input into the output.
We can also view an algorithm as a tool for solving a well-speciﬁed computa-
tional problem . The statement of the problem speciﬁes in general terms the desired
input/output relationship. The algorithm describes a speciﬁc computational proce-
dure for achieving that input/output relationship.
For example, we might need to sort a sequence of numbers into nondecreasing
order. This problem arises frequently in practice and provides fertile ground forintroducing many standard design techniques and analysis tools. Here is how weformally deﬁne the sorting problem :
Input: A sequence of nnumbersha
1;a2;:::;a ni.
Output: A permutation (reordering) ha0
1;a0
2;:::;a0
niof the input sequence such
thata0
1/DC4a0
2/DC4/SOH/SOH/SOH/DC4 a0
n.
For example, given the input sequence h31; 41; 59; 26; 41; 58 i, a sorting algorithm
returns as output the sequence h26; 31; 41; 41; 58; 59 i. Such an input sequence is
called an instance of the sorting problem. In general, an instance of a problem
consists of the input (satisfying whatever constraints are imposed in the problemstatement) needed to compute a solution to the problem.6 Chapter 1 The Role of Algorithms in Computing
Because many programs use it as an intermediate step, sorting is a fundamental
operation in computer science. As a result, we have a large number of good sortingalgorithms at our disposal. Which algorithm is best for a given application dependson—among other factors—the number of items to be sorted, the extent to whichthe items are already somewhat sorted, possible restrictions on the item values,the architecture of the computer, and the kind of storage devices to be used: mainmemory, disks, or even tapes.
An algorithm is said to be correct if, for every input instance, it halts with the
correct output. We say that a correct algorithm solves the given computational
problem. An incorrect algorithm might not halt at all on some input instances, or itmight halt with an incorrect answer. Contrary to what you might expect, incorrectalgorithms can sometimes be useful, if we can control their error rate. We shall seean example of an algorithm with a controllable error rate in Chapter 31 when westudy algorithms for ﬁnding large prime numbers. Ordinarily, however, we shallbe concerned only with correct algorithms.
An algorithm can be speciﬁed in English, as a computer program, or even as
a hardware design. The only requirement is that the speciﬁcation must provide a
precise description of the computational procedure to be followed.
What kinds of problems are solved by algorithms?
Sorting is by no means the only computational problem for which algorithms have
been developed. (You probably suspected as much when you saw the size of thisbook.) Practical applications of algorithms are ubiquitous and include the follow-ing examples:
/SIThe Human Genome Project has made great progress toward the goals of iden-tifying all the 100,000 genes in human DNA, determining the sequences of the3 billion chemical base pairs that make up human DNA, storing this informa-
tion in databases, and developing tools for data analysis. Each of these steps
requires sophisticated algorithms. Although the solutions to the various prob-lems involved are beyond the scope of this book, many methods to solve thesebiological problems use ideas from several of the chapters in this book, therebyenabling scientists to accomplish tasks while using resources efﬁciently. Thesavings are in time, both human and machine, and in money, as more informa-tion can be extracted from laboratory techniques.
/SIThe Internet enables people all around the world to quickly access and retrievelarge amounts of information. With the aid of clever algorithms, sites on theInternet are able to manage and manipulate this large volume of data. Examplesof problems that make essential use of algorithms include ﬁnding good routeson which the data will travel (techniques for solving such problems appear in1.1 Algorithms 7
Chapter 24), and using a search engine to quickly ﬁnd pages on which particular
information resides (related techniques are in Chapters 11 and 32).
/SIElectronic commerce enables goods and services to be negotiated and ex-changed electronically, and it depends on the privacy of personal informa-tion such as credit card numbers, passwords, and bank statements. The coretechnologies used in electronic commerce include public-key cryptography anddigital signatures (covered in Chapter 31), which are based on numerical algo-rithms and number theory.
/SIManufacturing and other commercial enterprises often need to allocate scarceresources in the most beneﬁcial way. An oil company may wish to know whereto place its wells in order to maximize its expected proﬁt. A political candidatemay want to determine where to spend money buying campaign advertising inorder to maximize the chances of winning an election. An airline may wishto assign crews to ﬂights in the least expensive way possible, making sure thateach ﬂight is covered and that government regulations regarding crew schedul-ing are met. An Internet service provider may wish to determine where to placeadditional resources in order to serve its customers more effectively. All ofthese are examples of problems that can be solved using linear programming,which we shall study in Chapter 29.
Although some of the details of these examples are beyond the scope of this
book, we do give underlying techniques that apply to these problems and problemareas. We also show how to solve many speciﬁc problems, including the following:
/SIWe are given a road map on which the distance between each pair of adjacentintersections is marked, and we wish to determine the shortest route from oneintersection to another. The number of possible routes can be huge, even if wedisallow routes that cross over themselves. How do we choose which of allpossible routes is the shortest? Here, we model the road map (which is itselfa model of the actual roads) as a graph (which we will meet in Part VI andAppendix B), and we wish to ﬁnd the shortest path from one vertex to anotherin the graph. We shall see how to solve this problem efﬁciently in Chapter 24.
/SIWe are given two ordered sequences of symbols, XDhx1;x2;:::;x miand
YDhy1;y2;:::;y ni, and we wish to ﬁnd a longest common subsequence of
XandY. A subsequence of Xis just Xwith some (or possibly all or none) of
its elements removed. For example, one subsequence of hA; B; C; D; E; F; G i
would behB; C; E; Gi. The length of a longest common subsequence of X
andYgives one measure of how similar these two sequences are. For example,
if the two sequences are base pairs in DNA strands, then we might considerthem similar if they have a long common subsequence. If Xhasmsymbols
andYhasnsymbols, then XandYhave 2
mand2npossible subsequences,8 Chapter 1 The Role of Algorithms in Computing
respectively. Selecting all possible subsequences of XandYand matching
them up could take a prohibitively long time unless mandnare very small.
We shall see in Chapter 15 how to use a general technique known as dynamicprogramming to solve this problem much more efﬁciently.
/SIWe are given a mechanical design in terms of a library of parts, where each partmay include instances of other parts, and we need to list the parts in order sothat each part appears before any part that uses it. If the design comprises n
parts, then there are nŠpossible orders, where nŠdenotes the factorial function.
Because the factorial function grows faster than even an exponential function,we cannot feasibly generate each possible order and then verify that, withinthat order, each part appears before the parts using it (unless we have only afew parts). This problem is an instance of topological sorting, and we shall seein Chapter 22 how to solve this problem efﬁciently.
/SIWe are given npoints in the plane, and we wish to ﬁnd the convex hull of
these points. The convex hull is the smallest convex polygon containing the
points. Intuitively, we can think of each point as being represented by a nail
sticking out from a board. The convex hull would be represented by a tight
rubber band that surrounds all the nails. Each nail around which the rubberband makes a turn is a vertex of the convex hull. (See Figure 33.6 on page 1029for an example.) Any of the 2
nsubsets of the points might be the vertices
of the convex hull. Knowing which points are vertices of the convex hull isnot quite enough, either, since we also need to know the order in which they
appear. There are many choices, therefore, for the vertices of the convex hull.
Chapter 33 gives two good methods for ﬁnding the convex hull.
These lists are far from exhaustive (as you again have probably surmised from
this book’s heft), but exhibit two characteristics that are common to many interest-ing algorithmic problems:
1. They have many candidate solutions, the overwhelming majority of which do
not solve the problem at hand. Finding one that does, or one that is “best,” can
present quite a challenge.
2. They have practical applications. Of the problems in the above list, ﬁnding the
shortest path provides the easiest examples. A transportation ﬁrm, such as atrucking or railroad company, has a ﬁnancial interest in ﬁnding shortest pathsthrough a road or rail network because taking shorter paths results in lowerlabor and fuel costs. Or a routing node on the Internet may need to ﬁnd the
shortest path through the network in order to route a message quickly. Or a
person wishing to drive from New York to Boston may want to ﬁnd drivingdirections from an appropriate Web site, or she may use her GPS while driving.1.1 Algorithms 9
Not every problem solved by algorithms has an easily identiﬁed set of candidate
solutions. For example, suppose we are given a set of numerical values represent-ing samples of a signal, and we want to compute the discrete Fourier transform ofthese samples. The discrete Fourier transform converts the time domain to the fre-quency domain, producing a set of numerical coefﬁcients, so that we can determinethe strength of various frequencies in the sampled signal. In addition to lying atthe heart of signal processing, discrete Fourier transforms have applications in datacompression and multiplying large polynomials and integers. Chapter 30 gives
an efﬁcient algorithm, the fast Fourier transform (commonl y called the FFT), for
this problem, and the chapter also sketches out the design of a hardware circuit tocompute the FFT.
Data structures
This book also contains several data structures. A data structure is a way to store
and organize data in order to facilitate access and modiﬁcations. No single datastructure works well for all purposes, and so it is important to know the strengthsand limitations of several of them.
Technique
Although you can use this book as a “cookbook” for algorithms, you may someday
encounter a problem for which you cannot readily ﬁnd a published algorithm (many
of the exercises and problems in this book, for example). This book will teach you
techniques of algorithm design and analysis so that you can develop algorithms onyour own, show that they give the correct answer, and understand their efﬁciency.Different chapters address different aspects of algorithmic problem solving. Somechapters address speciﬁc problems, such as ﬁnding medians and order statistics inChapter 9, computing minimum spanning trees in Chapter 23, and determining amaximum ﬂow in a network in Chapter 26. Other chapters address techniques,such as divide-and-conquer in Chapter 4, dynamic programming in Chapter 15,and amortized analysis in Chapter 17.
Hard problems
Most of this book is about efﬁcient algorithms. Our usual measure of efﬁciency
is speed, i.e., how long an algorithm takes to produce its result. There are someproblems, however, for which no efﬁcient solution is known. Chapter 34 studies
an interesting subset of these problems, which are known as NP-complete.
Why are NP-complete problems interesting? First, although no efﬁcient algo-
rithm for an NP-complete problem has ever been found, nobody has ever proven10 Chapter 1 The Role of Algorithms in Computing
that an efﬁcient algorithm for one cannot exist. In other words, no one knows
whether or not efﬁcient algorithms exist for NP-complete problems. Second, theset of NP-complete problems has the remarkable property that if an efﬁcient algo-rithm exists for any one of them, then efﬁcient algorithms exist for all of them. Thisrelationship among the NP-complete problems makes the lack of efﬁcient solutionsall the more tantalizing. Third, several NP-complete problems are similar, but notidentical, to problems for which we do know of efﬁcient algorithms. Computerscientists are intrigued by how a small change to the problem statement can cause
a big change to the efﬁciency of the best known algorithm.
You should know about NP-complete problems because some of them arise sur-
prisingly often in real applications. If you are called upon to produce an efﬁcientalgorithm for an NP-complete problem, you are likely to spend a lot of time in afruitless search. If you can show that the problem is NP-complete, you can insteadspend your time developing an efﬁcient algorithm that gives a good, but not thebest possible, solution.
As a concrete example, consider a delivery company with a central depot. Each
day, it loads up each delivery truck at the depot and sends it around to deliver goods
to several addresses. At the end of the day, each truck must end up back at the depot
so that it is ready to be loaded for the next day. To reduce costs, the company wantsto select an order of delivery stops that yields the lowest overall distance traveledby each truck. This problem is the well-known “traveling-salesman problem,” andit is NP-complete. It has no known efﬁcient algorithm. Under certain assumptions,
however, we know of efﬁcient algorithms that give an overall distance which is
not too far above the smallest possible. Chapter 35 discusses such “approximationalgorithms.”
Parallelism
For many years, we could count on processor clock speeds increasing at a steady
rate. Physical limitations present a fundamental roadblock to ever-increasing clockspeeds, however: because power density increases superlinearly with clock speed,chips run the risk of melting once their clock speeds become high enough. In orderto perform more computations per second, therefore, chips are being designed tocontain not just one but several processing “cores.” We can liken these multicorecomputers to several sequential computers on a single chip; in other words, they area type of “parallel computer.” In order to elicit the best performance from multicorecomputers, we need to design algorithms with parallelism in mind. Chapter 27presents a model for “multithreaded” algorithms, which take advantage of multiplecores. This model has advantages from a theoretical standpoint, and it forms thebasis of several successful computer programs, including a championship chessprogram.1.2 Algorithms as a technology 11
Exercises
1.1-1
Give a real-world example that requires sorting or a real-world example that re-quires computing a convex hull.
1.1-2
Other than speed, what other measures of efﬁciency might one use in a real-worldsetting?
1.1-3
Select a data structure that you have seen previously, and discuss its strengths andlimitations.
1.1-4
How are the shortest-path and traveling-salesman problems given above similar?How are they different?
1.1-5
Come up with a real-world problem in which only the best solution will do. Thencome up with one in which a solution that is “approximately” the best is goodenough.
1.2 Algorithms as a technology
Suppose computers were inﬁnitely fast and computer memory was free. Wouldyou have any reason to study algorithms? The answer is yes, if for no other reasonthan that you would still like to demonstrate that your solution method terminatesand does so with the correct answer.
If computers were inﬁnitely fast, any correct method for solving a problem
would do. You would probably want your implementation to be within the boundsof good software engineering practice (for example, your implementation should
be well designed and documented), but you would most often use whichever
method was the easiest to implement.
Of course, computers may be fast, but they are not inﬁnitely fast. And memory
may be inexpensive, but it is not free. Computing time is therefore a boundedresource, and so is space in memory. You should use these resources wisely, andalgorithms that are efﬁcient in terms of time or space will help you do so.12 Chapter 1 The Role of Algorithms in Computing
Efﬁciency
Different algorithms devised to solve the same problem often differ dramatically in
their efﬁciency. These differences can be much more signiﬁcant than differencesdue to hardware and software.
As an example, in Chapter 2, we will see two algorithms for sorting. The ﬁrst,
known as insertion sort , takes time roughly equal to c
1n2to sort nitems, where c1
is a constant that does not depend on n. That is, it takes time roughly proportional
ton2. The second, merge sort , takes time roughly equal to c2nlgn, where lg n
stands for log2nandc2is another constant that also does not depend on n. Inser-
tion sort typically has a smaller constant factor than merge sort, so that c1<c 2.
We shall see that the constant factors can have far less of an impact on the runningtime than the dependence on the input size n. Let’s write insertion sort’s running
time as c
1n/SOHnand merge sort’s running time as c2n/SOHlgn. Then we see that where
insertion sort has a factor of nin its running time, merge sort has a factor of lg n,
which is much smaller. (For example, when nD1000 ,l gnis approximately 10,
and when nequals one million, lg nis approximately only 20.) Although insertion
sort usually runs faster than merge sort for small input sizes, once the input size n
becomes large enough, merge sort’s advantage of lg nvs.nwill more than com-
pensate for the difference in constant factors. No matter how much smaller c1is
thanc2, there will always be a crossover point beyond which merge sort is faster.
For a concrete example, let us pit a faster computer (computer A) running inser-
tion sort against a slower computer (computer B) running merge sort. They eachmust sort an array of 10 million numbers. (Although 10 million numbers mightseem like a lot, if the numbers are eight-byte integers, then the input occupiesabout 80 megabytes, which ﬁts in the memory of even an inexpensive laptop com-puter many times over.) Suppose that computer A executes 10 billion instructionsper second (faster than any single sequential computer at the time of this writing)and computer B executes only 10 million instructions per second, so that com-
puter A is 1000 times faster than computer B in raw computing power. To make
the difference even more dramatic, suppose that the world’s craftiest programmercodes insertion sort in machine language for computer A, and the resulting coderequires 2n
2instructions to sort nnumbers. Suppose further that just an average
programmer implements merge sort, using a high-level language with an inefﬁcientcompiler, with the resulting code taking 50nlgninstructions. To sort 10 million
numbers, computer A takes
2/SOH.10
7/2instructions
1010instructions/secondD20,000 seconds (more than 5.5 hours) ;
while computer B takes1.2 Algorithms as a technology 13
50/SOH107lg107instructions
107instructions/second/EM1163 seconds (less than 20 minutes) :
By using an algorithm whose running time grows more slowly, even with a poor
compiler, computer B runs more than 17 times faster than computer A! The advan-tage of merge sort is even more pronounced when we sort 100 million numbers:where insertion sort takes more than 23 days, merge sort takes under four hours.In general, as the problem size increases, so does the relative advantage of mergesort.
Algorithms and other technologies
The example above shows that we should consider algorithms, like computer hard-
ware, as a technology . Total system performance depends on choosing efﬁcient
algorithms as much as on choosing fast hardware. Just as rapid advances are beingmade in other computer technologies, they are being made in algorithms as well.
You might wonder whether algorithms are truly that important on contemporary
computers in light of other advanced technologies, such as
/SIadvanced computer architectures and fabrication technologies,
/SIeasy-to-use, intuitive, graphical user interfaces (GUIs),
/SIobject-oriented systems,
/SIintegrated Web technologies, and
/SIfast networking, both wired and wireless.
The answer is yes. Although some applications do not explicitly require algorith-
mic content at the application level (such as some simple, Web-based applications),many do. For example, consider a Web-based service that determines how to travelfrom one location to another. Its implementation would rely on fast hardware, agraphical user interface, wide-area networking, and also possibly on object ori-entation. However, it would also require algorithms for certain operations, suchas ﬁnding routes (probably using a shortest-path algorithm), rendering maps, andinterpolating addresses.
Moreover, even an application that does not require algorithmic content at the
application level relies heavily upon algorithms. Does the application rely on fasthardware? The hardware design used algorithms. Does the application rely ongraphical user interfaces? The design of any GUI relies on algorithms. Does theapplication rely on networking? Routing in networks relies heavily on algorithms.Was the application written in a language other than machine code? Then it wasprocessed by a compiler, interpreter, or assembler, all of which make extensive use14 Chapter 1 The Role of Algorithms in Computing
of algorithms. Algorithms are at the core of most technologies used in contempo-
rary computers.
Furthermore, with the ever-increasing capacities of computers, we use them to
solve larger problems than ever before. As we saw in the above comparison be-tween insertion sort and merge sort, it is at larger problem sizes that the differencesin efﬁciency between algorithms become particularly prominent.
Having a solid base of algorithmic knowledge and technique is one characteristic
that separates the truly skilled programmers from the novices. With modern com-
puting technology, you can accomplish some tasks without knowing much about
algorithms, but with a good background in algorithms, you can do much, muchmore.
Exercises
1.2-1
Give an example of an application that requires algorithmic content at the applica-tion level, and discuss the function of the algorithms involved.
1.2-2
Suppose we are comparing implementations of insertion sort and merge sort on the
same machine. For inputs of size n, insertion sort runs in 8n
2steps, while merge
sort runs in 64nlgnsteps. For which values of ndoes insertion sort beat merge
sort?
1.2-3
What is the smallest value of nsuch that an algorithm whose running time is 100n2
runs faster than an algorithm whose running time is 2non the same machine?
Problems
1-1 Comparison of running times
For each function f .n/ and time tin the following table, determine the largest
sizenof a problem that can be solved in time t, assuming that the algorithm to
solve the problem takes f .n/ microseconds.Notes for Chapter 1 15
1
 1
 1
 1
 1
 1
 1
second
 minute
 hour
 day
 month
 year
 century
lgn
p
n
n
nlgn
n2
n3
2n
nŠ
Chapter notes
There are many excellent texts on the general topic of algorithms, including those
by Aho, Hopcroft, and Ullman [5, 6]; Baase and Van Gelder [28]; Brassard andBratley [54]; Dasgupta, Papadimitriou, and Vazirani [82]; Goodrich and Tamassia[148]; Hofri [175]; Horowitz, Sahni, and Rajasekaran [181]; Johnsonbaugh andSchaefer [193]; Kingston [205]; Kleinberg and Tardos [208]; Knuth [209, 210,211]; Kozen [220]; Levitin [235]; Manber [242]; Mehlhorn [249, 250, 251]; Pur-dom and Brown [287]; Reingold, Nievergelt, and Deo [293]; Sedgewick [306];Sedgewick and Flajolet [307]; Skiena [318]; and Wilf [356]. Some of the morepractical aspects of algorithm design are discussed by Bentley [42, 43] and Gonnet[145]. Surveys of the ﬁeld of algorithms can also be found in the Handbook of The-
oretical Computer Science, Volume A [342] and the CRC Algorithms and Theory of
Computation Handbook [25]. Overviews of the algorithms used in computational
biology can be found in textbooks by Gusﬁeld [156], Pevzner [275], Setubal andMeidanis [310], and Waterman [350].2 Getting Started
This chapter will familiarize you with the framework we shall use throughout the
book to think about the design and analysis of algorithms. It is self-contained, butit does include several references to material that we introduce in Chapters 3 and 4.(It also contains several summations, which Appendix A shows how to solve.)
We begin by examining the insertion sort algorithm to solve the sorting problem
introduced in Chapter 1. We deﬁne a “pseudocode” that should be familiar to you ifyou have done computer programming, and we use it to show how we shall specifyour algorithms. Having speciﬁed the insertion sort algorithm, we then argue that itcorrectly sorts, and we analyze its running time. The analysis introduces a notationthat focuses on how that time increases with the number of items to be sorted.Following our discussion of insertion sort, we introduce the divide-and-conquerapproach to the design of algorithms and use it to develop an algorithm calledmerge sort. We end with an analysis of merge sort’s running time.
2.1 Insertion sort
Our ﬁrst algorithm, insertion sort, solves the sorting problem introduced in Chap-
ter 1:
Input: A sequence of nnumbersha1;a2;:::;a ni.
Output: A permutation (reordering) ha0
1;a0
2;:::;a0
niof the input sequence such
thata0
1/DC4a0
2/DC4/SOH/SOH/SOH/DC4 a0
n.
The numbers that we wish to sort are also known as the keys. Although conceptu-
ally we are sorting a sequence, the input comes to us in the form of an array with n
elements.
In this book, we shall typically describe algorithms as programs written in a
pseudocode that is similar in many respects to C, C++, Java, Python, or Pascal. If
you have been introduced to any of these languages, you should have little trouble2.1 Insertion sort 17
2♣♣♣2♣4♣♣♣♣♣ 4♣5♣♣♣♣♣ 5♣♣7♣
♣♣♣
♣♣♣♣7♣10♣♣♣♣
♣♣♣♣♣
♣♣10♣
Figure 2.1 Sorting a hand of cards using insertion sort.
reading our algorithms. What separates pseudocode from “real” code is that in
pseudocode, we employ whatever expressive method is most clear and concise tospecify a given algorithm. Sometimes, the clearest method is English, so do notbe surprised if you come across an English phrase or sentence embedded withina section of “real” code. Another difference between pseudocode and real codeis that pseudocode is not typically concerned with issues of software engineering.
Issues of data abstraction, modularity, and error handling are often ignored in order
to convey the essence of the algorithm more concisely.
We start with insertion sort , which is an efﬁcient algorithm for sorting a small
number of elements. Insertion sort works the way many people sort a hand of
playing cards. We start with an empty left hand and the cards face down on thetable. We then remove one card at a time from the table and insert it into thecorrect position in the left hand. To ﬁnd the correct position for a card, we compareit with each of the cards already in the hand, from right to left, as illustrated inFigure 2.1. At all times, the cards held in the left hand are sorted, and these cardswere originally the top cards of the pile on the table.
We present our pseudocode for insertion sort as a procedure called I
NSERTION -
SORT, which takes as a parameter an array AŒ1 : : n/c141 containing a sequence of
length nthat is to be sorted. (In the code, the number nof elements in Ais denoted
byA:length .) The algorithm sorts the input numbers in place : it rearranges the
numbers within the array A, with at most a constant number of them stored outside
the array at any time. The input array Acontains the sorted output sequence when
the I NSERTION -SORT procedure is ﬁnished.18 Chapter 2 Getting Started
123456
524613 (a)123456
25 4613 (b)123456
245 613 (c)
123456
2456 13 (d)123456
24561 3 (e)123456
2 456 1 3 (f)
Figure 2.2 The operation of I NSERTION -SORT on the array ADh5; 2; 4; 6; 1; 3i. Array indices
appear above the rectangles, and values stored in the array positions appear within the rectangles.
(a)–(e) The iterations of the forloop of lines 1–8. In each iteration, the black rectangle holds the
key taken from AŒj /c141 , which is compared with the values in shaded rectangles to its left in the test of
line 5. Shaded arrows show array values moved one position to the right in line 6, and black arrows
indicate where the key moves to in line 8. (f)The ﬁnal sorted array.
INSERTION -SORT.A/
1forjD2toA:length
2 keyDAŒj /c141
3 //Insert AŒj /c141 into the sorted sequence AŒ1 : : j/NUL1/c141.
4 iDj/NUL1
5 while i>0 andAŒi/c141 > key
6 AŒiC1/c141DAŒi/c141
7 iDi/NUL1
8 AŒiC1/c141Dkey
Loop invariants and the correctness of insertion sort
Figure 2.2 shows how this algorithm works for ADh5; 2; 4; 6; 1; 3i.T h e i n -
dexjindicates the “current card” being inserted into the hand. At the beginning
of each iteration of the forloop, which is indexed by j, the subarray consisting
of elements AŒ1 : : j/NUL1/c141constitutes the currently sorted hand, and the remaining
subarray AŒjC1::n /c141 corresponds to the pile of cards still on the table. In fact,
elements AŒ1 : : j/NUL1/c141are the elements originally in positions 1 through j/NUL1,b u t
now in sorted order. We state these properties of AŒ1 : : j/NUL1/c141formally as a loop
invariant :
At the start of each iteration of the forloop of lines 1–8, the subarray
AŒ1 : : j/NUL1/c141consists of the elements originally in AŒ1 : : j/NUL1/c141, but in sorted
order.
We use loop invariants to help us understand why an algorithm is correct. We
must show three things about a loop invariant:2.1 Insertion sort 19
Initialization: It is true prior to the ﬁrst iteration of the loop.
Maintenance: If it is true before an iteration of the loop, it remains true before the
next iteration.
Termination: When the loop terminates, the invariant gives us a useful property
that helps show that the algorithm is correct.
When the ﬁrst two properties hold, the loop invariant is true prior to every iteration
of the loop. (Of course, we are free to use established facts other than the loopinvariant itself to prove that the loop invariant remains true before each iteration.)Note the similarity to mathematical induction, where to prove that a property holds,you prove a base case and an inductive step. Here, showing that the invariant holdsbefore the ﬁrst iteration corresponds to the base case, and showing that the invariantholds from iteration to iteration corresponds to the inductive step.
The third property is perhaps the most important one, since we are using the loop
invariant to show correctness. Typically, we use the loop invariant along with the
condition that caused the loop to terminate. The termination property differs from
how we usually use mathematical induction, in which we apply the inductive stepinﬁnitely; here, we stop the “induction” when the loop terminates.
Let us see how these properties hold for insertion sort.
Initialization: We start by showing that the loop invariant holds before the ﬁrst
loop iteration, when jD2.
1The subarray AŒ1 : : j/NUL1/c141, therefore, consists
of just the single element AŒ1/c141, which is in fact the original element in AŒ1/c141.
Moreover, this subarray is sorted (trivially, of course), which shows that theloop invariant holds prior to the ﬁrst iteration of the loop.
Maintenance: Next, we tackle the second property: showing that each iteration
maintains the loop invariant. Informally, the body of the forloop works by
moving AŒj/NUL1/c141,AŒj/NUL2/c141,AŒj/NUL3/c141, and so on by one position to the right
until it ﬁnds the proper position for AŒj /c141 (lines 4–7), at which point it inserts
the value of AŒj /c141 (line 8). The subarray AŒ1 : : j /c141 then consists of the elements
originally in AŒ1 : : j /c141 , but in sorted order. Incrementing jfor the next iteration
of the forloop then preserves the loop invariant.
A more formal treatment of the second property would require us to state and
show a loop invariant for the while loop of lines 5–7. At this point, however,
1When the loop is a forloop, the moment at which we check the loop invariant just prior to the ﬁrst
iteration is immediately after the initial assignment to the loop-counter variable and just before the
ﬁrst test in the loop header. In the case of I NSERTION -SORT, this time is after assigning 2to the
variable jbut before the ﬁrst test of whether j/DC4A:length .20 Chapter 2 Getting Started
we prefer not to get bogged down in such formalism, and so we rely on our
informal analysis to show that the second property holds for the outer loop.
Termination: Finally, we examine what happens when the loop terminates. The
condition causing the forloop to terminate is that j> A : lengthDn. Because
each loop iteration increases jby1,w em u s th a v e jDnC1at that time.
Substituting nC1forjin the wording of loop invariant, we have that the
subarray AŒ1 : : n/c141 consists of the elements originally in AŒ1 : : n/c141 , but in sorted
order. Observing that the subarray AŒ1 : : n/c141 is the entire array, we conclude that
the entire array is sorted. Hence, the algorithm is correct.
We shall use this method of loop invariants to show correctness later in this
chapter and in other chapters as well.
Pseudocode conventions
We use the following conventions in our pseudocode.
/SIIndentation indicates block structure. For example, the body of the forloop that
begins on line 1 consists of lines 2–8, and the body of the while loop that begins
on line 5 contains lines 6–7 but not line 8. Our indentation style applies to
if-elsestatements2as well. Using indentation instead of conventional indicators
of block structure, such as begin andend statements, greatly reduces clutter
while preserving, or even enhancing, clarity.3
/SIThe looping constructs while ,for,a n drepeat -until and the if-elseconditional
construct have interpretations similar to those in C, C++, Java, Python, andPascal.
4In this book, the loop counter retains its value after exiting the loop,
unlike some situations that arise in C++, Java, and Pascal. Thus, immediately
after a forloop, the loop counter’s value is the value that ﬁrst exceeded the for
loop bound. We used this property in our correctness argument for insertionsort. The forloop header in line 1 is forjD2toA:length , and so when
this loop terminates, jDA:lengthC1(or, equivalently, jDnC1,s i n c e
nDA:length ). We use the keyword towhen a forloop increments its loop
2In an if-elsestatement, we indent elseat the same level as its matching if. Although we omit the
keyword then , we occasionally refer to the portion executed when the test following ifis true as a
then clause . For multiway tests, we use elseif for tests after the ﬁrst one.
3Each pseudocode procedure in this book appears on one page so that you will not have to discern
levels of indentation in code that is split across pages.
4Most block-structured languages have equivalent constructs, though the exact syntax may differ.
Python lacks repeat -until loops, and its forloops operate a little differently from the forloops in
this book.2.1 Insertion sort 21
counter in each iteration, and we use the keyword downto when a forloop
decrements its loop counter. When the loop counter changes by an amountgreater than 1, the amount of change follows the optional keyword by.
/SIThe symbol “ //” indicates that the remainder of the line is a comment.
/SIA multiple assignment of the form iDjDeassigns to both variables iandj
the value of expression e; it should be treated as equivalent to the assignment
jDefollowed by the assignment iDj.
/SIVariables (such as i,j,a n d key) are local to the given procedure. We shall not
use global variables without explicit indication.
/SIWe access array elements by specifying the array name followed by the in-dex in square brackets. For example, AŒi/c141 indicates the ith element of the
array A. The notation “ ::” is used to indicate a range of values within an ar-
ray. Thus, AŒ1 : : j /c141 indicates the subarray of Aconsisting of the jelements
A Œ 1 /c141 ;A Œ 2 /c141 ;:::;A Œ j/c141 .
/SIWe typically organize compound data into objects , which are composed of
attributes . We access a particular attribute using the syntax found in many
object-oriented programming languages: the object name, followed by a dot,
followed by the attribute name. For example, we treat an array as an object
with the attribute length indicating how many elements it contains. To specify
the number of elements in an array A, we write A:length .
We treat a variable representing an array or object as a pointer to the data rep-
resenting the array or object. For all attributes fof an object x, setting yDx
causes y:fto equal x:f. Moreover, if we now set x:fD3, then afterward not
only does x:fequal 3,b u ty:fequals 3as well. In other words, xandypoint
to the same object after the assignment yDx.
Our attribute notation can “cascade.” For example, suppose that the attribute f
is itself a pointer to some type of object that has an attribute g. Then the notation
x:f:gis implicitly parenthesized as .x:f/:g. In other words, if we had assigned
yDx:f,t h e n x:f:gis the same as y:g.
Sometimes, a pointer will refer to no object at all. In this case, we give it the
special value NIL.
/SIWe pass parameters to a procedure by value : the called procedure receives its
own copy of the parameters, and if it assigns a value to a parameter, the changeisnotseen by the calling procedure. When objects are passed, the pointer to
the data representing the object is copied, but the object’s attributes are not. Forexample, if xis a parameter of a called procedure, the assignment xDywithin
the called procedure is not visible to the calling procedure. The assignmentx:fD3, however, is visible. Similarly, arrays are passed by pointer, so that22 Chapter 2 Getting Started
a pointer to the array is passed, rather than the entire array, and changes to
individual array elements are visible to the calling procedure.
/SIAreturn statement immediately transfers control back to the point of call in
the calling procedure. Most return statements also take a value to pass back to
the caller. Our pseudocode differs from many programming languages in thatwe allow multiple values to be returned in a single return statement.
/SIThe boolean operators “and” and “or” are short circuiting . That is, when we
evaluate the expression “ xandy”w eﬁ r s te v a l u a t e x.I fxevaluates to FALSE ,
then the entire expression cannot evaluate to TRUE , and so we do not evaluate y.
If, on the other hand, xevaluates to TRUE ,w em u s te v a l u a t e yto determine the
value of the entire expression. Similarly, in the expression “ xory”w ee v a l -
uate the expression yonly if xevaluates to FALSE . Short-circuiting operators
allow us to write boolean expressions such as “ x¤NILandx:fDy” without
worrying about what happens when we try to evaluate x:fwhen xisNIL.
/SIThe keyword error indicates that an error occurred because conditions were
wrong for the procedure to have been called. The calling procedure is respon-sible for handling the error, and so we do not specify what action to take.
Exercises
2.1-1
Using Figure 2.2 as a model, illustrate the operation of I
NSERTION -SORT on the
array ADh31; 41; 59; 26; 41; 58 i.
2.1-2
Rewrite the I NSERTION -SORT procedure to sort into nonincreasing instead of non-
decreasing order.
2.1-3
Consider the searching problem :
Input: A sequence of nnumbers ADha1;a2;:::;a niand a value /ETB.
Output: An index isuch that /ETBDAŒi/c141 or the special value NILif/ETBdoes not
appear in A.
Write pseudocode for linear search , which scans through the sequence, looking
for/ETB. Using a loop invariant, prove that your algorithm is correct. Make sure that
your loop invariant fulﬁlls the three necessary properties.
2.1-4
Consider the problem of adding two n-bit binary integers, stored in two n-element
arrays AandB. The sum of the two integers should be stored in binary form in2.2 Analyzing algorithms 23
an.nC1/-element array C. State the problem formally and write pseudocode for
adding the two integers.
2.2 Analyzing algorithms
Analyzing an algorithm has come to mean predicting the resources that the algo-
rithm requires. Occasionally, resources such as memory, communication band-width, or computer hardware are of primary concern, but most often it is compu-tational time that we want to measure. Generally, by analyzing several candidatealgorithms for a problem, we can identify a most efﬁcient one. Such analysis mayindicate more than one viable candidate, but we can often discard several inferioralgorithms in the process.
Before we can analyze an algorithm, we must have a model of the implemen-
tation technology that we will use, including a model for the resources of thattechnology and their costs. For most of this book, we shall assume a generic one-processor, random-access machine (RAM) model of computation as our imple-
mentation technology and understand that our algorithms will be implemented as
computer programs. In the RAM model, instructions are executed one after an-other, with no concurrent operations.
Strictly speaking, we should precisely deﬁne the instructions of the RAM model
and their costs. To do so, however, would be tedious and would yield little insight
into algorithm design and analysis. Yet we must be careful not to abuse the RAM
model. For example, what if a RAM had an instruction that sorts? Then we couldsort in just one instruction. Such a RAM would be unrealistic, since real computersdo not have such instructions. Our guide, therefore, is how real computers are de-signed. The RAM model contains instructions commonly found in real computers:arithmetic (such as add, subtract, multiply, divide, remainder, ﬂoor, ceiling), datamovement (load, store, copy), and control (conditional and unconditional branch,subroutine call and return). Each such instruction takes a constant amount of time.
The data types in the RAM model are integer and ﬂoating point (for storing real
numbers). Although we typically do not concern ourselves with precision in this
book, in some applications precision is crucial. We also assume a limit on the sizeof each word of data. For example, when working with inputs of size n, we typ-
ically assume that integers are represented by clgnbits for some constant c/NAK1.
We require c/NAK1so that each word can hold the value of n, enabling us to index the
individual input elements, and we restrict cto be a constant so that the word size
does not grow arbitrarily. (If the word size could grow arbitrarily, we could storehuge amounts of data in one word and operate on it all in constant time—clearlyan unrealistic scenario.)24 Chapter 2 Getting Started
Real computers contain instructions not listed above, and such instructions rep-
resent a gray area in the RAM model. For example, is exponentiation a constant-time instruction? In the general case, no; it takes several instructions to compute x
y
when xandyare real numbers. In restricted situations, however, exponentiation is
a constant-time operation. Many computers have a “shift left” instruction, whichin constant time shifts the bits of an integer by kpositions to the left. In most
computers, shifting the bits of an integer by one position to the left is equivalentto multiplication by 2, so that shifting the bits by kpositions to the left is equiv-
alent to multiplication by 2
k. Therefore, such computers can compute 2kin one
constant-time instruction by shifting the integer 1 by kpositions to the left, as long
askis no more than the number of bits in a computer word. We will endeavor to
avoid such gray areas in the RAM model, but we will treat computation of 2kas a
constant-time operation when kis a small enough positive integer.
In the RAM model, we do not attempt to model the memory hierarchy that is
common in contemporary computers. That is, we do not model caches or virtual
memory. Several computational models attempt to account for memory-hierarchy
effects, which are sometimes signiﬁcant in real programs on real machines. A
handful of problems in this book examine memory-hierarchy effects, but for the
most part, the analyses in this book will not consider them. Models that includethe memory hierarchy are quite a bit more complex than the RAM model, and sothey can be difﬁcult to work with. Moreover, RAM-model analyses are usuallyexcellent predictors of performance on actual machines.
Analyzing even a simple algorithm in the RAM model can be a challenge. The
mathematical tools required may include combinatorics, probability theory, alge-
braic dexterity, and the ability to identify the most signiﬁcant terms in a formula.Because the behavior of an algorithm may be different for each possible input, we
need a means for summarizing that behavior in simple, easily understood formulas.
Even though we typically select only one machine model to analyze a given al-
gorithm, we still face many choices in deciding how to express our analysis. Wewould like a way that is simple to write and manipulate, shows the important char-acteristics of an algorithm’s resource requirements, and suppresses tedious details.
Analysis of insertion sort
The time taken by the I
NSERTION -SORT procedure depends on the input: sorting a
thousand numbers takes longer than sorting three numbers. Moreover, I NSERTION -
SORT can take different amounts of time to sort two input sequences of the same
size depending on how nearly sorted they already are. In general, the time takenby an algorithm grows with the size of the input, so it is traditional to describe therunning time of a program as a function of the size of its input. To do so, we needto deﬁne the terms “running time” and “size of input” more carefully.2.2 Analyzing algorithms 25
The best notion for input size depends on the problem being studied. For many
problems, such as sorting or computing discrete Fourier transforms, the most nat-ural measure is the number of items in the input —for example, the array size n
for sorting. For many other problems, such as multiplying two integers, the bestmeasure of input size is the total number of bits needed to represent the input in
ordinary binary notation. Sometimes, it is more appropriate to describe the size ofthe input with two numbers rather than one. For instance, if the input to an algo-rithm is a graph, the input size can be described by the numbers of vertices and
edges in the graph. We shall indicate which input size measure is being used with
each problem we study.
Therunning time of an algorithm on a particular input is the number of primitive
operations or “steps” executed. It is convenient to deﬁne the notion of step sothat it is as machine-independent as possible. For the moment, let us adopt thefollowing view. A constant amount of time is required to execute each line of ourpseudocode. One line may take a different amount of time than another line, butwe shall assume that each execution of the ith line takes time c
i,w h e r e ciis a
constant. This viewpoint is in keeping with the RAM model, and it also reﬂects
how the pseudocode would be implemented on most actual computers.5
In the following discussion, our expression for the running time of I NSERTION -
SORT will evolve from a messy formula that uses all the statement costs cito a
much simpler notation that is more concise and more easily manipulated. This
simpler notation will also make it easy to determine whether one algorithm is more
efﬁcient than another.
We start by presenting the I NSERTION -SORT procedure with the time “cost”
of each statement and the number of times each statement is executed. For eachjD2;3 ;:::;n ,w h e r e nDA:length ,w el e t t
jdenote the number of times the
while loop test in line 5 is executed for that value of j.W h e na fororwhile loop
exits in the usual way (i.e., due to the test in the loop header), the test is executedone time more than the loop body. We assume that comments are not executablestatements, and so they take no time.
5There are some subtleties here. Computational steps that we specify in English are often variants
of a procedure that requires more than just a constant amount of time. For example, later in thisbook we might say “sort the points by x-coordinate,” which, as we shall see, takes more than a
constant amount of time. Also, note that a statement that calls a subroutine takes constant time,
though the subroutine, once invoked, may take more. That is, we separate the process of calling the
subroutine—passing parameters to it, etc.—from the process of executing the subroutine.26 Chapter 2 Getting Started
INSERTION -SORT.A/ cost times
1forjD2toA:length c1 n
2 keyDAŒj /c141 c 2 n/NUL1
3 //Insert AŒj /c141 into the sorted
sequence AŒ1 : : j/NUL1/c141.0 n/NUL1
4 iDj/NUL1c 4 n/NUL1
5 while i>0 andAŒi/c141 > key c5Pn
jD2tj
6 AŒiC1/c141DAŒi/c141 c 6Pn
jD2.tj/NUL1/
7 iDi/NUL1c 7Pn
jD2.tj/NUL1/
8 AŒiC1/c141Dkey c8 n/NUL1
The running time of the algorithm is the sum of running times for each state-
ment executed; a statement that takes cisteps to execute and executes ntimes will
contribute cinto the total running time.6To compute T .n/ , the running time of
INSERTION -SORT on an input of nvalues, we sum the products of the cost and
times columns, obtaining
T .n/Dc1nCc2.n/NUL1/Cc4.n/NUL1/Cc5nX
jD2tjCc6nX
jD2.tj/NUL1/
Cc7nX
jD2.tj/NUL1/Cc8.n/NUL1/ :
Even for inputs of a given size, an algorithm’s running time may depend on
which input of that size is given. For example, in I NSERTION -SORT, the best
case occurs if the array is already sorted. For each jD2;3 ;:::;n , we then ﬁnd
thatAŒi/c141/DC4keyin line 5 when ihas its initial value of j/NUL1. Thus tjD1for
jD2;3 ;:::;n , and the best-case running time is
T .n/Dc1nCc2.n/NUL1/Cc4.n/NUL1/Cc5.n/NUL1/Cc8.n/NUL1/
D.c1Cc2Cc4Cc5Cc8/n/NUL.c2Cc4Cc5Cc8/:
We can express this running time as anCbforconstants aandbthat depend on
the statement costs ci;i ti st h u sa linear function ofn.
If the array is in reverse sorted order—that is, in decreasing order—the worst
case results. We must compare each element AŒj /c141 with each element in the entire
sorted subarray AŒ1 : : j/NUL1/c141,a n ds o tjDjforjD2;3 ;:::;n . Noting that
6This characteristic does not necessarily hold for a resource such as memory. A statement that
references mwords of memory and is executed ntimes does not necessarily reference mndistinct
words of memory.2.2 Analyzing algorithms 27
nX
jD2jDn.nC1/
2/NUL1
and
nX
jD2.j/NUL1/Dn.n/NUL1/
2
(see Appendix A for a review of how to solve these summations), we ﬁnd that in
the worst case, the running time of I NSERTION -SORT is
T .n/Dc1nCc2.n/NUL1/Cc4.n/NUL1/Cc5/DC2n.nC1/
2/NUL1/DC3
Cc6/DC2n.n/NUL1/
2/DC3
Cc7/DC2n.n/NUL1/
2/DC3
Cc8.n/NUL1/
D/DLEc5
2Cc6
2Cc7
2/DC1
n2C/DLE
c1Cc2Cc4Cc5
2/NULc6
2/NULc7
2Cc8/DC1
n
/NUL.c2Cc4Cc5Cc8/:
We can express this worst-case running time as an2CbnCcfor constants a,b,
andcthat again depend on the statement costs ci; it is thus a quadratic function
ofn.
Typically, as in insertion sort, the running time of an algorithm is ﬁxed for a
given input, although in later chapters we shall see some interesting “randomized”algorithms whose behavior can vary even for a ﬁxed input.
Worst-case and average-case analysis
In our analysis of insertion sort, we looked at both the best case, in which the input
array was already sorted, and the worst case, in which the input array was reversesorted. For the remainder of this book, though, we shall usually concentrate on
ﬁnding only the worst-case running time , that is, the longest running time for any
input of size n. We give three reasons for this orientation.
/SIThe worst-case running time of an algorithm gives us an upper bound on the
running time for any input. Knowing it provides a guarantee that the algorithmwill never take any longer. We need not make some educated guess about therunning time and hope that it never gets much worse.
/SIFor some algorithms, the worst case occurs fairly often. For example, in search-ing a database for a particular piece of information, the searching algorithm’sworst case will often occur when the information is not present in the database.In some applications, searches for absent information may be frequent.28 Chapter 2 Getting Started
/SIThe “average case” is often roughly as bad as the worst case. Suppose that we
randomly choose nnumbers and apply insertion sort. How long does it take to
determine where in subarray AŒ1 : : j/NUL1/c141to insert element AŒj /c141 ?O na v e r a g e ,
half the elements in AŒ1 : : j/NUL1/c141are less than AŒj /c141 , and half the elements are
greater. On average, therefore, we check half of the subarray AŒ1 : : j/NUL1/c141,a n d
sotjis about j=2. The resulting average-case running time turns out to be a
quadratic function of the input size, just like the worst-case running time.
In some particular cases, we shall be interested in the average-case running time
of an algorithm; we shall see the technique of probabilistic analysis applied to
various algorithms throughout this book. The scope of average-case analysis islimited, because it may not be apparent what constitutes an “average” input fora particular problem. Often, we shall assume that all inputs of a given size areequally likely. In practice, this assumption may be violated, but we can sometimesuse arandomized algorithm , which makes random choices, to allow a probabilistic
analysis and yield an expected running time. We explore randomized algorithms
more in Chapter 5 and in several other subsequent chapters.
Order of growth
We used some simplifying abstractions to ease our analysis of the I
NSERTION -
SORT procedure. First, we ignored the actual cost of each statement, using the
constants cito represent these costs. Then, we observed that even these constants
give us more detail than we really need: we expressed the worst-case running time
asan2CbnCcfor some constants a,b,a n d cthat depend on the statement
costs ci. We thus ignored not only the actual statement costs, but also the abstract
costs ci.
We shall now make one more simplifying abstraction: it is the rate of growth ,
ororder of growth , of the running time that really interests us. We therefore con-
sider only the leading term of a formula (e.g., an2), since the lower-order terms are
relatively insigniﬁcant for large values of n. We also ignore the leading term’s con-
stant coefﬁcient, since constant factors are less signiﬁcant than the rate of growthin determining computational efﬁciency for large inputs. For insertion sort, whenwe ignore the lower-order terms and the leading term’s constant coefﬁcient, we are
left with the factor of n
2from the leading term. We write that insertion sort has a
worst-case running time of ‚.n2/(pronounced “theta of n-squared”). We shall use
‚-notation informally in this chapter, and we will deﬁne it precisely in Chapter 3.
We usually consider one algorithm to be more efﬁcient than another if its worst-
case running time has a lower order of growth. Due to constant factors and lower-order terms, an algorithm whose running time has a higher order of growth mighttake less time for small inputs than an algorithm whose running time has a lower2.3 Designing algorithms 29
order of growth. But for large enough inputs, a ‚.n2/algorithm, for example, will
run more quickly in the worst case than a ‚.n3/algorithm.
Exercises
2.2-1
Express the function n3=1000/NUL100n2/NUL100nC3in terms of ‚-notation.
2.2-2
Consider sorting nnumbers stored in array Aby ﬁrst ﬁnding the smallest element
ofAand exchanging it with the element in AŒ1/c141. Then ﬁnd the second smallest
element of A, and exchange it with AŒ2/c141. Continue in this manner for the ﬁrst n/NUL1
elements of A. Write pseudocode for this algorithm, which is known as selection
sort. What loop invariant does this algorithm maintain? Why does it need to run
for only the ﬁrst n/NUL1elements, rather than for all nelements? Give the best-case
and worst-case running times of selection sort in ‚-notation.
2.2-3
Consider linear search again (see Exercise 2.1-3). How many elements of the in-put sequence need to be checked on the average, assuming that the element being
searched for is equally likely to be any element in the array? How about in the
worst case? What are the average-case and worst-case running times of linearsearch in ‚-notation? Justify your answers.
2.2-4
How can we modify almost any algorithm to have a good best-case running time?
2.3 Designing algorithms
We can choose from a wide range of algorithm design techniques. For insertionsort, we used an incremental approach: having sorted the subarray AŒ1 : : j/NUL1/c141,
we inserted the single element AŒj /c141 into its proper place, yielding the sorted
subarray AŒ1 : : j /c141 .
In this section, we examine an alternative design approach, known as “divide-
and-conquer,” which we shall explore in more detail in Chapter 4. We’ll use divide-and-conquer to design a sorting algorithm whose worst-case running time is muchless than that of insertion sort. One advantage of divide-and-conquer algorithms isthat their running times are often easily determined using techniques that we willsee in Chapter 4.30 Chapter 2 Getting Started
2.3.1 The divide-and-conquer approach
Many useful algorithms are recursive in structure: to solve a given problem, they
call themselves recursively one or more times to deal with closely related sub-problems. These algorithms typically follow a divide-and-conquer approach: they
break the problem into several subproblems that are similar to the original prob-lem but smaller in size, solve the subproblems recursively, and then combine thesesolutions to create a solution to the original problem.
The divide-and-conquer paradigm involves three steps at each level of the recur-
sion:
Divide the problem into a number of subproblems that are smaller instances of the
same problem.
Conquer the subproblems by solving them recursively. If the subproblem sizes are
small enough, however, just solve the subproblems in a straightforward manner.
Combine the solutions to the subproblems into the solution for the original prob-
lem.
Themerge sort algorithm closely follows the divide-and-conquer paradigm. In-
tuitively, it operates as follows.
Divide: Divide the n-element sequence to be sorted into two subsequences of n=2
elements each.
Conquer: Sort the two subsequences recursively using merge sort.
Combine: Merge the two sorted subsequences to produce the sorted answer.
The recursion “bottoms out” when the sequence to be sorted has length 1, in which
case there is no work to be done, since every sequence of length 1 is already in
sorted order.
The key operation of the merge sort algorithm is the merging of two sorted
sequences in the “combine” step. We merge by calling an auxiliary procedure
M
ERGE . A ;p;q;r/ ,w h e r e Ais an array and p,q,a n d rare indices into the array
such that p/DC4q<r . The procedure assumes that the subarrays AŒp : : q/c141 and
AŒqC1::r/c141 are in sorted order. It merges them to form a single sorted subarray
that replaces the current subarray AŒp : : r/c141 .
Our M ERGE procedure takes time ‚.n/ ,w h e r e nDr/NULpC1is the total
number of elements being merged, and it works as follows. Returning to our card-
playing motif, suppose we have two piles of cards face up on a table. Each pile is
sorted, with the smallest cards on top. We wish to merge the two piles into a single
sorted output pile, which is to be face down on the table. Our basic step consistsof choosing the smaller of the two cards on top of the face-up piles, removing itfrom its pile (which exposes a new top card), and placing this card face down onto2.3 Designing algorithms 31
the output pile. We repeat this step until one input pile is empty, at which time
we just take the remaining input pile and place it face down onto the output pile.Computationally, each basic step takes constant time, since we are comparing justthe two top cards. Since we perform at most nbasic steps, merging takes ‚.n/
time.
The following pseudocode implements the above idea, but with an additional
twist that avoids having to check whether either pile is empty in each basic step.We place on the bottom of each pile a sentinel card, which contains a special value
that we use to simplify our code. Here, we use 1as the sentinel value, so that
whenever a card with 1is exposed, it cannot be the smaller card unless both piles
have their sentinel cards exposed. But once that happens, all the nonsentinel cardshave already been placed onto the output pile. Since we know in advance thatexactly r/NULpC1cards will be placed onto the output pile, we can stop once we
have performed that many basic steps.
M
ERGE . A ;p;q;r/
1n1Dq/NULpC1
2n2Dr/NULq
3l e t L Œ 1::n 1C1/c141andRŒ1: :n 2C1/c141be new arrays
4foriD1ton1
5 LŒi/c141DAŒpCi/NUL1/c141
6forjD1ton2
7 RŒj/c141DAŒqCj/c141
8LŒn 1C1/c141D1
9RŒn 2C1/c141D1
10iD1
11jD1
12forkDptor
13 ifLŒi/c141/DC4RŒj/c141
14 AŒk/c141DLŒi/c141
15 iDiC1
16 elseAŒk/c141DRŒj/c141
17 jDjC1
In detail, the M ERGE procedure works as follows. Line 1 computes the length n1
of the subarray AŒp : : q/c141 , and line 2 computes the length n2of the subarray
AŒqC1::r/c141 . We create arrays LandR(“left” and “right”), of lengths n1C1
andn2C1, respectively, in line 3; the extra position in each array will hold the
sentinel. The forloop of lines 4–5 copies the subarray AŒp : : q/c141 intoL Œ 1::n 1/c141,
and the forloop of lines 6–7 copies the subarray AŒqC1::r/c141 intoR Œ 1::n 2/c141.
Lines 8–9 put the sentinels at the ends of the arrays LandR. Lines 10–17, illus-32 Chapter 2 Getting Started
A
LR1234 1234
ijk
(a)2457 1236A
LR1234 1234
ijk
(b)24571
236124571236 4571236
A
LR9 1 01 11 21 31 41 51 6
1234 1234
ijk
(c)24571
23615712362 A
LR1234 1234
ijk
(d)24571
236171236 225
∞5
∞5
∞5
∞
5
∞5
∞5
∞5
∞9 1 01 11 21 31 41 51 6
9 1 01 11 21 31 41 51 69 1 01 11 21 31 41 51 6 8
…17
…
8
…17
…8
…17
…
8
…17
…
Figure 2.3 The operation of lines 10–17 in the call M ERGE .A; 9; 12; 16/ , when the subarray
AŒ9 : : 16/c141 contains the sequence h2; 4; 5; 7; 1; 2; 3; 6 i. After copying and inserting sentinels, the
array Lcontainsh2; 4; 5; 7;1i, and the array Rcontainsh1; 2; 3; 6;1i. Lightly shaded positions
inAcontain their ﬁnal values, and lightly shaded positions in LandRcontain values that have yet
to be copied back into A. Taken together, the lightly shaded positions always comprise the values
originally in AŒ9 : : 16/c141 , along with the two sentinels. Heavily shaded positions in Acontain values
that will be copied over, and heavily shaded positions in LandRcontain values that have already
been copied back into A.(a)–(h) The arrays A,L,a n d R, and their respective indices k,i,a n d j
prior to each iteration of the loop of lines 12–17.
trated in Figure 2.3, perform the r/NULpC1basic steps by maintaining the following
loop invariant:
At the start of each iteration of the forloop of lines 12–17, the subarray
AŒp : : k/NUL1/c141contains the k/NULpsmallest elements of L Œ 1::n 1C1/c141and
RŒ1: :n 2C1/c141, in sorted order. Moreover, LŒi/c141 andRŒj/c141 are the smallest
elements of their arrays that have not been copied back into A.
We must show that this loop invariant holds prior to the ﬁrst iteration of the for
loop of lines 12–17, that each iteration of the loop maintains the invariant, andthat the invariant provides a useful property to show correctness when the loopterminates.
Initialization: Prior to the ﬁrst iteration of the loop, we have kDp, so that the
subarray AŒp : : k/NUL1/c141is empty. This empty subarray contains the k/NULpD0
smallest elements of LandR, and since iDjD1, both LŒi/c141 andRŒj/c141 are the
smallest elements of their arrays that have not been copied back into A.2.3 Designing algorithms 33
A
LR1234 1234
ijk
(e)24571
23611236 223 A
LR1234 1234
ijk
(f)24571
2361236 2234
A
LR1234 1234
ijk
(g)24571
236136 22345 A
LR1234 1234
ijk
(h)24571
23616 223455
∞5
∞5
∞5
∞
5
∞5
∞5
∞5
∞6
A
LR1234 1234
ijk
(i)24571
23617 22345
5
∞5
∞69 1 01 11 21 31 41 51 6
9 1 01 11 21 31 41 51 6
9 1 01 11 21 31 41 51 69 1 01 11 21 31 41 51 6
9 1 01 11 21 31 41 51 68
…17
…
8
…17
…
8
…17
…8
…17
…
8
…17
…
Figure 2.3, continued (i) The arrays and indices at termination. At this point, the subarray in
AŒ9 : : 16/c141 is sorted, and the two sentinels in LandRare the only two elements in these arrays that
have not been copied into A.
Maintenance: To see that each iteration maintains the loop invariant, let us ﬁrst
suppose that LŒi/c141/DC4RŒj/c141 .T h e n LŒi/c141 is the smallest element not yet copied
back into A. Because AŒp : : k/NUL1/c141contains the k/NULpsmallest elements, after
line 14 copies LŒi/c141 intoAŒk/c141 , the subarray AŒp : : k/c141 will contain the k/NULpC1
smallest elements. Incrementing k(in the forloop update) and i(in line 15)
reestablishes the loop invariant for the next iteration. If instead LŒi/c141 > RŒj /c141 ,
then lines 16–17 perform the appropriate action to maintain the loop invariant.
Termination: At termination, kDrC1. By the loop invariant, the subarray
AŒp : : k/NUL1/c141,w h i c hi s AŒp : : r/c141 , contains the k/NULpDr/NULpC1smallest
elements of L Œ 1::n 1C1/c141andR Œ 1::n 2C1/c141, in sorted order. The arrays L
andRtogether contain n1Cn2C2Dr/NULpC3elements. All but the two
largest have been copied back into A, and these two largest elements are the
sentinels.34 Chapter 2 Getting Started
To see that the M ERGE procedure runs in ‚.n/ time, where nDr/NULpC1,
observe that each of lines 1–3 and 8–11 takes constant time, the forloops of
lines 4–7 take ‚.n 1Cn2/D‚.n/ time,7and there are niterations of the for
loop of lines 12–17, each of which takes constant time.
We can now use the M ERGE procedure as a subroutine in the merge sort al-
gorithm. The procedure M ERGE -SORT. A ;p;r/ sorts the elements in the subar-
rayAŒp : : r/c141 .I fp/NAKr, the subarray has at most one element and is therefore
already sorted. Otherwise, the divide step simply computes an index qthat par-
titions AŒp : : r/c141 into two subarrays: AŒp : : q/c141 , containingdn=2eelements, and
AŒqC1::r/c141 , containingbn=2celements.8
MERGE -SORT. A ;p;r/
1ifp<r
2 qDb.pCr/=2c
3M ERGE -SORT. A ;p;q/
4M ERGE -SORT.A; qC1; r/
5M ERGE . A ;p;q;r/
To sort the entire sequence ADhAŒ1/c141; AŒ2/c141; : : : ; AŒn/c141 i, we make the initial call
MERGE -SORT. A ;1 ;A: length /, where once again A:lengthDn. Figure 2.4 il-
lustrates the operation of the procedure bottom-up when ni sap o w e ro f 2.T h e
algorithm consists of merging pairs of 1-item sequences to form sorted sequences
of length 2, merging pairs of sequences of length 2 to form sorted sequences of
length 4, and so on, until two sequences of length n=2are merged to form the ﬁnal
sorted sequence of length n.
2.3.2 Analyzing divide-and-conquer algorithms
When an algorithm contains a recursive call to itself, we can often describe its
running time by a recurrence equation orrecurrence , which describes the overall
running time on a problem of size nin terms of the running time on smaller inputs.
We can then use mathematical tools to solve the recurrence and provide bounds onthe performance of the algorithm.
7We shall see in Chapter 3 how to formally interpret equations containing ‚-notation.
8The expressiondxedenotes the least integer greater than or equal to x,a n dbxcdenotes the greatest
integer less than or equal to x. These notations are deﬁned in Chapter 3. The easiest way to verify
that setting qtob.pCr/=2cyields subarrays AŒp : : q/c141 andAŒqC1::r/c141 of sizesdn=2eandbn=2c,
respectively, is to examine the four cases that arise depending on whether each of pandris odd or
even.2.3 Designing algorithms 35
5247132625 47 13 262457 123612234567
mergemerge
mergesorted sequence
initial sequencemerge merge merge merge
Figure 2.4 The operation of merge sort on the array ADh5; 2; 4; 7; 1; 3; 2; 6i. The lengths of the
sorted sequences being merged increase as the algorithm progresses from bottom to top.
A recurrence for the running time of a divide-and-conquer algorithm falls out
from the three steps of the basic paradigm. As before, we let T .n/ be the running
time on a problem of size n. If the problem size is small enough, say n/DC4c
for some constant c, the straightforward solution takes constant time, which we
write as ‚.1/ . Suppose that our division of the problem yields asubproblems,
each of which is 1=bthe size of the original. (For merge sort, both aandbare2,
but we shall see many divide-and-conquer algorithms in which a¤b.) It takes
timeT .n=b/ to solve one subproblem of size n=b, and so it takes time aT .n=b/
to solve aof them. If we take D.n/ time to divide the problem into subproblems
andC.n/ time to combine the solutions to the subproblems into the solution to the
original problem, we get the recurrence
T .n/D(
‚.1/ ifn/DC4c;
aT .n=b/CD.n/CC.n/ otherwise :
In Chapter 4, we shall see how to solve common recurrences of this form.
Analysis of merge sort
Although the pseudocode for M ERGE -SORT works correctly when the number of
elements is not even, our recurrence-based analysis is simpliﬁed if we assume that36 Chapter 2 Getting Started
the original problem size is a power of 2. Each divide step then yields two subse-
quences of size exactly n=2. In Chapter 4, we shall see that this assumption does
not affect the order of growth of the solution to the recurrence.
We reason as follows to set up the recurrence for T .n/ , the worst-case running
time of merge sort on nnumbers. Merge sort on just one element takes constant
time. When we have n>1 elements, we break down the running time as follows.
Divide: The divide step just computes the middle of the subarray, which takes
constant time. Thus, D.n/D‚.1/ .
Conquer: We recursively solve two subproblems, each of size n=2, which con-
tributes 2T .n=2/ to the running time.
Combine: We have already noted that the M ERGE procedure on an n-element
subarray takes time ‚.n/ ,a n ds o C.n/D‚.n/ .
When we add the functions D.n/ andC.n/ for the merge sort analysis, we are
adding a function that is ‚.n/ and a function that is ‚.1/ . This sum is a linear
function of n,t h a ti s , ‚.n/ . Adding it to the 2T .n=2/ term from the “conquer”
step gives the recurrence for the worst-case running time T .n/ of merge sort:
T .n/D(
‚.1/ ifnD1;
2T .n=2/C‚.n/ ifn>1:(2.1)
In Chapter 4, we shall see the “master theorem,” which we can use to show
thatT .n/ is‚.n lgn/, where lg nstands for log2n. Because the logarithm func-
tion grows more slowly than any linear function, for large enough inputs, merge
sort, with its ‚.n lgn/running time, outperforms insertion sort, whose running
time is ‚.n2/, in the worst case.
We do not need the master theorem to intuitively understand why the solution to
the recurrence (2.1) is T .n/D‚.n lgn/. Let us rewrite recurrence (2.1) as
T .n/D(
c ifnD1;
2T .n=2/Ccn ifn>1;(2.2)
where the constant crepresents the time required to solve problems of size 1as
well as the time per array element of the divide and combine steps.9
9It is unlikely that the same constant exactly represents both the time to solve problems of size 1
and the time per array element of the divide and combine steps. We can get around this problem by
letting cbe the larger of these times and understanding that our recurrence gives an upper bound on
the running time, or by letting cbe the lesser of these times and understanding that our recurrence
gives a lower bound on the running time. Both bounds are on the order of nlgnand, taken together,
give a ‚.n lgn/running time.2.3 Designing algorithms 37
Figure 2.5 shows how we can solve recurrence (2.2). For convenience, we as-
sume that nis an exact power of 2. Part (a) of the ﬁgure shows T .n/ ,w h i c hw e
expand in part (b) into an equivalent tree representing the recurrence. The cnterm
is the root (the cost incurred at the top level of recursion), and the two subtrees ofthe root are the two smaller recurrences T .n=2/ . Part (c) shows this process carried
one step further by expanding T .n=2/ . The cost incurred at each of the two sub-
nodes at the second level of recursion is cn=2 . We continue expanding each node
in the tree by breaking it into its constituent parts as determined by the recurrence,
until the problem sizes get down to 1, each with a cost of c. Part (d) shows the
resulting recursion tree .
Next, we add the costs across each level of the tree. The top level has total
costcn, the next level down has total cost c.n=2/Cc.n=2/Dcn, the level after
that has total cost c.n=4/Cc.n=4/Cc.n=4/Cc.n=4/Dcn, and so on. In general,
the level ibelow the top has 2
inodes, each contributing a cost of c.n=2i/,s ot h a t
theith level below the top has total cost 2ic.n=2i/Dcn. The bottom level has n
nodes, each contributing a cost of c, for a total cost of cn.
The total number of levels of the recursion tree in Figure 2.5 is lg nC1,w h e r e
nis the number of leaves, corresponding to the input size. An informal inductive
argument justiﬁes this claim. The base case occurs when nD1, in which case the
tree has only one level. Since lg 1D0,w eh a v et h a tl g nC1gives the correct
number of levels. Now assume as an inductive hypothesis that the number of levelsof a recursion tree with 2
ileaves is lg 2iC1DiC1(since for any value of i,
we have that lg 2iDi). Because we are assuming that the input size is a power
of2, the next input size to consider is 2iC1. A tree with nD2iC1leaves has
one more level than a tree with 2ileaves, and so the total number of levels is
.iC1/C1Dlg2iC1C1.
To compute the total cost represented by the recurrence (2.2), we simply add up
the costs of all the levels. The recursion tree has lg nC1levels, each costing cn,
for a total cost of cn.lgnC1/DcnlgnCcn. Ignoring the low-order term and
the constant cgives the desired result of ‚.n lgn/.
Exercises
2.3-1
Using Figure 2.4 as a model, illustrate the operation of merge sort on the arrayADh3; 41; 52; 26; 38; 57; 9; 49 i.
2.3-2
Rewrite the M
ERGE procedure so that it does not use sentinels, instead stopping
once either array LorRhas had all its elements copied back to Aand then copying
the remainder of the other array back into A.38 Chapter 2 Getting Started
cn
cn
…
Total: cn lg n + cncnlgncn
nc c c c c c c
…
(d)(c)cn
T(n/2) T(n/2)
(b)T(n)
(a)cn
cn/2
T(n/4) T(n/4)cn/2
T(n/4) T(n/4)
cn
cn/2
cn/4 cn/4cn/2
cn/4 cn/4
Figure 2.5 How to construct a recursion tree for the recurrence T .n/D2T .n=2/Ccn.
Part(a)shows T .n/ , which progressively expands in (b)–(d) to form the recursion tree. The fully
expanded tree in part (d) has lg nC1levels (i.e., it has height lg n, as indicated), and each level
contributes a total cost of cn. The total cost, therefore, is cnlgnCcn,w h i c hi s ‚.n lgn/.Problems for Chapter 2 39
2.3-3
Use mathematical induction to show that when nis an exact power of 2,t h es o l u -
tion of the recurrence
T .n/D(
2 ifnD2;
2T .n=2/CnifnD2k,f o rk>1
isT .n/Dnlgn.
2.3-4
We can express insertion sort as a recursive procedure as follows. In order to sortAŒ1 : : n/c141 , we recursively sort AŒ1 : : n/NUL1/c141and then insert AŒn/c141 into the sorted array
AŒ1 : : n/NUL1/c141. Write a recurrence for the running time of this recursive version of
insertion sort.
2.3-5
Referring back to the searching problem (see Exercise 2.1-3), observe that if the
sequence Ais sorted, we can check the midpoint of the sequence against /ETBand
eliminate half of the sequence from further consideration. The binary search al-
gorithm repeats this procedure, halving the size of the remaining portion of thesequence each time. Write pseudocode, either iterative or recursive, for binarysearch. Argue that the worst-case running time of binary search is ‚.lgn/.
2.3-6
Observe that the while loop of lines 5–7 of the I
NSERTION -SORT procedure in
Section 2.1 uses a linear search to scan (backward) through the sorted subarrayAŒ1 : : j/NUL1/c141. Can we use a binary search (see Exercise 2.3-5) instead to improve
the overall worst-case running time of insertion sort to ‚.n lgn/?
2.3-7 ?
Describe a ‚.n lgn/-time algorithm that, given a set Sofnintegers and another
integer x, determines whether or not there exist two elements in Swhose sum is
exactly x.
Problems
2-1 Insertion sort on small arrays in merge sort
Although merge sort runs in ‚.n lgn/worst-case time and insertion sort runs
in‚.n2/worst-case time, the constant factors in insertion sort can make it faster
in practice for small problem sizes on many machines. Thus, it makes sense tocoarsen the leaves of the recursion by using insertion sort within merge sort when40 Chapter 2 Getting Started
subproblems become sufﬁciently small. Consider a modiﬁcation to merge sort in
which n=k sublists of length kare sorted using insertion sort and then merged
using the standard merging mechanism, where kis a value to be determined.
a.Show that insertion sort can sort the n=k sublists, each of length k,i n‚.nk/
worst-case time.
b.Show how to merge the sublists in ‚.n lg.n=k// worst-case time.
c.Given that the modiﬁed algorithm runs in ‚.nkCnlg.n=k// worst-case time,
what is the largest value of kas a function of nfor which the modiﬁed algorithm
has the same running time as standard merge sort, in terms of ‚-notation?
d.How should we choose kin practice?
2-2 Correctness of bubblesort
Bubblesort is a popular, but inefﬁcient, sorting algorithm. It works by repeatedlyswapping adjacent elements that are out of order.
B
UBBLESORT .A/
1foriD1toA:length/NUL1
2 forjDA:length downto iC1
3 ifAŒj /c141 < AŒj/NUL1/c141
4 exchange AŒj /c141 withAŒj/NUL1/c141
a.LetA0denote the output of B UBBLESORT .A/. To prove that B UBBLESORT is
correct, we need to prove that it terminates and that
A0Œ1/c141/DC4A0Œ2/c141/DC4/SOH/SOH/SOH/DC4 A0Œn/c141 ; (2.3)
where nDA:length . In order to show that B UBBLESORT actually sorts, what
else do we need to prove?
The next two parts will prove inequality (2.3).b.State precisely a loop invariant for the forloop in lines 2–4, and prove that this
loop invariant holds. Your proof should use the structure of the loop invariant
proof presented in this chapter.
c.Using the termination condition of the loop invariant proved in part (b), state
a loop invariant for the forloop in lines 1–4 that will allow you to prove in-
equality (2.3). Your proof should use the structure of the loop invariant proofpresented in this chapter.Problems for Chapter 2 41
d.What is the worst-case running time of bubblesort? How does it compare to the
running time of insertion sort?
2-3 Correctness of Horner’s rule
The following code fragment implements Horner’s rule for evaluating a polynomial
P.x/DnX
kD0akxk
Da0Cx.a 1Cx.a 2C/SOH/SOH/SOHC x.a n/NUL1Cxan//SOH/SOH/SOH// ;
given the coefﬁcients a0;a1;:::;a nand a value for x:
1yD0
2foriDndownto 0
3 yDaiCx/SOHy
a.In terms of ‚-notation, what is the running time of this code fragment for
Horner’s rule?
b.Write pseudocode to implement the naive polynomial-evaluation algorithm that
computes each term of the polynomial from scratch. What is the running timeof this algorithm? How does it compare to Horner’s rule?
c.Consider the following loop invariant:
At the start of each iteration of the forloop of lines 2–3,
yDn/NUL.iC1/X
kD0akCiC1xk:
Interpret a summation with no terms as equaling 0. Following the structure of
the loop invariant proof presented in this chapter, use this loop invariant to showthat, at termination, yDP
n
kD0akxk.
d.Conclude by arguing that the given code fragment correctly evaluates a poly-
nomial characterized by the coefﬁcients a0;a1;:::;a n.
2-4 Inversions
LetAŒ1 : : n/c141 be an array of ndistinct numbers. If i<j andAŒi/c141 > AŒj /c141 , then the
pair.i; j / is called an inversion ofA.
a.List the ﬁve inversions of the array h2; 3; 8; 6; 1i.42 Chapter 2 Getting Started
b.What array with elements from the set f1 ;2;:::;nghas the most inversions?
How many does it have?
c.What is the relationship between the running time of insertion sort and the
number of inversions in the input array? Justify your answer.
d.Give an algorithm that determines the number of inversions in any permutation
onnelements in ‚.n lgn/worst-case time. ( Hint: Modify merge sort.)
Chapter notes
In 1968, Knuth published the ﬁrst of three volumes with the general title The Art of
Computer Programming [209, 210, 211]. The ﬁrst volume ushered in the modern
study of computer algorithms with a focus on the analysis of running time, and thefull series remains an engaging and worthwhile reference for many of the topics
presented here. According to Knuth, the word “algorithm” is derived from the
name “al-Khowˆ arizmˆ ı,” a ninth-century Persian mathematician.
Aho, Hopcroft, and Ullman [5] advocated the asymptotic analysis of algo-
rithms—using notations that Chapter 3 introduces, including ‚-notation—as a
means of comparing relative performance. They also popularized the use of re-currence relations to describe the running times of recursive algorithms.
Knuth [211] provides an encyclopedic treatment of many sorting algorithms. His
comparison of sorting algorithms (page 381) includes exact step-counting analyses,like the one we performed here for insertion sort. Knuth’s discussion of insertionsort encompasses several variations of the algorithm. The most important of theseis Shell’s sort, introduced by D. L. Shell, which uses insertion sort on periodicsubsequences of the input to produce a faster sorting algorithm.
Merge sort is also described by Knuth. He mentions that a mechanical colla-
tor capable of merging two decks of punched cards in a single pass was invented
in 1938. J. von Neumann, one of the pioneers of computer science, apparently
wrote a program for merge sort on the EDVAC computer in 1945.
The early history of proving programs correct is described by Gries [153], who
credits P. Naur with the ﬁrst article in this ﬁeld. Gries attributes loop invariants toR. W. Floyd. The textbook by Mitchell [256] describes more recent progress inproving programs correct.3 Growth of Functions
The order of growth of the running time of an algorithm, deﬁned in Chapter 2,
gives a simple characterization of the algorithm’s efﬁciency and also allows us tocompare the relative performance of alternative algorithms. Once the input size n
becomes large enough, merge sort, with its ‚.n lgn/worst-case running time,
beats insertion sort, whose worst-case running time is ‚.n
2/. Although we can
sometimes determine the exact running time of an algorithm, as we did for insertionsort in Chapter 2, the extra precision is not usually worth the effort of computingit. For large enough inputs, the multiplicative constants and lower-order terms ofan exact running time are dominated by the effects of the input size itself.
When we look at input sizes large enough to make only the order of growth of
the running time relevant, we are studying the asymptotic efﬁciency of algorithms.
That is, we are concerned with how the running time of an algorithm increases withthe size of the input in the limit , as the size of the input increases without bound.
Usually, an algorithm that is asymptotically more efﬁcient will be the best choice
for all but very small inputs.
This chapter gives several standard methods for simplifying the asymptotic anal-
ysis of algorithms. The next section begins by deﬁning several types of “asymp-totic notation,” of which we have already seen an example in ‚-notation. We then
present several notational conventions used throughout this book, and ﬁnally wereview the behavior of functions that commonly arise in the analysis of algorithms.
3.1 Asymptotic notation
The notations we use to describe the asymptotic running time of an algorithm
are deﬁned in terms of functions whose domains are the set of natural numbers
NDf0; 1; 2; : : :g. Such notations are convenient for describing the worst-case
running-time function T .n/ , which usually is deﬁned only on integer input sizes.
We sometimes ﬁnd it convenient, however, to abuse asymptotic notation in a va-44 Chapter 3 Growth of Functions
riety of ways. For example, we might extend the notation to the domain of real
numbers or, alternatively, restrict it to a subset of the natural numbers. We shouldmake sure, however, to understand the precise meaning of the notation so that whenwe abuse, we do not misuse it. This section deﬁnes the basic asymptotic notations
and also introduces some common abuses.
Asymptotic notation, functions, and running times
We will use asymptotic notation primarily to describe the running times of algo-
rithms, as when we wrote that insertion sort’s worst-case running time is ‚.n
2/.
Asymptotic notation actually applies to functions, however. Recall that we charac-terized insertion sort’s worst-case running time as an
2CbnCc, for some constants
a,b,a n d c. By writing that insertion sort’s running time is ‚.n2/, we abstracted
away some details of this function. Because asymptotic notation applies to func-
tions, what we were writing as ‚.n2/was the function an2CbnCc,w h i c hi n
that case happened to characterize the worst-case running time of insertion sort.
In this book, the functions to which we apply asymptotic notation will usually
characterize the running times of algorithms. But asymptotic notation can apply tofunctions that characterize some other aspect of algorithms (the amount of spacethey use, for example), or even to functions that have nothing whatsoever to dowith algorithms.
Even when we use asymptotic notation to apply to the running time of an al-
gorithm, we need to understand which running time we mean. Sometimes we are
interested in the worst-case running time. Often, however, we wish to characterizethe running time no matter what the input. In other words, we often wish to makea blanket statement that covers all inputs, not just the worst case. We shall seeasymptotic notations that are well suited to characterizing running times no matter
what the input.
‚-notation
In Chapter 2, we found that the worst-case running time of insertion sort is
T .n/D‚.n
2/. Let us deﬁne what this notation means. For a given function g.n/ ,
we denote by ‚.g.n// theset of functions
‚.g.n//Dff .n/Wthere exist positive constants c1,c2,a n d n0such that
0/DC4c1g.n//DC4f .n//DC4c2g.n/ for all n/NAKn0g:1
1Within set notation, a colon means “such that.”3.1 Asymptotic notation 45
(b) (c) (a)n n nn0 n0 n0f .n/D‚.g.n// f .n/ DO.g.n// f .n/ D/DEL.g.n//f .n/
f .n/f .n/
cg.n/cg.n/
c1g.n/c2g.n/
Figure 3.1 Graphic examples of the ‚,O,a n d /DELnotations. In each part, the value of n0shown
is the minimum possible value; any greater value would also work. (a)‚-notation bounds a func-
tion to within constant factors. We write f. n /D‚.g.n// if there exist positive constants n0,c1,
andc2such that at and to the right of n0,t h ev a l u eo f f. n / always lies between c1g.n/ andc2g.n/
inclusive. (b)O-notation gives an upper bound for a function to within a constant factor. We write
f. n /DO.g.n// if there are positive constants n0andcsuch that at and to the right of n0,t h ev a l u e
off. n / always lies on or below cg.n/ .(c)/DEL-notation gives a lower bound for a function to within
a constant factor. We write f. n /D/DEL.g.n// if there are positive constants n0andcsuch that at and
to the right of n0,t h ev a l u eo f f. n / always lies on or above cg.n/ .
A function f .n/ belongs to the set ‚.g.n// if there exist positive constants c1
andc2such that it can be “sandwiched” between c1g.n/ andc2g.n/ ,f o rs u f ﬁ -
ciently large n. Because ‚.g.n// is a set, we could write “ f .n/2‚.g.n// ”
to indicate that f .n/ is a member of ‚.g.n// . Instead, we will usually write
“f .n/D‚.g.n// ” to express the same notion. You might be confused because
we abuse equality in this way, but we shall see later in this section that doing sohas its advantages.
Figure 3.1(a) gives an intuitive picture of functions f .n/ andg.n/ ,w h e r e
f .n/D‚.g.n// . For all values of nat and to the right of n
0,t h ev a l u eo f f .n/
lies at or above c1g.n/ and at or below c2g.n/ . In other words, for all n/NAKn0,t h e
function f .n/ is equal to g.n/ to within a constant factor. We say that g.n/ is an
asymptotically tight bound forf .n/ .
The deﬁnition of ‚.g.n// requires that every member f .n/2‚.g.n// be
asymptotically nonnegative , that is, that f .n/ be nonnegative whenever nis suf-
ﬁciently large. (An asymptotically positive function is one that is positive for all
sufﬁciently large n.) Consequently, the function g.n/ itself must be asymptotically
nonnegative, or else the set ‚.g.n// is empty. We shall therefore assume that every
function used within ‚-notation is asymptotically nonnegative. This assumption
holds for the other asymptotic notations deﬁned in this chapter as well.46 Chapter 3 Growth of Functions
In Chapter 2, we introduced an informal notion of ‚-notation that amounted
to throwing away lower-order terms and ignoring the leading coefﬁcient of thehighest-order term. Let us brieﬂy justify this intuition by using the formal deﬁ-nition to show that
1
2n2/NUL3nD‚.n2/. To do so, we must determine positive
constants c1,c2,a n d n0such that
c1n2/DC41
2n2/NUL3n/DC4c2n2
for all n/NAKn0. Dividing by n2yields
c1/DC41
2/NUL3
n/DC4c2:
We can make the right-hand inequality hold for any value of n/NAK1by choosing any
constant c2/NAK1=2. Likewise, we can make the left-hand inequality hold for any
value of n/NAK7by choosing any constant c1/DC41=14 . Thus, by choosing c1D1=14 ,
c2D1=2,a n d n0D7, we can verify that1
2n2/NUL3nD‚.n2/. Certainly, other
choices for the constants exist, but the important thing is that some choice exists.
Note that these constants depend on the function1
2n2/NUL3n; a different function
belonging to ‚.n2/would usually require different constants.
We can also use the formal deﬁnition to verify that 6n3¤‚.n2/. Suppose
for the purpose of contradiction that c2andn0exist such that 6n3/DC4c2n2for
alln/NAKn0. But then dividing by n2yields n/DC4c2=6, which cannot possibly hold
for arbitrarily large n,s i n c e c2is constant.
Intuitively, the lower-order terms of an asymptotically positive function can be
ignored in determining asymptotically tight bounds because they are insigniﬁcantfor large n.W h e n nis large, even a tiny fraction of the highest-order term suf-
ﬁces to dominate the lower-order terms. Thus, setting c
1to a value that is slightly
smaller than the coefﬁcient of the highest-order term and setting c2to a value that
is slightly larger permits the inequalities in the deﬁnition of ‚-notation to be sat-
isﬁed. The coefﬁcient of the highest-order term can likewise be ignored, since itonly changes c
1andc2by a constant factor equal to the coefﬁcient.
As an example, consider any quadratic function f .n/Dan2CbnCc,w h e r e
a,b,a n d care constants and a>0 . Throwing away the lower-order terms and
ignoring the constant yields f .n/D‚.n2/. Formally, to show the same thing, we
take the constants c1Da=4,c2D7a=4 ,a n d n0D2/SOHmax.jbj=a;p
jcj=a/.Y o u
may verify that 0/DC4c1n2/DC4an2CbnCc/DC4c2n2for all n/NAKn0. In general,
for any polynomial p.n/DPd
iD0aini, where the aiare constants and ad>0,w e
have p.n/D‚.nd/(see Problem 3-1).
Since any constant is a degree- 0polynomial, we can express any constant func-
tion as ‚.n0/,o r‚.1/ . This latter notation is a minor abuse, however, because the3.1 Asymptotic notation 47
expression does not indicate what variable is tending to inﬁnity.2We shall often
use the notation ‚.1/ to mean either a constant or a constant function with respect
to some variable.
O-notation
The‚-notation asymptotically bounds a function from above and below. When
we have only an asymptotic upper bound ,w eu s e O-notation. For a given func-
tiong.n/ , we denote by O.g.n// (pronounced “big-oh of gofn” or sometimes
just “oh of gofn”) the set of functions
O.g.n//Dff .n/Wthere exist positive constants candn0such that
0/DC4f .n//DC4cg.n/ for all n/NAKn0g:
We use O-notation to give an upper bound on a function, to within a constant
factor. Figure 3.1(b) shows the intuition behind O-notation. For all values nat and
to the right of n0, the value of the function f .n/ is on or below cg.n/ .
We write f .n/DO.g.n// to indicate that a function f .n/ i sam e m b e ro ft h e
setO.g.n// . Note that f .n/D‚.g.n// implies f .n/DO.g.n// ,s i n c e ‚-
notation is a stronger notion than O-notation. Written set-theoretically, we have
‚.g.n///DC2O.g.n// . Thus, our proof that any quadratic function an2CbnCc,
where a>0 ,i si n ‚.n2/also shows that any such quadratic function is in O.n2/.
What may be more surprising is that when a>0 ,a n y linear function anCbis
inO.n2/, which is easily veriﬁed by taking cDaCjbjandn0Dmax.1;/NULb=a/ .
If you have seen O-notation before, you might ﬁnd it strange that we should
write, for example, nDO.n2/. In the literature, we sometimes ﬁnd O-notation
informally describing asymptotically tight bounds, that is, what we have deﬁned
using ‚-notation. In this book, however, when we write f .n/DO.g.n// ,w e
are merely claiming that some constant multiple of g.n/ is an asymptotic upper
bound on f .n/ , with no claim about how tight an upper bound it is. Distinguish-
ing asymptotic upper bounds from asymptotically tight bounds is standard in thealgorithms literature.
Using O-notation, we can often describe the running time of an algorithm
merely by inspecting the algorithm’s overall structure. For example, the doublynested loop structure of the insertion sort algorithm from Chapter 2 immediatelyyields an O.n
2/upper bound on the worst-case running time: the cost of each it-
eration of the inner loop is bounded from above by O.1/ (constant), the indices i
2The real problem is that our ordinary notation for functions does not distinguish functions from
values. In /NAK-calculus, the parameters to a function are clearly speciﬁed: the function n2could be
written as /NAKn:n2,o re v e n /NAKr:r2. Adopting a more rigorous notation, however, would complicate
algebraic manipulations, and so we choose to tolerate the abuse.48 Chapter 3 Growth of Functions
andjare both at most n, and the inner loop is executed at most once for each of
then2pairs of values for iandj.
Since O-notation describes an upper bound, when we use it to bound the worst-
case running time of an algorithm, we have a bound on the running time of the algo-rithm on every input—the blanket statement we discussed earlier. Thus, the O.n
2/
bound on worst-case running time of insertion sort also applies to its running timeon every input. The ‚.n
2/bound on the worst-case running time of insertion sort,
however, does not imply a ‚.n2/bound on the running time of insertion sort on
every input. For example, we saw in Chapter 2 that when the input is already
sorted, insertion sort runs in ‚.n/ time.
Technically, it is an abuse to say that the running time of insertion sort is O.n2/,
since for a given n, the actual running time varies, depending on the particular
input of size n. When we say “the running time is O.n2/,” we mean that there is a
function f .n/ that is O.n2/such that for any value of n, no matter what particular
input of size nis chosen, the running time on that input is bounded from above by
the value f .n/ . Equivalently, we mean that the worst-case running time is O.n2/.
/DEL-notation
Just as O-notation provides an asymptotic upper bound on a function, /DEL-notation
provides an asymptotic lower bound . For a given function g.n/ , we denote
by/DEL.g.n// (pronounced “big-omega of gofn” or sometimes just “omega of g
ofn”) the set of functions
/DEL.g.n//Dff .n/Wthere exist positive constants candn0such that
0/DC4cg.n//DC4f .n/ for all n/NAKn0g:
Figure 3.1(c) shows the intuition behind /DEL-notation. For all values nat or to the
right of n0,t h ev a l u eo f f .n/ is on or above cg.n/ .
From the deﬁnitions of the asymptotic notations we have seen thus far, it is easy
to prove the following important theorem (see Exercise 3.1-5).
Theorem 3.1
For any two functions f .n/ andg.n/ ,w eh a v e f .n/D‚.g.n// if and only if
f .n/DO.g.n// andf .n/D/DEL.g.n// .
As an example of the application of this theorem, our proof that an2CbnCcD
‚.n2/for any constants a,b,a n d c,w h e r e a>0 , immediately implies that
an2CbnCcD/DEL.n2/andan2CbnCcDO.n2/. In practice, rather than using
Theorem 3.1 to obtain asymptotic upper and lower bounds from asymptoticallytight bounds, as we did for this example, we usually use it to prove asymptoticallytight bounds from asymptotic upper and lower bounds.3.1 Asymptotic notation 49
When we say that the running time (no modiﬁer) of an algorithm is /DEL.g.n// ,
we mean that no matter what particular input of size nis chosen for each value
ofn, the running time on that input is at least a constant times g.n/ , for sufﬁciently
large n. Equivalently, we are giving a lower bound on the best-case running time
of an algorithm. For example, the best-case running time of insertion sort is /DEL.n/ ,
which implies that the running time of insertion sort is /DEL.n/ .
The running time of insertion sort therefore belongs to both /DEL.n/ andO.n2/,
since it falls anywhere between a linear function of nand a quadratic function of n.
Moreover, these bounds are asymptotically as tight as possible: for instance, the
running time of insertion sort is not /DEL.n2/, since there exists an input for which
insertion sort runs in ‚.n/ time (e.g., when the input is already sorted). It is not
contradictory, however, to say that the worst-case running time of insertion sort
is/DEL.n2/, since there exists an input that causes the algorithm to take /DEL.n2/time.
Asymptotic notation in equations and inequalities
We have already seen how asymptotic notation can be used within mathematical
formulas. For example, in introducing O-notation, we wrote “ nDO.n2/.” We
might also write 2n2C3nC1D2n2C‚.n/ . How do we interpret such formulas?
When the asymptotic notation stands alone (that is, not within a larger formula)
on the right-hand side of an equation (or inequality), as in nDO.n2/,w eh a v e
already deﬁned the equal sign to mean set membership: n2O.n2/. In general,
however, when asymptotic notation appears in a formula, we interpret it as stand-ing for some anonymous function that we do not care to name. For example, theformula 2n
2C3nC1D2n2C‚.n/ means that 2n2C3nC1D2n2Cf .n/ ,
where f .n/ is some function in the set ‚.n/ . In this case, we let f .n/D3nC1,
which indeed is in ‚.n/ .
Using asymptotic notation in this manner can help eliminate inessential detail
and clutter in an equation. For example, in Chapter 2 we expressed the worst-caserunning time of merge sort as the recurrence
T .n/D2T .n=2/C‚.n/ :
If we are interested only in the asymptotic behavior of T .n/ , there is no point in
specifying all the lower-order terms exactly; they are all understood to be includedin the anonymous function denoted by the term ‚.n/ .
The number of anonymous functions in an expression is understood to be equal
to the number of times the asymptotic notation appears. For example, in the ex-pression
nX
iD1O.i/ ;50 Chapter 3 Growth of Functions
there is only a single anonymous function (a function of i). This expression is thus
notthe same as O.1/CO.2/C/SOH/SOH/SOHC O.n/ , which doesn’t really have a clean
interpretation.
In some cases, asymptotic notation appears on the left-hand side of an equation,
as in
2n2C‚.n/D‚.n2/:
We interpret such equations using the following rule: No matter how the anony-
mous functions are chosen on the left of the equal sign, there is a way to choosethe anonymous functions on the right of the equal sign to make the equation valid .
Thus, our example means that for anyfunction f .n/2‚.n/ , there is some func-
tiong.n/2‚.n
2/such that 2n2Cf .n/Dg.n/ for all n. In other words, the
right-hand side of an equation provides a coarser level of detail than the left-handside.
We can chain together a number of such relationships, as in
2n
2C3nC1D2n2C‚.n/
D‚.n2/:
We can interpret each equation separately by the rules above. The ﬁrst equa-
tion says that there is some function f .n/2‚.n/ such that 2n2C3nC1D
2n2Cf .n/ for all n. The second equation says that for anyfunction g.n/2‚.n/
(such as the f .n/ just mentioned), there is some function h.n/2‚.n2/such
that2n2Cg.n/Dh.n/ for all n. Note that this interpretation implies that
2n2C3nC1D‚.n2/, which is what the chaining of equations intuitively gives
us.
o-notation
The asymptotic upper bound provided by O-notation may or may not be asymp-
totically tight. The bound 2n2DO.n2/is asymptotically tight, but the bound
2nDO.n2/is not. We use o-notation to denote an upper bound that is not asymp-
totically tight. We formally deﬁne o.g.n// (“little-oh of gofn”) as the set
o.g.n//Dff .n/Wfor any positive constant c>0 , there exists a constant
n0>0such that 0/DC4f .n/ < cg.n/ for all n/NAKn0g:
For example, 2nDo.n2/,b u t2n2¤o.n2/.
The deﬁnitions of O-notation and o-notation are similar. The main difference
is that in f .n/DO.g.n// , the bound 0/DC4f .n//DC4cg.n/ holds for some con-
stant c>0 ,b u ti n f .n/Do.g.n// , the bound 0/DC4f .n/ < cg.n/ holds for all
constants c>0 . Intuitively, in o-notation, the function f .n/ becomes insigniﬁcant
relative to g.n/ asnapproaches inﬁnity; that is,3.1 Asymptotic notation 51
lim
n!1f .n/
g.n/D0: (3.1)
Some authors use this limit as a deﬁnition of the o-notation; the deﬁnition in this
book also restricts the anonymous functions to be asymptotically nonnegative.
!-notation
By analogy, !-notation is to /DEL-notation as o-notation is to O-notation. We use
!-notation to denote a lower bound that is not asymptotically tight. One way to
deﬁne it is by
f .n/2!.g.n// if and only if g.n/2o.f .n// :
Formally, however, we deﬁne !.g.n// (“little-omega of gofn”) as the set
!.g.n//Dff .n/Wfor any positive constant c>0 , there exists a constant
n0>0such that 0/DC4cg.n/ < f .n/ for all n/NAKn0g:
For example, n2=2D!.n/ ,b u t n2=2¤!.n2/. The relation f .n/D!.g.n//
implies that
lim
n!1f .n/
g.n/D1 ;
if the limit exists. That is, f .n/ becomes arbitrarily large relative to g.n/ asn
approaches inﬁnity.
Comparing functions
Many of the relational properties of real numbers apply to asymptotic comparisons
as well. For the following, assume that f .n/ andg.n/ are asymptotically positive.
Transitivity:
f .n/D‚.g.n// andg.n/D‚.h.n// imply f .n/D‚.h.n// ;
f .n/DO.g.n// andg.n/DO.h.n// imply f .n/DO.h.n// ;
f .n/D/DEL.g.n// andg.n/D/DEL.h.n// imply f .n/D/DEL.h.n// ;
f .n/Do.g.n// andg.n/Do.h.n// imply f .n/Do.h.n// ;
f .n/D!.g.n// andg.n/D!.h.n// imply f .n/D!.h.n// :
Reﬂexivity:
f .n/D‚.f .n// ;
f .n/DO.f .n// ;
f .n/D/DEL.f .n// :52 Chapter 3 Growth of Functions
Symmetry:
f .n/D‚.g.n// if and only if g.n/D‚.f .n// :
Transpose symmetry:
f .n/DO.g.n// if and only if g.n/D/DEL.f .n// ;
f .n/Do.g.n// if and only if g.n/D!.f .n// :
Because these properties hold for asymptotic notations, we can draw an analogy
between the asymptotic comparison of two functions fandgand the comparison
of two real numbers aandb:
f .n/DO.g.n// is like a/DC4b;
f .n/D/DEL.g.n// is like a/NAKb;
f .n/D‚.g.n// is like aDb;
f .n/Do.g.n// is like a<b;
f .n/D!.g.n// is like a>b:
We say that f .n/ isasymptotically smaller thang.n/ iff .n/Do.g.n// ,a n d f .n/
isasymptotically larger thang.n/ iff .n/D!.g.n// .
One property of real numbers, however, does not carry over to asymptotic nota-
tion:
Trichotomy: For any two real numbers aandb, exactly one of the following must
hold: a<b ,aDb,o ra>b .
Although any two real numbers can be compared, not all functions are asymptot-
ically comparable. That is, for two functions f .n/ andg.n/ , it may be the case
that neither f .n/DO.g.n// norf .n/D/DEL.g.n// holds. For example, we cannot
compare the functions nandn1Csinnusing asymptotic notation, since the value of
the exponent in n1Csinnoscillates between 0 and 2, taking on all values in between.
Exercises
3.1-1
Letf .n/ andg.n/ be asymptotically nonnegative functions. Using the basic deﬁ-
nition of ‚-notation, prove that max .f .n/; g.n//D‚.f .n/Cg.n// .
3.1-2
Show that for any real constants aandb,w h e r e b>0 ,
.nCa/bD‚.nb/: (3.2)3.2 Standard notations and common functions 53
3.1-3
Explain why the statement, “The running time of algorithm Ais at least O.n2/,” is
meaningless.
3.1-4
Is2nC1DO.2n/?I s22nDO.2n/?
3.1-5
Prove Theorem 3.1.
3.1-6
Prove that the running time of an algorithm is ‚.g.n// if and only if its worst-case
running time is O.g.n// and its best-case running time is /DEL.g.n// .
3.1-7
Prove that o.g.n//\!.g.n// is the empty set.
3.1-8
We can extend our notation to the case of two parameters nandmthat can go to
inﬁnity independently at different rates. For a given function g.n;m/ , we denote
byO.g.n;m// the set of functions
O.g.n;m//Dff. n ;m /Wthere exist positive constants c,n0,a n d m0
such that 0/DC4f. n ;m //DC4cg.n;m/
for all n/NAKn0orm/NAKm0g:
Give corresponding deﬁnitions for /DEL.g.n; m// and‚.g.n; m// .
3.2 Standard notations and common functions
This section reviews some standard mathematical functions and notations and ex-
plores the relationships among them. It also illustrates the use of the asymptoticnotations.
Monotonicity
A function f .n/ ismonotonically increasing ifm/DC4nimplies f. m //DC4f .n/ .
Similarly, it is monotonically decreasing ifm/DC4nimplies f. m //NAKf .n/ .A
function f .n/ isstrictly increasing ifm<n implies f .m/ < f .n/ andstrictly
decreasing ifm<n implies f .m/ > f .n/ .54 Chapter 3 Growth of Functions
Floors and ceilings
For any real number x, we denote the greatest integer less than or equal to xbybxc
(read “the ﬂoor of x”) and the least integer greater than or equal to xbydxe(read
“the ceiling of x”). For all real x,
x/NUL1<bxc/DC4x/DC4dxe<xC1: (3.3)
For any integer n,
dn=2eCbn=2cDn;
and for any real number x/NAK0and integers a;b > 0 ,
/CANdx=ae
b/EM
Dlx
abm
; (3.4)
/SYNbx=ac
b/ETB
Djx
abk
; (3.5)
la
bm
/DC4aC.b/NUL1/
b; (3.6)
ja
bk
/NAKa/NUL.b/NUL1/
b: (3.7)
The ﬂoor function f. x/Dbxcis monotonically increasing, as is the ceiling func-
tionf. x/Ddxe.
Modular arithmetic
For any integer aand any positive integer n,t h ev a l u e amodnis the remainder
(orresidue ) of the quotient a=n:
amodnDa/NULnba=nc: (3.8)
It follows that
0/DC4amodn<n: (3.9)
Given a well-deﬁned notion of the remainder of one integer when divided by an-
other, it is convenient to provide special notation to indicate equality of remainders.If.amodn/D.bmodn/, we write a/DC1b.mod n/and say that aisequivalent
tob, modulo n. In other words, a/DC1b.mod n/ifaandbhave the same remain-
der when divided by n. Equivalently, a/DC1b.mod n/if and only if nis a divisor
ofb/NULa
. We write a6/DC1b.mod n/ifais not equivalent to b, modulo n.3.2 Standard notations and common functions 55
Polynomials
Given a nonnegative integer d,apolynomial in nof degree dis a function p.n/
of the form
p.n/DdX
iD0aini;
where the constants a0;a1;:::;a dare the coefﬁcients of the polynomial and
ad¤0. A polynomial is asymptotically positive if and only if ad>0.F o r a n
asymptotically positive polynomial p.n/ of degree d,w eh a v e p.n/D‚.nd/.F o r
any real constant a/NAK0, the function nais monotonically increasing, and for any
real constant a/DC40, the function nais monotonically decreasing. We say that a
function f .n/ ispolynomially bounded iff .n/DO.nk/for some constant k.
Exponentials
For all real a>0 ,m,a n d n, we have the following identities:
a0D1;
a1Da;
a/NUL1D1=a ;
.am/nDamn;
.am/nD.an/m;
amanDamCn:
For all nanda/NAK1, the function anis monotonically increasing in n.W h e n
convenient, we shall assume 00D1.
We can relate the rates of growth of polynomials and exponentials by the fol-
lowing fact. For all real constants aandbsuch that a>1 ,
lim
n!1nb
anD0; (3.10)
from which we can conclude that
nbDo.an/:
Thus, any exponential function with a base strictly greater than 1grows faster than
any polynomial function.
Using eto denote 2:71828 : : : , the base of the natural logarithm function, we
have for all real x,
exD1CxCx2
2ŠCx3
3ŠC/SOH/SOH/SOHD1X
iD0xi
iŠ; (3.11)56 Chapter 3 Growth of Functions
where “ Š” denotes the factorial function deﬁned later in this section. For all real x,
we have the inequality
ex/NAK1Cx; (3.12)
where equality holds only when xD0.W h e njxj/DC41, we have the approximation
1Cx/DC4ex/DC41CxCx2: (3.13)
When x!0, the approximation of exby1Cxis quite good:
exD1CxC‚.x2/:
(In this equation, the asymptotic notation is used to describe the limiting behavior
asx!0rather than as x!1 .) We have for all x,
lim
n!1/DLE
1Cx
n/DC1n
Dex: (3.14)
Logarithms
We shall use the following notations:
lgnDlog2n (binary logarithm) ,
lnnDlogen (natural logarithm) ,
lgknD.lgn/k(exponentiation) ,
lg lgnDlg.lgn/(composition) .
An important notational convention we shall adopt is that logarithm functions will
apply only to the next term in the formula ,s ot h a tl g nCkwill mean .lgn/Ck
and not lg .nCk/. If we hold b>1 constant, then for n>0 , the function logbn
is strictly increasing.
For all real a>0 ,b>0 ,c>0 ,a n d n,
aDblogba;
logc.ab/DlogcaClogcb;
logbanDnlogba;
logbaDlogca
logcb; (3.15)
logb.1=a/D/NUL logba;
logbaD1
logab;
alogbcDclogba; (3.16)
where, in each equation above, logarithm bases are not 1.3.2 Standard notations and common functions 57
By equation (3.15), changing the base of a logarithm from one constant to an-
other changes the value of the logarithm by only a constant factor, and so we shalloften use the notation “lg n” when we don’t care about constant factors, such as in
O-notation. Computer scientists ﬁnd 2to be the most natural base for logarithms
because so many algorithms and data structures involve splitting a problem intotwo parts.
There is a simple series expansion for ln .1Cx/whenjxj<1:
ln.1Cx/Dx/NULx
2
2Cx3
3/NULx4
4Cx5
5/NUL/SOH/SOH/SOH :
We also have the following inequalities for x>/NUL1:
x
1Cx/DC4ln.1Cx//DC4x; (3.17)
where equality holds only for xD0.
We say that a function f .n/ ispolylogarithmically bounded iff .n/DO.lgkn/
for some constant k. We can relate the growth of polynomials and polylogarithms
by substituting lg nfornand2aforain equation (3.10), yielding
lim
n!1lgbn
.2a/lgnDlim
n!1lgbn
naD0:
From this limit, we can conclude that
lgbnDo.na/
for any constant a>0 . Thus, any positive polynomial function grows faster than
any polylogarithmic function.
Factorials
The notation nŠ(read “ nfactorial”) is deﬁned for integers n/NAK0as
nŠD(
1 ifnD0;
n/SOH.n/NUL1/Š ifn>0:
Thus, nŠD1/SOH2/SOH3/SOH/SOH/SOHn.
A weak upper bound on the factorial function is nŠ/DC4nn, since each of the n
terms in the factorial product is at most n.Stirling’s approximation ,
nŠDp
2/EMn/DLEn
e/DC1n/DC2
1C‚/DC21
n/DC3/DC3
; (3.18)58 Chapter 3 Growth of Functions
where eis the base of the natural logarithm, gives us a tighter upper bound, and a
lower bound as well. As Exercise 3.2-3 asks you to prove,
nŠDo.nn/;
nŠD!.2n/;
lg.nŠ/D‚.n lgn/ ; (3.19)
where Stirling’s approximation is helpful in proving equation (3.19). The following
equation also holds for all n/NAK1:
nŠDp
2/EMn/DLEn
e/DC1n
e˛n(3.20)
where
1
12nC1<˛ n<1
12n: (3.21)
Functional iteration
We use the notation f.i/.n/to denote the function f .n/ iteratively applied itimes
to an initial value of n. Formally, let f .n/ be a function over the reals. For non-
negative integers i, we recursively deﬁne
f.i/.n/D(
n ifiD0;
f. f.i/NUL1/.n// ifi>0:
For example, if f .n/D2n,t h e n f.i/.n/D2in.
The iterated logarithm function
We use the notation lg/ETXn(read “log star of n”) to denote the iterated logarithm, de-
ﬁned as follows. Let lg.i/nbe as deﬁned above, with f .n/Dlgn. Because the log-
arithm of a nonpositive number is undeﬁned, lg.i/nis deﬁned only if lg.i/NUL1/n>0 .
Be sure to distinguish lg.i/n(the logarithm function applied itimes in succession,
starting with argument n) from lgin(the logarithm of nraised to the ith power).
Then we deﬁne the iterated logarithm function as
lg/ETXnDmin˚
i/NAK0Wlg.i/n/DC41/TAB
:
The iterated logarithm is a very slowly growing function:
lg/ETX2D1;
lg/ETX4D2;
lg/ETX16D3;
lg/ETX65536D4;
lg/ETX.265536/D5:3.2 Standard notations and common functions 59
Since the number of atoms in the observable universe is estimated to be about 1080,
which is much less than 265536, we rarely encounter an input size nsuch that
lg/ETXn>5 .
Fibonacci numbers
We deﬁne the Fibonacci numbers by the following recurrence:
F0D0;
F1D1; (3.22)
FiDFi/NUL1CFi/NUL2 fori/NAK2:
Thus, each Fibonacci number is the sum of the two previous ones, yielding the
sequence
0; 1; 1; 2; 3; 5; 8; 13; 21; 34; 55; : : : :
Fibonacci numbers are related to the golden ratio /RSand to its conjugate y/RS,w h i c h
are the two roots of the equation
x2DxC1 (3.23)
and are given by the following formulas (see Exercise 3.2-6):
/RSD1Cp
5
2(3.24)
D1:61803 : : : ;
y/RSD1/NULp
5
2
D/NUL :61803 : : : :
Speciﬁcally, we have
FiD/RSi/NULy/RSi
p
5;
which we can prove by induction (Exercise 3.2-7). Sinceˇˇy/RSˇˇ<1,w eh a v e
ˇˇy/RSiˇˇ
p
5<1
p
5
<1
2;
which implies that60 Chapter 3 Growth of Functions
FiD/SYN/RSi
p
5C1
2/ETB
; (3.25)
which is to say that the ith Fibonacci number Fiis equal to /RSi=p
5rounded to the
nearest integer. Thus, Fibonacci numbers grow exponentially.
Exercises
3.2-1
Show that if f .n/ andg.n/ are monotonically increasing functions, then so are
the functions f .n/Cg.n/ andf .g.n// ,a n di f f .n/ andg.n/ are in addition
nonnegative, then f .n//SOHg.n/ is monotonically increasing.
3.2-2
Prove equation (3.16).
3.2-3
Prove equation (3.19). Also prove that nŠD!.2n/andnŠDo.nn/.
3.2-4 ?
Is the functiondlgneŠpolynomially bounded? Is the function dlg lgneŠpolynomi-
ally bounded?
3.2-5 ?
Which is asymptotically larger: lg .lg/ETXn/or lg/ETX.lgn/?
3.2-6
Show that the golden ratio /RSand its conjugate y/RSboth satisfy the equation
x2DxC1.
3.2-7
Prove by induction that the ith Fibonacci number satisﬁes the equality
FiD/RSi/NULy/RSi
p
5;
where /RSis the golden ratio and y/RSis its conjugate.
3.2-8
Show that klnkD‚.n/ implies kD‚.n= lnn/.Problems for Chapter 3 61
Problems
3-1 Asymptotic behavior of polynomials
Let
p.n/DdX
iD0aini;
where ad>0,b ead e g r e e - dpolynomial in n,a n dl e t kbe a constant. Use the
deﬁnitions of the asymptotic notations to prove the following properties.
a.Ifk/NAKd,t h e n p.n/DO.nk/.
b.Ifk/DC4d,t h e n p.n/D/DEL.nk/.
c.IfkDd,t h e n p.n/D‚.nk/.
d.Ifk>d ,t h e n p.n/Do.nk/.
e.Ifk<d ,t h e n p.n/D!.nk/.
3-2 Relative asymptotic growths
Indicate, for each pair of expressions .A; B/ in the table below, whether AisO,o,
/DEL,!,o r‚ofB. Assume that k/NAK1,/SI>0 ,a n d c>1 are constants. Your answer
should be in the form of the table with “yes” or “no” written in each box.
AB
 O
 o
 /DEL
 !
 ‚
a. lgknn/SI
b. nkcn
c.p
nnsinn
d. 2n2n=2
e. nlgcclgn
f.lg.nŠ/ lg.nn/
3-3 Ordering by asymptotic growth rates
a.Rank the following functions by order of growth; that is, ﬁnd an arrangement
g1;g2;:::;g 30of the functions satisfying g1D/DEL.g 2/,g2D/DEL.g 3/, ...,
g29D/DEL.g 30/. Partition your list into equivalence classes such that functions
f .n/ andg.n/ are in the same class if and only if f .n/D‚.g.n// .62 Chapter 3 Growth of Functions
lg.lg/ETXn/ 2lg/ETXn.p
2/lgnn2nŠ . lgn/Š
.3
2/nn3lg2n lg.nŠ/ 22nn1=lgn
ln lnn lg/ETXnn/SOH2nnlg lgnlnn1
2lgn.lgn/lgnen4lgn.nC1/Šp
lgn
lg/ETX.lgn/ 2p
2lgnn2nnlgn22nC1
b.Give an example of a single nonnegative function f .n/ such that for all func-
tions gi.n/in part (a), f .n/ is neither O.g i.n// nor/DEL.g i.n//.
3-4 Asymptotic notation properties
Letf .n/ andg.n/ be asymptotically positive functions. Prove or disprove each of
the following conjectures.
a.f .n/DO.g.n// implies g.n/DO.f .n// .
b.f .n/Cg.n/D‚.min.f .n/; g.n/// .
c.f .n/DO.g.n// implies lg .f .n//DO.lg.g.n/// , where lg .g.n///NAK1and
f .n//NAK1for all sufﬁciently large n.
d.f .n/DO.g.n// implies 2f. n /DO/NUL
2g.n//SOH
.
e.f .n/DO ..f .n//2/.
f.f .n/DO.g.n// implies g.n/D/DEL.f .n// .
g.f .n/D‚.f .n=2// .
h.f .n/Co.f .n//D‚.f .n// .
3-5 Variations on Oand˝
Some authors deﬁne /DELin a slightly different way than we do; let’s use1/DEL(read
“omega inﬁnity”) for this alternative deﬁnition. We say that f .n/D1/DEL.g.n// if
there exists a positive constant csuch that f .n//NAKcg.n//NAK0for inﬁnitely many
integers n.
a.Show that for any two functions f .n/ andg.n/ that are asymptotically nonneg-
ative, either f .n/DO.g.n// orf .n/D1/DEL.g.n// or both, whereas this is not
true if we use /DELin place of1/DEL.Problems for Chapter 3 63
b.Describe the potential advantages and disadvantages of using1/DELinstead of /DELto
characterize the running times of programs.
Some authors also deﬁne Oin a slightly different manner; let’s use O0for the
alternative deﬁnition. We say that f .n/DO0.g.n// if and only ifjf .n/jD
O.g.n// .
c.What happens to each direction of the “if and only if” in Theorem 3.1 if we
substitute O0forObut still use /DEL?
Some authors deﬁne eO(read “soft-oh”) to mean Owith logarithmic factors ig-
nored:
eO.g.n//Dff .n/Wthere exist positive constants c,k,a n d n0such that
0/DC4f .n//DC4cg.n/ lgk.n/for all n/NAKn0g:
d.Deﬁnee/DELande‚in a similar manner. Prove the corresponding analog to Theo-
rem 3.1.
3-6 Iterated functions
We can apply the iteration operator/ETXused in the lg/ETXfunction to any monotonically
increasing function f .n/ over the reals. For a given constant c2R,w ed e ﬁ n et h e
iterated function f/ETX
cby
f/ETX
c.n/Dmin˚
i/NAK0Wf.i/.n//DC4c/TAB
;
which need not be well deﬁned in all cases. In other words, the quantity f/ETX
c.n/is
the number of iterated applications of the function frequired to reduce its argu-
ment down to cor less.
For each of the following functions f .n/ and constants c, give as tight a bound
as possible on f/ETX
c.n/.
f .n/ c
 f/ETX
c.n/
a. n/NUL10
b. lgn1
c. n=2 1
d. n=2 2
e.p
n2
f.p
n1
g. n1=32
h.n=lgn2
64 Chapter 3 Growth of Functions
Chapter notes
Knuth [209] traces the origin of the O-notation to a number-theory text by P. Bach-
mann in 1892. The o-notation was invented by E. Landau in 1909 for his discussion
of the distribution of prime numbers. The /DELand‚notations were advocated by
Knuth [213] to correct the popular, but technically sloppy, practice in the literature
of using O-notation for both upper and lower bounds. Many people continue to
use the O-notation where the ‚-notation is more technically precise. Further dis-
cussion of the history and development of asymptotic notations appears in worksby Knuth [209, 213] and Brassard and Bratley [54].
Not all authors deﬁne the asymptotic notations in the same way, although the
various deﬁnitions agree in most common situations. Some of the alternative def-initions encompass functions that are not asymptotically nonnegative, as long astheir absolute values are appropriately bounded.
Equation (3.20) is due to Robbins [297]. Other properties of elementary math-
ematical functions can be found in any good mathematical reference, such asAbramowitz and Stegun [1] or Zwillinger [362], or in a calculus book, such asApostol [18] or Thomas et al. [334]. Knuth [209] and Graham, Knuth, and Patash-nik [152] contain a wealth of material on discrete mathematics as used in computerscience.4 Divide-and-Conquer
In Section 2.3.1, we saw how merge sort serves as an example of the divide-and-
conquer paradigm. Recall that in divide-and-conquer, we solve a problem recur-sively, applying three steps at each level of the recursion:
Divide the problem into a number of subproblems that are smaller instances of the
same problem.
Conquer the subproblems by solving them recursively. If the subproblem sizes are
small enough, however, just solve the subproblems in a straightforward manner.
Combine the solutions to the subproblems into the solution for the original prob-
lem.
When the subproblems are large enough to solve recursively, we call that the recur-
sive case . Once the subproblems become small enough that we no longer recurse,
we say that the recursion “bottoms out” and that we have gotten down to the base
case. Sometimes, in addition to subproblems that are smaller instances of the same
problem, we have to solve subproblems that are not quite the same as the original
problem. We consider solving such subproblems as part of the combine step.
In this chapter, we shall see more algorithms based on divide-and-conquer. The
ﬁrst one solves the maximum-subarray problem: it takes as input an array of num-bers, and it determines the contiguous subarray whose values have the greatest sum.Then we shall see two divide-and-conquer algorithms for multiplying n/STXnmatri-
ces. One runs in ‚.n
3/time, which is no better than the straightforward method of
multiplying square matrices. But the other, Strassen’s algorithm, runs in O.n2:81/
time, which beats the straightforward method asymptotically.
Recurrences
Recurrences go hand in hand with the divide-and-conquer paradigm, because they
give us a natural way to characterize the running times of divide-and-conquer algo-rithms. A recurrence is an equation or inequality that describes a function in terms66 Chapter 4 Divide-and-Conquer
of its value on smaller inputs. For example, in Section 2.3.2 we described the
worst-case running time T .n/ of the M ERGE -SORT procedure by the recurrence
T .n/D(
‚.1/ ifnD1;
2T .n=2/C‚.n/ ifn>1;(4.1)
whose solution we claimed to be T .n/D‚.n lgn/.
Recurrences can take many forms. For example, a recursive algorithm might
divide subproblems into unequal sizes, such as a 2=3-to-1=3split. If the divide and
combine steps take linear time, such an algorithm would give rise to the recurrenceT .n/DT .2n=3/CT .n=3/C‚.n/ .
Subproblems are not necessarily constrained to being a constant fraction of
the original problem size. For example, a recursive version of linear search
(see Exercise 2.1-3) would create just one subproblem containing only one el-
ement fewer than the original problem. Each recursive call would take con-stant time plus the time for the recursive calls it makes, yielding the recurrenceT .n/DT. n/NUL1/C‚.1/ .
This chapter offers three methods for solving recurrences—that is, for obtaining
asymptotic “ ‚”o r“ O” bounds on the solution:
/SIIn the substitution method , we guess a bound and then use mathematical in-
duction to prove our guess correct.
/SITherecursion-tree method converts the recurrence into a tree whose nodes
represent the costs incurred at various levels of the recursion. We use techniques
for bounding summations to solve the recurrence.
/SIThemaster method provides bounds for recurrences of the form
T .n/DaT .n=b/Cf .n/ ; (4.2)
where a/NAK1,b>1 ,a n d f .n/ is a given function. Such recurrences arise
frequently. A recurrence of the form in equation (4.2) characterizes a divide-and-conquer algorithm that creates asubproblems, each of which is 1=b the
size of the original problem, and in which the divide and combine steps togethertakef .n/ time.
To use the master method, you will need to memorize three cases, but once
you do that, you will easily be able to determine asymptotic bounds for manysimple recurrences. We will use the master method to determine the runningtimes of the divide-and-conquer algorithms for the maximum-subarray problemand for matrix multiplication, as well as for other algorithms based on divide-and-conquer elsewhere in this book.Chapter 4 Divide-and-Conquer 67
Occasionally, we shall see recurrences that are not equalities but rather inequal-
ities, such as T .n//DC42T .n=2/C‚.n/ . Because such a recurrence states only
an upper bound on T .n/ , we will couch its solution using O-notation rather than
‚-notation. Similarly, if the inequality were reversed to T .n//NAK2T .n=2/C‚.n/ ,
then because the recurrence gives only a lower bound on T .n/ , we would use
/DEL-notation in its solution.
Technicalities in recurrences
In practice, we neglect certain technical details when we state and solve recur-
rences. For example, if we call M ERGE -SORT onnelements when nis odd, we
end up with subproblems of size bn=2canddn=2e. Neither size is actually n=2,
because n=2is not an integer when nis odd. Technically, the recurrence describing
the worst-case running time of M ERGE -SORT is really
T .n/D(
‚.1/ ifnD1;
T.dn=2e/CT.bn=2c/C‚.n/ ifn>1:(4.3)
Boundary conditions represent another class of details that we typically ignore.
Since the running time of an algorithm on a constant-sized input is a constant,the recurrences that arise from the running times of algorithms generally haveT .n/D‚.1/ for sufﬁciently small n. Consequently, for convenience, we shall
generally omit statements of the boundary conditions of recurrences and assumethatT .n/ is constant for small n. For example, we normally state recurrence (4.1)
as
T .n/D2T .n=2/C‚.n/ ; (4.4)
without explicitly giving values for small n. The reason is that although changing
the value of T. 1 / changes the exact solution to the recurrence, the solution typi-
cally doesn’t change by more than a constant factor, and so the order of growth is
unchanged.
When we state and solve recurrences, we often omit ﬂoors, ceilings, and bound-
ary conditions. We forge ahead without these details and later determine whetheror not they matter. They usually do not, but you should know when they do. Ex-perience helps, and so do some theorems stating that these details do not affect theasymptotic bounds of many recurrences characterizing divide-and-conquer algo-rithms (see Theorem 4.1). In this chapter, however, we shall address some of thesedetails and illustrate the ﬁne points of recurrence solution methods.68 Chapter 4 Divide-and-Conquer
4.1 The maximum-subarray problem
Suppose that you been offered the opportunity to invest in the Volatile Chemical
Corporation. Like the chemicals the company produces, the stock price of theVolatile Chemical Corporation is rather volatile. You are allowed to buy one unitof stock only one time and then sell it at a later date, buying and selling after the
close of trading for the day. To compensate for this restriction, you are allowed to
learn what the price of the stock will be in the future. Your goal is to maximizeyour proﬁt. Figure 4.1 shows the price of the stock over a 17-day period. Youmay buy the stock at any one time, starting after day 0, when the price is $100per share. Of course, you would want to “buy low, sell high”—buy at the lowestpossible price and later on sell at the highest possible price—to maximize yourproﬁt. Unfortunately, you might not be able to buy at the lowest price and then sellat the highest price within a given period. In Figure 4.1, the lowest price occursafter day 7, which occurs after the highest price, after day 1.
You might think that you can always maximize proﬁt by either buying at the
lowest price or selling at the highest price. For example, in Figure 4.1, we wouldmaximize proﬁt by buying at the lowest price, after day 7. If this strategy alwaysworked, then it would be easy to determine how to maximize proﬁt: ﬁnd the highestand lowest prices, and then work left from the highest price to ﬁnd the lowest prior
price, work right from the lowest price to ﬁnd the highest later price, and take
the pair with the greater difference. Figure 4.2 shows a simple counterexample,
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16120
110100
90807060
Day
 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
Price
 100 113 110 85 105 102 86 63 81 101 94 106 101 79 94 90 97
Change
 13/NUL3/NUL25 20/NUL3/NUL16/NUL23 18 20/NUL71 2/NUL5/NUL22 15/NUL47
Figure 4.1 Information about the price of stock in the V olatile Chemical Corporation after the close
of trading over a period of 17 days. The horizontal axis of the chart indicates the day, and the vertical
axis shows the price. The bottom row of the table gives the change in price from the previous day.4.1 The maximum-subarray problem 69
0123411
10
9876 Day
 01 23 4
Price
 10 11 7 10 6
Change
 1/NUL43/NUL4
Figure 4.2 An example showing that the maximum proﬁt does not always start at the lowest price
or end at the highest price. Again, the horizontal axis indicates the day, and the vertical axis shows
the price. Here, the maximum proﬁt of $3 per share would be earned by buying after day 2 and
selling after day 3. The price of $7 after day 2 is not the lowest price overall, and the price of $10
after day 3 is not the highest price overall.
demonstrating that the maximum proﬁt sometimes comes neither by buying at the
lowest price nor by selling at the highest price.
A brute-force solution
We can easily devise a brute-force solution to this problem: just try every possible
pair of buy and sell dates in which the buy date precedes the sell date. A period of n
days has/NULn
2/SOH
such pairs of dates. Since/NULn
2/SOH
is‚.n2/, and the best we can hope for
is to evaluate each pair of dates in constant time, this approach would take /DEL.n2/
time. Can we do better?
A transformation
In order to design an algorithm with an o.n2/running time, we will look at the
input in a slightly different way. We want to ﬁnd a sequence of days over which
the net change from the ﬁrst day to the last is maximum. Instead of looking at the
daily prices, let us instead consider the daily change in price, where the change on
dayiis the difference between the prices after day i/NUL1and after day i. The table
in Figure 4.1 shows these daily changes in the bottom row. If we treat this row as
an array A, shown in Figure 4.3, we now want to ﬁnd the nonempty, contiguous
subarray of Awhose values have the largest sum. We call this contiguous subarray
themaximum subarray . For example, in the array of Figure 4.3, the maximum
subarray of AŒ1 : : 16/c141 isAŒ8 : : 11/c141 , with the sum 43. Thus, you would want to buy
the stock just before day 8 (that is, after day 7) and sell it after day 11, earning a
proﬁt of $43 per share.
At ﬁrst glance, this transformation does not help. We still need to check/NULn/NUL1
2/SOH
D‚.n2/subarrays for a period of ndays. Exercise 4.1-2 asks you to show70 Chapter 4 Divide-and-Conquer
131
–32
–253
204
–35
–166
–237 8 9 10
maximum subarray11
1812
2013
–714
1215
716
–5 –22 15 –4 A
Figure 4.3 The change in stock prices as a maximum-subarray problem. Here, the subar-
rayAŒ8 : : 11/c141 ,w i t hs u m 43, has the greatest sum of any contiguous subarray of array A.
that although computing the cost of one subarray might take time proportional to
the length of the subarray, when computing all ‚.n2/subarray sums, we can orga-
nize the computation so that each subarray sum takes O.1/ time, given the values
of previously computed subarray sums, so that the brute-force solution takes ‚.n2/
time.
So let us seek a more efﬁcient solution to the maximum-subarray problem.
When doing so, we will usually speak of “a” maximum subarray rather than “the”maximum subarray, since there could be more than one subarray that achieves themaximum sum.
The maximum-subarray problem is interesting only when the array contains
some negative numbers. If all the array entries were nonnegative, then themaximum-subarray problem would present no challenge, since the entire arraywould give the greatest sum.
A solution using divide-and-conquer
Let’s think about how we might solve the maximum-subarray problem using
the divide-and-conquer technique. Suppose we want to ﬁnd a maximum subar-
ray of the subarray AŒlow::high/c141. Divide-and-conquer suggests that we divide
the subarray into two subarrays of as equal size as possible. That is, we ﬁnd
the midpoint, say mid, of the subarray, and consider the subarrays AŒlow::mid/c141
andAŒmidC1::high/c141. As Figure 4.4(a) shows, any contiguous subarray AŒi : : j /c141
ofAŒlow::high/c141must lie in exactly one of the following places:
/SIentirely in the subarray AŒlow::mid/c141,s ot h a t low/DC4i/DC4j/DC4mid,
/SIentirely in the subarray AŒmidC1::high/c141,s ot h a t mid<i/DC4j/DC4high,o r
/SIcrossing the midpoint, so that low/DC4i/DC4mid<j/DC4high.
Therefore, a maximum subarray of AŒlow::high/c141must lie in exactly one of these
places. In fact, a maximum subarray of AŒlow::high/c141must have the greatest
sum over all subarrays entirely in AŒlow::mid/c141, entirely in AŒmidC1::high/c141,
or crossing the midpoint. We can ﬁnd maximum subarrays of AŒlow::mid/c141and
AŒmidC1::high/c141recursively, because these two subproblems are smaller instances
of the problem of ﬁnding a maximum subarray. Thus, all that is left to do is ﬁnd a4.1 The maximum-subarray problem 71
(a) (b)low low mid mid high highcrosses the midpoint
midC1 midC1
entirely in AŒlow::mid/c141 entirely in AŒmidC1::high/c141i
j
AŒi : : mid/c141AŒmidC1::j/c141
Figure 4.4 (a) Possible locations of subarrays of AŒlow::high/c141: entirely in AŒlow::mid/c141,e n t i r e l y
inAŒmidC1::high/c141, or crossing the midpoint mid.(b)Any subarray of AŒlow::high/c141crossing
the midpoint comprises two subarrays AŒi : : mid/c141andAŒmidC1::j/c141 ,w h e r e low/DC4i/DC4midand
mid<j/DC4high.
maximum subarray that crosses the midpoint, and take a subarray with the largest
sum of the three.
We can easily ﬁnd a maximum subarray crossing the midpoint in time linear
in the size of the subarray AŒlow::high/c141. This problem is nota smaller instance
of our original problem, because it has the added restriction that the subarray it
chooses must cross the midpoint. As Figure 4.4(b) shows, any subarray crossingthe midpoint is itself made of two subarrays AŒi : : mid/c141andAŒmidC1::j/c141 ,w h e r e
low/DC4i/DC4midandmid<j/DC4high. Therefore, we just need to ﬁnd maximum
subarrays of the form AŒi : : mid/c141andAŒmidC1::j/c141 and then combine them. The
procedure F
IND-MAX-CROSSING -SUBARRAY takes as input the array Aand the
indices low,mid,a n d high, and it returns a tuple containing the indices demarcating
a maximum subarray that crosses the midpoint, along with the sum of the values ina maximum subarray.
F
IND-MAX-CROSSING -SUBARRAY .A;low;mid;high/
1left-sumD/NUL1
2sumD0
3foriDmiddownto low
4 sumDsumCAŒi/c141
5 ifsum>left-sum
6 left-sumDsum
7 max-leftDi
8right -sumD/NUL1
9sumD0
10forjDmidC1tohigh
11 sumDsumCAŒj /c141
12 ifsum>right -sum
13 right -sumDsum
14 max-rightDj
15return .max-left;max-right;left-sumCright -sum/72 Chapter 4 Divide-and-Conquer
This procedure works as follows. Lines 1–7 ﬁnd a maximum subarray of the
left half, AŒlow::mid/c141. Since this subarray must contain AŒmid/c141,t h eforloop of
lines 3–7 starts the index iatmidand works down to low, so that every subarray
it considers is of the form AŒi : : mid/c141. Lines 1–2 initialize the variables left-sum,
which holds the greatest sum found so far, and sum, holding the sum of the entries
inAŒi : : mid/c141. Whenever we ﬁnd, in line 5, a subarray AŒi : : mid/c141with a sum of
values greater than left-sum, we update left-sumto this subarray’s sum in line 6, and
in line 7 we update the variable max-leftto record this index i. Lines 8–14 work
analogously for the right half, AŒmidC1::high/c141. Here, the forloop of lines 10–14
starts the index jatmidC1and works up to high, so that every subarray it considers
is of the form AŒmidC1::j/c141 . Finally, line 15 returns the indices max-leftand
max-right that demarcate a maximum subarray crossing the midpoint, along with
the sum left-sumCright -sumof the values in the subarray AŒmax-left::max-right/c141.
If the subarray AŒlow::high/c141contains nentries (so that nDhigh/NULlowC1),
we claim that the call F IND-MAX-CROSSING -SUBARRAY .A;low;mid;high/
takes ‚.n/ time. Since each iteration of each of the two forloops takes ‚.1/
time, we just need to count up how many iterations there are altogether. The for
loop of lines 3–7 makes mid/NULlowC1iterations, and the forloop of lines 10–14
makes high/NULmiditerations, and so the total number of iterations is
.mid/NULlowC1/C.high/NULmid/Dhigh/NULlowC1
Dn:
With a linear-time F IND-MAX-CROSSING -SUBARRAY procedure in hand, we
can write pseudocode for a divide-and-conquer algorithm to solve the maximum-subarray problem:
F
IND-MAXIMUM -SUBARRAY .A;low;high/
1ifhigh ==low
2 return .low;high;A Œlow/c141/ //base case: only one element
3elsemidDb.lowChigh/=2c
4 .left-low;left-high;left-sum/D
FIND-MAXIMUM -SUBARRAY .A;low;mid/
5 .right -low;right -high;right -sum/D
FIND-MAXIMUM -SUBARRAY .A;midC1;high/
6 .cross -low;cross -high;cross -sum/D
FIND-MAX-CROSSING -SUBARRAY .A;low;mid;high/
7 ifleft-sum/NAKright -sum andleft-sum/NAKcross -sum
8 return .left-low;left-high;left-sum/
9 elseif right -sum/NAKleft-sumandright -sum/NAKcross -sum
10 return .right -low;right -high;right -sum/
11 else return .cross -low;cross -high;cross -sum/4.1 The maximum-subarray problem 73
The initial call F IND-MAXIMUM -SUBARRAY . A ;1 ;A: length /will ﬁnd a maxi-
mum subarray of AŒ1 : : n/c141 .
Similar to F IND-MAX-CROSSING -SUBARRAY , the recursive procedure F IND-
MAXIMUM -SUBARRAY returns a tuple containing the indices that demarcate a
maximum subarray, along with the sum of the values in a maximum subarray.Line 1 tests for the base case, where the subarray has just one element. A subar-ray with just one element has only one subarray—itself—and so line 2 returns atuple with the starting and ending indices of just the one element, along with its
value. Lines 3–11 handle the recursive case. Line 3 does the divide part, comput-
ing the index midof the midpoint. Let’s refer to the subarray AŒlow::mid/c141as the
left subarray and to AŒmidC1::high/c141as the right subarray . Because we know
that the subarray AŒlow::high/c141contains at least two elements, each of the left and
right subarrays must have at least one element. Lines 4 and 5 conquer by recur-sively ﬁnding maximum subarrays within the left and right subarrays, respectively.Lines 6–11 form the combine part. Line 6 ﬁnds a maximum subarray that crossesthe midpoint. (Recall that because line 6 solves a subproblem that is not a smallerinstance of the original problem, we consider it to be in the combine part.) Line 7tests whether the left subarray contains a subarray with the maximum sum, andline 8 returns that maximum subarray. Otherwise, line 9 tests whether the rightsubarray contains a subarray with the maximum sum, and line 10 returns that max-imum subarray. If neither the left nor right subarrays contain a subarray achievingthe maximum sum, then a maximum subarray must cross the midpoint, and line 11
returns it.
Analyzing the divide-and-conquer algorithm
Next we set up a recurrence that describes the running time of the recursive F
IND-
MAXIMUM -SUBARRAY procedure. As we did when we analyzed merge sort in
Section 2.3.2, we make the simplifying assumption that the original problem sizei sap o w e ro f 2, so that all subproblem sizes are integers. We denote by T .n/ the
running time of F
IND-MAXIMUM -SUBARRAY on a subarray of nelements. For
starters, line 1 takes constant time. The base case, when nD1, is easy: line 2
takes constant time, and so
T. 1 /D‚.1/ : (4.5)
The recursive case occurs when n>1 . Lines 1 and 3 take constant time. Each
of the subproblems solved in lines 4 and 5 is on a subarray of n=2 elements (our
assumption that the original problem size is a power of 2ensures that n=2 is an
integer), and so we spend T .n=2/ time solving each of them. Because we have
to solve two subproblems—for the left subarray and for the right subarray—the
contribution to the running time from lines 4 and 5 comes to 2T .n=2/ .A sw eh a v e74 Chapter 4 Divide-and-Conquer
already seen, the call to F IND-MAX-CROSSING -SUBARRAY in line 6 takes ‚.n/
time. Lines 7–11 take only ‚.1/ time. For the recursive case, therefore, we have
T .n/D‚.1/C2T .n=2/C‚.n/C‚.1/
D2T .n=2/C‚.n/ : (4.6)
Combining equations (4.5) and (4.6) gives us a recurrence for the running
timeT .n/ of F IND-MAXIMUM -SUBARRAY :
T .n/D(
‚.1/ ifnD1;
2T .n=2/C‚.n/ ifn>1:(4.7)
This recurrence is the same as recurrence (4.1) for merge sort. As we shall
see from the master method in Section 4.5, this recurrence has the solutionT .n/D‚.n lgn/. You might also revisit the recursion tree in Figure 2.5 to un-
derstand why the solution should be T .n/D‚.n lgn/.
Thus, we see that the divide-and-conquer method yields an algorithm that is
asymptotically faster than the brute-force method. With merge sort and now themaximum-subarray problem, we begin to get an idea of how powerful the divide-and-conquer method can be. Sometimes it will yield the asymptotically fastestalgorithm for a problem, and other times we can do even better. As Exercise 4.1-5shows, there is in fact a linear-time algorithm for the maximum-subarray problem,and it does not use divide-and-conquer.
Exercises
4.1-1
What does F
IND-MAXIMUM -SUBARRAY return when all elements of Aare nega-
tive?
4.1-2
Write pseudocode for the brute-force method of solving the maximum-subarrayproblem. Your procedure should run in ‚.n
2/time.
4.1-3
Implement both the brute-force and recursive algorithms for the maximum-
subarray problem on your own computer. What problem size n0gives the crossover
point at which the recursive algorithm beats the brute-force algorithm? Then,change the base case of the recursive algorithm to use the brute-force algorithmwhenever the problem size is less than n
0. Does that change the crossover point?
4.1-4
Suppose we change the deﬁnition of the maximum-subarray problem to allow theresult to be an empty subarray, where the sum of the values of an empty subar-4.2 Strassen’s algorithm for matrix multiplication 75
ray is 0. How would you change any of the algorithms that do not allow empty
subarrays to permit an empty subarray to be the result?
4.1-5
Use the following ideas to develop a nonrecursive, linear-time algorithm for themaximum-subarray problem. Start at the left end of the array, and progress towardthe right, keeping track of the maximum subarray seen so far. Knowing a maximumsubarray of AŒ1 : : j /c141 , extend the answer to ﬁnd a maximum subarray ending at in-
dexjC1by using the following observation: a maximum subarray of AŒ1 : : jC1/c141
is either a maximum subarray of AŒ1 : : j /c141 or a subarray AŒi : : jC1/c141,f o rs o m e
1/DC4i/DC4jC1. Determine a maximum subarray of the form AŒi : : jC1/c141in
constant time based on knowing a maximum subarray ending at index j.
4.2 Strassen’s algorithm for matrix multiplication
If you have seen matrices before, then you probably know how to multiply them.
(Otherwise, you should read Section D.1 in Appendix D.) If AD.aij/and
BD.bij/are square n/STXnmatrices, then in the product CDA/SOHB,w ed e ﬁ n et h e
entry cij,f o ri;jD1 ;2;:::;n ,b y
cijDnX
kD1aik/SOHbkj: (4.8)
We must compute n2matrix entries, and each is the sum of nvalues. The following
procedure takes n/STXnmatrices AandBand multiplies them, returning their n/STXn
product C. We assume that each matrix has an attribute rows , giving the number
of rows in the matrix.
SQUARE -MATRIX -MULTIPLY .A; B/
1nDA:rows
2l e t Cb ean e w n/STXnmatrix
3foriD1ton
4 forjD1ton
5 cijD0
6 forkD1ton
7 cijDcijCaik/SOHbkj
8return C
The S QUARE -MATRIX -MULTIPLY procedure works as follows. The forloop
of lines 3–7 computes the entries of each row i, and within a given row i,t h e76 Chapter 4 Divide-and-Conquer
forloop of lines 4–7 computes each of the entries cij, for each column j.L i n e5
initializes cijto0as we start computing the sum given in equation (4.8), and each
iteration of the forloop of lines 6–7 adds in one more term of equation (4.8).
Because each of the triply-nested forloops runs exactly niterations, and each
execution of line 7 takes constant time, the S QUARE -MATRIX -MULTIPLY proce-
dure takes ‚.n3/time.
You might at ﬁrst think that any matrix multiplication algorithm must take /DEL.n3/
time, since the natural deﬁnition of matrix multiplication requires that many mul-
tiplications. You would be incorrect, however: we have a way to multiply matrices
ino.n3/time. In this section, we shall see Strassen’s remarkable recursive algo-
rithm for multiplying n/STXnmatrices. It runs in ‚.nlg7/time, which we shall show
in Section 4.5. Since lg 7lies between 2:80 and2:81, Strassen’s algorithm runs in
O.n2:81/time, which is asymptotically better than the simple S QUARE -MATRIX -
MULTIPLY procedure.
A simple divide-and-conquer algorithm
To keep things simple, when we use a divide-and-conquer algorithm to compute
the matrix product CDA/SOHB, we assume that nis an exact power of 2in each of
then/STXnmatrices. We make this assumption because in each divide step, we will
divide n/STXnmatrices into four n=2/STXn=2matrices, and by assuming that nis an
exact power of 2, we are guaranteed that as long as n/NAK2, the dimension n=2is an
integer.
Suppose that we partition each of A,B,a n d Cinto four n=2/STXn=2matrices
AD/DC2A11A12
A21A22/DC3
;BD/DC2B11B12
B21B22/DC3
;CD/DC2C11C12
C21C22/DC3
; (4.9)
so that we rewrite the equation CDA/SOHBas
/DC2C11C12
C21C22/DC3
D/DC2A11A12
A21A22/DC3
/SOH/DC2B11B12
B21B22/DC3
: (4.10)
Equation (4.10) corresponds to the four equations
C11DA11/SOHB11CA12/SOHB21; (4.11)
C12DA11/SOHB12CA12/SOHB22; (4.12)
C21DA21/SOHB11CA22/SOHB21; (4.13)
C22DA21/SOHB12CA22/SOHB22: (4.14)
Each of these four equations speciﬁes two multiplications of n=2/STXn=2 matrices
and the addition of their n=2/STXn=2products. We can use these equations to create
a straightforward, recursive, divide-and-conquer algorithm:4.2 Strassen’s algorithm for matrix multiplication 77
SQUARE -MATRIX -MULTIPLY -RECURSIVE .A; B/
1nDA:rows
2l e t Cbe a new n/STXnmatrix
3ifn==1
4 c11Da11/SOHb11
5elsepartition A,B,a n d Cas in equations (4.9)
6 C11DSQUARE -MATRIX -MULTIPLY -RECURSIVE .A11;B11/
CSQUARE -MATRIX -MULTIPLY -RECURSIVE .A12;B21/
7 C12DSQUARE -MATRIX -MULTIPLY -RECURSIVE .A11;B12/
CSQUARE -MATRIX -MULTIPLY -RECURSIVE .A12;B22/
8 C21DSQUARE -MATRIX -MULTIPLY -RECURSIVE .A21;B11/
CSQUARE -MATRIX -MULTIPLY -RECURSIVE .A22;B21/
9 C22DSQUARE -MATRIX -MULTIPLY -RECURSIVE .A21;B12/
CSQUARE -MATRIX -MULTIPLY -RECURSIVE .A22;B22/
10return C
This pseudocode glosses over one subtle but important implementation detail.
How do we partition the matrices in line 5? If we were to create 12newn=2/STXn=2
matrices, we would spend ‚.n2/time copying entries. In fact, we can partition
the matrices without copying entries. The trick is to use index calculations. Weidentify a submatrix by a range of row indices and a range of column indices ofthe original matrix. We end up representing a submatrix a little differently fromhow we represent the original matrix, which is the subtlety we are glossing over.The advantage is that, since we can specify submatrices by index calculations,executing line 5 takes only ‚.1/ time (although we shall see that it makes no
difference asymptotically to the overall running time whether we copy or partition
in place).
Now, we derive a recurrence to characterize the running time of S
QUARE -
MATRIX -MULTIPLY -RECURSIVE .L e t T .n/ be the time to multiply two n/STXn
matrices using this procedure. In the base case, when nD1, we perform just the
one scalar multiplication in line 4, and so
T. 1 /D‚.1/ : (4.15)
The recursive case occurs when n>1 . As discussed, partitioning the matrices in
line 5 takes ‚.1/ time, using index calculations. In lines 6–9, we recursively call
SQUARE -MATRIX -MULTIPLY -RECURSIVE a total of eight times. Because each
recursive call multiplies two n=2/STXn=2matrices, thereby contributing T .n=2/ to
the overall running time, the time taken by all eight recursive calls is 8T .n=2/ .W e
also must account for the four matrix additions in lines 6–9. Each of these matricescontains n
2=4entries, and so each of the four matrix additions takes ‚.n2/time.
Since the number of matrix additions is a constant, the total time spent adding ma-78 Chapter 4 Divide-and-Conquer
trices in lines 6–9 is ‚.n2/. (Again, we use index calculations to place the results
of the matrix additions into the correct positions of matrix C, with an overhead
of‚.1/ time per entry.) The total time for the recursive case, therefore, is the sum
of the partitioning time, the time for all the recursive calls, and the time to add thematrices resulting from the recursive calls:
T .n/D‚.1/C8T .n=2/C‚.n
2/
D8T .n=2/C‚.n2/: (4.16)
Notice that if we implemented partitioning by copying matrices, which would cost
‚.n2/time, the recurrence would not change, and hence the overall running time
would increase by only a constant factor.
Combining equations (4.15) and (4.16) gives us the recurrence for the running
time of S QUARE -MATRIX -MULTIPLY -RECURSIVE :
T .n/D(
‚.1/ ifnD1;
8T .n=2/C‚.n2/ifn>1:(4.17)
As we shall see from the master method in Section 4.5, recurrence (4.17) has the
solution T .n/D‚.n3/. Thus, this simple divide-and-conquer approach is no
faster than the straightforward S QUARE -MATRIX -MULTIPLY procedure.
Before we continue on to examining Strassen’s algorithm, let us review where
the components of equation (4.16) came from. Partitioning each n/STXnmatrix by
index calculation takes ‚.1/ time, but we have two matrices to partition. Although
you could say that partitioning the two matrices takes ‚.2/ time, the constant of 2
is subsumed by the ‚-notation. Adding two matrices, each with, say, kentries,
takes ‚.k/ time. Since the matrices we add each have n2=4entries, you could
say that adding each pair takes ‚.n2=4/time. Again, however, the ‚-notation
subsumes the constant factor of 1=4, and we say that adding two n2=4/STXn2=4
matrices takes ‚.n2/time. We have four such matrix additions, and once again,
instead of saying that they take ‚.4n2/time, we say that they take ‚.n2/time.
(Of course, you might observe that we could say that the four matrix additionstake‚.4n
2=4/time, and that 4n2=4Dn2, but the point here is that ‚-notation
subsumes constant factors, whatever they are.) Thus, we end up with two termsof‚.n
2/, which we can combine into one.
When we account for the eight recursive calls, however, we cannot just sub-
sume the constant factor of 8. In other words, we must say that together they take
8T .n=2/ time, rather than just T .n=2/ time. You can get a feel for why by looking
back at the recursion tree in Figure 2.5, for recurrence (2.1) (which is identical torecurrence (4.7)), with the recursive case T .n/D2T .n=2/C‚.n/ . The factor of 2
determined how many children each tree node had, which in turn determined how
many terms contributed to the sum at each level of the tree. If we were to ignore4.2 Strassen’s algorithm for matrix multiplication 79
the factor of 8in equation (4.16) or the factor of 2in recurrence (4.1), the recursion
tree would just be linear, rather than “bushy,” and each level would contribute onlyone term to the sum.
Bear in mind, therefore, that although asymptotic notation subsumes constant
multiplicative factors, recursive notation such as T .n=2/ does not.
Strassen’s method
The key to Strassen’s method is to make the recursion tree slightly less bushy. That
is, instead of performing eight recursive multiplications of n=2/STXn=2 matrices,
it performs only seven. The cost of eliminating one matrix multiplication will beseveral new additions of n=2/STXn=2 matrices, but still only a constant number of
additions. As before, the constant number of matrix additions will be subsumed
by‚-notation when we set up the recurrence equation to characterize the running
time.
Strassen’s method is not at all obvious. (This might be the biggest understate-
ment in this book.) It has four steps:
1. Divide the input matrices AandBand output matrix Cinton=2/STXn=2subma-
trices, as in equation (4.9). This step takes ‚.1/ time by index calculation, just
as in S
QUARE -MATRIX -MULTIPLY -RECURSIVE .
2. Create 10matrices S1;S2;:::;S 10, each of which is n=2/STXn=2and is the sum
or difference of two matrices created in step 1. We can create all 10matrices in
‚.n2/time.
3. Using the submatrices created in step 1 and the 10matrices created in step 2,
recursively compute seven matrix products P1;P2;:::;P 7. Each matrix Piis
n=2/STXn=2.
4. Compute the desired submatrices C11;C12;C21;C22of the result matrix Cby
adding and subtracting various combinations of the Pimatrices. We can com-
pute all four submatrices in ‚.n2/time.
We shall see the details of steps 2–4 in a moment, but we already have enough
information to set up a recurrence for the running time of Strassen’s method. Let usassume that once the matrix size ngets down to 1, we perform a simple scalar mul-
tiplication, just as in line 4 of S
QUARE -MATRIX -MULTIPLY -RECURSIVE .W h e n
n>1 , steps 1, 2, and 4 take a total of ‚.n2/time, and step 3 requires us to per-
form seven multiplications of n=2/STXn=2matrices. Hence, we obtain the following
recurrence for the running time T .n/ of Strassen’s algorithm:
T .n/D(
‚.1/ ifnD1;
7T .n=2/C‚.n2/ifn>1:(4.18)80 Chapter 4 Divide-and-Conquer
We have traded off one matrix multiplication for a constant number of matrix ad-
ditions. Once we understand recurrences and their solutions, we shall see that thistradeoff actually leads to a lower asymptotic running time. By the master methodin Section 4.5, recurrence (4.18) has the solution T .n/D‚.n
lg7/.
We now proceed to describe the details. In step 2, we create the following 10
matrices:
S1DB12/NULB22;
S2DA11CA12;
S3DA21CA22;
S4DB21/NULB11;
S5DA11CA22;
S6DB11CB22;
S7DA12/NULA22;
S8DB21CB22;
S9DA11/NULA21;
S10DB11CB12:
Since we must add or subtract n=2/STXn=2matrices 10times, this step does indeed
take‚.n2/time.
In step 3, we recursively multiply n=2/STXn=2matrices seven times to compute the
following n=2/STXn=2matrices, each of which is the sum or difference of products
ofAandBsubmatrices:
P1DA11/SOHS1DA11/SOHB12/NULA11/SOHB22;
P2DS2/SOHB22DA11/SOHB22CA12/SOHB22;
P3DS3/SOHB11DA21/SOHB11CA22/SOHB11;
P4DA22/SOHS4DA22/SOHB21/NULA22/SOHB11;
P5DS5/SOHS6DA11/SOHB11CA11/SOHB22CA22/SOHB11CA22/SOHB22;
P6DS7/SOHS8DA12/SOHB21CA12/SOHB22/NULA22/SOHB21/NULA22/SOHB22;
P7DS9/SOHS10DA11/SOHB11CA11/SOHB12/NULA21/SOHB11/NULA21/SOHB12:
Note that the only multiplications we need to perform are those in the middle col-
umn of the above equations. The right-hand column just shows what these productsequal in terms of the original submatrices created in step 1.
Step 4 adds and subtracts the P
imatrices created in step 3 to construct the four
n=2/STXn=2submatrices of the product C. We start with
C11DP5CP4/NULP2CP6:4.2 Strassen’s algorithm for matrix multiplication 81
Expanding out the right-hand side, with the expansion of each Pion its own line
and vertically aligning terms that cancel out, we see that C11equals
A11/SOHB11CA11/SOHB22CA22/SOHB11CA22/SOHB22
/NULA22/SOHB11CA22/SOHB21
/NULA11/SOHB22 /NULA12/SOHB22
/NULA22/SOHB22/NULA22/SOHB21CA12/SOHB22CA12/SOHB21
A11/SOHB11 CA12/SOHB21;
which corresponds to equation (4.11).
Similarly, we set
C12DP1CP2;
and so C12equals
A11/SOHB12/NULA11/SOHB22
CA11/SOHB22CA12/SOHB22
A11/SOHB12CA12/SOHB22;
corresponding to equation (4.12).
Setting
C21DP3CP4
makes C21equal
A21/SOHB11CA22/SOHB11
/NULA22/SOHB11CA22/SOHB21
A21/SOHB11CA22/SOHB21;
corresponding to equation (4.13).
Finally, we set
C22DP5CP1/NULP3/NULP7;
so that C22equals
A11/SOHB11CA11/SOHB22CA22/SOHB11CA22/SOHB22
/NULA11/SOHB22 CA11/SOHB12
/NULA22/SOHB11 /NULA21/SOHB11
/NULA11/SOHB11 /NULA11/SOHB12CA21/SOHB11CA21/SOHB12
A22/SOHB22 CA21/SOHB12;82 Chapter 4 Divide-and-Conquer
which corresponds to equation (4.14). Altogether, we add or subtract n=2/STXn=2
matrices eight times in step 4, and so this step indeed takes ‚.n2/time.
Thus, we see that Strassen’s algorithm, comprising steps 1–4, produces the cor-
rect matrix product and that recurrence (4.18) characterizes its running time. Sincewe shall see in Section 4.5 that this recurrence has the solution T .n/D‚.n
lg7/,
Strassen’s method is asymptotically faster than the straightforward S QUARE -
MATRIX -MULTIPLY procedure. The notes at the end of this chapter discuss some
of the practical aspects of Strassen’s algorithm.
Exercises
Note: Although Exercises 4.2-3, 4.2-4, and 4.2-5 are about variants on Strassen’s
algorithm, you should read Section 4.5 before trying to solve them.
4.2-1
Use Strassen’s algorithm to compute the matrix product
/DC213
75/DC3/DC268
42/DC3
:
Show your work.
4.2-2
Write pseudocode for Strassen’s algorithm.
4.2-3
How would you modify Strassen’s algorithm to multiply n/STXnmatrices in which n
is not an exact power of 2? Show that the resulting algorithm runs in time ‚.nlg7/.
4.2-4
What is the largest ksuch that if you can multiply 3/STX3matrices using kmulti-
plications (not assuming commutativity of multiplication), then you can multiplyn/STXnmatrices in time o.n
lg7/? What would the running time of this algorithm be?
4.2-5
V. Pan has discovered a way of multiplying 68/STX68matrices using 132,464 mul-
tiplications, a way of multiplying 70/STX70matrices using 143,640 multiplications,
and a way of multiplying 72/STX72matrices using 155,424 multiplications. Which
method yields the best asymptotic running time when used in a divide-and-conquer
matrix-multiplication algorithm? How does it compare to Strassen’s algorithm?4.3 The substitution method for solving recurrences 83
4.2-6
How quickly can you multiply a kn/STXnmatrix by an n/STXknmatrix, using Strassen’s
algorithm as a subroutine? Answer the same question with the order of the inputmatrices reversed.
4.2-7
Show how to multiply the complex numbers aCbiandcCdiusing only three
multiplications of real numbers. The algorithm should take a,b,c,a n d das input
and produce the real component ac/NULbdand the imaginary component adCbc
separately.
4.3 The substitution method for solving recurrences
Now that we have seen how recurrences characterize the running times of divide-
and-conquer algorithms, we will learn how to solve recurrences. We start in thissection with the “substitution” method.
Thesubstitution method for solving recurrences comprises two steps:
1. Guess the form of the solution.
2. Use mathematical induction to ﬁnd the constants and show that the solution
works.
We substitute the guessed solution for the function when applying the inductive
hypothesis to smaller values; hence the name “substitution method.” This methodis powerful, but we must be able to guess the form of the answer in order to apply it.
We can use the substitution method to establish either upper or lower bounds on
a recurrence. As an example, let us determine an upper bound on the recurrence
T .n/D2T .bn=2c/Cn; (4.19)
which is similar to recurrences (4.3) and (4.4). We guess that the solution is
T .n/DO.n lgn/. The substitution method requires us to prove that T .n//DC4
cnlgnfor an appropriate choice of the constant c>0 . We start by assuming
that this bound holds for all positive m<n , in particular for mDbn=2c, yielding
T.bn=2c//DC4cbn=2clg.bn=2c/. Substituting into the recurrence yields
T .n//DC42.cbn=2clg.bn=2c//Cn
/DC4cnlg.n=2/Cn
Dcnlgn/NULcnlg2Cn
Dcnlgn/NULcnCn
/DC4cnlgn;84 Chapter 4 Divide-and-Conquer
where the last step holds as long as c/NAK1.
Mathematical induction now requires us to show that our solution holds for the
boundary conditions. Typically, we do so by showing that the boundary condi-tions are suitable as base cases for the inductive proof. For the recurrence (4.19),we must show that we can choose the constant clarge enough so that the bound
T .n//DC4cnlgnworks for the boundary conditions as well. This requirement
can sometimes lead to problems. Let us assume, for the sake of argument, thatT. 1 /D1is the sole boundary condition of the recurrence. Then for nD1,t h e
bound T .n//DC4cnlgnyields T. 1 //DC4c1lg1D0, which is at odds with T. 1 /D1.
Consequently, the base case of our inductive proof fails to hold.
We can overcome this obstacle in proving an inductive hypothesis for a spe-
ciﬁc boundary condition with only a little more effort. In the recurrence (4.19),for example, we take advantage of asymptotic notation requiring us only to proveT .n//DC4cnlgnforn/NAKn
0,w h e r e n0is a constant that we get to choose .W e
keep the troublesome boundary condition T. 1 /D1, but remove it from consid-
eration in the inductive proof. We do so by ﬁrst observing that for n>3 ,t h e
recurrence does not depend directly on T. 1 / . Thus, we can replace T. 1 / byT. 2 /
andT. 3 / as the base cases in the inductive proof, letting n0D2. Note that we
make a distinction between the base case of the recurrence ( nD1) and the base
cases of the inductive proof ( nD2andnD3). With T. 1 /D1, we derive from
the recurrence that T. 2 /D4andT. 3 /D5. Now we can complete the inductive
proof that T .n//DC4cnlgnfor some constant c/NAK1by choosing clarge enough
so that T. 2 //DC4c2lg2andT. 3 //DC4c3lg3. As it turns out, any choice of c/NAK2
sufﬁces for the base cases of nD2andnD3to hold. For most of the recurrences
we shall examine, it is straightforward to extend boundary conditions to make theinductive assumption work for small n, and we shall not always explicitly work out
the details.
Making a good guess
Unfortunately, there is no general way to guess the correct solutions to recurrences.
Guessing a solution takes experience and, occasionally, creativity. Fortunately,though, you can use some heuristics to help you become a good guesser. Youcan also use recursion trees, which we shall see in Section 4.4, to generate goodguesses.
If a recurrence is similar to one you have seen before, then guessing a similar
solution is reasonable. As an example, consider the recurrence
T .n/D2T .bn=2cC17/Cn;
which looks difﬁcult because of the added “ 17” in the argument to Ton the right-
hand side. Intuitively, however, this additional term cannot substantially affect the4.3 The substitution method for solving recurrences 85
solution to the recurrence. When nis large, the difference between bn=2cand
bn=2cC17is not that large: both cut nnearly evenly in half. Consequently, we
make the guess that T .n/DO.n lgn/, which you can verify as correct by using
the substitution method (see Exercise 4.3-6).
Another way to make a good guess is to prove loose upper and lower bounds on
the recurrence and then reduce the range of uncertainty. For example, we mightstart with a lower bound of T .n/D/DEL.n/ for the recurrence (4.19), since we
have the term nin the recurrence, and we can prove an initial upper bound of
T .n/DO.n
2/. Then, we can gradually lower the upper bound and raise the
lower bound until we converge on the correct, asymptotically tight solution ofT .n/D‚.n lgn/.
Subtleties
Sometimes you might correctly guess an asymptotic bound on the solution of a
recurrence, but somehow the math fails to work out in the induction. The problemfrequently turns out to be that the inductive assumption is not strong enough toprove the detailed bound. If you revise the guess by subtracting a lower-order termwhen you hit such a snag, the math often goes through.
Consider the recurrence
T .n/DT.bn=2c/CT.dn=2e/C1:
We guess that the solution is T .n/DO.n/ , and we try to show that T .n//DC4cnfor
an appropriate choice of the constant c. Substituting our guess in the recurrence,
we obtain
T .n//DC4cbn=2cCcdn=2eC1
DcnC1;
which does not imply T .n//DC4cnfor any choice of c. We might be tempted to try
a larger guess, say T .n/DO.n
2/. Although we can make this larger guess work,
our original guess of T .n/DO.n/ is correct. In order to show that it is correct,
however, we must make a stronger inductive hypothesis.
Intuitively, our guess is nearly right: we are off only by the constant 1,a
lower-order term. Nevertheless, mathematical induction does not work unless weprove the exact form of the inductive hypothesis. We overcome our difﬁcultybysubtracting a lower-order term from our previous guess. Our new guess is
T .n//DC4cn/NULd,w h e r e d/NAK0is a constant. We now have
T .n//DC4.cbn=2c/NULd/C.cdn=2e/NULd/C1
Dcn/NUL2dC1
/DC4cn/NULd;86 Chapter 4 Divide-and-Conquer
as long as d/NAK1. As before, we must choose the constant clarge enough to handle
the boundary conditions.
You might ﬁnd the idea of subtracting a lower-order term counterintuitive. Af-
ter all, if the math does not work out, we should increase our guess, right?Not necessarily! When proving an upper bound by induction, it may actually bemore difﬁcult to prove that a weaker upper bound holds, because in order to provethe weaker bound, we must use the same weaker bound inductively in the proof.In our current example, when the recurrence has more than one recursive term, we
get to subtract out the lower-order term of the proposed bound once per recursive
term. In the above example, we subtracted out the constant dtwice, once for the
T.bn=2c/term and once for the T.dn=2e/term. We ended up with the inequality
T .n//DC4cn/NUL2dC1, and it was easy to ﬁnd values of dto make cn/NUL2dC1be
less than or equal to cn/NULd.
Avoiding pitfalls
It is easy to err in the use of asymptotic notation. For example, in the recur-
rence (4.19) we can falsely “prove” T .n/DO.n/ by guessing T .n//DC4cnand
then arguing
T .n//DC42.cbn=2c/Cn
/DC4cnCn
DO.n/ ;/c143 wrong!!
since cis a constant. The error is that we have not proved the exact form of the
inductive hypothesis, that is, that T .n//DC4cn. We therefore will explicitly prove
thatT .n//DC4cnwhen we want to show that T .n/DO.n/ .
Changing variables
Sometimes, a little algebraic manipulation can make an unknown recurrence simi-
lar to one you have seen before. As an example, consider the recurrence
T .n/D2T
/NUL/EOTp
n˘/SOH
Clgn;
which looks difﬁcult. We can simplify this recurrence, though, with a change of
variables. For convenience, we shall not worry about rounding off values, suchasp
n, to be integers. Renaming mDlgnyields
T. 2m/D2T .2m=2/Cm:
We can now rename S.m/DT. 2m/to produce the new recurrence
S.m/D2S.m=2/Cm;4.3 The substitution method for solving recurrences 87
which is very much like recurrence (4.19). Indeed, this new recurrence has the
same solution: S.m/DO.m lgm/. Changing back from S.m/ toT .n/ , we obtain
T .n/DT. 2m/DS.m/DO.m lgm/DO.lgnlg lgn/ :
Exercises
4.3-1
Show that the solution of T .n/DT. n/NUL1/CnisO.n2/.
4.3-2
Show that the solution of T .n/DT.dn=2e/C1isO.lgn/.
4.3-3
We saw that the solution of T .n/D2T .bn=2c/CnisO.n lgn/. Show that the so-
lution of this recurrence is also /DEL.n lgn/. Conclude that the solution is ‚.n lgn/.
4.3-4
Show that by making a different inductive hypothesis, we can overcome the difﬁ-culty with the boundary condition T. 1 /D1for recurrence (4.19) without adjusting
the boundary conditions for the inductive proof.
4.3-5
Show that ‚.n lgn/is the solution to the “exact” recurrence (4.3) for merge sort.
4.3-6
Show that the solution to T .n/D2T .bn=2cC17/CnisO.n lgn/.
4.3-7
Using the master method in Section 4.5, you can show that the solution to therecurrence T .n/D4T .n=3/CnisT .n/D‚.n
log34/. Show that a substitution
proof with the assumption T .n//DC4cnlog34fails. Then show how to subtract off a
lower-order term to make a substitution proof work.
4.3-8
Using the master method in Section 4.5, you can show that the solution to therecurrence T .n/D4T .n=2/Cn
2isT .n/D‚.n2/. Show that a substitution
proof with the assumption T .n//DC4cn2fails. Then show how to subtract off a
lower-order term to make a substitution proof work.88 Chapter 4 Divide-and-Conquer
4.3-9
Solve the recurrence T .n/D3T .p
n/Clognby making a change of variables.
Your solution should be asymptotically tight. Do not worry about whether valuesare integral.
4.4 The recursion-tree method for solving recurrences
Although you can use the substitution method to provide a succinct proof thata solution to a recurrence is correct, you might have trouble coming up with agood guess. Drawing out a recursion tree, as we did in our analysis of the mergesort recurrence in Section 2.3.2, serves as a straightforward way to devise a goodguess. In a recursion tree , each node represents the cost of a single subproblem
somewhere in the set of recursive function invocations. We sum the costs withineach level of the tree to obtain a set of per-level costs, and then we sum all theper-level costs to determine the total cost of all levels of the recursion.
A recursion tree is best used to generate a good guess, which you can then verify
by the substitution method. When using a recursion tree to generate a good guess,
you can often tolerate a small amount of “sloppiness,” since you will be verifyingyour guess later on. If you are very careful when drawing out a recursion tree andsumming the costs, however, you can use a recursion tree as a direct proof of asolution to a recurrence. In this section, we will use recursion trees to generategood guesses, and in Section 4.6, we will use recursion trees directly to prove thetheorem that forms the basis of the master method.
For example, let us see how a recursion tree would provide a good guess for
the recurrence T .n/D3T .bn=4c/C‚.n
2/. We start by focusing on ﬁnding an
upper bound for the solution. Because we know that ﬂoors and ceilings usually donot matter when solving recurrences (here’s an example of sloppiness that we cantolerate), we create a recursion tree for the recurrence T .n/D3T .n=4/Ccn
2,
having written out the implied constant coefﬁcient c>0 .
Figure 4.5 shows how we derive the recursion tree for T .n/D3T .n=4/Ccn2.
For convenience, we assume that nis an exact power of 4(another example of
tolerable sloppiness) so that all subproblem sizes are integers. Part (a) of the ﬁgureshows T .n/ , which we expand in part (b) into an equivalent tree representing the
recurrence. The cn
2term at the root represents the cost at the top level of recursion,
and the three subtrees of the root represent the costs incurred by the subproblemsof size n=4. Part (c) shows this process carried one step further by expanding each
node with cost T .n=4/ from part (b). The cost for each of the three children of the
root is c.n=4/
2. We continue expanding each node in the tree by breaking it into
its constituent parts as determined by the recurrence.4.4 The recursion-tree method for solving recurrences 89
…
…
(d)(c) (b) (a)T .n/ cn2cn2
cn2T/NULn
4/SOH
T/NULn
4/SOH
T/NULn
4/SOH
T/NULn
16/SOH
T/NULn
16/SOH
T/NULn
16/SOH
T/NULn
16/SOH
T/NULn
16/SOH
T/NULn
16/SOH
T/NULn
16/SOH
T/NULn
16/SOH
T/NULn
16/SOH
cn2c/NULn
4/SOH2c/NULn
4/SOH2c/NULn
4/SOH2
c/NULn
4/SOH2c/NULn
4/SOH2c/NULn
4/SOH2
c/NULn
16/SOH2c/NULn
16/SOH2c/NULn
16/SOH2c/NULn
16/SOH2c/NULn
16/SOH2c/NULn
16/SOH2c/NULn
16/SOH2c/NULn
16/SOH2c/NULn
16/SOH23
16cn2
/NUL3
16/SOH2cn2log4n
nlog43T. 1 / T. 1 / T. 1 / T. 1 / T. 1 / T. 1 / T. 1 / T. 1 / T. 1 / T. 1 / T. 1 / T. 1 / T. 1 / ‚.nlog43/
Total: O.n2/
Figure 4.5 Constructing a recursion tree for the recurrence T .n/D3T .n=4/Ccn2.P a r t (a)
shows T .n/ , which progressively expands in (b)–(d) to form the recursion tree. The fully expanded
tree in part (d) has height log4n(it has log4nC1levels).90 Chapter 4 Divide-and-Conquer
Because subproblem sizes decrease by a factor of 4each time we go down one
level, we eventually must reach a boundary condition. How far from the root dowe reach one? The subproblem size for a node at depth iisn=4
i. Thus, the
subproblem size hits nD1when n=4iD1or, equivalently, when iDlog4n.
Thus, the tree has log4nC1levels (at depths 0; 1; 2; : : : ; log4n).
Next we determine the cost at each level of the tree. Each level has three times
more nodes than the level above, and so the number of nodes at depth iis3i.
Because subproblem sizes reduce by a factor of 4for each level we go down
from the root, each node at depth i,f o r iD0; 1; 2; : : : ; log4n/NUL1,h a sac o s t
ofc.n=4i/2. Multiplying, we see that the total cost over all nodes at depth i,f o r
iD0; 1; 2; : : : ; log4n/NUL1,i s3ic.n=4i/2D.3=16/icn2. The bottom level, at
depth log4n,h a s 3log4nDnlog43nodes, each contributing cost T. 1 / , for a total
cost of nlog43T. 1 / ,w h i c hi s ‚.nlog43/, since we assume that T. 1 / is a constant.
Now we add up the costs over all levels to determine the cost for the entire tree:
T .n/Dcn2C3
16cn2C/DC23
16/DC32
cn2C/SOH/SOH/SOHC/DC23
16/DC3log4n/NUL1
cn2C‚.nlog43/
Dlog4n/NUL1X
iD0/DC23
16/DC3i
cn2C‚.nlog43/
D.3=16/log4n/NUL1
.3=16//NUL1cn2C‚.nlog43/ (by equation (A.5)) :
This last formula looks somewhat messy until we realize that we can again take
advantage of small amounts of sloppiness and use an inﬁnite decreasing geometricseries as an upper bound. Backing up one step and applying equation (A.6), wehave
T .n/D
log4n/NUL1X
iD0/DC23
16/DC3i
cn2C‚.nlog43/
<1X
iD0/DC23
16/DC3i
cn2C‚.nlog43/
D1
1/NUL.3=16/cn2C‚.nlog43/
D16
13cn2C‚.nlog43/
DO.n2/:
Thus, we have derived a guess of T .n/DO.n2/for our original recurrence
T .n/D3T .bn=4c/C‚.n2/. In this example, the coefﬁcients of cn2form a
decreasing geometric series and, by equation (A.6), the sum of these coefﬁcients4.4 The recursion-tree method for solving recurrences 91……cn
cncncn
c/NULn
3/SOH
c/NUL2n
3/SOH
c/NULn
9/SOH
c/NUL2n
9/SOH
c/NUL2n
9/SOH
c/NUL4n
9/SOHlog3=2n
Total: O.n lgn/
Figure 4.6 A recursion tree for the recurrence T. n /DT .n=3/CT .2n=3/Ccn.
is bounded from above by the constant 16=13 . Since the root’s contribution to the
total cost is cn2, the root contributes a constant fraction of the total cost. In other
words, the cost of the root dominates the total cost of the tree.
In fact, if O.n2/is indeed an upper bound for the recurrence (as we shall verify in
a moment), then it must be a tight bound. Why? The ﬁrst recursive call contributesac o s to f ‚.n
2/,a n ds o /DEL.n2/must be a lower bound for the recurrence.
Now we can use the substitution method to verify that our guess was cor-
rect, that is, T .n/DO.n2/is an upper bound for the recurrence T .n/D
3T .bn=4c/C‚.n2/. We want to show that T .n//DC4dn2for some constant d>0 .
Using the same constant c>0 as before, we have
T .n//DC43T .bn=4c/Ccn2
/DC43dbn=4c2Ccn2
/DC43d.n=4/2Ccn2
D3
16dn2Ccn2
/DC4dn2;
where the last step holds as long as d/NAK.16=13/c .
In another, more intricate, example, Figure 4.6 shows the recursion tree for
T .n/DT .n=3/CT .2n=3/CO.n/ :
(Again, we omit ﬂoor and ceiling functions for simplicity.) As before, we let c
represent the constant factor in the O.n/ term. When we add the values across the
levels of the recursion tree shown in the ﬁgure, we get a value of cnfor every level.92 Chapter 4 Divide-and-Conquer
The longest simple path from the root to a leaf is n!.2=3/n!.2=3/2n!
/SOH/SOH/SOH! 1.S i n c e .2=3/knD1when kDlog3=2n, the height of the tree is log3=2n.
Intuitively, we expect the solution to the recurrence to be at most the number
of levels times the cost of each level, or O.cn log3=2n/DO.n lgn/. Figure 4.6
shows only the top levels of the recursion tree, however, and not every level in thetree contributes a cost of cn. Consider the cost of the leaves. If this recursion tree
were a complete binary tree of height log
3=2n, there would be 2log3=2nDnlog3=22
leaves. Since the cost of each leaf is a constant, the total cost of all leaves would
then be ‚.nlog3=22/which, since log3=22is a constant strictly greater than 1,
is!.nlgn/. This recursion tree is not a complete binary tree, however, and so
it has fewer than nlog3=22leaves. Moreover, as we go down from the root, more
and more internal nodes are absent. Consequently, levels toward the bottom of therecursion tree contribute less than cnto the total cost. We could work out an accu-
rate accounting of all costs, but remember that we are just trying to come up with aguess to use in the substitution method. Let us tolerate the sloppiness and attemptto show that a guess of O.n lgn/for the upper bound is correct.
Indeed, we can use the substitution method to verify that O.n lgn/is an upper
bound for the solution to the recurrence. We show that T .n//DC4dnlgn,w h e r e dis
a suitable positive constant. We have
T .n//DC4T .n=3/CT .2n=3/Ccn
/DC4d.n=3/ lg.n=3/Cd.2n=3/ lg.2n=3/Ccn
D.d.n=3/ lgn/NULd.n=3/ lg3/
C.d.2n=3/ lgn/NULd.2n=3/ lg.3=2//Ccn
Ddnlgn/NULd..n=3/ lg3C.2n=3/ lg.3=2//Ccn
Ddnlgn/NULd..n=3/ lg3C.2n=3/ lg3/NUL.2n=3/ lg2/Ccn
Ddnlgn/NULdn.lg3/NUL2=3/
Ccn
/DC4dnlgn;
as long as d/NAKc=.lg3/NUL.2=3// . Thus, we did not need to perform a more accurate
accounting of costs in the recursion tree.
Exercises
4.4-1
Use a recursion tree to determine a good asymptotic upper bound on the recurrenceT .n/D3T .bn=2c/Cn. Use the substitution method to verify your answer.
4.4-2
Use a recursion tree to determine a good asymptotic upper bound on the recurrenceT
.n/DT.n=2/Cn2. Use the substitution method to verify your answer.4.5 The master method for solving recurrences 93
4.4-3
Use a recursion tree to determine a good asymptotic upper bound on the recurrenceT .n/D4T .n=2C2/Cn. Use the substitution method to verify your answer.
4.4-4
Use a recursion tree to determine a good asymptotic upper bound on the recurrenceT .n/D2T .n/NUL1/C1. Use the substitution method to verify your answer.
4.4-5
Use a recursion tree to determine a good asymptotic upper bound on the recurrence
T .n/DT. n/NUL1/CT .n=2/Cn. Use the substitution method to verify your answer.
4.4-6
Argue that the solution to the recurrence T .n/DT .n=3/CT .2n=3/Ccn,w h e r e c
is a constant, is /DEL.n lgn/by appealing to a recursion tree.
4.4-7
Draw the recursion tree for T .n/D4T .bn=2c/Ccn,w h e r e cis a constant, and
provide a tight asymptotic bound on its solution. Verify your bound by the substi-tution method.
4.4-8
Use a recursion tree to give an asymptotically tight solution to the recurrenceT .n/DT. n/NULa/CT. a/Ccn,w h e r e a/NAK1andc>0 are constants.
4.4-9
Use a recursion tree to give an asymptotically tight solution to the recurrenceT .n/DT. ˛n /CT ..1/NUL˛/n/Ccn,w h e r e ˛is a constant in the range 0<˛<1
andc>0 is also a constant.
4.5 The master method for solving recurrences
The master method provides a “cookbook” method for solving recurrences of the
form
T .n/DaT .n=b/Cf .n/ ; (4.20)
where a/NAK1andb>1 are constants and f .n/ is an asymptotically positive
function. To use the master method, you will need to memorize three cases, butthen you will be able to solve many recurrences quite easily, often without penciland paper.94 Chapter 4 Divide-and-Conquer
The recurrence (4.20) describes the running time of an algorithm that divides a
problem of size nintoasubproblems, each of size n=b,w h e r e aandbare positive
constants. The asubproblems are solved recursively, each in time T .n=b/ .T h e
function f .n/ encompasses the cost of dividing the problem and combining the
results of the subproblems. For example, the recurrence arising from Strassen’salgorithm has aD7,bD2,a n d f .n/D‚.n
2/.
As a matter of technical correctness, the recurrence is not actually well deﬁned,
because n=b might not be an integer. Replacing each of the aterms T .n=b/ with
either T.bn=bc/orT.dn=be/will not affect the asymptotic behavior of the recur-
rence, however. (We will prove this assertion in the next section.) We normallyﬁnd it convenient, therefore, to omit the ﬂoor and ceiling functions when writingdivide-and-conquer recurrences of this form.
The master theorem
The master method depends on the following theorem.
Theorem 4.1 (Master theorem)
Leta/NAK1andb>1 be constants, let f .n/ be a function, and let T .n/ be deﬁned
on the nonnegative integers by the recurrence
T .n/DaT .n=b/Cf .n/ ;
where we interpret n=bto mean eitherbn=bcordn=be.T h e n T .n/ has the follow-
ing asymptotic bounds:
1. If f .n/DO.n
logba/NUL/SI/for some constant /SI>0 ,t h e n T .n/D‚.nlogba/.
2. If f .n/D‚.nlogba/,t h e n T .n/D‚.nlogbalgn/.
3. If f .n/D/DEL.nlogbaC/SI/for some constant /SI>0 ,a n di f af .n=b//DC4cf .n/ for
some constant c<1 and all sufﬁciently large n,t h e n T .n/D‚.f .n// .
Before applying the master theorem to some examples, let’s spend a moment
trying to understand what it says. In each of the three cases, we compare the
function f .n/ with the function nlogba. Intuitively, the larger of the two functions
determines the solution to the recurrence. If, as in case 1, the function nlogbais the
larger, then the solution is T .n/D‚.nlogba/. If, as in case 3, the function f .n/
is the larger, then the solution is T .n/D‚.f .n// . If, as in case 2, the two func-
tions are the same size, we multiply by a logarithmic factor, and the solution isT .n/D‚.n
logbalgn/D‚.f .n/ lgn/.
Beyond this intuition, you need to be aware of some technicalities. In the ﬁrst
case, not only must f .n/ be smaller than nlogba,i tm u s tb e polynomially smaller.4.5 The master method for solving recurrences 95
That is, f .n/ must be asymptotically smaller than nlogbaby a factor of n/SIfor some
constant /SI>0 . In the third case, not only must f .n/ be larger than nlogba,i ta l s o
must be polynomially larger and in addition satisfy the “regularity” condition thataf .n=b//DC4cf .n/ . This condition is satisﬁed by most of the polynomially bounded
functions that we shall encounter.
Note that the three cases do not cover all the possibilities for f .n/ . There is
a gap between cases 1 and 2 when f .n/ is smaller than n
logbabut not polynomi-
ally smaller. Similarly, there is a gap between cases 2 and 3 when f .n/ is larger
thannlogbabut not polynomially larger. If the function f .n/ falls into one of these
gaps, or if the regularity condition in case 3 fails to hold, you cannot use the mastermethod to solve the recurrence.
Using the master method
To use the master method, we simply determine which case (if any) of the master
theorem applies and write down the answer.
As a ﬁrst example, consider
T .n/D9T .n=3/Cn:
For this recurrence, we have aD9,bD3,f .n/Dn, and thus we have that
n
logbaDnlog39D‚.n2). Since f .n/DO.nlog39/NUL/SI/,w h e r e /SID1, we can apply
case 1 of the master theorem and conclude that the solution is T .n/D‚.n2/.
Now consider
T .n/DT .2n=3/C1;
in which aD1,bD3=2,f .n/D1,a n d nlogbaDnlog3=21Dn0D1. Case 2
applies, since f .n/D‚.nlogba/D‚.1/ , and thus the solution to the recurrence
isT .n/D‚.lgn/.
For the recurrence
T .n/D3T .n=4/Cnlgn;
we have aD3,bD4,f .n/Dnlgn,a n d nlogbaDnlog43DO.n0:793/.
Since f .n/D/DEL.nlog43C/SI/,w h e r e /SI/EM0:2, case 3 applies if we can show that
the regularity condition holds for f .n/ . For sufﬁciently large n,w eh a v et h a t
af .n=b/D3.n=4/ lg.n=4//DC4.3=4/n lgnDcf .n/ forcD3=4. Consequently,
by case 3, the solution to the recurrence is T .n/D‚.n lgn/.
The master method does not apply to the recurrence
T .n/D2T .n=2/Cnlgn;
even though it appears to have the proper form: aD2,bD2,f .n/Dnlgn,
andnlogbaDn. You might mistakenly think that case 3 should apply, since96 Chapter 4 Divide-and-Conquer
f .n/Dnlgnis asymptotically larger than nlogbaDn. The problem is that it
is not polynomially larger. The ratio f .n/=nlogbaD.nlgn/=nDlgnis asymp-
totically less than n/SIfor any positive constant /SI. Consequently, the recurrence falls
into the gap between case 2 and case 3. (See Exercise 4.6-2 for a solution.)
Let’s use the master method to solve the recurrences we saw in Sections 4.1
and 4.2. Recurrence (4.7),
T .n/D2T .n=2/C‚.n/ ;
characterizes the running times of the divide-and-conquer algorithm for both the
maximum-subarray problem and merge sort. (As is our practice, we omit statingthe base case in the recurrence.) Here, we have aD2,bD2,f .n/D‚.n/ ,a n d
thus we have that n
logbaDnlog22Dn. Case 2 applies, since f .n/D‚.n/ ,a n ds o
we have the solution T .n/D‚.n lgn/.
Recurrence (4.17),
T .n/D8T .n=2/C‚.n2/;
describes the running time of the ﬁrst divide-and-conquer algorithm that we saw
for matrix multiplication. Now we have aD8,bD2,a n d f .n/D‚.n2/,
and so nlogbaDnlog28Dn3.S i n c e n3is polynomially larger than f .n/ (that is,
f .n/DO.n3/NUL/SI/for/SID1), case 1 applies, and T .n/D‚.n3/.
Finally, consider recurrence (4.18),
T .n/D7T .n=2/C‚.n2/;
which describes the running time of Strassen’s algorithm. Here, we have aD7,
bD2,f .n/D‚.n2/, and thus nlogbaDnlog27. Rewriting log27as lg 7and
recalling that 2:80 < lg7<2 : 8 1 , we see that f .n/DO.nlg7/NUL/SI/for/SID0:8.
Again, case 1 applies, and we have the solution T .n/D‚.nlg7/.
Exercises
4.5-1
Use the master method to give tight asymptotic bounds for the following recur-rences.
a.T .n/D2T .n=4/C1.
b.T .n/D2T .n=4/Cp
n.
c.T .n/D2T .n=4/Cn.
d.T .n/D2T .n=4/Cn2.4.6 Proof of the master theorem 97
4.5-2
Professor Caesar wishes to develop a matrix-multiplication algorithm that isasymptotically faster than Strassen’s algorithm. His algorithm will use the divide-and-conquer method, dividing each matrix into pieces of size n=4/STXn=4,a n dt h e
divide and combine steps together will take ‚.n
2/time. He needs to determine
how many subproblems his algorithm has to create in order to beat Strassen’s algo-rithm. If his algorithm creates asubproblems, then the recurrence for the running
timeT .n/ becomes T .n/DaT .n=4/C‚.n
2/. What is the largest integer value
ofafor which Professor Caesar’s algorithm would be asymptotically faster than
Strassen’s algorithm?
4.5-3
Use the master method to show that the solution to the binary-search recurrenceT .n/DT .n=2/C‚.1/ isT .n/D‚.lgn/. (See Exercise 2.3-5 for a description
of binary search.)
4.5-4
Can the master method be applied to the recurrence T .n/D4T .n=2/Cn
2lgn?
Why or why not? Give an asymptotic upper bound for this recurrence.
4.5-5 ?
Consider the regularity condition af .n=b//DC4cf .n/ for some constant c<1 ,
which is part of case 3 of the master theorem. Give an example of constants a/NAK1
andb>1 and a function f .n/ that satisﬁes all the conditions in case 3 of the
master theorem except the regularity condition.
?4.6 Proof of the master theorem
This section contains a proof of the master theorem (Theorem 4.1). You do notneed to understand the proof in order to apply the master theorem.
The proof appears in two parts. The ﬁrst part analyzes the master recur-
rence (4.20), under the simplifying assumption that T .n/ is deﬁned only on ex-
act powers of b>1 ,t h a ti s ,f o r nD1; b; b
2;:::. This part gives all the intuition
needed to understand why the master theorem is true. The second part shows howto extend the analysis to all positive integers n; it applies mathematical technique
to the problem of handling ﬂoors and ceilings.
In this section, we shall sometimes abuse our asymptotic notation slightly by
using it to describe the behavior of functions that are deﬁned only over exact
powers of b. Recall that the deﬁnitions of asymptotic notations require that98 Chapter 4 Divide-and-Conquer
bounds be proved for all sufﬁciently large numbers, not just those that are pow-
ers of b. Since we could make new asymptotic notations that apply only to the set
fbiWiD0; 1; 2; : : :g, instead of to the nonnegative numbers, this abuse is minor.
Nevertheless, we must always be on guard when we use asymptotic notation over
a limited domain lest we draw improper conclusions. For example, proving thatT .n/DO.n/ when nis an exact power of 2does not guarantee that T .n/DO.n/ .
The function T .n/ could be deﬁned as
T .n/D(
n ifnD1;2;4;8;::: ;
n
2otherwise ;
in which case the best upper bound that applies to all values of nisT .n/DO.n2/.
Because of this sort of drastic consequence, we shall never use asymptotic notationover a limited domain without making it absolutely clear from the context that weare doing so.
4.6.1 The proof for exact powers
The ﬁrst part of the proof of the master theorem analyzes the recurrence (4.20)
T .n/DaT .n=b/Cf .n/ ;
for the master method, under the assumption that nis an exact power of b>1 ,
where bneed not be an integer. We break the analysis into three lemmas. The ﬁrst
reduces the problem of solving the master recurrence to the problem of evaluatingan expression that contains a summation. The second determines bounds on thissummation. The third lemma puts the ﬁrst two together to prove a version of themaster theorem for the case in which nis an exact power of b.
Lemma 4.2
Leta/NAK1andb>1 be constants, and let f .n/ be a nonnegative function deﬁned
on exact powers of b.D e ﬁ n e T .n/ on exact powers of bby the recurrence
T .n/D(
‚.1/ ifnD1;
aT .n=b/Cf .n/ ifnDb
i;
where iis a positive integer. Then
T .n/D‚.nlogba/Clogbn/NUL1X
jD0ajf .n=bj/: (4.21)
Proof We use the recursion tree in Figure 4.7. The root of the tree has cost f .n/ ,
and it has achildren, each with cost f .n=b/ . (It is convenient to think of aas being4.6 Proof of the master theorem 99
……
…
… … ……
… … ……
… … …
…f .n/ f .n/
a a aa
a a aa
a a aaa
f .n=b/ f .n=b/ f .n=b/
f .n=b2/ f .n=b2/ f .n=b2/ f .n=b2/ f .n=b2/ f .n=b2/ f .n=b2/ f .n=b2/ f .n=b2/af .n=b/
a2f .n=b2/logbn
nlogba‚.1/ ‚.1/ ‚.1/ ‚.1/ ‚.1/ ‚.1/ ‚.1/ ‚.1/ ‚.1/ ‚.1/ ‚.1/ ‚.1/ ‚.1/ ‚.nlogba/
Total: ‚.nlogba/Clogbn/NUL1X
jD0ajf .n=bj/
Figure 4.7 The recursion tree generated by T .n/DaT .n=b/Cf .n/ . The tree is a complete a-ary
tree with nlogbaleaves and height logbn. The cost of the nodes at each depth is shown at the right,
and their sum is given in equation (4.21).
an integer, especially when visualizing the recursion tree, but the mathematics does
not require it.) Each of these children has achildren, making a2nodes at depth 2,
and each of the achildren has cost f .n=b2/. In general, there are ajnodes at
depth j, and each has cost f .n=bj/. The cost of each leaf is T. 1 /D‚.1/ ,a n d
each leaf is at depth logbn,s i n c e n=blogbnD1.T h e r e a r e alogbnDnlogbaleaves
in the tree.
We can obtain equation (4.21) by summing the costs of the nodes at each depth
in the tree, as shown in the ﬁgure. The cost for all internal nodes at depth jis
ajf .n=bj/, and so the total cost of all internal nodes is
logbn/NUL1X
jD0ajf .n=bj/:
In the underlying divide-and-conquer algorithm, this sum represents the costs of
dividing problems into subproblems and then recombining the subproblems. The100 Chapter 4 Divide-and-Conquer
cost of all the leaves, which is the cost of doing all nlogbasubproblems of size 1,
is‚.nlogba/.
In terms of the recursion tree, the three cases of the master theorem correspond
to cases in which the total cost of the tree is (1) dominated by the costs in theleaves, (2) evenly distributed among the levels of the tree, or (3) dominated by thecost of the root.
The summation in equation (4.21) describes the cost of the dividing and com-
bining steps in the underlying divide-and-conquer algorithm. The next lemma pro-vides asymptotic bounds on the summation’s growth.
Lemma 4.3
Leta/NAK1andb>1 be constants, and let f .n/ be a nonnegative function deﬁned
on exact powers of b. A function g.n/ deﬁned over exact powers of bby
g.n/D
logbn/NUL1X
jD0ajf .n=bj/ (4.22)
has the following asymptotic bounds for exact powers of b:
1. If f .n/DO.nlogba/NUL/SI/for some constant /SI>0 ,t h e n g.n/DO.nlogba/.
2. If f .n/D‚.nlogba/,t h e n g.n/D‚.nlogbalgn/.
3. If af .n=b//DC4cf .n/ for some constant c<1 and for all sufﬁciently large n,
theng.n/D‚.f .n// .
Proof For case 1, we have f .n/DO.nlogba/NUL/SI/, which implies that f .n=bj/D
O..n=bj/logba/NUL/SI/. Substituting into equation (4.22) yields
g.n/DO logbn/NUL1X
jD0aj/DLEn
bj/DC1logba/NUL/SI!
: (4.23)
We bound the summation within the O-notation by factoring out terms and simpli-
fying, which leaves an increasing geometric series:
logbn/NUL1X
jD0aj/DLEn
bj/DC1logba/NUL/SI
Dnlogba/NUL/SIlogbn/NUL1X
jD0/DC2ab/SI
blogba/DC3j
Dnlogba/NUL/SIlogbn/NUL1X
jD0.b/SI/j
Dnlogba/NUL/SI/DC2b/SIlogbn/NUL1
b/SI/NUL1/DC34.6 Proof of the master theorem 101
Dnlogba/NUL/SI/DC2n/SI/NUL1
b/SI/NUL1/DC3
:
Since band/SIare constants, we can rewrite the last expression as nlogba/NUL/SIO.n/SI/D
O.nlogba/. Substituting this expression for the summation in equation (4.23) yields
g.n/DO.nlogba/;
thereby proving case 1.
Because case 2 assumes that f .n/D‚.nlogba/,w eh a v et h a t f .n=bj/D
‚..n=bj/logba/. Substituting into equation (4.22) yields
g.n/D‚ logbn/NUL1X
jD0aj/DLEn
bj/DC1logba!
: (4.24)
We bound the summation within the ‚-notation as in case 1, but this time we do not
obtain a geometric series. Instead, we discover that every term of the summation
is the same:
logbn/NUL1X
jD0aj/DLEn
bj/DC1logba
Dnlogbalogbn/NUL1X
jD0/DLEa
blogba/DC1j
Dnlogbalogbn/NUL1X
jD01
Dnlogbalogbn:
Substituting this expression for the summation in equation (4.24) yieldsg.n/D‚.n
logbalogbn/
D‚.nlogbalgn/ ;
proving case 2.
We prove case 3 similarly. Since f .n/ appears in the deﬁnition (4.22) of g.n/
and all terms of g.n/ are nonnegative, we can conclude that g.n/D/DEL.f .n// for
exact powers of b. We assume in the statement of the lemma that af .n=b//DC4cf .n/
for some constant c<1 and all sufﬁciently large n. We rewrite this assumption
asf .n=b//DC4.c=a/f .n/ and iterate jtimes, yielding f .n=bj//DC4.c=a/jf .n/ or,
equivalently, ajf .n=bj//DC4cjf .n/ , where we assume that the values we iterate
on are sufﬁciently large. Since the last, and smallest, such value is n=bj/NUL1,i ti s
enough to assume that n=bj/NUL1is sufﬁciently large.
Substituting into equation (4.22) and simplifying yields a geometric series, but
unlike the series in case 1, this one has decreasing terms. We use an O.1/ term to102 Chapter 4 Divide-and-Conquer
capture the terms that are not covered by our assumption that nis sufﬁciently large:
g.n/Dlogbn/NUL1X
jD0ajf .n=bj/
/DC4logbn/NUL1X
jD0cjf .n/CO.1/
/DC4f .n/1X
jD0cjCO.1/
Df .n//DC21
1/NULc/DC3
CO.1/
DO.f .n// ;
since cis a constant. Thus, we can conclude that g.n/D‚.f .n// for exact powers
ofb. With case 3 proved, the proof of the lemma is complete.
We can now prove a version of the master theorem for the case in which nis an
exact power of b.
Lemma 4.4
Leta/NAK1andb>1 be constants, and let f .n/ be a nonnegative function deﬁned
on exact powers of b.D e ﬁ n e T .n/ on exact powers of bby the recurrence
T .n/D(
‚.1/ ifnD1;
aT .n=b/Cf .n/ ifnDbi;
where iis a positive integer. Then T .n/ has the following asymptotic bounds for
exact powers of b:
1. If f .n/DO.nlogba/NUL/SI/for some constant /SI>0 ,t h e n T .n/D‚.nlogba/.
2. If f .n/D‚.nlogba/,t h e n T .n/D‚.nlogbalgn/.
3. If f .n/D/DEL.nlogbaC/SI/for some constant /SI>0 ,a n di f af .n=b//DC4cf .n/ for
some constant c<1 and all sufﬁciently large n,t h e n T .n/D‚.f .n// .
Proof We use the bounds in Lemma 4.3 to evaluate the summation (4.21) from
Lemma 4.2. For case 1, we have
T .n/D‚.nlogba/CO.nlogba/
D‚.nlogba/;4.6 Proof of the master theorem 103
and for case 2,
T .n/D‚.nlogba/C‚.nlogbalgn/
D‚.nlogbalgn/ :
For case 3,T .n/D‚.n
logba/C‚.f .n//
D‚.f .n// ;
because f .n/D/DEL.nlogbaC/SI/.
4.6.2 Floors and ceilings
To complete the proof of the master theorem, we must now extend our analysis to
the situation in which ﬂoors and ceilings appear in the master recurrence, so thatthe recurrence is deﬁned for all integers, not for just exact powers of b. Obtaining
a lower bound on
T .n/DaT.dn=be/Cf .n/ (4.25)
and an upper bound onT .n/DaT.bn=bc/Cf .n/ (4.26)
is routine, since we can push through the bound dn=be/NAKn=b in the ﬁrst case to
yield the desired result, and we can push through the bound bn=bc/DC4n=b in the
second case. We use much the same technique to lower-bound the recurrence (4.26)as to upper-bound the recurrence (4.25), and so we shall present only this latterbound.
We modify the recursion tree of Figure 4.7 to produce the recursion tree in Fig-
ure 4.8. As we go down in the recursion tree, we obtain a sequence of recursiveinvocations on the arguments
n;
dn=be;
ddn=be=be;
dddn=be=be=be;
:::
Let us denote the jth element in the sequence by n
j,w h e r e
njD(
n ifjD0;
dnj/NUL1=beifj> 0 :(4.27)104 Chapter 4 Divide-and-Conquer
……
…
… … ……
… … ……
… … …
…f .n/ f .n/
a a aa
a a aa
a a aaa
f. n 1/ f. n 1/ f. n 1/
f. n 2/ f. n 2/ f. n 2/ f. n 2/ f. n 2/ f. n 2/ f. n 2/ f. n 2/ f. n 2/af .n 1/
a2f. n 2/blogbnc
‚.nlogba/‚.1/ ‚.1/ ‚.1/ ‚.1/ ‚.1/ ‚.1/ ‚.1/ ‚.1/ ‚.1/ ‚.1/ ‚.1/ ‚.1/ ‚.1/ ‚.nlogba/
Total: ‚.nlogba/Cblogbnc/NUL1X
jD0ajf. n j/
Figure 4.8 The recursion tree generated by T .n/DaT .dn=be/Cf .n/ . The recursive argument nj
is given by equation (4.27).
Our ﬁrst goal is to determine the depth ksuch that nkis a constant. Using the
inequalitydxe/DC4xC1, we obtain
n0/DC4n;
n1/DC4n
bC1;
n2/DC4n
b2C1
bC1;
n3/DC4n
b3C1
b2C1
bC1;
:::
In general, we have4.6 Proof of the master theorem 105
nj/DC4n
bjCj/NUL1X
iD01
bi
<n
bjC1X
iD01
bi
Dn
bjCb
b/NUL1:
Letting jDblogbnc, we obtain
nblogbnc<n
bblogbncCb
b/NUL1
<n
blogbn/NUL1Cb
b/NUL1
Dn
n=bCb
b/NUL1
DbCb
b/NUL1
DO.1/ ;
and thus we see that at depth blogbnc, the problem size is at most a constant.
From Figure 4.8, we see that
T .n/D‚.nlogba/Cblogbnc/NUL1X
jD0ajf. n j/; (4.28)
which is much the same as equation (4.21), except that nis an arbitrary integer and
not restricted to be an exact power of b.
We can now evaluate the summation
g.n/Dblogbnc/NUL1X
jD0ajf. n j/ (4.29)
from equation (4.28) in a manner analogous to the proof of Lemma 4.3. Beginning
with case 3, if af .dn=be//DC4cf .n/ forn>bCb=.b/NUL1/,w h e r e c<1 is a constant,
then it follows that ajf. n j//DC4cjf .n/ . Therefore, we can evaluate the sum in
equation (4.29) just as in Lemma 4.3. For case 2, we have f .n/D‚.nlogba/.I fw e
can show that f. n j/DO.nlogba=aj/DO..n=bj/logba/, then the proof for case 2
of Lemma 4.3 will go through. Observe that j/DC4blogbncimplies bj=n/DC41.T h e
bound f .n/DO.nlogba/implies that there exists a constant c>0 such that for all
sufﬁciently large nj,106 Chapter 4 Divide-and-Conquer
f. n j//DC4c/DC2n
bjCb
b/NUL1/DC3logba
Dc/DC2n
bj/DC2
1Cbj
n/SOHb
b/NUL1/DC3/DC3 logba
Dc/DC2nlogba
aj/DC3/DC2
1C/DC2bj
n/SOHb
b/NUL1/DC3/DC3 logba
/DC4c/DC2nlogba
aj/DC3/DC2
1Cb
b/NUL1/DC3logba
DO/DC2nlogba
aj/DC3
;
since c.1Cb=.b/NUL1//logbais a constant. Thus, we have proved case 2. The proof
of case 1 is almost identical. The key is to prove the bound f. n j/DO.nlogba/NUL/SI/,
which is similar to the corresponding proof of case 2, though the algebra is moreintricate.
We have now proved the upper bounds in the master theorem for all integers n.
The proof of the lower bounds is similar.
Exercises
4.6-1 ?
Give a simple and exact expression for n
jin equation (4.27) for the case in which b
is a positive integer instead of an arbitrary real number.
4.6-2 ?
Show that if f .n/D‚.nlogbalgkn/,w h e r e k/NAK0, then the master recurrence has
solution T .n/D‚.nlogbalgkC1n/. For simplicity, conﬁne your analysis to exact
powers of b.
4.6-3 ?
Show that case 3 of the master theorem is overstated, in the sense that the regularitycondition af .n=b//DC4cf .n/ for some constant c<1 implies that there exists a
constant /SI>0 such that f .n/D/DEL.n
logbaC/SI/.Problems for Chapter 4 107
Problems
4-1 Recurrence examples
Give asymptotic upper and lower bounds for T .n/ in each of the following recur-
rences. Assume that T .n/ is constant for n/DC42. Make your bounds as tight as
possible, and justify your answers.
a.T .n/D2T .n=2/Cn4.
b.T .n/DT .7n=10/Cn.
c.T .n/D16T .n=4/Cn2.
d.T .n/D7T .n=3/Cn2.
e.T .n/D7T .n=2/Cn2.
f.T .n/D2T .n=4/Cp
n.
g.T .n/DT. n/NUL2/Cn2.
4-2 Parameter-passing costs
Throughout this book, we assume that parameter passing during procedure callstakes constant time, even if an N-element array is being passed. This assumption
is valid in most systems because a pointer to the array is passed, not the array itself.
This problem examines the implications of three parameter-passing strategies:
1. An array is passed by pointer. Time D‚.1/ .
2. An array is passed by copying. Time D‚.N / ,w h e r e Nis the size of the array.
3. An array is passed by copying only the subrange that might be accessed by the
called procedure. Time D‚.q/NULpC1/if the subarray AŒp : : q/c141 is passed.
a.Consider the recursive binary search algorithm for ﬁnding a number in a sorted
array (see Exercise 2.3-5). Give recurrences for the worst-case running times
of binary search when arrays are passed using each of the three methods above,and give good upper bounds on the solutions of the recurrences. Let Nbe the
size of the original problem and nbe the size of a subproblem.
b.Redo part (a) for the M
ERGE -SORT algorithm from Section 2.3.1.108 Chapter 4 Divide-and-Conquer
4-3 More recurrence examples
Give asymptotic upper and lower bounds for T .n/ in each of the following recur-
rences. Assume that T .n/ is constant for sufﬁciently small n. Make your bounds
as tight as possible, and justify your answers.
a.T .n/D4T .n=3/Cnlgn.
b.T .n/D3T .n=3/Cn=lgn.
c.T .n/D4T .n=2/Cn2p
n.
d.T .n/D3T .n=3/NUL2/Cn=2.
e.T .n/D2T .n=2/Cn=lgn.
f.T .n/DT .n=2/CT .n=4/CT .n=8/Cn.
g.T .n/DT. n/NUL1/C1=n.
h.T .n/DT. n/NUL1/Clgn.
i.T .n/DT. n/NUL2/C1=lgn.
j.T .n/Dp
nT .p
n/Cn.
4-4 Fibonacci numbers
This problem develops properties of the Fibonacci numbers, which are deﬁnedby recurrence (3.22). We shall use the technique of generating functions to solvethe Fibonacci recurrence. Deﬁne the generating function (orformal power se-
ries)Fas
F.´/D
1X
iD0Fi´i
D0C´C´2C2´3C3´4C5´5C8´6C13´7C21´8C/SOH/SOH/SOH ;
where Fiis the ith Fibonacci number.
a.Show that F.´/D´C´F.´/C´2F.´/.Problems for Chapter 4 109
b.Show that
F.´/D´
1/NUL´/NUL´2
D´
.1/NUL/RS´/.1/NULy/RS´/
D1
p
5/DC21
1/NUL/RS´/NUL1
1/NULy/RS´/DC3
;
where
/RSD1Cp
5
2D1:61803 : : :
and
y/RSD1/NULp
5
2D/NUL0:61803 : : : :
c.Show that
F.´/D1X
iD01
p
5./RSi/NULy/RSi/´i:
d.Use part (c) to prove that FiD/RSi=p
5fori>0 , rounded to the nearest integer.
(Hint: Observe thatˇˇy/RSˇˇ<1.)
4-5 Chip testing
Professor Diogenes has nsupposedly identical integrated-circuit chips that in prin-
ciple are capable of testing each other. The professor’s test jig accommodates twochips at a time. When the jig is loaded, each chip tests the other and reports whetherit is good or bad. A good chip always reports accurately whether the other chip isgood or bad, but the professor cannot trust the answer of a bad chip. Thus, the fourpossible outcomes of a test are as follows:
Chip Asays Chip Bsays Conclusion
Bis good Ais good both are good, or both are bad
Bis good Ais bad at least one is bad
Bis bad Ais good at least one is bad
Bis bad Ais bad at least one is bad
a.Show that if more than n=2chips are bad, the professor cannot necessarily de-
termine which chips are good using any strategy based on this kind of pairwisetest. Assume that the bad chips can conspire to fool the professor.110 Chapter 4 Divide-and-Conquer
b.Consider the problem of ﬁnding a single good chip from among nchips, as-
suming that more than n=2 of the chips are good. Show that bn=2cpairwise
tests are sufﬁcient to reduce the problem to one of nearly half the size.
c.Show that the good chips can be identiﬁed with ‚.n/ pairwise tests, assuming
that more than n=2 of the chips are good. Give and solve the recurrence that
describes the number of tests.
4-6 Monge arrays
Anm/STXnarray Aof real numbers is a Monge array if for all i,j,k,a n d lsuch
that1/DC4i<k/DC4mand1/DC4j< l/DC4n,w eh a v e
AŒi; j /c141CAŒk; l/c141/DC4AŒi; l/c141CAŒk; j /c141 :
In other words, whenever we pick two rows and two columns of a Monge array and
consider the four elements at the intersections of the rows and the columns, the sumof the upper-left and lower-right elements is less than or equal to the sum of thelower-left and upper-right elements. For example, the following array is Monge:
10 17 13 28 23
17 22 16 29 2324 28 22 34 2411 13 6 17 745 44 32 37 2336 33 19 21 675 66 51 53 34
a.Prove that an array is Monge if and only if for all iD1; 2; :::; m/NUL1and
jD1; 2; :::; n/NUL1,w eh a v e
AŒi; j /c141CAŒiC1; jC1/c141/DC4AŒi; jC1/c141CAŒiC1; j /c141 :
(Hint: For the “if” part, use induction separately on rows and columns.)
b.The following array is not Monge. Change one element in order to make it
Monge. ( Hint: Use part (a).)
37 23 22 32
21 6 7 10
53 34 30 3132 13 9 643 21 15 8Notes for Chapter 4 111
c.Letf. i/ be the index of the column containing the leftmost minimum element
of row i. Prove that f. 1 //DC4f. 2 //DC4/SOH/SOH/SOH/DC4 f. m / for any m/STXnMonge array.
d.Here is a description of a divide-and-conquer algorithm that computes the left-
most minimum element in each row of an m/STXnMonge array A:
Construct a submatrix A0ofAconsisting of the even-numbered rows of A.
Recursively determine the leftmost minimum for each row of A0.T h e n
compute the leftmost minimum in the odd-numbered rows of A.
Explain how to compute the leftmost minimum in the odd-numbered rows of A
(given that the leftmost minimum of the even-numbered rows is known) in
O.mCn/time.
e.Write the recurrence describing the running time of the algorithm described in
part (d). Show that its solution is O.mCnlogm/.
Chapter notes
Divide-and-conquer as a technique for designing algorithms dates back to at least
1962 in an article by Karatsuba and Ofman [194]. It might have been used well be-fore then, however; according to Heideman, Johnson, and Burrus [163], C. F. Gaussdevised the ﬁrst fast Fourier transform algorithm in 1805, and Gauss’s formulationbreaks the problem into smaller subproblems whose solutions are combined.
The maximum-subarray problem in Section 4.1 is a minor variation on a problem
studied by Bentley [43, Chapter 7].
Strassen’s algorithm [325] caused much excitement when it was published
in 1969. Before then, few imagined the possibility of an algorithm asymptoticallyfaster than the basic S
QUARE -MATRIX -MULTIPLY procedure. The asymptotic
upper bound for matrix multiplication has been improved since then. The most
asymptotically efﬁcient algorithm for multiplying n/STXnmatrices to date, due to
Coppersmith and Winograd [78], has a running time of O.n2:376/. The best lower
bound known is just the obvious /DEL.n2/bound (obvious because we must ﬁll in n2
elements of the product matrix).
From a practical point of view, Strassen’s algorithm is often not the method of
choice for matrix multiplication, for four reasons:
1. The constant factor hidden in the ‚.nlg7/running time of Strassen’s algo-
rithm is larger than the constant factor in the ‚.n3/-time S QUARE -MATRIX -
MULTIPLY procedure.
2. When the matrices are sparse, methods tailored for sparse matrices are faster.112 Chapter 4 Divide-and-Conquer
3. Strassen’s algorithm is not quite as numerically stable as S QUARE -MATRIX -
MULTIPLY . In other words, because of the limited precision of computer arith-
metic on noninteger values, larger errors accumulate in Strassen’s algorithmthan in S
QUARE -MATRIX -MULTIPLY .
4. The submatrices formed at the levels of recursion consume space.
The latter two reasons were mitigated around 1990. Higham [167] demonstrated
that the difference in numerical stability had been overemphasized; althoughStrassen’s algorithm is too numerically unstable for some applications, it is withinacceptable limits for others. Bailey, Lee, and Simon [32] discuss techniques forreducing the memory requirements for Strassen’s algorithm.
In practice, fast matrix-multiplication implementations for dense matrices use
Strassen’s algorithm for matrix sizes above a “crossover point,” and they switchto a simpler method once the subproblem size reduces to below the crossoverpoint. The exact value of the crossover point is highly system dependent. Analysesthat count operations but ignore effects from caches and pipelining have produced
crossover points as low as nD8(by Higham [167]) or nD12(by Huss-Lederman
et al. [186]). D’Alberto and Nicolau [81] developed an adaptive scheme, whichdetermines the crossover point by benchmarking when their software package isinstalled. They found crossover points on various systems ranging from nD400
tonD2150 , and they could not ﬁnd a crossover point on a couple of systems.
Recurrences were studied as early as 1202 by L. Fibonacci, for whom the Fi-
bonacci numbers are named. A. De Moivre introduced the method of generating
functions (see Problem 4-4) for solving recurrences. The master method is adaptedfrom Bentley, Haken, and Saxe [44], which provides the extended method justiﬁedby Exercise 4.6-2. Knuth [209] and Liu [237] show how to solve linear recurrencesusing the method of generating functions. Purdom and Brown [287] and Graham,Knuth, and Patashnik [152] contain extended discussions of recurrence solving.
Several researchers, including Akra and Bazzi [13], Roura [299], Verma [346],
and Yap [360], have given methods for solving more general divide-and-conquer
recurrences than are solved by the master method. We describe the result of Akra
and Bazzi here, as modiﬁed by Leighton [228]. The Akra-Bazzi method works forrecurrences of the form
T. x/D(
‚.1/ if1/DC4x/DC4x
0;Pk
iD1aiT. b ix/Cf. x/ ifx>x 0;(4.30)
where
/SIx/NAK1is a real number,
/SIx0is a constant such that x0/NAK1=b iandx0/NAK1=.1/NULbi/foriD1 ;2;:::;k ,
/SIaiis a positive constant for iD1 ;2;:::;k ,Notes for Chapter 4 113
/SIbiis a constant in the range 0<b i<1foriD1 ;2;:::;k ,
/SIk/NAK1is an integer constant, and
/SIf. x/ is a nonnegative function that satisﬁes the polynomial-growth condi-
tion: there exist positive constants c1andc2such that for all x/NAK1,f o r
iD1 ;2;:::;k , and for all usuch that bix/DC4u/DC4x,w eh a v e c1f. x//DC4
f. u //DC4c2f. x/ . (Ifjf0.x/jis upper-bounded by some polynomial in x,t h e n
f. x/ satisﬁes the polynomial-growth condition. For example, f. x/Dx˛lgˇx
satisﬁes this condition for any real constants ˛andˇ.)
Although the master method does not apply to a recurrence such as T .n/D
T.bn=3c/CT.b2n=3c/CO.n/ , the Akra-Bazzi method does. To solve the re-
currence (4.30), we ﬁrst ﬁnd the unique real number psuch thatPk
iD1aibp
iD1.
(Such a palways exists.) The solution to the recurrence is then
T .n/D‚/DC2
xp/DC2
1CZx
1f. u /
upC1du/DC3/DC3
:
The Akra-Bazzi method can be somewhat difﬁcult to use, but it serves in solving
recurrences that model division of the problem into substantially unequally sizedsubproblems. The master method is simpler to use, but it applies only when sub-problem sizes are equal.5 Probabilistic Analysis and Randomized
Algorithms
This chapter introduces probabilistic analysis and randomized algorithms. If you
are unfamiliar with the basics of probability theory, you should read Appendix C,which reviews this material. We shall revisit probabilistic analysis and randomizedalgorithms several times throughout this book.
5.1 The hiring problem
Suppose that you need to hire a new ofﬁce assistant. Your previous attempts athiring have been unsuccessful, and you decide to use an employment agency. Theemployment agency sends you one candidate each day. You interview that person
and then decide either to hire that person or not. You must pay the employment
agency a small fee to interview an applicant. To actually hire an applicant is morecostly, however, since you must ﬁre your current ofﬁce assistant and pay a substan-tial hiring fee to the employment agency. You are committed to having, at all times,the best possible person for the job. Therefore, you decide that, after interviewingeach applicant, if that applicant is better qualiﬁed than the current ofﬁce assistant,you will ﬁre the current ofﬁce assistant and hire the new applicant. You are willingto pay the resulting price of this strategy, but you wish to estimate what that pricewill be.
The procedure H
IRE-ASSISTANT , given below, expresses this strategy for hiring
in pseudocode. It assumes that the candidates for the ofﬁce assistant job are num-
bered 1through n. The procedure assumes that you are able to, after interviewing
candidate i, determine whether candidate iis the best candidate you have seen so
far. To initialize, the procedure creates a dummy candidate, numbered 0, who is
less qualiﬁed than each of the other candidates.5.1 The hiring problem 115
HIRE-ASSISTANT .n/
1bestD0 //candidate 0 is a least-qualiﬁed dummy candidate
2foriD1ton
3 interview candidate i
4 ifcandidate iis better than candidate best
5 bestDi
6 hire candidate i
The cost model for this problem differs from the model described in Chapter 2.
We focus not on the running time of H IRE-ASSISTANT , but instead on the costs
incurred by interviewing and hiring. On the surface, analyzing the cost of this algo-rithm may seem very different from analyzing the running time of, say, merge sort.The analytical techniques used, however, are identical whether we are analyzing
cost or running time. In either case, we are counting the number of times certain
basic operations are executed.
Interviewing has a low cost, say c
i, whereas hiring is expensive, costing ch.L e t -
tingmbe the number of people hired, the total cost associated with this algorithm
isO.c inCchm/. No matter how many people we hire, we always interview n
candidates and thus always incur the cost cinassociated with interviewing. We
therefore concentrate on analyzing chm, the hiring cost. This quantity varies with
each run of the algorithm.
This scenario serves as a model for a common computational paradigm. We of-
ten need to ﬁnd the maximum or minimum value in a sequence by examining eachelement of the sequence and maintaining a current “winner.” The hiring problemmodels how often we update our notion of which element is currently winning.
Worst-case analysis
In the worst case, we actually hire every candidate that we interview. This situation
occurs if the candidates come in strictly increasing order of quality, in which casewe hire ntimes, for a total hiring cost of O.c
hn/.
Of course, the candidates do not always come in increasing order of quality. In
fact, we have no idea about the order in which they arrive, nor do we have any
control over this order. Therefore, it is natural to ask what we expect to happen ina typical or average case.
Probabilistic analysis
Probabilistic analysis is the use of probability in the analysis of problems. Most
commonly, we use probabilistic analysis to analyze the running time of an algo-rithm. Sometimes we use it to analyze other quantities, such as the hiring cost116 Chapter 5 Probabilistic Analysis and Randomized Algorithms
in procedure H IRE-ASSISTANT . In order to perform a probabilistic analysis, we
must use knowledge of, or make assumptions about, the distribution of the inputs.Then we analyze our algorithm, computing an average-case running time, wherewe take the average over the distribution of the possible inputs. Thus we are, ineffect, averaging the running time over all possible inputs. When reporting such arunning time, we will refer to it as the average-case running time .
We must be very careful in deciding on the distribution of inputs. For some
problems, we may reasonably assume something about the set of all possible in-
puts, and then we can use probabilistic analysis as a technique for designing an
efﬁcient algorithm and as a means for gaining insight into a problem. For otherproblems, we cannot describe a reasonable input distribution, and in these caseswe cannot use probabilistic analysis.
For the hiring problem, we can assume that the applicants come in a random
order. What does that mean for this problem? We assume that we can compareany two candidates and decide which one is better qualiﬁed; that is, there is atotal order on the candidates. (See Appendix B for the deﬁnition of a total or-der.) Thus, we can rank each candidate with a unique number from 1through n,
using rank.i/to denote the rank of applicant i, and adopt the convention that a
higher rank corresponds to a better qualiﬁed applicant. The ordered list hrank.1/;
rank. 2 / ;:::; rank.n/iis a permutation of the list h1 ;2;:::;ni. Saying that the
applicants come in a random order is equivalent to saying that this list of ranks isequally likely to be any one of the nŠpermutations of the numbers 1through n.
Alternatively, we say that the ranks form a uniform random permutation ;t h a ti s ,
each of the possible nŠpermutations appears with equal probability.
Section 5.2 contains a probabilistic analysis of the hiring problem.
Randomized algorithms
In order to use probabilistic analysis, we need to know something about the distri-
bution of the inputs. In many cases, we know very little about the input distribution.
Even if we do know something about the distribution, we may not be able to modelthis knowledge computationally. Yet we often can use probability and randomness
as a tool for algorithm design and analysis, by making the behavior of part of thealgorithm random.
In the hiring problem, it may seem as if the candidates are being presented to us
in a random order, but we have no way of knowing whether or not they really are.Thus, in order to develop a randomized algorithm for the hiring problem, we musthave greater control over the order in which we interview the candidates. We will,therefore, change the model slightly. We say that the employment agency has n
candidates, and they send us a list of the candidates in advance. On each day, wechoose, randomly, which candidate to interview. Although we know nothing about5.1 The hiring problem 117
the candidates (besides their names), we have made a signiﬁcant change. Instead
of relying on a guess that the candidates come to us in a random order, we haveinstead gained control of the process and enforced a random order.
More generally, we call an algorithm randomized if its behavior is determined
not only by its input but also by values produced by a random-number gener-
ator. We shall assume that we have at our disposal a random-number generator
R
ANDOM . A call to R ANDOM .a; b/ returns an integer between aandb,i n c l u -
sive, with each such integer being equally likely. For example, R ANDOM .0; 1/
produces 0with probability 1=2, and it produces 1with probability 1=2. A call to
RANDOM .3; 7/ returns either 3,4,5,6,o r7, each with probability 1=5. Each inte-
ger returned by R ANDOM is independent of the integers returned on previous calls.
You may imagine R ANDOM as rolling a .b/NULaC1/-sided die to obtain its out-
put. (In practice, most programming environments offer a pseudorandom-number
generator : a deterministic algorithm returning numbers that “look” statistically
random.)
When analyzing the running time of a randomized algorithm, we take the expec-
tation of the running time over the distribution of values returned by the random
number generator. We distinguish these algorithms from those in which the input
is random by referring to the running time of a randomized algorithm as an ex-
pected running time . In general, we discuss the average-case running time when
the probability distribution is over the inputs to the algorithm, and we discuss theexpected running time when the algorithm itself makes random choices.
Exercises
5.1-1
Show that the assumption that we are always able to determine which candidate isbest, in line 4 of procedure H
IRE-ASSISTANT , implies that we know a total order
on the ranks of the candidates.
5.1-2 ?
Describe an implementation of the procedure R ANDOM .a; b/ that only makes calls
to R ANDOM .0; 1/ . What is the expected running time of your procedure, as a
function of aandb?
5.1-3 ?
Suppose that you want to output 0with probability 1=2and1with probability 1=2.
At your disposal is a procedure B IASED -RANDOM , that outputs either 0or1.I t
outputs 1with some probability pand0with probability 1/NULp,w h e r e 0<p<1 ,
but you do not know what pis. Give an algorithm that uses B IASED -RANDOM
as a subroutine, and returns an unbiased answer, returning 0with probability 1=2118 Chapter 5 Probabilistic Analysis and Randomized Algorithms
and1with probability 1=2. What is the expected running time of your algorithm
as a function of p?
5.2 Indicator random variables
In order to analyze many algorithms, including the hiring problem, we use indicator
random variables. Indicator random variables provide a convenient method forconverting between probabilities and expectations. Suppose we are given a samplespace Sand an event A. Then the indicator random variable IfAgassociated with
event Ais deﬁned as
IfAgD(
1ifAoccurs ;
0ifAdoes not occur :(5.1)
As a simple example, let us determine the expected number of heads that we
obtain when ﬂipping a fair coin. Our sample space is SDfH;Tg, with PrfHgD
PrfTgD1=2. We can then deﬁne an indicator random variable X
H, associated
with the coin coming up heads, which is the event H. This variable counts the
number of heads obtained in this ﬂip, and it is 1if the coin comes up heads and 0
otherwise. We write
XHDIfHg
D(
1ifHoccurs ;
0ifToccurs :
The expected number of heads obtained in one ﬂip of the coin is simply the ex-
pected value of our indicator variable XH:
EŒXH/c141DEŒIfHg/c141
D1/SOHPrfHgC0/SOHPrfTg
D1/SOH.1=2/C0/SOH.1=2/
D1=2 :
Thus the expected number of heads obtained by one ﬂip of a fair coin is 1=2.A s
the following lemma shows, the expected value of an indicator random variable
associated with an event Ais equal to the probability that Aoccurs.
Lemma 5.1
Given a sample space Sa n da ne v e n t Ain the sample space S,l e tXADIfAg.
Then E ŒXA/c141DPrfAg.5.2 Indicator random variables 119
Proof By the deﬁnition of an indicator random variable from equation (5.1) and
the deﬁnition of expected value, we have
EŒXA/c141DEŒIfAg/c141
D1/SOHPrfAgC0/SOHPr˚
A/TAB
DPrfAg;
where
 Adenotes S/NULA, the complement of A.
Although indicator random variables may seem cumbersome for an application
such as counting the expected number of heads on a ﬂip of a single coin, they are
useful for analyzing situations in which we perform repeated random trials. Forexample, indicator random variables give us a simple way to arrive at the resultof equation (C.37). In this equation, we compute the number of heads in ncoin
ﬂips by considering separately the probability of obtaining 0heads, 1head, 2heads,
etc. The simpler method proposed in equation (C.38) instead uses indicator randomvariables implicitly. Making this argument more explicit, we let X
ibe the indicator
random variable associated with the event in which the ith ﬂip comes up heads:
XiDIftheith ﬂip results in the event Hg.L e tXbe the random variable denoting
the total number of heads in the ncoin ﬂips, so that
XDnX
iD1Xi:
We wish to compute the expected number of heads, and so we take the expectation
of both sides of the above equation to obtain
EŒX/c141DE"nX
iD1Xi#
:
The above equation gives the expectation of the sum of nindicator random vari-
ables. By Lemma 5.1, we can easily compute the expectation of each of the random
variables. By equation (C.21)—linearity of expectation—it is easy to compute the
expectation of the sum: it equals the sum of the expectations of the nrandom
variables. Linearity of expectation makes the use of indicator random variables a
powerful analytical technique; it applies even when there is dependence among the
random variables. We now can easily compute the expected number of heads:120 Chapter 5 Probabilistic Analysis and Randomized Algorithms
EŒX/c141DE"nX
iD1Xi#
DnX
iD1EŒXi/c141
DnX
iD11=2
Dn=2 :
Thus, compared to the method used in equation (C.37), indicator random variables
greatly simplify the calculation. We shall use indicator random variables through-
out this book.
Analysis of the hiring problem using indicator random variables
Returning to the hiring problem, we now wish to compute the expected number of
times that we hire a new ofﬁce assistant. In order to use a probabilistic analysis, weassume that the candidates arrive in a random order, as discussed in the previoussection. (We shall see in Section 5.3 how to remove this assumption.) Let Xbe the
random variable whose value equals the number of times we hire a new ofﬁce as-
sistant. We could then apply the deﬁnition of expected value from equation (C.20)
to obtain
EŒX/c141D
nX
xD1xPrfXDxg;
but this calculation would be cumbersome. We shall instead use indicator random
variables to greatly simplify the calculation.
To use indicator random variables, instead of computing E ŒX/c141by deﬁning one
variable associated with the number of times we hire a new ofﬁce assistant, wedeﬁne nvariables related to whether or not each particular candidate is hired. In
particular, we let X
ibe the indicator random variable associated with the event in
which the ith candidate is hired. Thus,
XiDIfcandidate iis hiredg
D(
1if candidate iis hired ;
0if candidate iis not hired ;
and
XDX1CX2C/SOH/SOH/SOHC Xn: (5.2)5.2 Indicator random variables 121
By Lemma 5.1, we have that
EŒXi/c141DPrfcandidate iis hiredg;
and we must therefore compute the probability that lines 5–6 of H IRE-ASSISTANT
are executed.
Candidate iis hired, in line 6, exactly when candidate iis better than each of
candidates 1through i/NUL1. Because we have assumed that the candidates arrive in
a random order, the ﬁrst icandidates have appeared in a random order. Any one of
these ﬁrst icandidates is equally likely to be the best-qualiﬁed so far. Candidate i
has a probability of 1=iof being better qualiﬁed than candidates 1through i/NUL1
and thus a probability of 1=iof being hired. By Lemma 5.1, we conclude that
EŒXi/c141D1=i : (5.3)
Now we can compute E ŒX/c141:
EŒX/c141DE"nX
iD1Xi#
(by equation (5.2)) (5.4)
DnX
iD1EŒXi/c141 (by linearity of expectation)
DnX
iD11=i (by equation (5.3))
DlnnCO.1/ (by equation (A.7)) . (5.5)
Even though we interview npeople, we actually hire only approximately ln nof
them, on average. We summarize this result in the following lemma.
Lemma 5.2
Assuming that the candidates are presented in a random order, algorithm H IRE-
ASSISTANT has an average-case total hiring cost of O.c hlnn/.
Proof The bound follows immediately from our deﬁnition of the hiring cost
and equation (5.5), which shows that the expected number of hires is approxi-mately ln n.
The average-case hiring cost is a signiﬁcant improvement over the worst-case
hiring cost of O.c hn/.122 Chapter 5 Probabilistic Analysis and Randomized Algorithms
Exercises
5.2-1
In H IRE-ASSISTANT , assuming that the candidates are presented in a random or-
der, what is the probability that you hire exactly one time? What is the probability
that you hire exactly ntimes?
5.2-2
In H IRE-ASSISTANT , assuming that the candidates are presented in a random or-
der, what is the probability that you hire exactly twice?
5.2-3
Use indicator random variables to compute the expected value of the sum of ndice.
5.2-4
Use indicator random variables to solve the following problem, which is known asthehat-check problem . Each of ncustomers gives a hat to a hat-check person at a
restaurant. The hat-check person gives the hats back to the customers in a randomorder. What is the expected number of customers who get back their own hat?
5.2-5
LetAŒ1 : : n/c141 be an array of ndistinct numbers. If i<j andAŒi/c141 > AŒj /c141 ,t h e n
the pair .i; j / is called an inversion ofA. (See Problem 2-4 for more on inver-
sions.) Suppose that the elements of Aform a uniform random permutation of
h1 ;2;:::;ni. Use indicator random variables to compute the expected number of
inversions.
5.3 Randomized algorithms
In the previous section, we showed how knowing a distribution on the inputs canhelp us to analyze the average-case behavior of an algorithm. Many times, we donot have such knowledge, thus precluding an average-case analysis. As mentioned
in Section 5.1, we may be able to use a randomized algorithm.
For a problem such as the hiring problem, in which it is helpful to assume that
all permutations of the input are equally likely, a probabilistic analysis can guidethe development of a randomized algorithm. Instead of assuming a distributionof inputs, we impose a distribution. In particular, before running the algorithm,we randomly permute the candidates in order to enforce the property that everypermutation is equally likely. Although we have modiﬁed the algorithm, we stillexpect to hire a new ofﬁce assistant approximately ln ntimes. But now we expect5.3 Randomized algorithms 123
this to be the case for anyinput, rather than for inputs drawn from a particular
distribution.
Let us further explore the distinction between probabilistic analysis and random-
ized algorithms. In Section 5.2, we claimed that, assuming that the candidates ar-rive in a random order, the expected number of times we hire a new ofﬁce assistantis about ln n. Note that the algorithm here is deterministic; for any particular input,
the number of times a new ofﬁce assistant is hired is always the same. Furthermore,the number of times we hire a new ofﬁce assistant differs for different inputs, and it
depends on the ranks of the various candidates. Since this number depends only on
the ranks of the candidates, we can represent a particular input by listing, in order,the ranks of the candidates, i.e., hrank.1/;rank. 2 / ;:::; rank.n/i. Given the rank
listA
1Dh1;2;3;4;5;6; 7; 8; 9; 10 i, a new ofﬁce assistant is always hired 10times,
since each successive candidate is better than the previous one, and lines 5–6 areexecuted in each iteration. Given the list of ranks A
2Dh10; 9; 8; 7; 6; 5; 4; 3; 2; 1 i,
a new ofﬁce assistant is hired only once, in the ﬁrst iteration. Given a list of ranks
A3Dh5; 2; 1; 8; 4; 7; 10; 9; 3; 6 i, a new ofﬁce assistant is hired three times,
upon interviewing the candidates with ranks 5,8,a n d 10. Recalling that the cost
of our algorithm depends on how many times we hire a new ofﬁce assistant, we
see that there are expensive inputs such as A1, inexpensive inputs such as A2,a n d
moderately expensive inputs such as A3.
Consider, on the other hand, the randomized algorithm that ﬁrst permutes the
candidates and then determines the best candidate. In this case, we randomize in
the algorithm, not in the input distribution. Given a particular input, say A3above,
we cannot say how many times the maximum is updated, because this quantitydiffers with each run of the algorithm. The ﬁrst time we run the algorithm on A
3,
it may produce the permutation A1and perform 10updates; but the second time
we run the algorithm, we may produce the permutation A2and perform only one
update. The third time we run it, we may perform some other number of updates.Each time we run the algorithm, the execution depends on the random choicesmade and is likely to differ from the previous execution of the algorithm. For thisalgorithm and many other randomized algorithms, no particular input elicits its
worst-case behavior . Even your worst enemy cannot produce a bad input array,
since the random permutation makes the input order irrelevant. The randomizedalgorithm performs badly only if the random-number generator produces an “un-lucky” permutation.
For the hiring problem, the only change needed in the code is to randomly per-
mute the array.124 Chapter 5 Probabilistic Analysis and Randomized Algorithms
RANDOMIZED -HIRE-ASSISTANT .n/
1 randomly permute the list of candidates
2bestD0 //candidate 0 is a least-qualiﬁed dummy candidate
3foriD1ton
4 interview candidate i
5 ifcandidate iis better than candidate best
6 bestDi
7 hire candidate i
With this simple change, we have created a randomized algorithm whose perfor-
mance matches that obtained by assuming that the candidates were presented in arandom order.
Lemma 5.3
The expected hiring cost of the procedure R
ANDOMIZED -HIRE-ASSISTANT is
O.c hlnn/.
Proof After permuting the input array, we have achieved a situation identical to
that of the probabilistic analysis of H IRE-ASSISTANT .
Comparing Lemmas 5.2 and 5.3 highlights the difference between probabilistic
analysis and randomized algorithms. In Lemma 5.2, we make an assumption aboutthe input. In Lemma 5.3, we make no such assumption, although randomizing theinput takes some additional time. To remain consistent with our terminology, we
couched Lemma 5.2 in terms of the average-case hiring cost and Lemma 5.3 in
terms of the expected hiring cost. In the remainder of this section, we discuss someissues involved in randomly permuting inputs.
Randomly permuting arrays
Many randomized algorithms randomize the input by permuting the given input
array. (There are other ways to use randomization.) Here, we shall discuss twomethods for doing so. We assume that we are given an array Awhich, without loss
of generality, contains the elements 1through n. Our goal is to produce a random
permutation of the array.
One common method is to assign each element AŒi/c141 of the array a random pri-
ority PŒ i/c141, and then sort the elements of Aaccording to these priorities. For ex-
ample, if our initial array is ADh1; 2; 3; 4iand we choose random priorities
PDh36; 3; 62; 19i, we would produce an array BDh2;4;1;3i, since the second
priority is the smallest, followed by the fourth, then the ﬁrst, and ﬁnally the third.
We call this procedure P
ERMUTE -BY-SORTING :5.3 Randomized algorithms 125
PERMUTE -BY-SORTING .A/
1nDA:length
2l e t PŒ 1::n /c141 be a new array
3foriD1ton
4 PŒ i/c141DRANDOM .1; n3/
5s o r t A,u s i n g Pas sort keys
Line 4 chooses a random number between 1andn3. We use a range of 1ton3
to make it likely that all the priorities in Pare unique. (Exercise 5.3-5 asks you
to prove that the probability that all entries are unique is at least 1/NUL1=n,a n d
Exercise 5.3-6 asks how to implement the algorithm even if two or more prioritiesare identical.) Let us assume that all the priorities are unique.
The time-consuming step in this procedure is the sorting in line 5. As we shall
see in Chapter 8, if we use a comparison sort, sorting takes /DEL.n lgn/time. We
can achieve this lower bound, since we have seen that merge sort takes ‚.n lgn/
time. (We shall see other comparison sorts that take ‚.n lgn/time in Part II.
Exercise 8.3-4 asks you to solve the very similar problem of sorting numbers in the
range 0ton
3/NUL1inO.n/ time.) After sorting, if PŒ i/c141 is the jth smallest priority,
thenAŒi/c141 lies in position jof the output. In this manner we obtain a permutation. It
remains to prove that the procedure produces a uniform random permutation ,t h a t
is, that the procedure is equally likely to produce every permutation of the numbers
1through n.
Lemma 5.4
Procedure P ERMUTE -BY-SORTING produces a uniform random permutation of the
input, assuming that all priorities are distinct.
Proof We start by considering the particular permutation in which each ele-
ment AŒi/c141 receives the ith smallest priority. We shall show that this permutation
occurs with probability exactly 1=nŠ .F o r iD1 ;2;:::;n ,l e tEibe the event
that element AŒi/c141 receives the ith smallest priority. Then we wish to compute the
probability that for all i,e v e n t Eioccurs, which is
PrfE1\E2\E3\/SOH/SOH/SOH\ En/NUL1\Eng:
Using Exercise C.2-5, this probability is equal to
PrfE1g/SOHPrfE2jE1g/SOHPrfE3jE2\E1g/SOHPrfE4jE3\E2\E1g
/SOH/SOH/SOHPrfEijEi/NUL1\Ei/NUL2\/SOH/SOH/SOH\ E1g/SOH/SOH/SOHPrfEnjEn/NUL1\/SOH/SOH/SOH\ E1g:
We have that PrfE1gD1=n because it is the probability that one priority
chosen randomly out of a set of nis the smallest priority. Next, we observe126 Chapter 5 Probabilistic Analysis and Randomized Algorithms
that PrfE2jE1gD1=.n/NUL1/because given that element AŒ1/c141 has the small-
est priority, each of the remaining n/NUL1elements has an equal chance of hav-
ing the second smallest priority. In general, for iD2;3 ;:::;n ,w eh a v et h a t
PrfEijEi/NUL1\Ei/NUL2\/SOH/SOH/SOH\ E1gD1=.n/NULiC1/, since, given that elements AŒ1/c141
through AŒi/NUL1/c141have the i/NUL1smallest priorities (in order), each of the remaining
n/NUL.i/NUL1/elements has an equal chance of having the ith smallest priority. Thus,
we have
PrfE1\E2\E3\/SOH/SOH/SOH\ En/NUL1\EngD/DC21
n/DC3/DC21
n/NUL1/DC3
/SOH/SOH/SOH/DC21
2/DC3/DC21
1/DC3
D1
nŠ;
and we have shown that the probability of obtaining the identity permutation
is1=nŠ .
We can extend this proof to work for any permutation of priorities. Consider
any ﬁxed permutation /ESCDh/ESC.1/; /ESC.2/; : : : ; /ESC.n/ iof the setf1 ;2;:::;ng.L e tu s
denote by rithe rank of the priority assigned to element AŒi/c141, where the element
with the jth smallest priority has rank j.I f w e d e ﬁ n e Eias the event in which
element AŒi/c141 receives the /ESC.i/th smallest priority, or riD/ESC.i/, the same proof
still applies. Therefore, if we calculate the probability of obtaining any particularpermutation, the calculation is identical to the one above, so that the probability ofobtaining this permutation is also 1=nŠ .
You might think that to prove that a permutation is a uniform random permuta-
tion, it sufﬁces to show that, for each element AŒi/c141, the probability that the element
winds up in position jis1=n. Exercise 5.3-4 shows that this weaker condition is,
in fact, insufﬁcient.
A better method for generating a random permutation is to permute the given
array in place. The procedure R ANDOMIZE -IN-PLACE does so in O.n/ time. In
itsith iteration, it chooses the element AŒi/c141 randomly from among elements AŒi/c141
through AŒn/c141. Subsequent to the ith iteration, AŒi/c141 is never altered.
RANDOMIZE -IN-PLACE .A/
1nDA:length
2foriD1ton
3s w a p AŒi/c141 withAŒRANDOM .i; n//c141
We shall use a loop invariant to show that procedure R ANDOMIZE -IN-PLACE
produces a uniform random permutation. A k-permutation on a set of nele-
ments is a sequence containing kof the nelements, with no repetitions. (See
Appendix C.) There are nŠ=.n/NULk/Šsuch possible k-permutations.5.3 Randomized algorithms 127
Lemma 5.5
Procedure R ANDOMIZE -IN-PLACE computes a uniform random permutation.
Proof We use the following loop invariant:
Just prior to the ith iteration of the forloop of lines 2–3, for each possible
.i/NUL1/-permutation of the nelements, the subarray AŒ1 : : i/NUL1/c141contains
this.i/NUL1/-permutation with probability .n/NULiC1/Š=nŠ .
We need to show that this invariant is true prior to the ﬁrst loop iteration, that each
iteration of the loop maintains the invariant, and that the invariant provides a usefulproperty to show correctness when the loop terminates.
Initialization: Consider the situation just before the ﬁrst loop iteration, so that
iD1. The loop invariant says that for each possible 0-permutation, the sub-
array AŒ1 : : 0/c141 contains this 0-permutation with probability .n/NULiC1/Š=nŠD
nŠ=nŠD1. The subarray AŒ1 : : 0/c141 is an empty subarray, and a 0-permutation
has no elements. Thus, AŒ1 : : 0/c141 contains any 0-permutation with probability 1,
and the loop invariant holds prior to the ﬁrst iteration.
Maintenance: We assume that just before the ith iteration, each possible
.i/NUL1/-permutation appears in the subarray AŒ1 : : i/NUL1/c141with probability
.n/NULiC1/Š=nŠ , and we shall show that after the ith iteration, each possible
i-permutation appears in the subarray AŒ1 : : i/c141 with probability .n/NULi/Š=nŠ .
Incrementing ifor the next iteration then maintains the loop invariant.
Let us examine the ith iteration. Consider a particular i
-permutation, and de-
note the elements in it by hx1;x2;:::;x ii. This permutation consists of an
.i/NUL1/-permutationhx1;:::;x i/NUL1ifollowed by the value xithat the algorithm
places in AŒi/c141.L e t E1denote the event in which the ﬁrst i/NUL1iterations have
created the particular .i/NUL1/-permutationhx1;:::;x i/NUL1iinAŒ1 : : i/NUL1/c141.B yt h e
loop invariant, PrfE1gD.n/NULiC1/Š=nŠ .L e t E2be the event that ith iteration
putsxiin position AŒi/c141.T h e i-permutationhx1;:::;x iiappears in AŒ1 : : i/c141 pre-
cisely when both E1andE2occur, and so we wish to compute Pr fE2\E1g.
Using equation (C.14), we have
PrfE2\E1gDPrfE2jE1gPrfE1g:
The probability PrfE2jE1gequals 1=.n/NULiC1/because in line 3 the algorithm
chooses xirandomly from the n/NULiC1values in positions AŒi : : n/c141 . Thus, we
have128 Chapter 5 Probabilistic Analysis and Randomized Algorithms
PrfE2\E1gDPrfE2jE1gPrfE1g
D1
n/NULiC1/SOH.n/NULiC1/Š
nŠ
D.n/NULi/Š
nŠ:
Termination: At termination, iDnC1, and we have that the subarray AŒ1 : : n/c141
is a given n-permutation with probability .n/NUL.nC1/C1/=nŠD0Š=nŠD1=nŠ .
Thus, R ANDOMIZE -IN-PLACE produces a uniform random permutation.
A randomized algorithm is often the simplest and most efﬁcient way to solve a
problem. We shall use randomized algorithms occasionally throughout this book.
Exercises
5.3-1
Professor Marceau objects to the loop invariant used in the proof of Lemma 5.5. Hequestions whether it is true prior to the ﬁrst iteration. He reasons that we could justas easily declare that an empty subarray contains no 0-permutations. Therefore,
the probability that an empty subarray contains a 0-permutation should be 0, thus
invalidating the loop invariant prior to the ﬁrst iteration. Rewrite the procedure
R
ANDOMIZE -IN-PLACE so that its associated loop invariant applies to a nonempty
subarray prior to the ﬁrst iteration, and modify the proof of Lemma 5.5 for yourprocedure.
5.3-2
Professor Kelp decides to write a procedure that produces at random any permuta-tion besides the identity permutation. He proposes the following procedure:
P
ERMUTE -WITHOUT -IDENTITY .A/
1nDA:length
2foriD1ton/NUL1
3s w a p AŒi/c141 withAŒRANDOM .iC1; n//c141
Does this code do what Professor Kelp intends?
5.3-3
Suppose that instead of swapping element AŒi/c141 with a random element from the
subarray AŒi : : n/c141 , we swapped it with a random element from anywhere in the
array:5.3 Randomized algorithms 129
PERMUTE -WITH-ALL.A/
1nDA:length
2foriD1ton
3s w a p AŒi/c141 withAŒRANDOM .1; n//c141
Does this code produce a uniform random permutation? Why or why not?
5.3-4
Professor Armstrong suggests the following procedure for generating a uniformrandom permutation:
P
ERMUTE -BY-CYCLIC .A/
1nDA:length
2l e t BŒ 1::n /c141 be a new array
3offsetDRANDOM .1; n/
4foriD1ton
5 destDiCoffset
6 ifdest>n
7 destDdest/NULn
8 BŒdest/c141DAŒi/c141
9return B
Show that each element AŒi/c141 has a 1=nprobability of winding up in any particular
position in B. Then show that Professor Armstrong is mistaken by showing that
the resulting permutation is not uniformly random.
5.3-5 ?
Prove that in the array Pin procedure P ERMUTE -BY-SORTING , the probability
that all elements are unique is at least 1/NUL1=n.
5.3-6
Explain how to implement the algorithm P ERMUTE -BY-SORTING to handle the
case in which two or more priorities are identical. That is, your algorithm should
produce a uniform random permutation, even if two or more priorities are identical.
5.3-7
Suppose we want to create a random sample of the setf1; 2; 3; : : : ; ng,t h a ti s ,
anm-element subset S,w h e r e 0/DC4m/DC4n, such that each m-subset is equally
likely to be created. One way would be to set AŒi/c141DiforiD1; 2; 3; : : : ; n ,
call R ANDOMIZE -IN-PLACE .A/, and then take just the ﬁrst marray elements.
This method would make ncalls to the R ANDOM procedure. If nis much larger
thanm, we can create a random sample with fewer calls to R ANDOM . Show that130 Chapter 5 Probabilistic Analysis and Randomized Algorithms
the following recursive procedure returns a random m-subset Soff1; 2; 3; : : : ; ng,
in which each m-subset is equally likely, while making only mcalls to R ANDOM :
RANDOM -SAMPLE .m; n/
1ifm==0
2 return;
3elseSDRANDOM -SAMPLE .m/NUL1; n/NUL1/
4 iDRANDOM .1; n/
5 ifi2S
6 SDS[fng
7 elseSDS[fig
8 return S
?5.4 Probabilistic analysis and further uses of indicator random variables
This advanced section further illustrates probabilistic analysis by way of four ex-
amples. The ﬁrst determines the probability that in a room of kpeople, two of
them share the same birthday. The second example examines what happens whenwe randomly toss balls into bins. The third investigates “streaks” of consecutive
heads when we ﬂip coins. The ﬁnal example analyzes a variant of the hiring prob-
lem in which you have to make decisions without actually interviewing all thecandidates.
5.4.1 The birthday paradox
Our ﬁrst example is the birthday paradox . How many people must there be in a
room before there is a 50% chance that two of them were born on the same day ofthe year? The answer is surprisingly few. The paradox is that it is in fact far fewerthan the number of days in a year, or even half the number of days in a year, as weshall see.
To answer this question, we index the people in the room with the integers
1 ;2;:::;k ,w h e r e kis the number of people in the room. We ignore the issue
of leap years and assume that all years have nD365days. For iD1 ;2;:::;k ,
letb
ibe the day of the year on which person i’s birthday falls, where 1/DC4bi/DC4n.
We also assume that birthdays are uniformly distributed across the ndays of the
year, so that PrfbiDrgD1=nforiD1 ;2;:::;k andrD1 ;2;:::;n .
The probability that two given people, say iandj, have matching birthdays
depends on whether the random selection of birthdays is independent. We assumefrom now on that birthdays are independent, so that the probability that i’s birthday5.4 Probabilistic analysis and further uses of indicator random variables 131
andj’s birthday both fall on day ris
PrfbiDrandbjDrgDPrfbiDrgPrfbjDrg
D1=n2:
Thus, the probability that they both fall on the same day is
PrfbiDbjgDnX
rD1PrfbiDrandbjDrg
DnX
rD1.1=n2/
D1=n : (5.6)
More intuitively, once biis chosen, the probability that bjis chosen to be the same
day is 1=n. Thus, the probability that iandjhave the same birthday is the same
as the probability that the birthday of one of them falls on a given day. Notice,however, that this coincidence depends on the assumption that the birthdays areindependent.
We can analyze the probability of at least 2out of kpeople having matching
birthdays by looking at the complementary event. The probability that at least twoof the birthdays match is 1minus the probability that all the birthdays are different.
The event that kpeople have distinct birthdays is
B
kDk\
iD1Ai;
where Aiis the event that person i’s birthday is different from person j’s for
allj< i . Since we can write BkDAk\Bk/NUL1, we obtain from equation (C.16)
the recurrence
PrfBkgDPrfBk/NUL1gPrfAkjBk/NUL1g; (5.7)
where we take PrfB1gDPrfA1gD1as an initial condition. In other words,
the probability that b1;b2;:::;b kare distinct birthdays is the probability that
b1;b2;:::;b k/NUL1are distinct birthdays times the probability that bk¤bifor
iD1 ;2;:::;k/NUL1, given that b1;b2;:::;b k/NUL1are distinct.
Ifb1;b2;:::;b k/NUL1are distinct, the conditional probability that bk¤bifor
iD1 ;2;:::;k/NUL1is PrfAkjBk/NUL1gD.n/NULkC1/=n , since out of the ndays,
n/NUL.k/NUL1/days are not taken. We iteratively apply the recurrence (5.7) to obtain132 Chapter 5 Probabilistic Analysis and Randomized Algorithms
PrfBkgDPrfBk/NUL1gPrfAkjBk/NUL1g
DPrfBk/NUL2gPrfAk/NUL1jBk/NUL2gPrfAkjBk/NUL1g
:::
DPrfB1gPrfA2jB1gPrfA3jB2g/SOH/SOH/SOHPrfAkjBk/NUL1g
D1/SOH/DC2n/NUL1
n/DC3/DC2n/NUL2
n/DC3
/SOH/SOH/SOH/DC2n/NULkC1
n/DC3
D1/SOH/DC2
1/NUL1
n/DC3/DC2
1/NUL2
n/DC3
/SOH/SOH/SOH/DC2
1/NULk/NUL1
n/DC3
:
Inequality (3.12), 1Cx/DC4ex,g i v e su s
PrfBkg/DC4e/NUL1=ne/NUL2=n/SOH/SOH/SOHe/NUL.k/NUL1/=n
De/NULPk/NUL1
iD1i=n
De/NULk.k/NUL1/=2n
/DC41=2
when/NULk.k/NUL1/=2n/DC4ln.1=2/ . The probability that all kbirthdays are distinct
is at most 1=2when k.k/NUL1//NAK2nln2or, solving the quadratic equation, when
k/NAK.1Cp
1C.8ln2/n/=2 .F o r nD365,w em u s th a v e k/NAK23. Thus, if at
least 23 people are in a room, the probability is at least 1=2that at least two people
have the same birthday. On Mars, a year is 669 Martian days long; it thereforetakes 31Martians to get the same effect.
An analysis using indicator random variables
We can use indicator random variables to provide a simpler but approximate anal-
ysis of the birthday paradox. For each pair .i; j / of the kpeople in the room, we
deﬁne the indicator random variable X
ij,f o r1/DC4i<j/DC4k,b y
XijDIfperson iand person jhave the same birthday g
D(
1if person iand person jhave the same birthday ;
0otherwise :
By equation (5.6), the probability that two people have matching birthdays is 1=n,
and thus by Lemma 5.1, we have
EŒXij/c141DPrfperson iand person jhave the same birthday g
D1=n :
Letting Xbe the random variable that counts the number of pairs of individuals
having the same birthday, we have5.4 Probabilistic analysis and further uses of indicator random variables 133
XDkX
iD1kX
jDiC1Xij:
Taking expectations of both sides and applying linearity of expectation, we obtain
EŒX/c141DE"kX
iD1kX
jDiC1Xij#
DkX
iD1kX
jDiC1EŒXij/c141
D 
k
2!
1
n
Dk.k/NUL1/
2n:
When k.k/NUL1//NAK2n, therefore, the expected number of pairs of people with the
same birthday is at least 1. Thus, if we have at leastp
2nC1individuals in a room,
we can expect at least two to have the same birthday. For nD365,i fkD28,t h e
expected number of pairs with the same birthday is .28/SOH27/=.2/SOH365//EM1:0356 .
Thus, with at least 28 people, we expect to ﬁnd at least one matching pair of birth-days. On Mars, where a year is 669Martian days long, we need at least 38Mar-
tians.
The ﬁrst analysis, which used only probabilities, determined the number of peo-
ple required for the probability to exceed 1=2 that a matching pair of birthdays
exists, and the second analysis, which used indicator random variables, determined
the number such that the expected number of matching birthdays is 1. Although
the exact numbers of people differ for the two situations, they are the same asymp-
totically: ‚.p
n/.
5.4.2 Balls and bins
Consider a process in which we randomly toss identical balls into bbins, numbered
1 ;2;:::;b . The tosses are independent, and on each toss the ball is equally likely
to end up in any bin. The probability that a tossed ball lands in any given bin is 1=b.
Thus, the ball-tossing process is a sequence of Bernoulli trials (see Appendix C.4)with a probability 1=b of success, where success means that the ball falls in the
given bin. This model is particularly useful for analyzing hashing (see Chapter 11),and we can answer a variety of interesting questions about the ball-tossing process.(Problem C-1 asks additional questions about balls and bins.)134 Chapter 5 Probabilistic Analysis and Randomized Algorithms
How many balls fall in a given bin? The number of balls that fall in a given bin
follows the binomial distribution b.kIn; 1=b/ .I f w e t o s s nballs, equation (C.37)
tells us that the expected number of balls that fall in the given bin is n=b.
How many balls must we toss, on the average, until a given bin contains a ball?
The number of tosses until the given bin receives a ball follows the geometricdistribution with probability 1=band, by equation (C.32), the expected number of
tosses until success is 1=.1=b/Db.
How many balls must we toss until every bin contains at least one ball? Let us
call a toss in which a ball falls into an empty bin a “hit.” We want to know the
expected number nof tosses required to get bhits.
Using the hits, we can partition the ntosses into stages. The ith stage consists of
the tosses after the .i/NUL1/st hit until the ith hit. The ﬁrst stage consists of the ﬁrst
toss, since we are guaranteed to have a hit when all bins are empty. For each tossduring the ith stage, i/NUL1bins contain balls and b/NULiC1bins are empty. Thus,
for each toss in the ith stage, the probability of obtaining a hit is .b/NULiC1/=b .
Letn
idenote the number of tosses in the ith stage. Thus, the number of tosses
required to get bhits is nDPb
iD1ni. Each random variable nihas a geometric
distribution with probability of success .b/NULiC1/=b and thus, by equation (C.32),
we have
EŒni/c141Db
b/NULiC1:
By linearity of expectation, we have
EŒn/c141DE"bX
iD1ni#
DbX
iD1EŒni/c141
DbX
iD1b
b/NULiC1
DbbX
iD11
i
Db.lnbCO.1// (by equation (A.7)) .
It therefore takes approximately blnbtosses before we can expect that every bin
has a ball. This problem is also known as the coupon collector’s problem ,w h i c h
says that a person trying to collect each of bdifferent coupons expects to acquire
approximately blnbrandomly obtained coupons in order to succeed.5.4 Probabilistic analysis and further uses of indicator random variables 135
5.4.3 Streaks
Suppose you ﬂip a fair coin ntimes. What is the longest streak of consecutive
heads that you expect to see? The answer is ‚.lgn/, as the following analysis
shows.
We ﬁrst prove that the expected length of the longest streak of heads is O.lgn/.
The probability that each coin ﬂip is a head is 1=2.L e t Aikbe the event that a
streak of heads of length at least kbegins with the ith coin ﬂip or, more precisely,
the event that the kconsecutive coin ﬂips i;iC1 ;:::;iCk/NUL1yield only heads,
where 1/DC4k/DC4nand1/DC4i/DC4n/NULkC1. Since coin ﬂips are mutually independent,
for any given event Aik, the probability that all kﬂips are heads is
PrfAikgD1=2k: (5.8)
ForkD2dlgne,
PrfAi;2dlgnegD1=22dlgne
/DC41=22lgn
D1=n2;
and thus the probability that a streak of heads of length at least 2dlgnebegins in
position iis quite small. There are at most n/NUL2dlgneC1positions where such
a streak can begin. The probability that a streak of heads of length at least 2dlgne
begins anywhere is therefore
Pr(n/NUL2dlgneC1[
iD1Ai;2dlgne)
/DC4n/NUL2dlgneC1X
iD11=n2
<nX
iD11=n2
D1=n ; (5.9)
since by Boole’s inequality (C.19), the probability of a union of events is at most
the sum of the probabilities of the individual events. (Note that Boole’s inequality
holds even for events such as these that are not independent.)
We now use inequality (5.9) to bound the length of the longest streak. For
jD0; 1; 2; : : : ; n ,l e tLjbe the event that the longest streak of heads has length ex-
actly j,a n dl e t Lbe the length of the longest streak. By the deﬁnition of expected
value, we have
EŒL/c141DnX
jD0jPrfLjg: (5.10)136 Chapter 5 Probabilistic Analysis and Randomized Algorithms
We could try to evaluate this sum using upper bounds on each Pr fLjgsimilar to
those computed in inequality (5.9). Unfortunately, this method would yield weakbounds. We can use some intuition gained by the above analysis to obtain a goodbound, however. Informally, we observe that for no individual term in the sum-mation in equation (5.10) are both the factors jand PrfL
jglarge. Why? When
j/NAK2dlgne,t h e nP rfLjgis very small, and when j< 2dlgne,t h e n jis fairly
small. More formally, we note that the events LjforjD0; 1; : : : ; n are disjoint,
and so the probability that a streak of heads of length at least 2dlgnebegins any-
where isPn
jD2dlgnePrfLjg. By inequality (5.9), we havePn
jD2dlgnePrfLjg<1 = n .
Also, noting thatPn
jD0PrfLjgD1,w eh a v et h a tP2dlgne/NUL1
jD0 PrfLjg/DC41. Thus,
we obtain
EŒL/c141DnX
jD0jPrfLjg
D2dlgne/NUL1X
jD0jPrfLjgCnX
jD2dlgnejPrfLjg
<2dlgne/NUL1X
jD0.2dlgne/PrfLjgCnX
jD2dlgnenPrfLjg
D2dlgne2dlgne/NUL1X
jD0PrfLjgCnnX
jD2dlgnePrfLjg
<2dlgne/SOH1Cn/SOH.1=n/
DO.lgn/ :
The probability that a streak of heads exceeds rdlgneﬂips diminishes quickly
withr.F o r r/NAK1, the probability that a streak of at least rdlgneheads starts in
position iis
PrfAi;rdlgnegD1=2rdlgne
/DC41=nr:
Thus, the probability is at most n=nrD1=nr/NUL1that the longest streak is at
leastrdlgne, or equivalently, the probability is at least 1/NUL1=nr/NUL1that the longest
streak has length less than rdlgne.
As an example, for nD1000 coin ﬂips, the probability of having a streak of at
least2dlgneD20heads is at most 1=nD1=1000 . The chance of having a streak
longer than 3dlgneD30heads is at most 1=n2D1=1,000,000.
We now prove a complementary lower bound: the expected length of the longest
streak of heads in ncoin ﬂips is /DEL.lgn/. To prove this bound, we look for streaks5.4 Probabilistic analysis and further uses of indicator random variables 137
of length sby partitioning the nﬂips into approximately n=s groups of sﬂips
each. If we choose sDb.lgn/=2c, we can show that it is likely that at least one
of these groups comes up all heads, and hence it is likely that the longest streakhas length at least sD/DEL.lgn/. We then show that the longest streak has expected
length /DEL.lgn/.
We partition the ncoin ﬂips into at least bn=b.lgn/=2ccgroups ofb.lgn/=2c
consecutive ﬂips, and we bound the probability that no group comes up all heads.
By equation (5.8), the probability that the group starting in position icomes up all
heads is
PrfA
i;b.lgn/=2 cgD1=2b.lgn/=2 c
/NAK1=p
n:
The probability that a streak of heads of length at least b.lgn/=2cdoes not begin
in position iis therefore at most 1/NUL1=p
n. Since thebn=b.lgn/=2ccgroups are
formed from mutually exclusive, independent coin ﬂips, the probability that everyone of these groups fails to be a streak of length b.lgn/=2cis at most
/NUL
1/NUL1=p
n/SOHbn=b.lgn/=2 cc/DC4/NUL
1/NUL1=p
n/SOHn=b.lgn/=2 c/NUL1
/DC4/NUL
1/NUL1=p
n/SOH2n= lgn/NUL1
/DC4e/NUL.2n= lgn/NUL1/=p
n
DO.e/NULlgn/
DO.1=n/ :
For this argument, we used inequality (3.12), 1Cx/DC4ex, and the fact, which you
might want to verify, that .2n= lgn/NUL1/=p
n/NAKlgnfor sufﬁciently large n.
Thus, the probability that the longest streak exceeds b.lgn/=2cis
nX
jDb.lgn/=2 cC1PrfLjg/NAK1/NULO.1=n/ : (5.11)
We can now calculate a lower bound on the expected length of the longest streak,
beginning with equation (5.10) and proceeding in a manner similar to our analysisof the upper bound:138 Chapter 5 Probabilistic Analysis and Randomized Algorithms
EŒL/c141DnX
jD0jPrfLjg
Db.lgn/=2 cX
jD0jPrfLjgCnX
jDb.lgn/=2 cC1jPrfLjg
/NAKb.lgn/=2 cX
jD00/SOHPrfLjgCnX
jDb.lgn/=2 cC1b.lgn/=2cPrfLjg
D0/SOHb.lgn/=2 cX
jD0PrfLjgCb.lgn/=2cnX
jDb.lgn/=2 cC1PrfLjg
/NAK0Cb.lgn/=2c.1/NULO.1=n// (by inequality (5.11))
D/DEL.lgn/ :
As with the birthday paradox, we can obtain a simpler but approximate analysis
using indicator random variables. We let XikDIfAikgbe the indicator random
variable associated with a streak of heads of length at least kbeginning with the
ith coin ﬂip. To count the total number of such streaks, we deﬁne
XDn/NULkC1X
iD1Xik:
Taking expectations and using linearity of expectation, we have
EŒX/c141DE"n/NULkC1X
iD1Xik#
Dn/NULkC1X
iD1EŒXik/c141
Dn/NULkC1X
iD1PrfAikg
Dn/NULkC1X
iD11=2k
Dn/NULkC1
2k:
By plugging in various values for k, we can calculate the expected number of
streaks of length k. If this number is large (much greater than 1), then we expect
many streaks of length kto occur and the probability that one occurs is high. If5.4 Probabilistic analysis and further uses of indicator random variables 139
this number is small (much less than 1), then we expect few streaks of length kto
occur and the probability that one occurs is low. If kDclgn, for some positive
constant c, we obtain
EŒX/c141Dn/NULclgnC1
2clgn
Dn/NULclgnC1
nc
D1
nc/NUL1/NUL.clgn/NUL1/=n
nc/NUL1
D‚.1=nc/NUL1/:
Ifcis large, the expected number of streaks of length clgnis small, and we con-
clude that they are unlikely to occur. On the other hand, if cD1=2, then we obtain
EŒX/c141D‚.1=n1=2/NUL1/D‚.n1=2/, and we expect that there are a large number
of streaks of length .1=2/ lgn. Therefore, one streak of such a length is likely to
occur. From these rough estimates alone, we can conclude that the expected lengthof the longest streak is ‚.lgn/.
5.4.4 The on-line hiring problem
As a ﬁnal example, we consider a variant of the hiring problem. Suppose now that
we do not wish to interview all the candidates in order to ﬁnd the best one. Wealso do not wish to hire and ﬁre as we ﬁnd better and better applicants. Instead, weare willing to settle for a candidate who is close to the best, in exchange for hiringexactly once. We must obey one company requirement: after each interview wemust either immediately offer the position to the applicant or immediately reject the
applicant. What is the trade-off between minimizing the amount of interviewing
and maximizing the quality of the candidate hired?
We can model this problem in the following way. After meeting an applicant,
we are able to give each one a score; let score .i/denote the score we give to the ith
applicant, and assume that no two applicants receive the same score. After we haveseenjapplicants, we know which of the jhas the highest score, but we do not
know whether any of the remaining n/NULjapplicants will receive a higher score. We
decide to adopt the strategy of selecting a positive integer k<n , interviewing and
then rejecting the ﬁrst kapplicants, and hiring the ﬁrst applicant thereafter who has
a higher score than all preceding applicants. If it turns out that the best-qualiﬁedapplicant was among the ﬁrst kinterviewed, then we hire the nth applicant. We
formalize this strategy in the procedure O
N-LINE-MAXIMUM .k; n/ , which returns
the index of the candidate we wish to hire.140 Chapter 5 Probabilistic Analysis and Randomized Algorithms
ON-LINE-MAXIMUM .k; n/
1bestscoreD/NUL1
2foriD1tok
3 ifscore .i/ > bestscore
4 bestscoreDscore .i/
5foriDkC1ton
6 ifscore .i/ > bestscore
7 return i
8return n
We wish to determine, for each possible value of k, the probability that we
hire the most qualiﬁed applicant. We then choose the best possible k,a n d
implement the strategy with that value. For the moment, assume that kis
ﬁxed. Let M.j/Dmax 1/DC4i/DC4jfscore .i/gdenote the maximum score among ap-
plicants 1through j.L e t Sbe the event that we succeed in choosing the best-
qualiﬁed applicant, and let Sibe the event that we succeed when the best-qualiﬁed
applicant is the ith one interviewed. Since the various Siare disjoint, we have
that PrfSgDPn
iD1PrfSig. Noting that we never succeed when the best-qualiﬁed
applicant is one of the ﬁrst k,w eh a v et h a tP rfSigD0foriD1 ;2;:::;k . Thus,
we obtain
PrfSgDnX
iDkC1PrfSig: (5.12)
We now compute Pr fSig. In order to succeed when the best-qualiﬁed applicant
is the ith one, two things must happen. First, the best-qualiﬁed applicant must be
in position i, an event which we denote by Bi. Second, the algorithm must not
select any of the applicants in positions kC1through i/NUL1, which happens only if,
for each jsuch that kC1/DC4j/DC4i/NUL1,w eﬁ n dt h a t score .j / < bestscore in line 6.
(Because scores are unique, we can ignore the possibility of score .j /Dbestscore .)
In other words, all of the values score .kC1/through score .i/NUL1/must be less
thanM.k/ ; if any are greater than M.k/ , we instead return the index of the ﬁrst
one that is greater. We use Oito denote the event that none of the applicants in
position kC1through i/NUL1are chosen. Fortunately, the two events BiandOi
are independent. The event Oidepends only on the relative ordering of the values
in positions 1through i/NUL1, whereas Bidepends only on whether the value in
position iis greater than the values in all other positions. The ordering of the
values in positions 1through i/NUL1does not affect whether the value in position i
is greater than all of them, and the value in position idoes not affect the ordering
of the values in positions 1through i/NUL1. Thus we can apply equation (C.15) to
obtain5.4 Probabilistic analysis and further uses of indicator random variables 141
PrfSigDPrfBi\OigDPrfBigPrfOig:
The probability PrfBigis clearly 1=n, since the maximum is equally likely to
be in any one of the npositions. For event Oito occur, the maximum value in
positions 1through i/NUL1, which is equally likely to be in any of these i/NUL1positions,
must be in one of the ﬁrst kpositions. Consequently, Pr fOigDk=.i/NUL1/and
PrfSigDk=.n.i/NUL1//. Using equation (5.12), we have
PrfSgDnX
iDkC1PrfSig
DnX
iDkC1k
n.i/NUL1/
Dk
nnX
iDkC11
i/NUL1
Dk
nn/NUL1X
iDk1
i:
We approximate by integrals to bound this summation from above and below. By
the inequalities (A.12), we have
Zn
k1
xdx/DC4n/NUL1X
iDk1
i/DC4Zn/NUL1
k/NUL11
xdx :
Evaluating these deﬁnite integrals gives us the bounds
k
n.lnn/NULlnk//DC4PrfSg/DC4k
n.ln.n/NUL1//NULln.k/NUL1// ;
which provide a rather tight bound for Pr fSg. Because we wish to maximize our
probability of success, let us focus on choosing the value of kthat maximizes the
lower bound on PrfSg. (Besides, the lower-bound expression is easier to maximize
than the upper-bound expression.) Differentiating the expression .k=n/. lnn/NULlnk/
with respect to k, we obtain
1
n.lnn/NULlnk/NUL1/ :
Setting this derivative equal to 0, we see that we maximize the lower bound on the
probability when ln kDlnn/NUL1Dln.n=e/ or, equivalently, when kDn=e. Thus,
if we implement our strategy with kDn=e, we succeed in hiring our best-qualiﬁed
applicant with probability at least 1=e.142 Chapter 5 Probabilistic Analysis and Randomized Algorithms
Exercises
5.4-1
How many people must there be in a room before the probability that someonehas the same birthday as you do is at least 1=2? How many people must there be
before the probability that at least two people have a birthday on July 4 is greater
than1=2?
5.4-2
Suppose that we toss balls into bbins until some bin contains two balls. Each toss
is independent, and each ball is equally likely to end up in any bin. What is the
expected number of ball tosses?
5.4-3 ?
For the analysis of the birthday paradox, is it important that the birthdays be mutu-ally independent, or is pairwise independence sufﬁcient? Justify your answer.
5.4-4 ?
How many people should be invited to a party in order to make it likely that there
arethree people with the same birthday?
5.4-5 ?
What is the probability that a k-string over a set of size nforms a k-permutation?
How does this question relate to the birthday paradox?
5.4-6 ?
Suppose that nballs are tossed into nbins, where each toss is independent and the
ball is equally likely to end up in any bin. What is the expected number of emptybins? What is the expected number of bins with exactly one ball?
5.4-7 ?
Sharpen the lower bound on streak length by showing that in nﬂips of a fair coin,
the probability is less than 1=nthat no streak longer than lg n/NUL2lg lgnconsecutive
heads occurs.Problems for Chapter 5 143
Problems
5-1 Probabilistic counting
With a b-bit counter, we can ordinarily only count up to 2b/NUL1. With R. Morris’s
probabilistic counting , we can count up to a much larger value at the expense of
some loss of precision.
We let a counter value of irepresent a count of niforiD0; 1; : : : ; 2b/NUL1,w h e r e
theniform an increasing sequence of nonnegative values. We assume that the ini-
tial value of the counter is 0, representing a count of n0D0.T h e I NCREMENT
operation works on a counter containing the value iin a probabilistic manner. If
iD2b/NUL1, then the operation reports an overﬂow error. Otherwise, the I NCRE -
MENT operation increases the counter by 1with probability 1=.n iC1/NULni/,a n di t
leaves the counter unchanged with probability 1/NUL1=.n iC1/NULni/.
If we select niDifor all i/NAK0, then the counter is an ordinary one. More
interesting situations arise if we select, say, niD2i/NUL1fori>0 orniDFi(the
ith Fibonacci number—see Section 3.2).
For this problem, assume that n2b/NUL1is large enough that the probability of an
overﬂow error is negligible.
a.Show that the expected value represented by the counter after nINCREMENT
operations have been performed is exactly n.
b.The analysis of the variance of the count represented by the counter depends
on the sequence of the ni. Let us consider a simple case: niD100i for
alli/NAK0. Estimate the variance in the value represented by the register after n
INCREMENT operations have been performed.
5-2 Searching an unsorted array
This problem examines three algorithms for searching for a value xin an unsorted
array Aconsisting of nelements.
Consider the following randomized strategy: pick a random index iintoA.I f
AŒi/c141Dx, then we terminate; otherwise, we continue the search by picking a new
random index into A. We continue picking random indices into Auntil we ﬁnd an
index jsuch that AŒj /c141Dxor until we have checked every element of A.N o t e
that we pick from the whole set of indices each time, so that we may examine agiven element more than once.
a.Write pseudocode for a procedure R
ANDOM -SEARCH to implement the strat-
egy above. Be sure that your algorithm terminates when all indices into Ahave
been picked.144 Chapter 5 Probabilistic Analysis and Randomized Algorithms
b.Suppose that there is exactly one index isuch that AŒi/c141Dx. What is the
expected number of indices into Athat we must pick before we ﬁnd xand
RANDOM -SEARCH terminates?
c.Generalizing your solution to part (b), suppose that there are k/NAK1indices i
such that AŒi/c141Dx. What is the expected number of indices into Athat we
must pick before we ﬁnd xand R ANDOM -SEARCH terminates? Your answer
should be a function of nandk.
d.Suppose that there are no indices isuch that AŒi/c141Dx. What is the expected
number of indices into Athat we must pick before we have checked all elements
ofAand R ANDOM -SEARCH terminates?
Now consider a deterministic linear search algorithm, which we refer to as
DETERMINISTIC -SEARCH . Speciﬁcally, the algorithm searches Aforxin order,
considering A Œ 1 /c141 ;A Œ 2 /c141 ;A Œ 3 /c141 ;:::;A Œ n /c141 until either it ﬁnds AŒi/c141Dxor it reaches
the end of the array. Assume that all possible permutations of the input array are
equally likely.
e.Suppose that there is exactly one index isuch that AŒi/c141Dx. What is the
average-case running time of D ETERMINISTIC -SEARCH ? What is the worst-
case running time of D ETERMINISTIC -SEARCH ?
f.Generalizing your solution to part (e), suppose that there are k/NAK1indices i
such that AŒi/c141Dx. What is the average-case running time of D ETERMINISTIC -
SEARCH ? What is the worst-case running time of D ETERMINISTIC -SEARCH ?
Your answer should be a function of nandk.
g.Suppose that there are no indices isuch that AŒi/c141Dx. What is the average-case
running time of D ETERMINISTIC -SEARCH ? What is the worst-case running
time of D ETERMINISTIC -SEARCH ?
Finally, consider a randomized algorithm S CRAMBLE -SEARCH that works by
ﬁrst randomly permuting the input array and then running the deterministic lin-
ear search given above on the resulting permuted array.
h.Letting kbe the number of indices isuch that AŒi/c141Dx, give the worst-case and
expected running times of S CRAMBLE -SEARCH for the cases in which kD0
andkD1. Generalize your solution to handle the case in which k/NAK1.
i.Which of the three searching algorithms would you use? Explain your answer.Notes for Chapter 5 145
Chapter notes
Bollob´ as [53], Hofri [174], and Spencer [321] contain a wealth of advanced prob-
abilistic techniques. The advantages of randomized algorithms are discussed andsurveyed by Karp [200] and Rabin [288]. The textbook by Motwani and Raghavan[262] gives an extensive treatment of randomized algorithms.
Several variants of the hiring problem have been widely studied. These problems
are more commonly referred to as “secretary problems.” An example of work inthis area is the paper by Ajtai, Meggido, and Waarts [11].II Sorting and Order StatisticsIntroduction
This part presents several algorithms that solve the following sorting problem :
Input: A sequence of nnumbersha1;a2;:::;a ni.
Output: A permutation (reordering) ha0
1;a0
2;:::;a0
niof the input sequence such
thata0
1/DC4a0
2/DC4/SOH/SOH/SOH/DC4 a0
n.
The input sequence is usually an n-element array, although it may be represented
in some other fashion, such as a linked list.
The structure of the data
In practice, the numbers to be sorted are rarely isolated values. Each is usually part
of a collection of data called a record . Each record contains a key, which is the
value to be sorted. The remainder of the record consists of satellite data , which are
usually carried around with the key. In practice, when a sorting algorithm permutesthe keys, it must permute the satellite data as well. If each record includes a large
amount of satellite data, we often permute an array of pointers to the records rather
than the records themselves in order to minimize data movement.
In a sense, it is these implementation details that distinguish an algorithm from
a full-blown program. A sorting algorithm describes the method by which we
determine the sorted order, regardless of whether we are sorting individual numbersor large records containing many bytes of satellite data. Thus, when focusing on theproblem of sorting, we typically assume that the input consists only of numbers.Translating an algorithm for sorting numbers into a program for sorting records148 Part II Sorting and Order Statistics
is conceptually straightforward, although in a given engineering situation other
subtleties may make the actual programming task a challenge.
Why sorting?
Many computer scientists consider sorting to be the most fundamental problem in
the study of algorithms. There are several reasons:
/SISometimes an application inherently needs to sort information. For example,in order to prepare customer statements, banks need to sort checks by checknumber.
/SIAlgorithms often use sorting as a key subroutine. For example, a program thatrenders graphical objects which are layered on top of each other might have
to sort the objects according to an “above” relation so that it can draw these
objects from bottom to top. We shall see numerous algorithms in this text thatuse sorting as a subroutine.
/SIWe can draw from among a wide variety of sorting algorithms, and they em-ploy a rich set of techniques. In fact, many important techniques used through-out algorithm design appear in the body of sorting algorithms that have beendeveloped over the years. In this way, sorting is also a problem of historicalinterest.
/SIWe can prove a nontrivial lower bound for sorting (as we shall do in Chapter 8).Our best upper bounds match the lower bound asymptotically, and so we knowthat our sorting algorithms are asymptotically optimal. Moreover, we can usethe lower bound for sorting to prove lower bounds for certain other problems.
/SIMany engineering issues come to the fore when implementing sorting algo-rithms. The fastest sorting program for a particular situation may depend onmany factors, such as prior knowledge about the keys and satellite data, thememory hierarchy (caches and virtual memory) of the host computer, and thesoftware environment. Many of these issues are best dealt with at the algorith-mic level, rather than by “tweaking” the code.
Sorting algorithms
We introduced two algorithms that sort nreal numbers in Chapter 2. Insertion sort
takes ‚.n
2/time in the worst case. Because its inner loops are tight, however,
it is a fast in-place sorting algorithm for small input sizes. (Recall that a sortingalgorithm sorts in place if only a constant number of elements of the input ar-
ray are ever stored outside the array.) Merge sort has a better asymptotic runningtime, ‚.n lgn/,b u tt h eM
ERGE procedure it uses does not operate in place.Part II Sorting and Order Statistics 149
In this part, we shall introduce two more algorithms that sort arbitrary real num-
bers. Heapsort, presented in Chapter 6, sorts nnumbers in place in O.n lgn/time.
It uses an important data structure, called a heap, with which we can also imple-ment a priority queue.
Quicksort, in Chapter 7, also sorts nnumbers in place, but its worst-case running
time is ‚.n
2/. Its expected running time is ‚.n lgn/, however, and it generally
outperforms heapsort in practice. Like insertion sort, quicksort has tight code, andso the hidden constant factor in its running time is small. It is a popular algorithm
for sorting large input arrays.
Insertion sort, merge sort, heapsort, and quicksort are all comparison sorts: they
determine the sorted order of an input array by comparing elements. Chapter 8 be-gins by introducing the decision-tree model in order to study the performance limi-tations of comparison sorts. Using this model, we prove a lower bound of /DEL.n lgn/
on the worst-case running time of any comparison sort on ninputs, thus showing
that heapsort and merge sort are asymptotically optimal comparison sorts.
Chapter 8 then goes on to show that we can beat this lower bound of /DEL.n lgn/
if we can gather information about the sorted order of the input by means other
than comparing elements. The counting sort algorithm, for example, assumes that
the input numbers are in the set f0; 1; : : : ; kg. By using array indexing as a tool
for determining relative order, counting sort can sort nnumbers in ‚.kCn/time.
Thus, when kDO.n/ , counting sort runs in time that is linear in the size of the
input array. A related algorithm, radix sort, can be used to extend the range of
counting sort. If there are nintegers to sort, each integer has ddigits, and each
digit can take on up to kpossible values, then radix sort can sort the numbers
in‚.d.nCk//time. When dis a constant and kisO.n/ , radix sort runs in
linear time. A third algorithm, bucket sort, requires knowledge of the probabilisticdistribution of numbers in the input array. It can sort nreal numbers uniformly
distributed in the half-open interval Œ0; 1/ in average-case O.n/ time.
The following table summarizes the running times of the sorting algorithms from
Chapters 2 and 6–8. As usual, ndenotes the number of items to sort. For counting
sort, the items to sort are integers in the set f0; 1; : : : ; kg. For radix sort, each item
is ad-digit number, where each digit takes on kpossible values. For bucket sort,
we assume that the keys are real numbers uniformly distributed in the half-openinterval Œ0; 1/ . The rightmost column gives the average-case or expected running
time, indicating which it gives when it differs from the worst-case running time.We omit the average-case running time of heapsort because we do not analyze it inthis book.150 Part II Sorting and Order Statistics
Worst-case Average-case/expected
Algorithm
 running time running time
Insertion sort
 ‚.n2/ ‚.n2/
Merge sort
 ‚.n lgn/ ‚.n lgn/
Heapsort
 O.n lgn/ —
Quicksort
 ‚.n2/ ‚.n lgn/ (expected)
Counting sort
 ‚.kCn/ ‚.kCn/
Radix sort
 ‚.d.nCk// ‚.d.nCk//
Bucket sort
 ‚.n2/ ‚.n/ (average-case)
Order statistics
Theith order statistic of a set of nnumbers is the ith smallest number in the set.
We can, of course, select the ith order statistic by sorting the input and indexing
theith element of the output. With no assumptions about the input distribution,
this method runs in /DEL.n lgn/time, as the lower bound proved in Chapter 8 shows.
In Chapter 9, we show that we can ﬁnd the ith smallest element in O.n/ time,
even when the elements are arbitrary real numbers. We present a randomized algo-rithm with tight pseudocode that runs in ‚.n
2/time in the worst case, but whose
expected running time is O.n/ . We also give a more complicated algorithm that
runs in O.n/ worst-case time.
Background
Although most of this part does not rely on difﬁcult mathematics, some sections
do require mathematical sophistication. In particular, analyses of quicksort, bucketsort, and the order-statistic algorithm use probability, which is reviewed in Ap-pendix C, and the material on probabilistic analysis and randomized algorithms inChapter 5. The analysis of the worst-case linear-time algorithm for order statis-tics involves somewhat more sophisticated mathematics than the other worst-caseanalyses in this part.6H e a p s o r t
In this chapter, we introduce another sorting algorithm: heapsort. Like merge sort,
but unlike insertion sort, heapsort’s running time is O.n lgn/. Like insertion sort,
but unlike merge sort, heapsort sorts in place: only a constant number of arrayelements are stored outside the input array at any time. Thus, heapsort combinesthe better attributes of the two sorting algorithms we have already discussed.
Heapsort also introduces another algorithm design technique: using a data struc-
ture, in this case one we call a “heap,” to manage information. Not only is the heapdata structure useful for heapsort, but it also makes an efﬁcient priority queue. Theheap data structure will reappear in algorithms in later chapters.
The term “heap” was originally coined in the context of heapsort, but it has since
come to refer to “garbage-collected storage,” such as the programming languages
Java and Lisp provide. Our heap data structure is notgarbage-collected storage,
and whenever we refer to heaps in this book, we shall mean a data structure ratherthan an aspect of garbage collection.
6.1 Heaps
The(binary) heap data structure is an array object that we can view as a
nearly complete binary tree (see Section B.5.3), as shown in Figure 6.1. Eachnode of the tree corresponds to an element of the array. The tree is com-pletely ﬁlled on all levels except possibly the lowest, which is ﬁlled from theleft up to a point. An array Athat represents a heap is an object with two at-
tributes: A:length , which (as usual) gives the number of elements in the array, and
A:heap -size, which represents how many elements in the heap are stored within
array A. That is, although AŒ1 : : A: length /c141may contain numbers, only the ele-
ments in AŒ1 : : A: heap -size/c141,w h e r e 0/DC4A:heap -size/DC4A:length , are valid ele-
ments of the heap. The root of the tree is AŒ1/c141, and given the index iof a node, we
can easily compute the indices of its parent, left child, and right child:152 Chapter 6 Heapsort
(a)1 6 1 4 1 0 8793241123456789 1 0
(b)1
23
45 67
89 1 016
14 10
87 93
241
Figure 6.1 A max-heap viewed as (a)ab i n a r yt r e ea n d (b)an array. The number within the circle
at each node in the tree is the value stored at that node. The number above a node is the corresponding
index in the array. Above and below the array are lines showing parent-child relationships; parents
are always to the left of their children. The tree has height three; the node at index 4 (with value 8)has height one.
PARENT .i/
1returnbi=2c
LEFT.i/
1return 2i
RIGHT.i/
1return 2iC1
On most computers, the L EFT procedure can compute 2iin one instruction by
simply shifting the binary representation of ileft by one bit position. Similarly, the
RIGHT procedure can quickly compute 2iC1by shifting the binary representation
ofileft by one bit position and then adding in a 1as the low-order bit. The
PARENT procedure can compute bi=2cby shifting iright one bit position. Good
implementations of heapsort often implement these procedures as “macros” or “in-line” procedures.
There are two kinds of binary heaps: max-heaps and min-heaps. In both kinds,
the values in the nodes satisfy a heap property , the speciﬁcs of which depend on
the kind of heap. In a max-heap ,t h emax-heap property is that for every node i
other than the root,
AŒP
ARENT .i//c141/NAKAŒi/c141 ;
that is, the value of a node is at most the value of its parent. Thus, the largest
element in a max-heap is stored at the root, and the subtree rooted at a node contains6.1 Heaps 153
values no larger than that contained at the node itself. A min-heap is organized in
the opposite way; the min-heap property is that for every node iother than the
root,
AŒPARENT .i//c141/DC4AŒi/c141 :
The smallest element in a min-heap is at the root.
For the heapsort algorithm, we use max-heaps. Min-heaps commonly imple-
ment priority queues, which we discuss in Section 6.5. We shall be precise in
specifying whether we need a max-heap or a min-heap for any particular applica-tion, and when properties apply to either max-heaps or min-heaps, we just use theterm “heap.”
Viewing a heap as a tree, we deﬁne the height of a node in a heap to be the
number of edges on the longest simple downward path from the node to a leaf, andwe deﬁne the height of the heap to be the height of its root. Since a heap of nele-
ments is based on a complete binary tree, its height is ‚.lgn/(see Exercise 6.1-2).
We shall see that the basic operations on heaps run in time at most proportionalto the height of the tree and thus take O.lgn/time. The remainder of this chapter
presents some basic procedures and shows how they are used in a sorting algorithmand a priority-queue data structure.
/SIThe M AX-HEAPIFY procedure, which runs in O.lgn/time, is the key to main-
taining the max-heap property.
/SIThe B UILD -MAX-HEAP procedure, which runs in linear time, produces a max-
heap from an unordered input array.
/SIThe H EAPSORT procedure, which runs in O.n lgn/time, sorts an array in
place.
/SIThe M AX-HEAP-INSERT ,H EAP-EXTRACT -MAX,H EAP-INCREASE -KEY,
and H EAP-MAXIMUM procedures, which run in O.lgn/time, allow the heap
data structure to implement a priority queue.
Exercises
6.1-1
What are the minimum and maximum numbers of elements in a heap of height h?
6.1-2
Show that an n-element heap has height blgnc.
6.1-3
Show that in any subtree of a max-heap, the root of the subtree contains the largestvalue occurrin
ganywhere in that subtree.154 Chapter 6 Heapsort
6.1-4
Where in a max-heap might the smallest element reside, assuming that all elementsare distinct?
6.1-5
Is an array that is in sorted order a min-heap?
6.1-6
Is the array with values h23; 17; 14; 6; 13; 10; 1; 5; 7; 12 ia max-heap?
6.1-7
Show that, with the array representation for storing an n-element heap, the leaves
are the nodes indexed by bn=2cC1;bn=2cC2;:::;n .
6.2 Maintaining the heap property
In order to maintain the max-heap property, we call the procedure M AX-HEAPIFY .
Its inputs are an array Aa n da ni n d e x iinto the array. When it is called, M AX-
HEAPIFY assumes that the binary trees rooted at L EFT.i/and R IGHT.i/are max-
heaps, but that AŒi/c141 might be smaller than its children, thus violating the max-heap
property. M AX-HEAPIFY lets the value at AŒi/c141 “ﬂoat down” in the max-heap so
that the subtree rooted at index iobeys the max-heap property.
MAX-HEAPIFY .A; i/
1lDLEFT.i/
2rDRIGHT.i/
3ifl/DC4A:heap -sizeandAŒl/c141 > AŒi/c141
4 largestDl
5elselargestDi
6ifr/DC4A:heap -sizeandAŒr/c141 > AŒ largest /c141
7 largestDr
8iflargest¤i
9 exchange AŒi/c141 withAŒlargest /c141
10 M AX-HEAPIFY .A;largest /
Figure 6.2 illustrates the action of M AX-HEAPIFY . At each step, the largest of
the elements AŒi/c141,AŒLEFT.i//c141,a n d AŒRIGHT.i//c141is determined, and its index is
stored in largest .I fAŒi/c141 is largest, then the subtree rooted at node iis already a
max-heap and the procedure terminates. Otherwise, one of the two children has thelargest element, and AŒi/c141 is swapped with AŒlargest /c141, which causes node iand its6.2 Maintaining the heap property 155
16
41 0
14 7 9
281
(a)16
14 10
47 93
281
(b)
16
14 10
87 93
241
(c)31
3
45 67
91 02
81
3
45 67
91 02
8
1
3
45 67
91 02
8i
i
i
Figure 6.2 The action of M AX-HEAPIFY .A; 2/ ,w h e r e A:heap -sizeD10.(a)The initial con-
ﬁguration, with AŒ2/c141 at node iD2violating the max-heap property since it is not larger than
both children. The max-heap property is restored for node 2in(b)by exchanging AŒ2/c141 withAŒ4/c141,
which destroys the max-heap property for node 4. The recursive call M AX-HEAPIFY .A; 4/ now
hasiD4. After swapping AŒ4/c141 withAŒ9/c141,a ss h o w ni n (c), node 4is ﬁxed up, and the recursive call
MAX-HEAPIFY .A; 9/ yields no further change to the data structure.
children to satisfy the max-heap property. The node indexed by largest ,h o w e v e r ,
now has the original value AŒi/c141, and thus the subtree rooted at largest might violate
the max-heap property. Consequently, we call M AX-HEAPIFY recursively on that
subtree.
The running time of M AX-HEAPIFY on a subtree of size nrooted at a given
node iis the ‚.1/ time to ﬁx up the relationships among the elements AŒi/c141,
AŒLEFT.i//c141,a n d AŒRIGHT.i//c141, plus the time to run M AX-HEAPIFY on a subtree
rooted at one of the children of node i(assuming that the recursive call occurs).
The children’s subtrees each have size at most 2n=3 —the worst case occurs when
the bottom level of the tree is exactly half full—and therefore we can describe the
running time of M AX-HEAPIFY by the recurrence
T .n//DC4T .2n=3/C‚.1/ :156 Chapter 6 Heapsort
The solution to this recurrence, by case 2 of the master theorem (Theorem 4.1),
isT .n/DO.lgn/. Alternatively, we can characterize the running time of M AX-
HEAPIFY on a node of height hasO.h/ .
Exercises
6.2-1
Using Figure 6.2 as a model, illustrate the operation of M AX-HEAPIFY .A; 3/ on
the array ADh27; 17; 3; 16; 13; 10; 1; 5; 7; 12; 4; 8; 9; 0 i.
6.2-2
Starting with the procedure M AX-HEAPIFY , write pseudocode for the procedure
MIN-HEAPIFY .A; i/ , which performs the corresponding manipulation on a min-
heap. How does the running time of M IN-HEAPIFY compare to that of M AX-
HEAPIFY ?
6.2-3
What is the effect of calling M AX-HEAPIFY .A; i/ when the element AŒi/c141 is larger
than its children?
6.2-4
What is the effect of calling M AX-HEAPIFY .A; i/ fori>A : heap -size=2?
6.2-5
The code for M AX-HEAPIFY is quite efﬁcient in terms of constant factors, except
possibly for the recursive call in line 10, which might cause some compilers to
produce inefﬁcient code. Write an efﬁcient M AX-HEAPIFY that uses an iterative
control construct (a loop) instead of recursion.
6.2-6
Show that the worst-case running time of M AX-HEAPIFY on a heap of size n
is/DEL.lgn/.(Hint: For a heap with nnodes, give node values that cause M AX-
HEAPIFY to be called recursively at every node on a simple path from the root
down to a leaf.)
6.3 Building a heap
We can use the procedure M AX-HEAPIFY in a bottom-up manner to convert an
array AŒ1 : : n/c141 ,w h e r e nDA:length , into a max-heap. By Exercise 6.1-7, the
elements in the subarray AŒ.bn=2cC1 /::n /c141 are all leaves of the tree, and so each is6.3 Building a heap 157
a1-element heap to begin with. The procedure B UILD -MAX-HEAP goes through
the remaining nodes of the tree and runs M AX-HEAPIFY on each one.
BUILD -MAX-HEAP.A/
1A:heap -sizeDA:length
2foriDbA:length =2cdownto 1
3M AX-HEAPIFY .A; i/
Figure 6.3 shows an example of the action of B UILD -MAX-HEAP.
To show why B UILD -MAX-HEAP works correctly, we use the following loop
invariant:
At the start of each iteration of the forloop of lines 2–3, each node iC1;
iC2 ;:::;n is the root of a max-heap.
We need to show that this invariant is true prior to the ﬁrst loop iteration, that each
iteration of the loop maintains the invariant, and that the invariant provides a usefulproperty to show correctness when the loop terminates.
Initialization: Prior to the ﬁrst iteration of the loop, iDbn=2c. Each node
bn=2cC1;bn=2cC2;:::;n is a leaf and is thus the root of a trivial max-heap.
Maintenance: To see that each iteration maintains the loop invariant, observe that
the children of node iare numbered higher than i. By the loop invariant, there-
fore, they are both roots of max-heaps. This is precisely the condition requiredfor the call M
AX-HEAPIFY .A; i/ to make node ia max-heap root. Moreover,
the M AX-HEAPIFY call preserves the property that nodes iC1; iC2;:::;n
are all roots of max-heaps. Decrementing iin the forloop update reestablishes
the loop invariant for the next iteration.
Termination: At termination, iD0. By the loop invariant, each node 1 ;2;:::;n
is the root of a max-heap. In particular, node 1is.
We can compute a simple upper bound on the running time of B UILD -MAX-
HEAP as follows. Each call to M AX-HEAPIFY costs O.lgn/time, and B UILD -
MAX-HEAP makes O.n/ such calls. Thus, the running time is O.n lgn/.T h i s
upper bound, though correct, is not asymptotically tight.
We can derive a tighter bound by observing that the time for M AX-HEAPIFY to
run at a node varies with the height of the node in the tree, and the heights of mostnodes are small. Our tighter analysis relies on the properties that an n-element heap
has heightblgnc(see Exercise 6.1-2) and at most˙
n=2
hC1/BEL
nodes of any height h
(see Exercise 6.3-3).
The time required by M AX-HEAPIFY when called on a node of height hisO.h/ ,
and so we can express the total cost of B UILD -MAX-HEAP as being bounded from
above by158 Chapter 6 Heapsort
1
23
45 67
89 1 01
23
45 67
89 1 01
23
45 67
89 1 01
23
45 67
89 1 01
23
45 67
89 1 01
23
45 67
89 1 04
13
29 1 0
14 8 7
(a)1641 2 3 16 9 10 14 8 7
4
13
29 1 0
14 8 7
(b)16
4
13
14 9 10
287
(c)164
11 0
14 9 3
287
(d)16
4
16 10
14 9 3
281
(e)716
14 10
89 3
241
(f)7A
ii
i i
i
Figure 6.3 The operation of B UILD -MAX-HEAP, showing the data structure before the call to
MAX-HEAPIFY in line 3 of B UILD -MAX-HEAP.(a)A 10-element input array Aand the bi-
nary tree it represents. The ﬁgure shows that the loop index irefers to node 5before the call
MAX-HEAPIFY .A; i/ .(b)The data structure that results. The loop index ifor the next iteration
refers to node 4.(c)–(e) Subsequent iterations of the forloop in B UILD -MAX-HEAP. Observe that
whenever M AX-HEAPIFY is called on a node, the two subtrees of that node are both max-heaps.
(f)The max-heap after B UILD -MAX-HEAPﬁnishes.6.4 The heapsort algorithm 159
blgncX
hD0ln
2hC1m
O.h/DO 
nblgncX
hD0h
2h!
:
We evalaute the last summation by substituting xD1=2 in the formula (A.8),
yielding
1X
hD0h
2hD1=2
.1/NUL1=2/2
D2:
Thus, we can bound the running time of B UILD -MAX-HEAP as
O 
nblgncX
hD0h
2h!
DO 
n1X
hD0h
2h!
DO.n/ :
Hence, we can build a max-heap from an unordered array in linear time.
We can build a min-heap by the procedure B UILD -MIN-HEAP, which is the
same as B UILD -MAX-HEAP but with the call to M AX-HEAPIFY in line 3 replaced
by a call to M IN-HEAPIFY (see Exercise 6.2-2). B UILD -MIN-HEAP produces a
min-heap from an unordered linear array in linear time.
Exercises
6.3-1
Using Figure 6.3 as a model, illustrate the operation of B UILD -MAX-HEAP on the
array ADh5; 3; 17; 10; 84; 19; 6; 22; 9 i.
6.3-2
Why do we want the loop index iin line 2 of B UILD -MAX-HEAP to decrease from
bA:length =2cto1rather than increase from 1tobA:length =2c?
6.3-3
Show that there are at most˙
n=2hC1/BEL
nodes of height hin any n-element heap.
6.4 The heapsort algorithm
The heapsort algorithm starts by using B UILD -MAX-HEAP to build a max-heap
on the input array AŒ1 : : n/c141 ,w h e r e nDA:length . Since the maximum element
of the array is stored at the root AŒ1/c141, we can put it into its correct ﬁnal position160 Chapter 6 Heapsort
by exchanging it with AŒn/c141. If we now discard node nfrom the heap—and we
can do so by simply decrementing A:heap -size—we observe that the children of
the root remain max-heaps, but the new root element might violate the max-heapproperty. All we need to do to restore the max-heap property, however, is call
M
AX-HEAPIFY .A; 1/ , which leaves a max-heap in AŒ1 : : n/NUL1/c141. The heapsort
algorithm then repeats this process for the max-heap of size n/NUL1down to a heap
of size 2. (See Exercise 6.4-2 for a precise loop invariant.)
HEAPSORT .A/
1B UILD -MAX-HEAP.A/
2foriDA:length downto 2
3 exchange AŒ1/c141 withAŒi/c141
4 A:heap -sizeDA:heap -size/NUL1
5M AX-HEAPIFY .A; 1/
Figure 6.4 shows an example of the operation of H EAPSORT after line 1 has built
the initial max-heap. The ﬁgure shows the max-heap before the ﬁrst iteration oftheforloop of lines 2–5 and after each iteration.
The H
EAPSORT procedure takes time O.n lgn/, since the call to B UILD -MAX-
HEAP takes time O.n/ and each of the n/NUL1calls to M AX-HEAPIFY takes
timeO.lgn/.
Exercises
6.4-1
Using Figure 6.4 as a model, illustrate the operation of H EAPSORT on the array
ADh5; 13; 2; 25; 7; 17; 20; 8; 4 i.
6.4-2
Argue the correctness of H EAPSORT using the following loop invariant:
At the start of each iteration of the forloop of lines 2–5, the subarray
AŒ1 : : i/c141 is a max-heap containing the ismallest elements of AŒ1 : : n/c141 ,a n d
the subarray AŒiC1::n /c141 contains the n/NULilargest elements of AŒ1 : : n/c141 ,
sorted.
6.4-3
What is the running time of H EAPSORT on an array Aof length nthat is already
sorted in increasing order? What about decreasing order?
6.4-4
Show that the worst-case running time of H EAPSORT is/DEL.n lgn/.6.4 The heapsort algorithm 161
(a) (b) (c)
(d) (e) (f)
(g) (h) (i)
(j) (k)1234789 1 0 1 4 1 6102
13
47 89
16 14
1
23
47 89
16 14 103
21
9 8 7 4
10 14 164
23
9 8 7 1
10 14 168
3 7
42 19
16 14 107
43
9 8 2 1
10 14 169
83
2 1 7 4
16 14 1010
89
3 1 7 4
16 14214
81 0
3 9 7 4
161 216
14 10
3 9 7 8
1 4 2
Aii
iii
iii
i
Figure 6.4 The operation of H EAPSORT .(a)The max-heap data structure just after B UILD -MAX-
HEAP has built it in line 1. (b)–(j) The max-heap just after each call of M AX-HEAPIFY in line 5,
showing the value of iat that time. Only lightly shaded nodes remain in the heap. (k)The resulting
sorted array A.162 Chapter 6 Heapsort
6.4-5 ?
Show that when all elements are distinct, the best-case running time of H EAPSORT
is/DEL.n lgn/.
6.5 Priority queues
Heapsort is an excellent algorithm, but a good implementation of quicksort, pre-
sented in Chapter 7, usually beats it in practice. Nevertheless, the heap data struc-ture itself has many uses. In this section, we present one of the most popular ap-plications of a heap: as an efﬁcient priority queue. As with heaps, priority queuescome in two forms: max-priority queues and min-priority queues. We will focushere on how to implement max-priority queues, which are in turn based on max-heaps; Exercise 6.5-3 asks you to write the procedures for min-priority queues.
Apriority queue is a data structure for maintaining a set Sof elements, each
with an associated value called a key.Amax-priority queue supports the following
operations:
I
NSERT .S; x/ inserts the element xinto the set S, which is equivalent to the oper-
ation SDS[fxg.
MAXIMUM .S/returns the element of Swith the largest key.
EXTRACT -MAX.S/removes and returns the element of Swith the largest key.
INCREASE -KEY. S;x;k/ increases the value of element x’s key to the new value k,
which is assumed to be at least as large as x’s current key value.
Among their other applications, we can use max-priority queues to schedule
jobs on a shared computer. The max-priority queue keeps track of the jobs tobe performed and their relative priorities. When a job is ﬁnished or interrupted,the scheduler selects the highest-priority job from among those pending by calling
E
XTRACT -MAX. The scheduler can add a new job to the queue at any time by
calling I NSERT .
Alternatively, a min-priority queue supports the operations I NSERT ,MINIMUM ,
EXTRACT -MIN,a n dD ECREASE -KEY. A min-priority queue can be used in an
event-driven simulator. The items in the queue are events to be simulated, eachwith an associated time of occurrence that serves as its key. The events must besimulated in order of their time of occurrence, because the simulation of an eventcan cause other events to be simulated in the future. The simulation program calls
E
XTRACT -MINat each step to choose the next event to simulate. As new events are
produced, the simulator inserts them into the min-priority queue by calling I NSERT .6.5 Priority queues 163
We shall see other uses for min-priority queues, highlighting the D ECREASE -KEY
operation, in Chapters 23 and 24.
Not surprisingly, we can use a heap to implement a priority queue. In a given ap-
plication, such as job scheduling or event-driven simulation, elements of a priorityqueue correspond to objects in the application. We often need to determine whichapplication object corresponds to a given priority-queue element, and vice versa.When we use a heap to implement a priority queue, therefore, we often need tostore a handle to the corresponding application object in each heap element. The
exact makeup of the handle (such as a pointer or an integer) depends on the ap-
plication. Similarly, we need to store a handle to the corresponding heap elementin each application object. Here, the handle would typically be an array index.Because heap elements change locations within the array during heap operations,an actual implementation, upon relocating a heap element, would also have to up-date the array index in the corresponding application object. Because the detailsof accessing application objects depend heavily on the application and its imple-mentation, we shall not pursue them here, other than noting that in practice, thesehandles do need to be correctly maintained.
Now we discuss how to implement the operations of a max-priority queue. The
procedure H
EAP-MAXIMUM implements the M AXIMUM operation in ‚.1/ time.
HEAP-MAXIMUM .A/
1return AŒ1/c141
The procedure H EAP-EXTRACT -MAXimplements the E XTRACT -MAXopera-
tion. It is similar to the forloop body (lines 3–5) of the H EAPSORT procedure.
HEAP-EXTRACT -MAX.A/
1ifA:heap -size<1
2 error “heap underﬂow”
3maxDAŒ1/c141
4AŒ1/c141DAŒA: heap -size/c141
5A:heap -sizeDA:heap -size/NUL1
6M AX-HEAPIFY .A; 1/
7return max
The running time of H EAP-EXTRACT -MAXisO.lgn/, since it performs only a
constant amount of work on top of the O.lgn/time for M AX-HEAPIFY .
The procedure H EAP-INCREASE -KEYimplements the I NCREASE -KEYopera-
tion. An index iinto the array identiﬁes the priority-queue element whose key we
wish to increase. The procedure ﬁrst updates the key of element AŒi/c141 to its new
value. Because increasing the key of AŒi/c141 might violate the max-heap property,164 Chapter 6 Heapsort
the procedure then, in a manner reminiscent of the insertion loop (lines 5–7) of
INSERTION -SORT from Section 2.1, traverses a simple path from this node toward
the root to ﬁnd a proper place for the newly increased key. As H EAP-INCREASE -
KEYtraverses this path, it repeatedly compares an element to its parent, exchang-
ing their keys and continuing if the element’s key is larger, and terminating if the el-ement’s key is smaller, since the max-heap property now holds. (See Exercise 6.5-5for a precise loop invariant.)
H
EAP-INCREASE -KEY.A; i; key/
1ifkey< AŒi/c141
2 error “new key is smaller than current key”
3AŒi/c141Dkey
4while i>1 andAŒPARENT .i//c141 < AŒi/c141
5 exchange AŒi/c141 withAŒPARENT .i//c141
6 iDPARENT .i/
Figure 6.5 shows an example of a H EAP-INCREASE -KEYoperation. The running
time of H EAP-INCREASE -KEYon an n-element heap is O.lgn/, since the path
traced from the node updated in line 3 to the root has length O.lgn/.
The procedure M AX-HEAP-INSERT implements the I NSERT operation. It takes
as an input the key of the new element to be inserted into max-heap A. The proce-
dure ﬁrst expands the max-heap by adding to the tree a new leaf whose key is /NUL1.
Then it calls H EAP-INCREASE -KEYto set the key of this new node to its correct
value and maintain the max-heap property.
MAX-HEAP-INSERT .A;key/
1A:heap -sizeDA:heap -sizeC1
2AŒA: heap -size/c141D/NUL1
3H EAP-INCREASE -KEY.A; A: heap -size;key/
The running time of M AX-HEAP-INSERT on an n-element heap is O.lgn/.
In summary, a heap can support any priority-queue operation on a set of size n
inO.lgn/time.
Exercises
6.5-1
Illustrate the operation of H EAP-EXTRACT -MAXon the heap ADh15; 13; 9; 5;
1 2;8 ;7 ;4;0 ;6 ;2;1i.6.5 Priority queues 165
16
14 10
87 93
241
(a)i16
14 10
87 93
2 15 1
(b)
16
14 10
879 3
215
1
(c)i
i16
1410
879 3
215
1
(d)i
Figure 6.5 The operation of H EAP-INCREASE -KEY.(a)The max-heap of Figure 6.4(a) with a
node whose index is iheavily shaded. (b)This node has its key increased to 15.(c)After one
iteration of the while loop of lines 4–6, the node and its parent have exchanged keys, and the index i
moves up to the parent. (d)The max-heap after one more iteration of the while loop. At this point,
AŒPARENT .i//c141/NAKAŒi/c141. The max-heap property now holds and the procedure terminates.
6.5-2
Illustrate the operation of M AX-HEAP-INSERT .A; 10/ on the heap ADh15; 13; 9;
5 ;1 2;8 ;7 ;4;0 ;6 ;2;1 i.
6.5-3
Write pseudocode for the procedures H EAP-MINIMUM ,H EAP-EXTRACT -MIN,
HEAP-DECREASE -KEY,a n dM IN-HEAP-INSERT that implement a min-priority
queue with a min-heap.
6.5-4
Why do we bother setting the key of the inserted node to /NUL1 in line 2 of M AX-
HEAP-INSERT when the next thing we do is increase its key to the desired value?166 Chapter 6 Heapsort
6.5-5
Argue the correctness of H EAP-INCREASE -KEYusing the following loop invari-
ant:
At the start of each iteration of the while loop of lines 4–6, the subarray
AŒ1 : : A: heap -size/c141satisﬁes the max-heap property, except that there may
be one violation: AŒi/c141 may be larger than AŒPARENT .i//c141.
You may assume that the subarray AŒ1 : : A: heap -size/c141satisﬁes the max-heap prop-
erty at the time H EAP-INCREASE -KEYis called.
6.5-6
Each exchange operation on line 5 of H EAP-INCREASE -KEYtypically requires
three assignments. Show how to use the idea of the inner loop of I NSERTION -
SORT to reduce the three assignments down to just one assignment.
6.5-7
Show how to implement a ﬁrst-in, ﬁrst-out queue with a priority queue. Showhow to implement a stack with a priority queue. (Queues and stacks are deﬁned inSection 10.1.)
6.5-8
The operation H
EAP-DELETE .A; i/ deletes the item in node ifrom heap A.G i v e
an implementation of H EAP-DELETE that runs in O.lgn/time for an n-element
max-heap.
6.5-9
Give an O.n lgk/-time algorithm to merge ksorted lists into one sorted list,
where nis the total number of elements in all the input lists. ( Hint: Use a min-
heap for k-way merging.)
Problems
6-1 Building a heap using insertion
We can build a heap by repeatedly calling M AX-HEAP-INSERT to insert the ele-
ments into the heap. Consider the following variation on the B UILD -MAX-HEAP
procedure:Problems for Chapter 6 167
BUILD -MAX-HEAP0.A/
1A:heap -sizeD1
2foriD2toA:length
3M AX-HEAP-INSERT .A; AŒi/c141/
a.Do the procedures B UILD -MAX-HEAP and B UILD -MAX-HEAP0always create
the same heap when run on the same input array? Prove that they do, or providea counterexample.
b.Show that in the worst case, B
UILD -MAX-HEAP0requires ‚.n lgn/time to
build an n-element heap.
6-2 Analysis of d-ary heaps
Ad-ary heap is like a binary heap, but (with one possible exception) non-leaf
nodes have dchildren instead of 2children.
a.How would you represent a d-ary heap in an array?
b.What is the height of a d-ary heap of nelements in terms of nandd?
c.Give an efﬁcient implementation of E XTRACT -MAXin ad-ary max-heap. An-
alyze its running time in terms of dandn.
d.Give an efﬁcient implementation of I NSERT in ad-ary max-heap. Analyze its
running time in terms of dandn.
e.Give an efﬁcient implementation of I NCREASE -KEY. A ;i;k/ , which ﬂags an
error if k < AŒi/c141 , but otherwise sets AŒi/c141Dkand then updates the d-ary max-
heap structure appropriately. Analyze its running time in terms of dandn.
6-3 Young tableaus
Anm/STXnYoung tableau is an m/STXnmatrix such that the entries of each row are
in sorted order from left to right and the entries of each column are in sorted order
from top to bottom. Some of the entries of a Young tableau may be 1,w h i c hw e
treat as nonexistent elements. Thus, a Young tableau can be used to hold r/DC4mn
ﬁnite numbers.
a.Draw a 4/STX4Young tableau containing the elements f9; 16; 3; 2; 4; 8; 5; 14; 12 g.
b.Argue that an m/STXnYoung tableau Yis empty if Y Œ1; 1/c141D1 . Argue that Y
is full (contains mnelements) if YŒ m ;n /c141<1.168 Chapter 6 Heapsort
c.Give an algorithm to implement E XTRACT -MINon a nonempty m/STXnYoung
tableau that runs in O.mCn/time. Your algorithm should use a recur-
sive subroutine that solves an m/STXnproblem by recursively solving either
an.m/NUL1//STXnor an m/STX.n/NUL1/subproblem. ( Hint: Think about M AX-
HEAPIFY .) Deﬁne T. p/ ,w h e r e pDmCn, to be the maximum running time
of E XTRACT -MINon any m/STXnYoung tableau. Give and solve a recurrence
forT. p/ that yields the O.mCn/time bound.
d.Show how to insert a new element into a nonfull m/STXnYoung tableau in
O.mCn/time.
e.Using no other sorting method as a subroutine, show how to use an n/STXnYoung
tableau to sort n2numbers in O.n3/time.
f.Give an O.mCn/-time algorithm to determine whether a given number is
stored in a given m/STXnYoung tableau.
Chapter notes
The heapsort algorithm was invented by Williams [357], who also described how
to implement a priority queue with a heap. The B UILD -MAX-HEAP procedure
was suggested by Floyd [106].
We use min-heaps to implement min-priority queues in Chapters 16, 23, and 24.
We also give an implementation with improved time bounds for certain operationsin Chapter 19 and, assuming that the keys are drawn from a bounded set of non-negative integers, Chapter 20.
If the data are b-bit integers, and the computer memory consists of addressable
b-bit words, Fredman and Willard [115] showed how to implement M
INIMUM in
O.1/ time and I NSERT and E XTRACT -MINinO.p
lgn/time. Thorup [337] has
improved the O.p
lgn/bound to O.lg lgn/time. This bound uses an amount of
space unbounded in n, but it can be implemented in linear space by using random-
ized hashing.
An important special case of priority queues occurs when the sequence of
EXTRACT -MINoperations is monotone , that is, the values returned by succes-
sive E XTRACT -MINoperations are monotonically increasing over time. This case
arises in several important applications, such as Dijkstra’s single-source shortest-paths algorithm, which we discuss in Chapter 24, and in discrete-event simula-tion. For Dijkstra’s algorithm it is particularly important that the D
ECREASE -KEY
operation be implemented efﬁciently. For the monotone case, if the data are in-tegers in the range 1 ;2;:::;C , Ahuja, Mehlhorn, Orlin, and Tarjan [8] describeNotes for Chapter 6 169
how to implement E XTRACT -MINand I NSERT inO.lgC/amortized time (see
Chapter 17 for more on amortized analysis) and D ECREASE -KEYinO.1/ time,
using a data structure called a radix heap. The O.lgC/bound can be improved
toO.p
lgC/using Fibonacci heaps (see Chapter 19) in conjunction with radix
heaps. Cherkassky, Goldberg, and Silverstein [65] further improved the bound toO.lg
1=3C/SIC/expected time by combining the multilevel bucketing structure of
Denardo and Fox [85] with the heap of Thorup mentioned earlier. Raman [291]further improved these results to obtain a bound of O.min.lg
1=4C/SIC;lg1=3C/SIn//,
for any ﬁxed /SI>0 .7 Quicksort
The quicksort algorithm has a worst-case running time of ‚.n2/on an input array
ofnnumbers. Despite this slow worst-case running time, quicksort is often the best
practical choice for sorting because it is remarkably efﬁcient on the average: itsexpected running time is ‚.n lgn/, and the constant factors hidden in the ‚.n lgn/
notation are quite small. It also has the advantage of sorting in place (see page 17),and it works well even in virtual-memory environments.
Section 7.1 describes the algorithm and an important subroutine used by quick-
sort for partitioning. Because the behavior of quicksort is complex, we start with
an intuitive discussion of its performance in Section 7.2 and postpone its precise
analysis to the end of the chapter. Section 7.3 presents a version of quicksort that
uses random sampling. This algorithm has a good expected running time, and no
particular input elicits its worst-case behavior. Section 7.4 analyzes the random-ized algorithm, showing that it runs in ‚.n
2/time in the worst case and, assuming
distinct elements, in expected O.n lgn/time.
7.1 Description of quicksort
Quicksort, like merge sort, applies the divide-and-conquer paradigm introduced
in Section 2.3.1. Here is the three-step divide-and-conquer process for sorting atypical subarray AŒp : : r/c141 :
Divide: Partition (rearrange) the array AŒp : : r/c141 into two (possibly empty) subar-
raysAŒp : : q/NUL1/c141andAŒqC1::r/c141 such that each element of AŒp : : q/NUL1/c141is
less than or equal to AŒq/c141, which is, in turn, less than or equal to each element
ofAŒqC1::r/c141 . Compute the index qas part of this partitioning procedure.
Conquer: Sort the two subarrays AŒp : : q/NUL1/c141andAŒqC1::r/c141 by recursive calls
to quicksort.7.1 Description of quicksort 171
Combine: Because the subarrays are already sorted, no work is needed to combine
them: the entire array AŒp : : r/c141 is now sorted.
The following procedure implements quicksort:
QUICKSORT . A ;p;r/
1ifp<r
2 qDPARTITION . A ;p;r/
3Q UICKSORT . A ;p;q/NUL1/
4Q UICKSORT .A; qC1; r/
To sort an entire array A, the initial call is Q UICKSORT . A ;1 ;A: length /.
Partitioning the array
The key to the algorithm is the P ARTITION procedure, which rearranges the subar-
rayAŒp : : r/c141 in place.
PARTITION . A ;p;r/
1xDAŒr/c141
2iDp/NUL1
3forjDptor/NUL1
4 ifAŒj /c141/DC4x
5 iDiC1
6 exchange AŒi/c141 withAŒj /c141
7 exchange AŒiC1/c141withAŒr/c141
8return iC1
F i g u r e7 . 1s h o w sh o wP ARTITION works on an 8-element array. P ARTITION
always selects an element xDAŒr/c141 as apivot element around which to partition the
subarray AŒp : : r/c141 . As the procedure runs, it partitions the array into four (possibly
empty) regions. At the start of each iteration of the forloop in lines 3–6, the regions
satisfy certain properties, shown in Figure 7.2. We state these properties as a loopinvariant:
At the beginning of each iteration of the loop of lines 3–6, for any array
index k,
1. Ifp/DC4k/DC4i,t h e n AŒk/c141/DC4x.
2. IfiC1/DC4k/DC4j/NUL1,t h e n AŒk/c141 > x .
3. IfkDr,t h e n AŒk/c141Dx.172 Chapter 7 Quicksort
28713564p,j ri
(a)
28713564p,i r j
(b)
28713564p,i r j
(c)
28713564p,i r j
(d)
28 71 3564pr j
(e)i
28 713 564pr j
(f)i
28 713 564pr j
(g)i
28 713 564pr
(h)i
28 7 13 56 4pr
(i)i
Figure 7.1 The operation of P ARTITION on a sample array. Array entry AŒr/c141 becomes the pivot
element x. Lightly shaded array elements are all in the ﬁrst partition with values no greater than x.
Heavily shaded elements are in the second partition with values greater than x. The unshaded el-
ements have not yet been put in one of the ﬁrst two partitions, and the ﬁnal white element is the
pivot x.(a)The initial array and variable settings. None of the elements have been placed in either
of the ﬁrst two partitions. (b)The value 2is “swapped with itself” and put in the partition of smaller
values. (c)–(d) The values 8and7are added to the partition of larger values. (e)The values 1and8
are swapped, and the smaller partition grows. (f)The values 3and7are swapped, and the smaller
partition grows. (g)–(h) The larger partition grows to include 5and6, and the loop terminates. (i)In
lines 7–8, the pivot element is swapped so that it lies between the two partitions.
The indices between jandr/NUL1are not covered by any of the three cases, and the
values in these entries have no particular relationship to the pivot x.
We need to show that this loop invariant is true prior to the ﬁrst iteration, that
each iteration of the loop maintains the invariant, and that the invariant provides auseful property to show correctness when the loop terminates.7.1 Description of quicksort 173
≤x >x unrestrictedxpi j r
Figure 7.2 The four regions maintained by the procedure P ARTITION on a subarray AŒp : : r/c141 .T h e
values in AŒp : : i/c141 are all less than or equal to x,t h ev a l u e si n AŒiC1::j/NUL1/c141are all greater than x,
andAŒr/c141Dx. The subarray A Œ j ::r/NUL1/c141can take on any values.
Initialization: Prior to the ﬁrst iteration of the loop, iDp/NUL1andjDp.B e -
cause no values lie between pandiand no values lie between iC1andj/NUL1,
the ﬁrst two conditions of the loop invariant are trivially satisﬁed. The assign-ment in line 1 satisﬁes the third condition.
Maintenance: As Figure 7.3 shows, we consider two cases, depending on the
outcome of the test in line 4. Figure 7.3(a) shows what happens when AŒj /c141 > x ;
the only action in the loop is to increment j. After jis incremented, condition 2
holds for AŒj/NUL1/c141and all other entries remain unchanged. Figure 7.3(b) shows
what happens when AŒj /c141/DC4x; the loop increments i,s w a p s AŒi/c141 andAŒj /c141 ,
and then increments j. Because of the swap, we now have that AŒi/c141/DC4x,a n d
condition 1is satisﬁed. Similarly, we also have that AŒj/NUL1/c141 > x , since the
item that was swapped into AŒj/NUL1/c141is, by the loop invariant, greater than x.
Termination: At termination, jDr. Therefore, every entry in the array is in one
of the three sets described by the invariant, and we have partitioned the values
in the array into three sets: those less than or equal to x, those greater than x,
and a singleton set containing x.
The ﬁnal two lines of P
ARTITION ﬁnish up by swapping the pivot element with
the leftmost element greater than x, thereby moving the pivot into its correct place
in the partitioned array, and then returning the pivot’s new index. The output of
PARTITION now satisﬁes the speciﬁcations given for the divide step. In fact, it
satisﬁes a slightly stronger condition: after line 2 of Q UICKSORT ,AŒq/c141 is strictly
less than every element of AŒqC1::r/c141 .
The running time of P ARTITION on the subarray AŒp : : r/c141 is‚.n/ ,w h e r e
nDr/NULpC1(see Exercise 7.1-3).
Exercises
7.1-1
Using Figure 7.1 as a model, illustrate the operation of P ARTITION on the array
ADh13; 19; 9; 5; 12; 8; 7; 4; 21; 2; 6; 11 i.174 Chapter 7 Quicksort
≤x >xxpi j r
>x (a)
≤x >xxpi j r
≤x >xxpi j r
≤x (b)
≤x >xxpi jr
Figure 7.3 The two cases for one iteration of procedure P ARTITION .(a)IfAŒj /c141 > x , the only
action is to increment j, which maintains the loop invariant. (b)IfAŒj /c141/DC4x, index iis incremented,
AŒi/c141 andAŒj /c141 are swapped, and then jis incremented. Again, the loop invariant is maintained.
7.1-2
What value of qdoes P ARTITION return when all elements in the array AŒp : : r/c141
have the same value? Modify P ARTITION so that qDb.pCr/=2cwhen all
elements in the array AŒp : : r/c141 have the same value.
7.1-3
Give a brief argument that the running time of P ARTITION on a subarray of size n
is‚.n/ .
7.1-4
How would you modify Q UICKSORT to sort into nonincreasing order?
7.2 Performance of quicksort
The running time of quicksort depends on whether the partitioning is balanced orunbalanced, which in turn depends on which elements are used for partitioning.
If the partitioning is balanced, the algorithm runs asymptotically as fast as merge7.2 Performance of quicksort 175
sort. If the partitioning is unbalanced, however, it can run asymptotically as slowly
as insertion sort. In this section, we shall informally investigate how quicksortperforms under the assumptions of balanced versus unbalanced partitioning.
Worst-case partitioning
The worst-case behavior for quicksort occurs when the partitioning routine pro-
duces one subproblem with n/NUL1elements and one with 0elements. (We prove
this claim in Section 7.4.1.) Let us assume that this unbalanced partitioning arisesin each recursive call. The partitioning costs ‚.n/ time. Since the recursive call
on an array of size 0just returns, T. 0 /D‚.1/ , and the recurrence for the running
time is
T .n/DT. n/NUL1/CT. 0 /C‚.n/
DT. n/NUL1/C‚.n/ :
Intuitively, if we sum the costs incurred at each level of the recursion, we get
an arithmetic series (equation (A.2)), which evaluates to ‚.n
2/. Indeed, it is
straightforward to use the substitution method to prove that the recurrence T .n/D
T. n/NUL1/C‚.n/ has the solution T .n/D‚.n2/. (See Exercise 7.2-1.)
Thus, if the partitioning is maximally unbalanced at every recursive level of the
algorithm, the running time is ‚.n2/. Therefore the worst-case running time of
quicksort is no better than that of insertion sort. Moreover, the ‚.n2/running time
occurs when the input array is already completely sorted—a common situation inwhich insertion sort runs in O.n/ time.
Best-case partitioning
In the most even possible split, P
ARTITION produces two subproblems, each of
size no more than n=2, since one is of size bn=2cand one of sizedn=2e/NUL1.I nt h i s
case, quicksort runs much faster. The recurrence for the running time is then
T .n/D2T .n=2/C‚.n/ ;
where we tolerate the sloppiness from ignoring the ﬂoor and ceiling and from sub-
tracting 1. By case 2 of the master theorem (Theorem 4.1), this recurrence has the
solution T .n/D‚.n lgn/. By equally balancing the two sides of the partition at
every level of the recursion, we get an asymptotically faster algorithm.
Balanced partitioning
The average-case running time of quicksort is much closer to the best case than to
the worst case, as the analyses in Section 7.4 will show. The key to understand-176 Chapter 7 Quicksort
n
cncncncn
/DC4cn/DC4cn
11
O.n lgn/log10n
log10=9n1
10n9
10n
1
100n9
100n9
100n81
100n
81
1000n729
1000n
Figure 7.4 A recursion tree for Q UICKSORT in which P ARTITION always produces a 9-to-1split,
yielding a running time of O.n lgn/. Nodes show subproblem sizes, with per-level costs on the right.
The per-level costs include the constant cimplicit in the ‚.n/ term.
ing why is to understand how the balance of the partitioning is reﬂected in the
recurrence that describes the running time.
Suppose, for example, that the partitioning algorithm always produces a 9-to-1
proportional split, which at ﬁrst blush seems quite unbalanced. We then obtain therecurrence
T .n/DT .9n=10/CT .n=10/Ccn ;
on the running time of quicksort, where we have explicitly included the constant c
hidden in the ‚.n/ term. Figure 7.4 shows the recursion tree for this recurrence.
Notice that every level of the tree has cost cn, until the recursion reaches a bound-
ary condition at depth log
10nD‚.lgn/, and then the levels have cost at most cn.
The recursion terminates at depth log10=9nD‚.lgn/. The total cost of quick-
sort is therefore O.n lgn/. Thus, with a 9-to-1proportional split at every level of
recursion, which intuitively seems quite unbalanced, quicksort runs in O.n lgn/
time—asymptotically the same as if the split were right down the middle. Indeed,
even a 99-to-1split yields an O.n lgn/running time. In fact, any split of constant
proportionality yields a recursion tree of depth ‚.lgn/, where the cost at each level
isO.n/ . The running time is therefore O.n lgn/whenever the split has constant
proportionality.7.2 Performance of quicksort 177
n
0 n–1
(n–1)/2 – 1 ( n–1)/2n
(n–1)/2
(a) (b)(n–1)/2Θ(n) Θ(n)
Figure 7.5 (a) Two levels of a recursion tree for qui cksort. The partitioni ng at the root costs n
and produces a “bad” split: two subarrays of sizes 0andn/NUL1. The partitioning of the subarray of
sizen/NUL1costs n/NUL1and produces a “good” split: subarrays of size .n/NUL1/=2/NUL1and.n/NUL1/=2.
(b)A single level of a recursion tree that is very well balanced. In both parts, the partitioning cost for
the subproblems shown with elliptical shading is ‚.n/ . Yet the subproblems remaining to be solved
in (a), shown with square shading, are no larger than the corresponding subproblems remaining to be
solved in (b).
Intuition for the average case
To develop a clear notion of the randomized behavior of quicksort, we must make
an assumption about how frequently we expect to encounter the various inputs.
The behavior of quicksort depends on the relative ordering of the values in the
array elements given as the input, and not by the particular values in the array. Asin our probabilistic analysis of the hiring problem in Section 5.2, we will assumefor now that all permutations of the input numbers are equally likely.
When we run quicksort on a random input array, the partitioning is highly un-
likely to happen in the same way at every level, as our informal analysis has as-sumed. We expect that some of the splits will be reasonably well balanced andthat some will be fairly unbalanced. For example, Exercise 7.2-6 asks you to showthat about 80 percent of the time P
ARTITION produces a split that is more balanced
than9to1, and about 20 percent of the time it produces a split that is less balanced
than9to1.
In the average case, P ARTITION produces a mix of “good” and “bad” splits. In a
recursion tree for an average-case execution of P ARTITION , the good and bad splits
are distributed randomly throughout the tree. Suppose, for the sake of intuition,
that the good and bad splits alternate levels in the tree, and that the good splits
are best-case splits and the bad splits are worst-case splits. Figure 7.5(a) shows
the splits at two consecutive levels in the recursion tree. At the root of the tree,the cost is nfor partitioning, and the subarrays produced have sizes n/NUL1and0:
the worst case. At the next level, the subarray of size n/NUL1undergoes best-case
partitioning into subarrays of size .n/NUL1/=2/NUL1and.n/NUL1/=2 . Let’s assume that
the boundary-condition cost is 1for the subarray of size 0.178 Chapter 7 Quicksort
The combination of the bad split followed by the good split produces three sub-
arrays of sizes 0,.n/NUL1/=2/NUL1,a n d .n/NUL1/=2 at a combined partitioning cost
of‚.n/C‚.n/NUL1/D‚.n/ . Certainly, this situation is no worse than that in
Figure 7.5(b), namely a single level of partitioning that produces two subarrays ofsize.n/NUL1/=2 , at a cost of ‚.n/ . Yet this latter situation is balanced! Intuitively,
the‚.n/NUL1/cost of the bad split can be absorbed into the ‚.n/ cost of the good
split, and the resulting split is good. Thus, the running time of quicksort, when lev-els alternate between good and bad splits, is like the running time for good splits
alone: still O.n lgn/, but with a slightly larger constant hidden by the O-notation.
We shall give a rigorous analysis of the expected running time of a randomizedversion of quicksort in Section 7.4.2.
Exercises
7.2-1
Use the substitution method to prove that the recurrence T .n/DT. n/NUL1/C‚.n/
has the solution T .n/D‚.n
2/, as claimed at the beginning of Section 7.2.
7.2-2
What is the running time of Q UICKSORT when all elements of array Ahave the
same value?
7.2-3
Show that the running time of Q UICKSORT is‚.n2/when the array Acontains
distinct elements and is sorted in decreasing order.
7.2-4
Banks often record transactions on an account in order of the times of the transac-
tions, but many people like to receive their bank statements with checks listed in
order by check number. People usually write checks in order by check number, andmerchants usually cash them with reasonable dispatch. The problem of convertingtime-of-transaction ordering to check-number ordering is therefore the problem ofsorting almost-sorted input. Argue that the procedure I
NSERTION -SORT would
tend to beat the procedure Q UICKSORT on this problem.
7.2-5
Suppose that the splits at every level of quicksort are in the proportion 1/NUL˛to˛,
where 0<˛/DC41=2is a constant. Show that the minimum depth of a leaf in the re-
cursion tree is approximately /NULlgn=lg˛and the maximum depth is approximately
/NULlgn=lg.1/NUL˛/. (Don’t worry about integer round-off.)7.3 A randomized version of quicksort 179
7.2-6 ?
Argue that for any constant 0<˛/DC41=2, the probability is approximately 1/NUL2˛
that on a random input array, P ARTITION produces a split more balanced than 1/NUL˛
to˛.
7.3 A randomized version of quicksort
In exploring the average-case behavior of quicksort, we have made an assumption
that all permutations of the input numbers are equally likely. In an engineeringsituation, however, we cannot always expect this assumption to hold. (See Exer-cise 7.2-4.) As we saw in Section 5.3, we can sometimes add randomization to analgorithm in order to obtain good expected performance over all inputs. Many peo-ple regard the resulting randomized version of quicksort as the sorting algorithmof choice for large enough inputs.
In Section 5.3, we randomized our algorithm by explicitly permuting the in-
put. We could do so for quicksort also, but a different randomization technique,
called random sampling , yields a simpler analysis. Instead of always using AŒr/c141
as the pivot, we will select a randomly chosen element from the subarray AŒp : : r/c141 .
We do so by ﬁrst exchanging element AŒr/c141 with an element chosen at random
from AŒp : : r/c141 . By randomly sampling the range p ;:::;r , we ensure that the pivot
element xDAŒr/c141 is equally likely to be any of the r/NULpC1elements in the
subarray. Because we randomly choose the pivot element, we expect the split of
the input array to be reasonably well balanced on average.
The changes to P
ARTITION and Q UICKSORT are small. In the new partition
procedure, we simply implement the swap before actually partitioning:
RANDOMIZED -PARTITION . A ;p;r/
1iDRANDOM .p; r/
2 exchange AŒr/c141 withAŒi/c141
3return PARTITION . A ;p;r/
The new quicksort calls R ANDOMIZED -PARTITION in place of P ARTITION :
RANDOMIZED -QUICKSORT . A ;p;r/
1ifp<r
2 qDRANDOMIZED -PARTITION . A ;p;r/
3R ANDOMIZED -QUICKSORT . A ;p;q/NUL1/
4R ANDOMIZED -QUICKSORT .A; qC1; r/
We analyze this algorithm in the next section.180 Chapter 7 Quicksort
Exercises
7.3-1
Why do we analyze the expected running time of a randomized algorithm and notits worst-case running time?
7.3-2
When R
ANDOMIZED -QUICKSORT runs, how many calls are made to the random-
number generator R ANDOM in the worst case? How about in the best case? Give
your answer in terms of ‚-notation.
7.4 Analysis of quicksort
Section 7.2 gave some intuition for the worst-case behavior of quicksort and for
why we expect it to run quickly. In this section, we analyze the behavior of quick-
sort more rigorously. We begin with a worst-case analysis, which applies to either
QUICKSORT or R ANDOMIZED -QUICKSORT , and conclude with an analysis of the
expected running time of R ANDOMIZED -QUICKSORT .
7.4.1 Worst-case analysis
We saw in Section 7.2 that a worst-case split at every level of recursion in quicksort
produces a ‚.n2/running time, which, intuitively, is the worst-case running time
of the algorithm. We now prove this assertion.
Using the substitution method (see Section 4.3), we can show that the running
time of quicksort is O.n2/.L e t T .n/ be the worst-case time for the procedure
QUICKSORT on an input of size n. We have the recurrence
T .n/Dmax
0/DC4q/DC4n/NUL1.T .q/CT. n/NULq/NUL1//C‚.n/ ; (7.1)
where the parameter qranges from 0ton/NUL1because the procedure P ARTITION
produces two subproblems with total size n/NUL1. We guess that T .n//DC4cn2for
some constant c. Substituting this guess into recurrence (7.1), we obtain
T .n//DC4 max
0/DC4q/DC4n/NUL1.cq2Cc.n/NULq/NUL1/2/C‚.n/
Dc/SOHmax
0/DC4q/DC4n/NUL1.q2C.n/NULq/NUL1/2/C‚.n/ :
The expression q2C.n/NULq/NUL1/2achieves a maximum over the parameter’s
range 0/DC4q/DC4n/NUL1at either endpoint. To verify this claim, note that the second
derivative of the expression with respect to qis positive (see Exercise 7.4-3). This7.4 Analysis of quicksort 181
observation gives us the bound max 0/DC4q/DC4n/NUL1.q2C.n/NULq/NUL1/2//DC4.n/NUL1/2D
n2/NUL2nC1. Continuing with our bounding of T .n/ , we obtain
T .n//DC4cn2/NULc.2n/NUL1/C‚.n/
/DC4cn2;
since we can pick the constant clarge enough so that the c.2n/NUL1/term dom-
inates the ‚.n/ term. Thus, T .n/DO.n2/. We saw in Section 7.2 a speciﬁc
case in which quicksort takes /DEL.n2/time: when partitioning is unbalanced. Al-
ternatively, Exercise 7.4-1 asks you to show that recurrence (7.1) has a solution ofT .n/D/DEL.n
2/. Thus, the (worst-case) running time of quicksort is ‚.n2/.
7.4.2 Expected running time
We have already seen the intuition behind why the expected running time of
RANDOMIZED -QUICKSORT isO.n lgn/: if, in each level of recursion, the split
induced by R ANDOMIZED -PARTITION puts any constant fraction of the elements
on one side of the partition, then the recursion tree has depth ‚.lgn/,a n d O.n/
work is performed at each level. Even if we add a few new levels with the most un-balanced split possible between these levels, the total time remains O.n lgn/.W e
can analyze the expected running time of R
ANDOMIZED -QUICKSORT precisely
by ﬁrst understanding how the partitioning procedure operates and then using this
understanding to derive an O.n lgn/bound on the expected running time. This
upper bound on the expected running time, combined with the ‚.n lgn/best-case
bound we saw in Section 7.2, yields a ‚.n lgn/expected running time. We assume
throughout that the values of the elements being sorted are distinct.
Running time and comparisons
The Q UICKSORT and R ANDOMIZED -QUICKSORT procedures differ only in how
they select pivot elements; they are the same in all other respects. We can therefore
couch our analysis of R ANDOMIZED -QUICKSORT by discussing the Q UICKSORT
and P ARTITION procedures, but with the assumption that pivot elements are se-
lected randomly from the subarray passed to R ANDOMIZED -PARTITION .
The running time of Q UICKSORT is dominated by the time spent in the P ARTI -
TION procedure. Each time the P ARTITION procedure is called, it selects a pivot
element, and this element is never included in any future recursive calls to Q UICK -
SORT and P ARTITION . Thus, there can be at most ncalls to P ARTITION over the
entire execution of the quicksort algorithm. One call to P ARTITION takes O.1/
time plus an amount of time that is proportional to the number of iterations of theforloop in lines 3–6. Each iteration of this forloop performs a comparison in
line 4, comparing the pivot element to another element of the array A. Therefore,182 Chapter 7 Quicksort
if we can count the total number of times that line 4 is executed, we can bound the
total time spent in the forloop during the entire execution of Q UICKSORT .
Lemma 7.1
LetXbe the number of comparisons performed in line 4 of P ARTITION over the
entire execution of Q UICKSORT on an n-element array. Then the running time of
QUICKSORT isO.nCX/.
Proof By the discussion above, the algorithm makes at most ncalls to P ARTI -
TION , each of which does a constant amount of work and then executes the for
loop some number of times. Each iteration of the forloop executes line 4.
Our goal, therefore, is to compute X, the total number of comparisons performed
in all calls to P ARTITION . We will not attempt to analyze how many comparisons
are made in each call to P ARTITION . Rather, we will derive an overall bound on the
total number of comparisons. To do so, we must understand when the algorithmcompares two elements of the array and when it does not. For ease of analysis, werename the elements of the array Aas´
1;´2;:::;´ n, with ´ibeing the ith smallest
element. We also deﬁne the set ZijDf´i;´iC1;:::;´ jgto be the set of elements
between ´iand´j,i n c l u s i v e .
When does the algorithm compare ´iand´j? To answer this question, we ﬁrst
observe that each pair of elements is compared at most once. Why? Elementsare compared only to the pivot element and, after a particular call of P
ARTITION
ﬁnishes, the pivot element used in that call is never again compared to any otherelements.
Our analysis uses indicator random variables (see Section 5.2). We deﬁne
X
ijDIf´iis compared to ´jg;
where we are considering whether the comparison takes place at any time during
the execution of the algorithm, not just during one iteration or one call of P ARTI -
TION . Since each pair is compared at most once, we can easily characterize the
total number of comparisons performed by the algorithm:
XDn/NUL1X
iD1nX
jDiC1Xij:
Taking expectations of both sides, and then using linearity of expectation and
Lemma 5.1, we obtain
EŒX/c141DE"n/NUL1X
iD1nX
jDiC1Xij#7.4 Analysis of quicksort 183
Dn/NUL1X
iD1nX
jDiC1EŒXij/c141
Dn/NUL1X
iD1nX
jDiC1Prf´iis compared to ´jg: (7.2)
It remains to compute Pr f´iis compared to ´jg. Our analysis assumes that the
RANDOMIZED -PARTITION procedure chooses each pivot randomly and indepen-
dently.
Let us think about when two items are notcompared. Consider an input to
quicksort of the numbers 1through 10(in any order), and suppose that the ﬁrst
pivot element is 7. Then the ﬁrst call to P ARTITION separates the numbers into two
sets:f1; 2; 3; 4; 5; 6gandf8; 9; 10g. In doing so, the pivot element 7is compared
to all other elements, but no number from the ﬁrst set (e.g., 2) is or ever will becompared to any number from the second set (e.g., 9).
In general, because we assume that element values are distinct, once a pivot x
is chosen with ´
i<x<´ j, we know that ´iand´jcannot be compared at any
subsequent time. If, on the other hand, ´iis chosen as a pivot before any other item
inZij,t h e n ´iwill be compared to each item in Zij, except for itself. Similarly,
if´jis chosen as a pivot before any other item in Zij,t h e n ´jwill be compared to
each item in Zij, except for itself. In our example, the values 7and9are compared
because 7is the ﬁrst item from Z7;9to be chosen as a pivot. In contrast, 2and9will
never be compared because the ﬁrst pivot element chosen from Z2;9is7. Thus, ´i
and´jare compared if and only if the ﬁrst element to be chosen as a pivot from Zij
is either ´ior´j.
We now compute the probability that this event occurs. Prior to the point at
which an element from Zijhas been chosen as a pivot, the whole set Zijis together
in the same partition. Therefore, any element of Zijis equally likely to be the ﬁrst
one chosen as a pivot. Because the set Zijhasj/NULiC1elements, and because pivots
are chosen randomly and independently, the probability that any given element isthe ﬁrst one chosen as a pivot is 1=.j/NULiC1/. Thus, we have
Prf´
iis compared to ´jgDPrf´ior´jis ﬁrst pivot chosen from Zijg
DPrf´iis ﬁrst pivot chosen from Zijg
CPrf´jis ﬁrst pivot chosen from Zijg
D1
j/NULiC1C1
j/NULiC1
D2
j/NULiC1: (7.3)184 Chapter 7 Quicksort
The second line follows because the two events are mutually exclusive. Combining
equations (7.2) and (7.3), we get that
EŒX/c141Dn/NUL1X
iD1nX
jDiC12
j/NULiC1:
We can evaluate this sum using a change of variables ( kDj/NULi) and the bound
on the harmonic series in equation (A.7):
EŒX/c141Dn/NUL1X
iD1nX
jDiC12
j/NULiC1
Dn/NUL1X
iD1n/NULiX
kD12
kC1
<n/NUL1X
iD1nX
kD12
k
Dn/NUL1X
iD1O.lgn/
DO.n lgn/ : (7.4)
Thus we conclude that, using R ANDOMIZED -PARTITION , the expected running
time of quicksort is O.n lgn/when element values are distinct.
Exercises
7.4-1
Show that in the recurrence
T .n/Dmax
0/DC4q/DC4n/NUL1.T .q/CT. n/NULq/NUL1//C‚.n/ ;
T .n/D/DEL.n2/.
7.4-2
Show that quicksort’s best-case running time is /DEL.n lgn/.
7.4-3
Show that the expression q2C.n/NULq/NUL1/2achieves a maximum over qD
0; 1; : : : ; n/NUL1when qD0orqDn/NUL1.
7.4-4
Show that R ANDOMIZED -QUICKSORT ’s expected running time is /DEL.n lgn/.Problems for Chapter 7 185
7.4-5
We can improve the running time of quicksort in practice by taking advantage of thefast running time of insertion sort when its input is “nearly” sorted. Upon callingquicksort on a subarray with fewer than kelements, let it simply return without
sorting the subarray. After the top-level call to quicksort returns, run insertion sorton the entire array to ﬁnish the sorting process. Argue that this sorting algorithmruns in O.nkCnlg.n=k// expected time. How should we pick k, both in theory
and in practice?
7.4-6 ?
Consider modifying the P
ARTITION procedure by randomly picking three elements
from array Aand partitioning about their median (the middle value of the three
elements). Approximate the probability of getting at worst an ˛-to-.1/NUL˛/split, as
a function of ˛in the range 0<˛<1 .
Problems
7-1 Hoare partition correctness
The version of P ARTITION given in this chapter is not the original partitioning
algorithm. Here is the original partition algorithm, which is due to C. A. R. Hoare:
HOARE -PARTITION . A ;p;r/
1xDAŒp/c141
2iDp/NUL1
3jDrC1
4while TRUE
5 repeat
6 jDj/NUL1
7 until AŒj /c141/DC4x
8 repeat
9 iDiC1
10 until AŒi/c141/NAKx
11 ifi<j
12 exchange AŒi/c141 withAŒj /c141
13 else return j
a.Demonstrate the operation of H OARE -PARTITION on the array ADh13; 19; 9;
5; 12; 8; 7; 4; 11; 2; 6; 21 i, showing the values of the array and auxiliary values
after each iteration of the while loop in lines 4–13.186 Chapter 7 Quicksort
The next three questions ask you to give a careful argument that the procedure
HOARE -PARTITION is correct. Assuming that the subarray AŒp : : r/c141 contains at
least two elements, prove the following:
b.The indices iandjare such that we never access an element of Aoutside the
subarray AŒp : : r/c141 .
c.When H OARE -PARTITION terminates, it returns a value jsuch that p/DC4j< r .
d.Every element of AŒp : : j /c141 is less than or equal to every element of AŒjC1::r/c141
when H OARE -PARTITION terminates.
The P ARTITION procedure in Section 7.1 separates the pivot value (originally
inAŒr/c141) from the two partitions it forms. The H OARE -PARTITION procedure, on
the other hand, always places the pivot value (originally in AŒp/c141 ) into one of the
two partitions AŒp : : j /c141 andAŒjC1::r/c141 .S i n c e p/DC4j< r , this split is always
nontrivial.
e.Rewrite the Q UICKSORT procedure to use H OARE -PARTITION .
7-2 Quicksort with equal element values
The analysis of the expected running time of randomized quicksort in Section 7.4.2assumes that all element values are distinct. In this problem, we examine whathappens when they are not.
a.Suppose that all element values are equal. What would be randomized quick-
sort’s running time in this case?
b.The P
ARTITION procedure returns an index qsuch that each element of
AŒp : : q/NUL1/c141is less than or equal to AŒq/c141 and each element of AŒqC1::r/c141
is greater than AŒq/c141. Modify the P ARTITION procedure to produce a procedure
PARTITION0. A ;p;r/ , which permutes the elements of AŒp : : r/c141 and returns two
indices qandt,w h e r e p/DC4q/DC4t/DC4r, such that
/SIall elements of AŒq : : t/c141 are equal,
/SIeach element of AŒp : : q/NUL1/c141is less than AŒq/c141,a n d
/SIeach element of AŒtC1::r/c141 is greater than AŒq/c141.
Like P ARTITION , your P ARTITION0procedure should take ‚.r/NULp/time.
c.Modify the R ANDOMIZED -QUICKSORT procedure to call P ARTITION0,a n d
name the new procedure R ANDOMIZED -QUICKSORT0. Then modify the
QUICKSORT procedure to produce a procedure Q UICKSORT0.p; r/ that callsProblems for Chapter 7 187
RANDOMIZED -PARTITION0and recurses only on partitions of elements not
known to be equal to each other.
d.Using Q UICKSORT0, how would you adjust the analysis in Section 7.4.2 to
avoid the assumption that all elements are distinct?
7-3 Alternative quicksort analysis
An alternative analysis of the running time of randomized quicksort focuses onthe expected running time of each individual recursive call to R
ANDOMIZED -
QUICKSORT , rather than on the number of comparisons performed.
a.Argue that, given an array of size n, the probability that any particular element
is chosen as the pivot is 1=n. Use this to deﬁne indicator random variables
XiDIfith smallest element is chosen as the pivot g.W h a ti sE ŒXi/c141?
b.LetT .n/ be a random variable denoting the running time of quicksort on an
array of size n. Argue that
EŒT .n//c141DE"nX
qD1Xq.T .q/NUL1/CT. n/NULq/C‚.n//#
: (7.5)
c.Show that we can rewrite equation (7.5) as
EŒT .n//c141D2
nn/NUL1X
qD2EŒT .q//c141C‚.n/ : (7.6)
d.Show that
n/NUL1X
kD2klgk/DC41
2n2lgn/NUL1
8n2: (7.7)
(Hint: Split the summation into two parts, one for kD2;3 ;:::;dn=2e/NUL1and
one for kDdn=2e;:::;n/NUL1.)
e.Using the bound from equation (7.7), show that the recurrence in equation (7.6)
has the solution E ŒT .n//c141D‚.n lgn/.(Hint: Show, by substitution, that
EŒT .n//c141/DC4anlgnfor sufﬁciently large nand for some positive constant a.)188 Chapter 7 Quicksort
7-4 Stack depth for quicksort
The Q UICKSORT algorithm of Section 7.1 contains two recursive calls to itself.
After Q UICKSORT calls P ARTITION , it recursively sorts the left subarray and then
it recursively sorts the right subarray. The second recursive call in Q UICKSORT
is not really necessary; we can avoid it by using an iterative control structure.This technique, called tail recursion , is provided automatically by good compilers.
Consider the following version of quicksort, which simulates tail recursion:
T
AIL-RECURSIVE -QUICKSORT . A ;p;r/
1while p<r
2 //Partition and sort left subarray.
3 qDPARTITION . A ;p;r/
4T AIL-RECURSIVE -QUICKSORT . A ;p;q/NUL1/
5 pDqC1
a.Argue that T AIL-RECURSIVE -QUICKSORT . A ;1 ;A: length /correctly sorts the
array A.
Compilers usually execute recursive procedures by using a stack that contains per-
tinent information, including the parameter values, for each recursive call. Theinformation for the most recent call is at the top of the stack, and the informationfor the initial call is at the bottom. Upon calling a procedure, its information ispushed onto the stack; when it terminates, its information is popped .S i n c e w e
assume that array parameters are represented by pointers, the information for each
procedure call on the stack requires O.1/ stack space. The stack depth is the max-
imum amount of stack space used at any time during a computation.
b.Describe a scenario in which T
AIL-RECURSIVE -QUICKSORT ’s stack depth is
‚.n/ on an n-element input array.
c.Modify the code for T AIL-RECURSIVE -QUICKSORT so that the worst-case
stack depth is ‚.lgn/. Maintain the O.n lgn/expected running time of the
algorithm.
7-5 Median-of-3 partition
One way to improve the R ANDOMIZED -QUICKSORT procedure is to partition
around a pivot that is chosen more carefully than by picking a random elementfrom the subarray. One common approach is the median-of-3 method: choose
the pivot as the median (middle element) of a set of 3 elements randomly selected
from the subarray. (See Exercise 7.4-6.) For this problem, let us assume that theelements in the input array AŒ1 : : n/c141 are distinct and that n/NAK3. We denote theProblems for Chapter 7 189
sorted output array by A0Œ 1::n /c141 . Using the median-of-3 method to choose the
pivot element x,d e ﬁ n e piDPrfxDA0Œi/c141g.
a.Give an exact formula for pia saf u n c t i o no f nandiforiD2;3 ;:::;n/NUL1.
(Note that p1DpnD0.)
b.By what amount have we increased the likelihood of choosing the pivot as
xDA0Œb.nC1/=2c/c141, the median of AŒ1 : : n/c141 , compared with the ordinary
implementation? Assume that n!1 , and give the limiting ratio of these
probabilities.
c.If we deﬁne a “good” split to mean choosing the pivot as xDA0Œi/c141,w h e r e
n=3/DC4i/DC42n=3 , by what amount have we increased the likelihood of getting
a good split compared with the ordinary implementation? ( Hint: Approximate
the sum by an integral.)
d.Argue that in the /DEL.n lgn/running time of quicksort, the median-of-3 method
affects only the constant factor.
7-6 Fuzzy sorting of intervals
Consider a sorting problem in which we do not know the numbers exactly. In-
stead, for each number, we know an interval on the real line to which it belongs.
That is, we are given nclosed intervals of the form Œai;bi/c141,w h e r e ai/DC4bi.W e
wish to fuzzy-sort these intervals, i.e., to produce a permutation hi1;i2;:::;i niof
the intervals such that for jD1 ;2;:::;n , there exist cj2Œaij;bij/c141satisfying
c1/DC4c2/DC4/SOH/SOH/SOH/DC4 cn.
a.Design a randomized algorithm for fuzzy-sorting nintervals. Your algorithm
should have the general structure of an algorithm that quicksorts the left end-points (the a
ivalues), but it should take advantage of overlapping intervals to
improve the running time. (As the intervals overlap more and more, the prob-lem of fuzzy-sorting the intervals becomes progressively easier. Your algorithmshould take advantage of such overlapping, to the extent that it exists.)
b.Argue that your algorithm runs in expected time ‚.n lgn/in general, but runs
in expected time ‚.n/ when all of the intervals overlap (i.e., when there exists a
value xsuch that x2Œa
i;bi/c141for all i). Your algorithm should not be checking
for this case explicitly; rather, its performance should naturally improve as theamount of overlap increases.190 Chapter 7 Quicksort
Chapter notes
The quicksort procedure was invented by Hoare [170]; Hoare’s version appears in
Problem 7-1. The P ARTITION procedure given in Section 7.1 is due to N. Lomuto.
The analysis in Section 7.4 is due to Avrim Blum. Sedgewick [305] and Bent-ley [43] provide a good reference on the details of implementation and how they
matter.
McIlroy [248] showed how to engineer a “killer adversary” that produces an
array on which virtually any implementation of quicksort takes ‚.n
2/time. If the
implementation is randomized, the adversary produces the array after seeing therandom choices of the quicksort algorithm.8 Sorting in Linear Time
We have now introduced several algorithms that can sort nnumbers in O.n lgn/
time. Merge sort and heapsort achieve this upper bound in the worst case; quicksortachieves it on average. Moreover, for each of these algorithms, we can produce asequence of ninput numbers that causes the algorithm to run in /DEL.n lgn/time.
These algorithms share an interesting property: the sorted order they determine
is based only on comparisons between the input elements . We call such sorting
algorithms comparison sorts . All the sorting algorithms introduced thus far are
comparison sorts.
In Section 8.1, we shall prove that any comparison sort must make /DEL.n lgn/
comparisons in the worst case to sort nelements. Thus, merge sort and heapsort
are asymptotically optimal, and no comparison sort exists that is faster by more
than a constant factor.
Sections 8.2, 8.3, and 8.4 examine three sorting algorithms—counting sort, radix
sort, and bucket sort—that run in linear time. Of course, these algorithms use
operations other than comparisons to determine the sorted order. Consequently,
the/DEL.n lgn/lower bound does not apply to them.
8.1 Lower bounds for sorting
In a comparison sort, we use only comparisons between elements to gain order
information about an input sequence ha1;a2;:::;a ni. That is, given two elements
aiandaj, we perform one of the tests ai<a j,ai/DC4aj,aiDaj,ai/NAKaj,o r
ai>a jto determine their relative order. We may not inspect the values of the
elements or gain order information about them in any other way.
In this section, we assume without loss of generality that all the input elements
are distinct. Given this assumption, comparisons of the form aiDajare useless,
so we can assume that no comparisons of this form are made. We also note thatthe comparisons a
i/DC4aj,ai/NAKaj,ai>a j,a n d ai<a jare all equivalent in that192 Chapter 8 Sorting in Linear Time
≤ >
≤ >1:2
2:3 1:3
〈1,2,3 〉 1:3 〈2,1,3 〉 2:3
〈1,3,2 〉 〈3,1,2 〉 〈3,2,1 〉≤ >
≤ >
≤ >
〈2,3,1 〉
Figure 8.1 The decision tree for insertion sort operating on three elements. An internal node an-
notated by i:jindicates a comparison between aiandaj. A leaf annotated by the permutation
h/EM.1/; /EM.2/; : : : ; /EM.n/ iindicates the ordering a/EM.1//DC4a/EM.2//DC4/SOH/SOH/SOH/DC4 a/EM.n/. The shaded path
indicates the decisions made when sorting the input sequence ha1D6; a2D8; a3D5i;t h e
permutationh3; 1; 2iat the leaf indicates that the sorted ordering is a3D5/DC4a1D6/DC4a2D8.
There are 3ŠD6possible permutations of the input elements, and so the decision tree must have at
least6leaves.
they yield identical information about the relative order of aiandaj. We therefore
assume that all comparisons have the form ai/DC4aj.
The decision-tree model
We can view comparison sorts abstractly in terms of decision trees. A decision
tree is a full binary tree that represents the comparisons between elements that
are performed by a particular sorting algorithm operating on an input of a givensize. Control, data movement, and all other aspects of the algorithm are ignored.Figure 8.1 shows the decision tree corresponding to the insertion sort algorithmfrom Section 2.1 operating on an input sequence of three elements.
In a decision tree, we annotate each internal node by i:jfor some iandjin the
range 1/DC4i;j/DC4n,w h e r e nis the number of elements in the input sequence. We
also annotate each leaf by a permutation h/EM.1/; /EM.2/; : : : ; /EM.n/ i. (See Section C.1
for background on permutations.) The execution of the sorting algorithm corre-sponds to tracing a simple path from the root of the decision tree down to a leaf.Each internal node indicates a comparison a
i/DC4aj. The left subtree then dictates
subsequent comparisons once we know that ai/DC4aj, and the right subtree dictates
subsequent comparisons knowing that ai>a j. When we come to a leaf, the sort-
ing algorithm has established the ordering a/EM.1//DC4a/EM.2//DC4/SOH/SOH/SOH/DC4 a/EM.n/. Because
any correct sorting algorithm must be able to produce each permutation of its input,each of the nŠpermutations on nelements must appear as one of the leaves of the
decision tree for a comparison sort to be correct. Furthermore, each of these leaves
must be reachable from the root by a downward path corresponding to an actual8.1 Lower bounds for sorting 193
execution of the comparison sort. (We shall refer to such leaves as “reachable.”)
Thus, we shall consider only decision trees in which each permutation appears asa reachable leaf.
A lower bound for the worst case
The length of the longest simple path from the root of a decision tree to any of
its reachable leaves represents the worst-case number of comparisons that the cor-responding sorting algorithm performs. Consequently, the worst-case number ofcomparisons for a given comparison sort algorithm equals the height of its decisiontree. A lower bound on the heights of all decision trees in which each permutationappears as a reachable leaf is therefore a lower bound on the running time of anycomparison sort algorithm. The following theorem establishes such a lower bound.
Theorem 8.1
Any comparison sort algorithm requires /DEL.n lgn/comparisons in the worst case.
Proof From the preceding discussion, it sufﬁces to determine the height of a
decision tree in which each permutation appears as a reachable leaf. Consider a
decision tree of height hwith lreachable leaves corresponding to a comparison
sort on nelements. Because each of the nŠpermutations of the input appears as
some leaf, we have nŠ/DC4l. Since a binary tree of height hhas no more than 2
h
leaves, we have
nŠ/DC4l/DC42h;
which, by taking logarithms, impliesh/NAKlg.nŠ/ (since the lg function is monotonically increasing)
D/DEL.n lgn/(by equation (3.19)) .
Corollary 8.2
Heapsort and merge sort are asymptotically optimal comparison sorts.
Proof TheO.n lgn/upper bounds on the running times for heapsort and merge
sort match the /DEL.n lgn/worst-case lower bound from Theorem 8.1.
Exercises
8.1-1
What is the smallest possible depth of a leaf in a decision tree for a comparisonsort?194 Chapter 8 Sorting in Linear Time
8.1-2
Obtain asymptotically tight bounds on lg .nŠ/ without using Stirling’s approxi-
mation. Instead, evaluate the summationPn
kD1lgkusing techniques from Sec-
tion A.2.
8.1-3
Show that there is no comparison sort whose running time is linear for at least halfof the nŠinputs of length n. What about a fraction of 1=nof the inputs of length n?
What about a fraction 1=2
n?
8.1-4
Suppose that you are given a sequence of nelements to sort. The input sequence
consists of n=k subsequences, each containing kelements. The elements in a given
subsequence are all smaller than the elements in the succeeding subsequence andlarger than the elements in the preceding subsequence. Thus, all that is needed tosort the whole sequence of length nis to sort the kelements in each of the n=k
subsequences. Show an /DEL.n lgk/lower bound on the number of comparisons
needed to solve this variant of the sorting problem. ( Hint: It is not rigorous to
simply combine the lower bounds for the individual subsequences.)
8.2 Counting sort
Counting sort assumes that each of the ninput elements is an integer in the range
0tok, for some integer k.W h e n kDO.n/ , the sort runs in ‚.n/ time.
Counting sort determines, for each input element x, the number of elements less
thanx. It uses this information to place element xdirectly into its position in the
output array. For example, if 17elements are less than x,t h e n xbelongs in output
position 18. We must modify this scheme slightly to handle the situation in which
several elements have the same value, since we do not want to put them all in thesame position.
In the code for counting sort, we assume that the input is an array AŒ1 : : n/c141 ,a n d
thusA:lengthDn. We require two other arrays: the array BŒ 1::n /c141 holds the
sorted output, and the array CŒ 0::k/c141 provides temporary working storage.8.2 Counting sort 195
2530230312345678
20230112345A
C
(a)224778C
(b)312345678
224678B
C
(c)
312345678
124678B
C
(d)0 312345678
124578B
C
(e)03
312345678
B
(f)030 223 50123450
123450 12345 0123450
Figure 8.2 The operation of C OUNTING -SORT on an input array A Œ 1::8 /c141 , where each element
ofAis a nonnegative integer no larger than kD5.(a)The array Aand the auxiliary array Cafter
line 5. (b)The array Cafter line 8. (c)–(e) The output array Band the auxiliary array Cafter one,
two, and three iterations of the loop in lines 10–12, respectively. Only the lightly shaded elements ofarray Bhave been ﬁlled in. (f)The ﬁnal sorted output array B.
COUNTING -SORT. A ;B;k/
1l e t CŒ 0::k/c141 b ean e wa r r a y
2foriD0tok
3 CŒ i/c141D0
4forjD1toA:length
5 C ŒAŒj /c141/c141DC ŒAŒj /c141/c141C1
6//CŒ i/c141 now contains the number of elements equal to i.
7foriD1tok
8 CŒ i/c141DCŒ i/c141CCŒ i/NUL1/c141
9//CŒ i/c141 now contains the number of elements less than or equal to i.
10forjDA:length downto 1
11 BŒC ŒAŒj /c141/c141/c141DAŒj /c141
12 C ŒAŒj /c141/c141DC ŒAŒj /c141/c141/NUL1
Figure 8.2 illustrates counting sort. After the forloop of lines 2–3 initializes the
array Cto all zeros, the forloop of lines 4–5 inspects each input element. If the
value of an input element is i, we increment CŒ i/c141. Thus, after line 5, CŒ i/c141 holds
the number of input elements equal to ifor each integer iD0; 1; : : : ; k . Lines 7–8
determine for each iD0; 1; : : : ; k how many input elements are less than or equal
toiby keeping a running sum of the array C.196 Chapter 8 Sorting in Linear Time
Finally, the forloop of lines 10–12 places each element AŒj /c141 into its correct
sorted position in the output array B.I fa l l nelements are distinct, then when we
ﬁrst enter line 10, for each AŒj /c141 ,t h ev a l u e C ŒAŒj /c141/c141 is the correct ﬁnal position
ofAŒj /c141 in the output array, since there are C ŒAŒj /c141/c141 elements less than or equal
toAŒj /c141 . Because the elements might not be distinct, we decrement C ŒAŒj /c141/c141 each
time we place a value AŒj /c141 into the Barray. Decrementing C ŒAŒj /c141/c141 causes the
next input element with a value equal to AŒj /c141 , if one exists, to go to the position
immediately before AŒj /c141 in the output array.
How much time does counting sort require? The forloop of lines 2–3 takes
time‚.k/ ,t h eforloop of lines 4–5 takes time ‚.n/ ,t h eforloop of lines 7–8 takes
time‚.k/ ,a n dt h e forloop of lines 10–12 takes time ‚.n/ . Thus, the overall time
is‚.kCn/. In practice, we usually use counting sort when we have kDO.n/ ,i n
which case the running time is ‚.n/ .
Counting sort beats the lower bound of /DEL.n lgn/proved in Section 8.1 because
it is not a comparison sort. In fact, no comparisons between input elements occur
anywhere in the code. Instead, counting sort uses the actual values of the elements
to index into an array. The /DEL.n lgn/lower bound for sorting does not apply when
we depart from the comparison sort model.
An important property of counting sort is that it is stable : numbers with the same
value appear in the output array in the same order as they do in the input array. Thatis, it breaks ties between two numbers by the rule that whichever number appears
ﬁrst in the input array appears ﬁrst in the output array. Normally, the property of
stability is important only when satellite data are carried around with the element
being sorted. Counting sort’s stability is important for another reason: countingsort is often used as a subroutine in radix sort. As we shall see in the next section,in order for radix sort to work correctly, counting sort must be stable.
Exercises
8.2-1
Using Figure 8.2 as a model, illustrate the operation of C
OUNTING -SORT on the
array ADh6; 0; 2; 0; 1; 3; 4; 6; 1; 3; 2 i.
8.2-2
Prove that C OUNTING -SORT is stable.
8.2-3
Suppose that we were to rewrite the forloop header in line 10 of the C OUNTING -
SORT as
10forjD1toA:length
Show that the algorithm still works properly. Is the modiﬁed algorithm stable?8.3 Radix sort 197
8.2-4
Describe an algorithm that, given nintegers in the range 0tok, preprocesses its
input and then answers any query about how many of the nintegers fall into a
range Œ a::b/c141 inO.1/ time. Your algorithm should use ‚.nCk/preprocessing
time.
8.3 Radix sort
Radix sort is the algorithm used by the card-sorting machines you now ﬁnd only in
computer museums. The cards have 80 columns, and in each column a machine canpunch a hole in one of 12 places. The sorter can be mechanically “programmed”to examine a given column of each card in a deck and distribute the card into oneof 12 bins depending on which place has been punched. An operator can thengather the cards bin by bin, so that cards with the ﬁrst place punched are on top ofcards with the second place punched, and so on.
For decimal digits, each column uses only 10 places. (The other two places
are reserved for encoding nonnumeric characters.) A d-digit number would then
occupy a ﬁeld of dcolumns. Since the card sorter can look at only one column
at a time, the problem of sorting ncards on a d-digit number requires a sorting
algorithm.
Intuitively, you might sort numbers on their most signiﬁcant digit, sort each of
the resulting bins recursively, and then combine the decks in order. Unfortunately,since the cards in 9of the 10bins must be put aside to sort each of the bins, this
procedure generates many intermediate piles of cards that you would have to keep
track of. (See Exercise 8.3-5.)
Radix sort solves the problem of card sorting—counterintuitively—by sorting on
theleast signiﬁcant digit ﬁrst. The algorithm then combines the cards into a single
deck, with the cards in the 0bin preceding the cards in the 1bin preceding the
cards in the 2bin, and so on. Then it sorts the entire deck again on the second-least
signiﬁcant digit and recombines the deck in a like manner. The process continues
until the cards have been sorted on all ddigits. Remarkably, at that point the cards
are fully sorted on the d-digit number. Thus, only dpasses through the deck are
required to sort. Figure 8.3 shows how radix sort operates on a “deck” of seven3-digit numbers.
In order for radix sort to work correctly, the digit sorts must be stable. The sort
performed by a card sorter is stable, but the operator has to be wary about notchanging the order of the cards as they come out of a bin, even though all the cardsin a bin have the same digit in the chosen column.198 Chapter 8 Sorting in Linear Time
329
457657839
436
720355
329457
657
839436720
355 329
457
657839436720
355329
457
657
839436
720355
Figure 8.3 The operation of radix sort on a list of seven 3-digit numbers. The leftmost column is
the input. The remaining columns show the list after successive sorts on increasingly signiﬁcant digit
positions. Shading indicates the digit position sorted on to produce each list from the previous one.
In a typical computer, which is a sequential random-access machine, we some-
times use radix sort to sort records of information that are keyed by multiple ﬁelds.
For example, we might wish to sort dates by three keys: year, month, and day. Wecould run a sorting algorithm with a comparison function that, given two dates,compares years, and if there is a tie, compares months, and if another tie occurs,compares days. Alternatively, we could sort the information three times with astable sort: ﬁrst on day, next on month, and ﬁnally on year.
The code for radix sort is straightforward. The following procedure assumes that
each element in the n-element array Ahasddigits, where digit 1is the lowest-order
digit and digit dis the highest-order digit.
R
ADIX -SORT.A; d/
1foriD1tod
2 use a stable sort to sort array Aon digit i
Lemma 8.3
Given nd-digit numbers in which each digit can take on up to kpossible values,
RADIX -SORT correctly sorts these numbers in ‚.d.nCk//time if the stable sort
it uses takes ‚.nCk/time.
Proof The correctness of radix sort follows by induction on the column being
sorted (see Exercise 8.3-3). The analysis of the running time depends on the stable
sort used as the intermediate sorting algorithm. When each digit is in the range 0
tok/NUL1(so that it can take on kpossible values), and kis not too large, counting sort
is the obvious choice. Each pass over nd-digit numbers then takes time ‚.nCk/.
There are dpasses, and so the total time for radix sort is ‚.d.nCk//.
When dis constant and kDO.n/ , we can make radix sort run in linear time.
More generally, we have some ﬂexibility in how to break each key into digits.8.3 Radix sort 199
Lemma 8.4
Given nb-bit numbers and any positive integer r/DC4b,RADIX -SORT correctly sorts
these numbers in ‚..b=r/.nC2r//time if the stable sort it uses takes ‚.nCk/
time for inputs in the range 0tok.
Proof For a value r/DC4b, we view each key as having dDdb=redigits of rbits
each. Each digit is an integer in the range 0 to 2r/NUL1, so that we can use counting
sort with kD2r/NUL1. (For example, we can view a 32-bit word as having four 8-bit
digits, so that bD32,rD8,kD2r/NUL1D255,a n d dDb=rD4.) Each pass of
counting sort takes time ‚.nCk/D‚.nC2r/and there are dpasses, for a total
running time of ‚.d.nC2r//D‚..b=r/.nC2r//.
For given values of nandb, we wish to choose the value of r, with r/DC4b,
that minimizes the expression .b=r/.nC2r/.I fb<blgnc, then for any value
ofr/DC4b,w eh a v et h a t .nC2r/D‚.n/ . Thus, choosing rDbyields a running
time of .b=b/.nC2b/D‚.n/ , which is asymptotically optimal. If b/NAKblgnc,
then choosing rDblgncgives the best time to within a constant factor, which
we can see as follows. Choosing rDblgncyields a running time of ‚.bn= lgn/.
As we increase raboveblgnc,t h e2rterm in the numerator increases faster than
therterm in the denominator, and so increasing raboveblgncyields a running
time of /DEL.bn= lgn/. If instead we were to decrease rbelowblgnc, then the b=r
term increases and the nC2rterm remains at ‚.n/ .
Is radix sort preferable to a comparison-based sorting algorithm, such as quick-
sort? If bDO.lgn/, as is often the case, and we choose r/EMlgn, then radix sort’s
running time is ‚.n/ , which appears to be better than quicksort’s expected running
time of ‚.n lgn/. The constant factors hidden in the ‚-notation differ, however.
Although radix sort may make fewer passes than quicksort over the nkeys, each
pass of radix sort may take signiﬁcantly longer. Which sorting algorithm we preferdepends on the characteristics of the implementations, of the underlying machine(e.g., quicksort often uses hardware caches more effectively than radix sort), andof the input data. Moreover, the version of radix sort that uses counting sort as theintermediate stable sort does not sort in place, which many of the ‚.n lgn/-time
comparison sorts do. Thus, when primary memory storage is at a premium, wemight prefer an in-place algorithm such as quicksort.
Exercises
8.3-1
Using Figure 8.3 as a model, illustrate the operation of R
ADIX -SORT on the fol-
lowing list of English words: COW, DOG, SEA, RUG, ROW, MOB, BOX, TAB,BAR, EAR, TAR, DIG, BIG, TEA, NOW, FOX.200 Chapter 8 Sorting in Linear Time
8.3-2
Which of the following sorting algorithms are stable: insertion sort, merge sort,heapsort, and quicksort? Give a simple scheme that makes any sorting algorithmstable. How much additional time and space does your scheme entail?
8.3-3
Use induction to prove that radix sort works. Where does your proof need theassumption that the intermediate sort is stable?
8.3-4
Show how to sort nintegers in the range 0ton
3/NUL1inO.n/ time.
8.3-5 ?
In the ﬁrst card-sorting algorithm in this section, exactly how many sorting passesare needed to sort d-digit decimal numbers in the worst case? How many piles of
cards would an operator need to keep track of in the worst case?
8.4 Bucket sort
Bucket sort assumes that the input is drawn from a uniform distribution and has an
average-case running time of O.n/ . Like counting sort, bucket sort is fast because
it assumes something about the input. Whereas counting sort assumes that the inputconsists of integers in a small range, bucket sort assumes that the input is generatedby a random process that distributes elements uniformly and independently overthe interval Œ0; 1/ . (See Section C.2 for a deﬁnition of uniform distribution.)
Bucket sort divides the interval Œ0; 1/ intonequal-sized subintervals, or buckets ,
and then distributes the ninput numbers into the buckets. Since the inputs are uni-
formly and independently distributed over Œ0; 1/ , we do not expect many numbers
to fall into each bucket. To produce the output, we simply sort the numbers in each
bucket and then go through the buckets in order, listing the elements in each.
Our code for bucket sort assumes that the input is an n-element array Aand
that each element AŒi/c141 in the array satisﬁes 0/DC4AŒi/c141 < 1 . The code requires an
auxiliary array BŒ 0::n/NUL1/c141of linked lists (buckets) and assumes that there is a
mechanism for maintaining such lists. (Section 10.2 describes how to implement
basic operations on linked lists.)8.4 Bucket sort 201
1
23
4
567
8
9
10.78
.17.39
.72
.94.21.12.23.68A
(a)1
23
4
567
8
9B
(b)0
.12 .17
.21 .23
.26.26
.39
.68
.72 .78
.94
Figure 8.4 The operation of B UCKET -SORT fornD10.(a)The input array AŒ1 : : 10/c141 .(b)The
array BŒ0::9/c141 of sorted lists (buckets) after line 8 of the algorithm. Bucket iholds values in the
half-open interval Œi=10; .iC1/=10/ . The sorted output consists of a concatenation in order of the
listsBŒ0/c141; BŒ1/c141; : : : ; BŒ9/c141 .
BUCKET -SORT.A/
1l e t BŒ 0::n/NUL1/c141b ean e wa r r a y
2nDA:length
3foriD0ton/NUL1
4m a k e BŒi/c141 an empty list
5foriD1ton
6 insert AŒi/c141 into list BŒbnAŒi/c141c/c141
7foriD0ton/NUL1
8 sort list BŒi/c141 with insertion sort
9 concatenate the lists BŒ0/c141; BŒ1/c141; : : : ; BŒn /NUL1/c141together in order
Figure 8.4 shows the operation of bucket sort on an input array of 10numbers.
To see that this algorithm works, consider two elements AŒi/c141 andAŒj /c141 . Assume
without loss of generality that AŒi/c141/DC4AŒj /c141 .S i n c ebnAŒi/c141c/DC4bnAŒj /c141c, either
element AŒi/c141 goes into the same bucket as AŒj /c141 or it goes into a bucket with a lower
index. If AŒi/c141 andAŒj /c141 go into the same bucket, then the forloop of lines 7–8 puts
them into the proper order. If AŒi/c141 andAŒj /c141 go into different buckets, then line 9
puts them into the proper order. Therefore, bucket sort works correctly.
To analyze the running time, observe that all lines except line 8 take O.n/ time
in the worst case. We need to analyze the total time taken by the ncalls to insertion
sort in line 8.202 Chapter 8 Sorting in Linear Time
To analyze the cost of the calls to insertion sort, let nibe the random variable
denoting the number of elements placed in bucket BŒi/c141. Since insertion sort runs
in quadratic time (see Section 2.2), the running time of bucket sort is
T .n/D‚.n/Cn/NUL1X
iD0O.n2
i/:
We now analyze the average-case running time of bucket sort, by computing the
expected value of the running time, where we take the expectation over the inputdistribution. Taking expectations of both sides and using linearity of expectation,we have
EŒT .n//c141DE"
‚.n/C
n/NUL1X
iD0O.n2
i/#
D‚.n/Cn/NUL1X
iD0E/STX
O.n2
i//ETX
(by linearity of expectation)
D‚.n/Cn/NUL1X
iD0O/NUL
E/STX
n2
i/ETX/SOH
(by equation (C.22)) . (8.1)
We claim that
E/STX
n2
i/ETX
D2/NUL1=n (8.2)
foriD0; 1; : : : ; n/NUL1. It is no surprise that each bucket ihas the same value of
EŒn2
i/c141, since each value in the input array Ais equally likely to fall in any bucket.
To prove equation (8.2), we deﬁne indicator random variables
XijDIfAŒj /c141 falls in bucket ig
foriD0; 1; : : : ; n/NUL1andjD1 ;2;:::;n . Thus,
niDnX
jD1Xij:
To compute E Œn2
i/c141, we expand the square and regroup terms:8.4 Bucket sort 203
E/STX
n2
i/ETX
DE" nX
jD1Xij!2#
DE"nX
jD1nX
kD1XijXik#
DE2
4nX
jD1X2
ijCX
1/DC4j/DC4nX
1/DC4k/DC4n
k¤jXijXik3
5
DnX
jD1E/STX
X2
ij/ETX
CX
1/DC4j/DC4nX
1/DC4k/DC4n
k¤jEŒXijXik/c141; (8.3)
where the last line follows by linearity of expectation. We evaluate the two sum-
mations separately. Indicator random variable Xijis1with probability 1=n and0
otherwise, and therefore
E/STX
X2
ij/ETX
D12/SOH1
nC02/SOH/DC2
1/NUL1
n/DC3
D1
n:
When k¤j, the variables XijandXikare independent, and hence
EŒXijXik/c141DEŒXij/c141EŒXik/c141
D1
n/SOH1
n
D1
n2:
Substituting these two expected values in equation (8.3), we obtain
E/STX
n2
i/ETX
DnX
jD11
nCX
1/DC4j/DC4nX
1/DC4k/DC4n
k¤j1
n2
Dn/SOH1
nCn.n/NUL1//SOH1
n2
D1Cn/NUL1
n
D2/NUL1
n;
which proves equation (8.2).204 Chapter 8 Sorting in Linear Time
Using this expected value in equation (8.1), we conclude that the average-case
running time for bucket sort is ‚.n/Cn/SOHO.2/NUL1=n/D‚.n/ .
Even if the input is not drawn from a uniform distribution, bucket sort may still
run in linear time. As long as the input has the property that the sum of the squaresof the bucket sizes is linear in the total number of elements, equation (8.1) tells usthat bucket sort will run in linear time.
Exercises
8.4-1
Using Figure 8.4 as a model, illustrate the operation of B
UCKET -SORT on the array
ADh:79; :13; :16; :64; :39; :20; :89; :53; :71; :42 i.
8.4-2
Explain why the worst-case running time for bucket sort is ‚.n2/. What simple
change to the algorithm preserves its linear average-case running time and makesits worst-case running time O.n lgn/?
8.4-3
LetXbe a random variable that is equal to the number of heads in two ﬂips of a
fair coin. What is E ŒX
2/c141?W h a ti sE2ŒX/c141?
8.4-4 ?
We are given npoints in the unit circle, piD.xi;yi/, such that 0<x2
iCy2
i/DC41
foriD1 ;2;:::;n . Suppose that the points are uniformly distributed; that is, the
probability of ﬁnding a point in any region of the circle is proportional to the area
of that region. Design an algorithm with an average-case running time of ‚.n/ to
sort the npoints by their distances diDp
x2
iCy2
ifrom the origin. ( Hint: Design
the bucket sizes in B UCKET -SORT to reﬂect the uniform distribution of the points
in the unit circle.)
8.4-5 ?
Aprobability distribution function P.x/ for a random variable Xis deﬁned
byP.x/DPrfX/DC4xg. Suppose that we draw a list of nrandom variables
X1;X2;:::;X nfrom a continuous probability distribution function Pthat is com-
putable in O.1/ time. Give an algorithm that sorts these numbers in linear average-
case time.Problems for Chapter 8 205
Problems
8-1 Probabilistic lower bounds on comparison sorting
In this problem, we prove a probabilistic /DEL.n lgn/lower bound on the running time
of any deterministic or randomized comparison sort on ndistinct input elements.
We begin by examining a deterministic comparison sort Awith decision tree TA.
We assume that every permutation of A’s inputs is equally likely.
a.Suppose that each leaf of TAis labeled with the probability that it is reached
given a random input. Prove that exactly nŠleaves are labeled 1=nŠ and that the
rest are labeled 0.
b.LetD.T / denote the external path length of a decision tree T;t h a ti s , D.T /
is the sum of the depths of all the leaves of T.L e t Tbe a decision tree with
k>1 leaves, and let LTandRTbe the left and right subtrees of T. Show that
D.T /DD.LT/CD.RT/Ck.
c.Letd.k/ be the minimum value of D.T / over all decision trees Twithk>1
leaves. Show that d.k/Dmin 1/DC4i/DC4k/NUL1fd.i/Cd.k/NULi/Ckg.(Hint: Consider
a decision tree Twithkleaves that achieves the minimum. Let i0be the number
of leaves in LTandk/NULi0the number of leaves in RT.)
d.Prove that for a given value of k>1 andiin the range 1/DC4i/DC4k/NUL1,t h e
function ilgiC.k/NULi/lg.k/NULi/is minimized at iDk=2. Conclude that
d.k/D/DEL.k lgk/.
e.Prove that D.T A/D/DEL.nŠ lg.nŠ// , and conclude that the average-case time to
sortnelements is /DEL.n lgn/.
Now, consider a randomized comparison sort B. We can extend the decision-
tree model to handle randomization by incorporating two kinds of nodes: ordinary
comparison nodes and “randomization” nodes. A randomization node models arandom choice of the form R
ANDOM .1; r/ made by algorithm B; the node has r
children, each of which is equally likely to be chosen during an execution of thealgorithm.
f.Show that for any randomized comparison sort B, there exists a deterministic
comparison sort Awhose expected number of comparisons is no more than
those made by B.206 Chapter 8 Sorting in Linear Time
8-2 Sorting in place in linear time
Suppose that we have an array of ndata records to sort and that the key of each
record has the value 0or1. An algorithm for sorting such a set of records might
possess some subset of the following three desirable characteristics:
1. The algorithm runs in O.n/ time.
2. The algorithm is stable.3. The algorithm sorts in place, using no more than a constant amount of storage
space in addition to the original array.
a.Give an algorithm that satisﬁes criteria 1 and 2 above.
b.Give an algorithm that satisﬁes criteria 1 and 3 above.
c.Give an algorithm that satisﬁes criteria 2 and 3 above.
d.Can you use any of your sorting algorithms from parts (a)–(c) as the sorting
method used in line 2 of R
ADIX -SORT,s ot h a tR ADIX -SORT sorts nrecords
withb-bit keys in O.bn/ time? Explain how or why not.
e.Suppose that the nrecords have keys in the range from 1tok.S h o w h o w t o
modify counting sort so that it sorts the records in place in O.nCk/time. You
may use O.k/ storage outside the input array. Is your algorithm stable? ( Hint:
How would you do it for kD3?)
8-3 Sorting variable-length items
a.You are given an array of integers, where different integers may have different
numbers of digits, but the total number of digits over allthe integers in the array
isn. Show how to sort the array in O.n/ time.
b.You are given an array of strings, where different strings may have different
numbers of characters, but the total number of characters over all the stringsisn. Show how to sort the strings in O.n/ time.
(Note that the desired order here is the standard alphabetical order; for example,
a<ab<b.)
8-4 Water jugs
Suppose that you are given nred and nblue water jugs, all of different shapes and
sizes. All red jugs hold different amounts of water, as do the blue ones. Moreover,for every red jug, there is a blue jug that holds the same amount of water, and viceversa.Problems for Chapter 8 207
Your task is to ﬁnd a grouping of the jugs into pairs of red and blue jugs that hold
the same amount of water. To do so, you may perform the following operation: picka pair of jugs in which one is red and one is blue, ﬁll the red jug with water, andthen pour the water into the blue jug. This operation will tell you whether the redor the blue jug can hold more water, or that they have the same volume. Assumethat such a comparison takes one time unit. Your goal is to ﬁnd an algorithm thatmakes a minimum number of comparisons to determine the grouping. Rememberthat you may not directly compare two red jugs or two blue jugs.
a.Describe a deterministic algorithm that uses ‚.n
2/comparisons to group the
jugs into pairs.
b.Prove a lower bound of /DEL.n lgn/for the number of comparisons that an algo-
rithm solving this problem must make.
c.Give a randomized algorithm whose expected number of comparisons is
O.n lgn/, and prove that this bound is correct. What is the worst-case num-
ber of comparisons for your algorithm?
8-5 Average sorting
Suppose that, instead of sorting an array, we just require that the elements increaseon average. More precisely, we call an n-element array Ak-sorted if, for all
iD1 ;2;:::;n/NULk, the following holds:
P
iCk/NUL1
jDiAŒj /c141
k/DC4PiCk
jDiC1AŒj /c141
k:
a.What does it mean for an array to be 1-sorted?
b.Give a permutation of the numbers 1 ;2;:::;1 0 that is 2-sorted, but not sorted.
c.Prove that an n-element array is k-sorted if and only if AŒi/c141/DC4AŒiCk/c141for all
iD1 ;2;:::;n/NULk.
d.Give an algorithm that k-sorts an n-element array in O.n lg.n=k// time.
We can also show a lower bound on the time to produce a k-sorted array, when k
is a constant.
e.Show that we can sort a k-sorted array of length ninO.n lgk/time. ( Hint:
Use the solution to Exercise 6.5-9. )
f.Show that when kis a constant, k-sorting an n-element array requires /DEL.n lgn/
time. ( Hint: Use the solution to the previous part along with the lower bound
on comparison sorts.)208 Chapter 8 Sorting in Linear Time
8-6 Lower bound on merging sorted lists
The problem of merging two sorted lists arises frequently. We have seen a pro-cedure for it as the subroutine M
ERGE in Section 2.3.1. In this problem, we will
prove a lower bound of 2n/NUL1on the worst-case number of comparisons required
to merge two sorted lists, each containing nitems.
First we will show a lower bound of 2n/NULo.n/ comparisons by using a decision
tree.
a.Given 2nnumbers, compute the number of possible ways to divide them into
two sorted lists, each with nnumbers.
b.Using a decision tree and your answer to part (a), show that any algorithm that
correctly merges two sorted lists must perform at least 2n/NULo.n/ comparisons.
Now we will show a slightly tighter 2n/NUL1bound.
c.Show that if two elements are consecutive in the sorted order and from different
lists, then they must be compared.
d.Use your answer to the previous part to show a lower bound of 2n/NUL1compar-
isons for merging two sorted lists.
8-7 The 0-1 sorting lemma and columnsort
Acompare-exchange operation on two array elements AŒi/c141 andAŒj /c141 ,w h e r e i<j ,
has the form
COMPARE -EXCHANGE . A ;i;j/
1ifAŒi/c141 > AŒj /c141
2 exchange AŒi/c141 withAŒj /c141
After the compare-exchange operation, we know that AŒi/c141/DC4AŒj /c141 .
Anoblivious compare-exchange algorithm operates solely by a sequence of
prespeciﬁed compare-exchange operations. The indices of the positions comparedin the sequence must be determined in advance, and although they can dependon the number of elements being sorted, they cannot depend on the values beingsorted, nor can they depend on the result of any prior compare-exchange operation.
For example, here is insertion sort expressed as an oblivious compare-exchange
algorithm:
I
NSERTION -SORT.A/
1forjD2toA:length
2 foriDj/NUL1downto 1
3C OMPARE -EXCHANGE . A ;i;iC1/Problems for Chapter 8 209
The0-1 sorting lemma provides a powerful way to prove that an oblivious
compare-exchange algorithm produces a sorted result. It states that if an oblivi-ous compare-exchange algorithm correctly sorts all input sequences consisting ofonly 0s and 1s, then it correctly sorts all inputs containing arbitrary values.
You will prove the 0-1 sorting lemma by proving its contrapositive: if an oblivi-
ous compare-exchange algorithm fails to sort an input containing arbitrary values,then it fails to sort some 0-1 input. Assume that an oblivious compare-exchange al-gorithm X fails to correctly sort the array AŒ1 : : n/c141 .L e t AŒp/c141 be the smallest value
inAthat algorithm X puts into the wrong location, and let AŒq/c141 be the value that
algorithm X moves to the location into which AŒp/c141 should have gone. Deﬁne an
array BŒ 1::n /c141 of 0s and 1s as follows:
BŒi/c141D(
0ifAŒi/c141/DC4AŒp/c141 ;
1ifAŒi/c141 > AŒp/c141 :
a.Argue that AŒq/c141 > AŒp/c141 ,s ot h a t BŒp/c141D0andBŒq/c141D1.
b.To complete the proof of the 0-1 sorting lemma, prove that algorithm X fails to
sort array Bcorrectly.
Now you will use the 0-1 sorting lemma to prove that a particular sorting algo-
rithm works correctly. The algorithm, columnsort , works on a rectangular array
ofnelements. The array has rrows and scolumns (so that nDrs), subject to
three restrictions:
/SIrmust be even,
/SIsmust be a divisor of r,a n d
/SIr/NAK2s2.
When columnsort completes, the array is sorted in column-major order : reading
down the columns, from left to right, the elements monotonically increase.
Columnsort operates in eight steps, regardless of the value of n. The odd steps
are all the same: sort each column individually. Each even step is a ﬁxed permuta-tion. Here are the steps:
1. Sort each column.
2. Transpose the array, but reshape it back to rrows and scolumns. In other
words, turn the leftmost column into the top r=srows, in order; turn the next
column into the next r=srows, in order; and so on.
3. Sort each column.
4. Perform the inverse of the permutation performed in step 2.210 Chapter 8 Sorting in Linear Time
10 14 5
87 1 7
12 1 6
16 9 11
41 52
18 3 13
(a)412
835
10 7 6
12 9 11
16 14 13
18 15 17
(b)48 1 0
12 16 18
137
91 4 1 5
256
11 13 17
(c)136
257
48 1 0
91 3 1 5
11 14 1712 16 18
(d)141 1
381 4
61 01 7
291 2
51 31 671 51 8
(e)
141 1
281 2
391 4
51 01 661 31 7
71 51 8
(f)51 01 6
61 31 7
71 51 8
141 1281 2
391 4
(g)41 01 6
51 11 7
61 21 8
171 3281 4
391 5
(h)171 3
281 4
391 5
41 01 6
51 11 7
61 21 8
(i)
Figure 8.5 The steps of columnsort. (a)The input array with 6 rows and 3 columns. (b)After
sorting each column in step 1. (c)After transposing and reshaping in step 2. (d)After sorting each
c o l u m ni ns t e p3 . (e)After performing step 4, which inverts the permutation from step 2. (f)After
sorting each column in step 5. (g)After shifting by half a column in step 6. (h)After sorting each
column in step 7. (i)After performing step 8, which inverts the permutation from step 6. The array
is now sorted in column-major order.
5. Sort each column.
6. Shift the top half of each column into the bottom half of the same column, and
shift the bottom half of each column into the top half of the next column to theright. Leave the top half of the leftmost column empty. Shift the bottom halfof the last column into the top half of a new rightmost column, and leave thebottom half of this new column empty.
7. Sort each column.8. Perform the inverse of the permutation performed in step 6.
Figure 8.5 shows an example of the steps of columnsort with rD6andsD3.
(Even though this example violates the requirement that r/NAK2s
2, it happens to
work.)
c.Argue that we can treat columnsort as an oblivious compare-exchange algo-
rithm, even if we do not know what sorting method the odd steps use.
Although it might seem hard to believe that columnsort actually sorts, you will
use the 0-1 sorting lemma to prove that it does. The 0-1 sorting lemma appliesbecause we can treat columnsort as an oblivious compare-exchange algorithm. ANotes for Chapter 8 211
couple of deﬁnitions will help you apply the 0-1 sorting lemma. We say that an area
of an array is clean if we know that it contains either all 0s or all 1s. Otherwise,
the area might contain mixed 0s and 1s, and it is dirty. From here on, assume that
the input array contains only 0s and 1s, and that we can treat it as an array with r
rows and scolumns.
d.Prove that after steps 1–3, the array consists of some clean rows of 0s at the top,
some clean rows of 1s at the bottom, and at most sdirty rows between them.
e.Prove that after step 4, the array, read in column-major order, starts with a clean
area of 0s, ends with a clean area of 1s, and has a dirty area of at most s2
elements in the middle.
f.Prove that steps 5–8 produce a fully sorted 0-1 output. Conclude that column-
sort correctly sorts all inputs containing arbitrary values.
g.Now suppose that sdoes not divide r. Prove that after steps 1–3, the array
consists of some clean rows of 0s at the top, some clean rows of 1s at thebottom, and at most 2s/NUL1dirty rows between them. How large must rbe,
compared with s, for columnsort to correctly sort when sdoes not divide r?
h.Suggest a simple change to step 1 that allows us to maintain the requirement
thatr/NAK2s
2even when sdoes not divide r, and prove that with your change,
columnsort correctly sorts.
Chapter notes
The decision-tree model for studying comparison sorts was introduced by Fordand Johnson [110]. Knuth’s comprehensive treatise on sorting [211] covers manyvariations on the sorting problem, including the information-theoretic lower boundon the complexity of sorting given here. Ben-Or [39] studied lower bounds forsorting using generalizations of the decision-tree model.
Knuth credits H. H. Seward with inventing counting sort in 1954, as well as with
the idea of combining counting sort with radix sort. Radix sorting starting with theleast signiﬁcant digit appears to be a folk algorithm widely used by operators ofmechanical card-sorting machines. According to Knuth, the ﬁrst published refer-ence to the method is a 1929 document by L. J. Comrie describing punched-cardequipment. Bucket sorting has been in use since 1956, when the basic idea wasproposed by E. J. Isaac and R. C. Singleton [188].
Munro and Raman [263] give a stable sorting algorithm that performs O.n
1C/SI/
comparisons in the worst case, where 0</SI/DC41is any ﬁxed constant. Although212 Chapter 8 Sorting in Linear Time
any of the O.n lgn/-time algorithms make fewer comparisons, the algorithm by
Munro and Raman moves data only O.n/ times and operates in place.
The case of sorting nb-bit integers in o.nlgn/time has been considered by
many researchers. Several positive results have been obtained, each under slightlydifferent assumptions about the model of computation and the restrictions placedon the algorithm. All the results assume that the computer memory is divided intoaddressable b-bit words. Fredman and Willard [115] introduced the fusion tree data
structure and used it to sort nintegers in O.n lgn=lg lgn/time. This bound was
later improved to O.np
lgn/time by Andersson [16]. These algorithms require
the use of multiplication and several precomputed constants. Andersson, Hagerup,Nilsson, and Raman [17] have shown how to sort nintegers in O.n lg lgn/time
without using multiplication, but their method requires storage that can be un-bounded in terms of n. Using multiplicative hashing, we can reduce the storage
needed to O.n/ , but then the O.n lg lgn/worst-case bound on the running time
becomes an expected-time bound. Generalizing the exponential search trees ofAndersson [16], Thorup [335] gave an O.n. lg lgn/
2/-time sorting algorithm that
does not use multiplication or randomization, and it uses linear space. Combiningthese techniques with some new ideas, Han [158] improved the bound for sortingtoO.n lg lgnlg lg lg n/time. Although these algorithms are important theoretical
breakthroughs, they are all fairly complicated and at the present time seem unlikely
to compete with existing sorting algorithms in practice.
The columnsort algorithm in Problem 8-7 is by Leighton [227].9 Medians and Order Statistics
Theithorder statistic of a set of nelements is the ith smallest element. For
example, the minimum of a set of elements is the ﬁrst order statistic ( iD1),
and the maximum is the nth order statistic ( iDn). Amedian , informally, is
the “halfway point” of the set. When nis odd, the median is unique, occurring at
iD.nC1/=2 .W h e n nis even, there are two medians, occurring at iDn=2and
iDn=2C1. Thus, regardless of the parity of n, medians occur at iDb.nC1/=2c
(thelower median )a n d iDd.nC1/=2e(theupper median ). For simplicity in
this text, however, we consistently use the phrase “the median” to refer to the lower
median.
This chapter addresses the problem of selecting the ith order statistic from a
set of ndistinct numbers. We assume for convenience that the set contains dis-
tinct numbers, although virtually everything that we do extends to the situation inwhich a set contains repeated values. We formally specify the selection problem
as follows:
Input: A set Aofn(distinct) numbers and an integer i, with 1/DC4i/DC4n.
Output: The element x2Athat is larger than exactly
i/NUL1other elements of A.
We can solve the selection problem in O.n lgn/time, since we can sort the num-
bers using heapsort or merge sort and then simply index the ith element in the
output array. This chapter presents faster algorithms.
In Section 9.1, we examine the problem of selecting the minimum and maxi-
mum of a set of elements. More interesting is the general selection problem, whichwe investigate in the subsequent two sections. Section 9.2 analyzes a practicalrandomized algorithm that achieves an O.n/ expected running time, assuming dis-
tinct elements. Section 9.3 contains an algorithm of more theoretical interest thatachieves the O.n/ running time in the worst case.214 Chapter 9 Medians and Order Statistics
9.1 Minimum and maximum
How many comparisons are necessary to determine the minimum of a set of n
elements? We can easily obtain an upper bound of n/NUL1comparisons: examine
each element of the set in turn and keep track of the smallest element seen sofar. In the following procedure, we assume that the set resides in array A,w h e r e
A:lengthDn.
M
INIMUM .A/
1minDAŒ1/c141
2foriD2toA:length
3 ifmin> AŒi/c141
4 minDAŒi/c141
5return min
We can, of course, ﬁnd the maximum with n/NUL1comparisons as well.
Is this the best we can do? Yes, since we can obtain a lower bound of n/NUL1
comparisons for the problem of determining the minimum. Think of any algorithm
that determines the minimum as a tournament among the elements. Each compar-
ison is a match in the tournament in which the smaller of the two elements wins.Observing that every element except the winner must lose at least one match, weconclude that n/NUL1comparisons are necessary to determine the minimum. Hence,
the algorithm M
INIMUM is optimal with respect to the number of comparisons
performed.
Simultaneous minimum and maximum
In some applications, we must ﬁnd both the minimum and the maximum of a set
ofnelements. For example, a graphics program may need to scale a set of .x; y/
data to ﬁt onto a rectangular display screen or other graphical output device. To
do so, the program must ﬁrst determine the minimum and maximum value of each
coordinate.
At this point, it should be obvious how to determine both the minimum and the
maximum of nelements using ‚.n/ comparisons, which is asymptotically optimal:
simply ﬁnd the minimum and maximum independently, using n/NUL1comparisons
for each, for a total of 2n/NUL2comparisons.
In fact, we can ﬁnd both the minimum and the maximum using at most 3bn=2c
comparisons. We do so by maintaining both the minimum and maximum elementsseen thus far. Rather than processing each element of the input by comparing it
against the current minimum and maximum, at a cost of 2comparisons per element,9.2 Selection in expected linear time 215
we process elements in pairs. We compare pairs of elements from the input ﬁrst
with each other , and then we compare the smaller with the current minimum and
the larger to the current maximum, at a cost of 3comparisons for every 2elements.
How we set up initial values for the current minimum and maximum depends
on whether nis odd or even. If nis odd, we set both the minimum and maximum
to the value of the ﬁrst element, and then we process the rest of the elements inpairs. If nis even, we perform 1comparison on the ﬁrst 2elements to determine
the initial values of the minimum and maximum, and then process the rest of the
elements in pairs as in the case for odd n.
Let us analyze the total number of comparisons. If nis odd, then we perform
3bn=2ccomparisons. If nis even, we perform 1initial comparison followed by
3.n/NUL2/=2 comparisons, for a total of 3n=2/NUL2. Thus, in either case, the total
number of comparisons is at most 3bn=2c.
Exercises
9.1-1
Show that the second smallest of nelements can be found with nCdlgne/NUL2
comparisons in the worst case. ( Hint: Also ﬁnd the smallest element.)
9.1-2 ?
Prove the lower bound of d3n=2e/NUL2comparisons in the worst case to ﬁnd both
the maximum and minimum of nnumbers. ( Hint: Consider how many numbers
are potentially either the maximum or minimum, and investigate how a comparison
affects these counts.)
9.2 Selection in expected linear time
The general selection problem appears more difﬁcult than the simple problem ofﬁnding a minimum. Yet, surprisingly, the asymptotic running time for both prob-lems is the same: ‚.n/ . In this section, we present a divide-and-conquer algorithm
for the selection problem. The algorithm R
ANDOMIZED -SELECT is modeled after
the quicksort algorithm of Chapter 7. As in quicksort, we partition the input arrayrecursively. But unlike quicksort, which recursively processes both sides of thepartition, R
ANDOMIZED -SELECT works on only one side of the partition. This
difference shows up in the analysis: whereas quicksort has an expected runningtime of ‚.n lgn/, the expected running time of R
ANDOMIZED -SELECT is‚.n/ ,
assuming that the elements are distinct.216 Chapter 9 Medians and Order Statistics
RANDOMIZED -SELECT uses the procedure R ANDOMIZED -PARTITION intro-
duced in Section 7.3. Thus, like R ANDOMIZED -QUICKSORT , it is a randomized al-
gorithm, since its behavior is determined in part by the output of a random-numbergenerator. The following code for R
ANDOMIZED -SELECT returns the ith smallest
element of the array AŒp : : r/c141 .
RANDOMIZED -SELECT . A ;p;r ;i/
1ifp==r
2 return AŒp/c141
3qDRANDOMIZED -PARTITION . A ;p;r/
4kDq/NULpC1
5ifi==k //the pivot value is the answer
6 return AŒq/c141
7elseif i<k
8 return RANDOMIZED -SELECT . A ;p;q/NUL1; i/
9else return RANDOMIZED -SELECT .A; qC1; r; i/NULk/
The R ANDOMIZED -SELECT procedure works as follows. Line 1 checks for the
base case of the recursion, in which the subarray AŒp : : r/c141 consists of just one
element. In this case, imust equal 1, and we simply return AŒp/c141 in line 2 as the
ith smallest element. Otherwise, the call to R ANDOMIZED -PARTITION in line 3
partitions the array AŒp : : r/c141 into two (possibly empty) subarrays AŒp : : q/NUL1/c141
andAŒqC1::r/c141 such that each element of AŒp : : q/NUL1/c141is less than or equal
toAŒq/c141, which in turn is less than each element of AŒqC1::r/c141 . As in quicksort,
we will refer to AŒq/c141 as the pivot element. Line 4 computes the number kof
elements in the subarray AŒp : : q/c141 , that is, the number of elements in the low side
of the partition, plus one for the pivot element. Line 5 then checks whether AŒq/c141 is
theith smallest element. If it is, then line 6 returns AŒq/c141. Otherwise, the algorithm
determines in which of the two subarrays AŒp : : q/NUL1/c141andAŒqC1::r/c141 theith
smallest element lies. If i<k , then the desired element lies on the low side of
the partition, and line 8 recursively selects it from the subarray. If i>k ,h o w e v e r ,
then the desired element lies on the high side of the partition. Since we alreadyknow kvalues that are smaller than the ith smallest element of AŒp : : r/c141 —namely,
the elements of AŒp : : q/c141 —the desired element is the .i/NULk/th smallest element
ofAŒqC1::r/c141 , which line 9 ﬁnds recursively. The code appears to allow recursive
calls to subarrays with 0elements, but Exercise 9.2-1 asks you to show that this
situation cannot happen.
The worst-case running time for R
ANDOMIZED -SELECT is‚.n2/,e v e nt oﬁ n d
the minimum, because we could be extremely unlucky and always partition aroundthe largest remaining element, and partitioning takes ‚.n/ time. We will see that9.2 Selection in expected linear time 217
the algorithm has a linear expected running time, though, and because it is random-
ized, no particular input elicits the worst-case behavior.
To analyze the expected running time of R ANDOMIZED -SELECT , we let the run-
ning time on an input array AŒp : : r/c141 ofnelements be a random variable that we
denote by T .n/ , and we obtain an upper bound on E ŒT .n//c141 as follows. The pro-
cedure R ANDOMIZED -PARTITION is equally likely to return any element as the
pivot. Therefore, for each ksuch that 1/DC4k/DC4n, the subarray AŒp : : q/c141 haskele-
ments (all less than or equal to the pivot) with probability 1=n.F o r kD1 ;2;:::;n ,
we deﬁne indicator random variables Xkwhere
XkDIfthe subarray AŒp : : q/c141 has exactly kelementsg;
and so, assuming that the elements are distinct, we haveEŒX
k/c141D1=n : (9.1)
When we call R ANDOMIZED -SELECT and choose AŒq/c141 as the pivot element, we
do not know, a priori, if we will terminate immediately with the correct answer,recurse on the subarray AŒp : : q/NUL1/c141, or recurse on the subarray AŒqC1::r/c141 .
This decision depends on where the ith smallest element falls relative to AŒq/c141.
Assuming that T .n/ is monotonically increasing, we can upper-bound the time
needed for the recursive call by the time needed for the recursive call on the largest
possible input. In other words, to obtain an upper bound, we assume that the ith
element is always on the side of the partition with the greater number of elements.
For a given call of R
ANDOMIZED -SELECT , the indicator random variable Xkhas
the value 1for exactly one value of k, and it is 0for all other k.W h e n XkD1,t h e
two subarrays on which we might recurse have sizes k/NUL1andn/NULk. Hence, we
have the recurrence
T .n//DC4nX
kD1Xk/SOH.T .max.k/NUL1; n/NULk//CO.n//
DnX
kD1Xk/SOHT.max.k/NUL1; n/NULk//CO.n/ :218 Chapter 9 Medians and Order Statistics
Taking expected values, we have
EŒT .n//c141
/DC4E"nX
kD1Xk/SOHT.max.k/NUL1; n/NULk//CO.n/#
DnX
kD1EŒXk/SOHT.max.k/NUL1; n/NULk///c141CO.n/ (by linearity of expectation)
DnX
kD1EŒXk/c141/SOHEŒT .max.k/NUL1; n/NULk///c141CO.n/ (by equation (C.24))
DnX
kD11
n/SOHEŒT .max.k/NUL1; n/NULk///c141CO.n/ (by equation (9.1)) .
In order to apply equation (C.24), we rely on XkandT.max.k/NUL1; n/NULk//being
independent random variables. Exercise 9.2-2 asks you to justify this assertion.
Let us consider the expression max .k/NUL1; n/NULk/.W eh a v e
max.k/NUL1; n/NULk/D(
k/NUL1ifk>dn=2e;
n/NULkifk/DC4dn=2e:
Ifnis even, each term from T.dn=2e/up to T. n/NUL1/appears exactly twice in
the summation, and if nis odd, all these terms appear twice and T.bn=2c/appears
once. Thus, we have
EŒT .n//c141/DC42
nn/NUL1X
kDbn=2cEŒT .k//c141CO.n/ :
We show that E ŒT .n//c141DO.n/ by substitution. Assume that E ŒT .n//c141/DC4cnfor
some constant cthat satisﬁes the initial conditions of the recurrence. We assume
thatT .n/DO.1/ fornless than some constant; we shall pick this constant later.
We also pick a constant asuch that the function described by the O.n/ term above
(which describes the non-recursive component of the running time of the algo-rithm) is bounded from above by anfor all n>0 . Using this inductive hypothesis,
we have
EŒT .n//c141/DC42
nn/NUL1X
kDbn=2cckCan
D2c
n n/NUL1X
kD1k/NULbn=2c/NUL1X
kD1k!
Can9.2 Selection in expected linear time 219
D2c
n/DC2.n/NUL1/n
2/NUL.bn=2c/NUL1/bn=2c
2/DC3
Can
/DC42c
n/DC2.n/NUL1/n
2/NUL.n=2/NUL2/.n=2/NUL1/
2/DC3
Can
D2c
n/DC2n2/NULn
2/NULn2=4/NUL3n=2C2
2/DC3
Can
Dc
n/DC23n2
4Cn
2/NUL2/DC3
Can
Dc/DC23n
4C1
2/NUL2
n/DC3
Can
/DC43cn
4Cc
2Can
Dcn/NUL/DLEcn
4/NULc
2/NULan/DC1
:
In order to complete the proof, we need to show that for sufﬁciently large n,t h i s
last expression is at most cnor, equivalently, that cn=4/NULc=2/NULan/NAK0.I f w e
addc=2to both sides and factor out n,w eg e t n.c=4/NULa//NAKc=2. As long as we
choose the constant cso that c=4/NULa>0 , i.e., c>4 a , we can divide both sides
byc=4/NULa,g i v i n g
n/NAKc=2
c=4/NULaD2c
c/NUL4a:
Thus, if we assume that T .n/DO.1/ forn<2 c = . c/NUL4a/,t h e nE ŒT .n//c141DO.n/ .
We conclude that we can ﬁnd any order statistic, and in particular the median, inexpected linear time, assuming that the elements are distinct.
Exercises
9.2-1
Show that R
ANDOMIZED -SELECT never makes a recursive call to a 0-length array.
9.2-2
Argue that the indicator random variable Xkand the value T.max.k/NUL1; n/NULk//
are independent.
9.2-3
Write an iterative version of R ANDOMIZED -SELECT .220 Chapter 9 Medians and Order Statistics
9.2-4
Suppose we use R ANDOMIZED -SELECT to select the minimum element of the
array ADh3; 2; 9; 0; 7; 5; 4; 8; 6; 1 i. Describe a sequence of partitions that results
in a worst-case performance of R ANDOMIZED -SELECT .
9.3 Selection in worst-case linear time
We now examine a selection algorithm whose running time is O.n/ in the worst
case. Like R ANDOMIZED -SELECT , the algorithm S ELECT ﬁnds the desired ele-
ment by recursively partitioning the input array. Here, however, we guarantee a
good split upon partitioning the array. S ELECT uses the deterministic partitioning
algorithm P ARTITION from quicksort (see Section 7.1), but modiﬁed to take the
element to partition around as an input parameter.
The S ELECT algorithm determines the ith smallest of an input array of n>1
distinct elements by executing the following steps. (If nD1,t h e nS ELECT merely
returns its only input value as the ith smallest.)
1. Divide the nelements of the input array into bn=5cgroups of 5elements each
and at most one group made up of the remaining nmod5elements.
2. Find the median of each of the dn=5egroups by ﬁrst insertion-sorting the ele-
ments of each group (of which there are at most 5) and then picking the median
from the sorted list of group elements.
3. Use S ELECT recursively to ﬁnd the median xof thedn=5emedians found in
step 2. (If there are an even number of medians, then by our convention, xis
the lower median.)
4. Partition the input array around the median-of-medians xusing the modiﬁed
version of P ARTITION .L e t kbe one more than the number of elements on the
low side of the partition, so that xis the kth smallest element and there are n/NULk
elements on the high side of the partition.
5. If iDk, then return x. Otherwise, use S ELECT recursively to ﬁnd the ith
smallest element on the low side if i<k ,o rt h e .i/NULk/th smallest element on
the high side if i>k .
To analyze the running time of S ELECT , we ﬁrst determine a lower bound on the
number of elements that are greater than the partitioning element x. Figure 9.1
helps us to visualize this bookkeeping. At least half of the medians found in9.3 Selection in worst-case linear time 221
x
Figure 9.1 Analysis of the algorithm S ELECT .T h e nelements are represented by small circles,
and each group of 5elements occupies a column. The medians of the groups are whitened, and the
median-of-medians xis labeled. (When ﬁnding the median of an even number of elements, we use
the lower median.) Arrows go from larger elements to smaller, from which we can see that 3out
of every full group of 5elements to the right of xare greater than x,a n d 3out of every group of 5
elements to the left of xare less than x. The elements known to be greater than xappear on a shaded
background.
step 2 are greater than or equal to the median-of-medians x.1Thus, at least half
of thedn=5egroups contribute at least 3elements that are greater than x, except
for the one group that has fewer than 5elements if 5does not divide nexactly, and
the one group containing xitself. Discounting these two groups, it follows that the
number of elements greater than xis at least
3/DC2/CAN1
2ln
5m/EM
/NUL2/DC3
/NAK3n
10/NUL6:
Similarly, at least 3n=10/NUL6elements are less than x. Thus, in the worst case,
step 5 calls S ELECT recursively on at most 7n=10C6elements.
We can now develop a recurrence for the worst-case running time T .n/ of the
algorithm S ELECT . Steps 1, 2, and 4 take O.n/ time. (Step 2 consists of O.n/
calls of insertion sort on sets of size O.1/ .) Step 3 takes time T.dn=5e/, and step 5
takes time at most T. 7 n = 1 0C6/, assuming that Tis monotonically increasing.
We make the assumption, which seems unmotivated at ﬁrst, that any input of fewerthan 140 elements requires O.1/ time; the origin of the magic constant 140 will be
clear shortly. We can therefore obtain the recurrence
1Because of our assumption that the numbers are distinct, all medians except xare either greater
than or less than x.222 Chapter 9 Medians and Order Statistics
T .n//DC4(
O.1/ ifn < 140 ;
T.dn=5e/CT. 7 n = 1 0C6/CO.n/ ifn/NAK140 :
We show that the running time is linear by substitution. More speciﬁcally, we will
show that T .n//DC4cnfor some suitably large constant cand all n>0 .W eb e g i nb y
assuming that T .n//DC4cnfor some suitably large constant cand all n < 140 ;t h i s
assumption holds if cis large enough. We also pick a constant asuch that the func-
tion described by the O.n/ term above (which describes the non-recursive compo-
nent of the running time of the algorithm) is bounded above by anfor all n>0 .
Substituting this inductive hypothesis into the right-hand side of the recurrenceyields
T .n//DC4cdn=5eCc.7n=10C6/Can
/DC4cn=5CcC7cn=10C6cCan
D9cn=10C7cCan
DcnC./NULcn=10C7cCan/ ;
which is at most cnif
/NULcn=10C7cCan
/DC40: (9.2)
Inequality (9.2) is equivalent to the inequality c/NAK10a.n=.n/NUL70// when n>7 0 .
Because we assume that n/NAK140,w eh a v e n=.n/NUL70//DC42, and so choos-
ingc/NAK20awill satisfy inequality (9.2). (Note that there is nothing special about
the constant 140; we could replace it by any integer strictly greater than 70 andthen choose caccordingly.) The worst-case running time of S
ELECT is therefore
linear.
As in a comparison sort (see Section 8.1), S ELECT and R ANDOMIZED -SELECT
determine information about the relative order of elements only by comparing ele-ments. Recall from Chapter 8 that sorting requires /DEL.n lgn/time in the compari-
son model, even on average (see Problem 8-1). The linear-time sorting algorithmsin Chapter 8 make assumptions about the input. In contrast, the linear-time se-lection algorithms in this chapter do not require any assumptions about the input.They are not subject to the /DEL.n lgn/lower bound because they manage to solve
the selection problem without sorting. Thus, solving the selection problem by sort-ing and indexing, as presented in the introduction to this chapter, is asymptoticallyinefﬁcient.9.3 Selection in worst-case linear time 223
Exercises
9.3-1
In the algorithm S ELECT , the input elements are divided into groups of 5. Will
the algorithm work in linear time if they are divided into groups of 7? Argue that
SELECT does not run in linear time if groups of 3are used.
9.3-2
Analyze S ELECT to show that if n/NAK140, then at leastdn=4eelements are greater
than the median-of-medians xand at leastdn=4eelements are less than x.
9.3-3
Show how quicksort can be made to run in O.n lgn/time in the worst case, as-
suming that all elements are distinct.
9.3-4 ?
Suppose that an algorithm uses only comparisons to ﬁnd the ith smallest element
in a set of nelements. Show that it can also ﬁnd the i/NUL1smaller elements and
then/NULilarger elements without performing any additional comparisons.
9.3-5
Suppose that you have a “black-box” worst-case linear-time median subroutine.Give a simple, linear-time algorithm that solves the selection problem for an arbi-trary order statistic.
9.3-6
Thekthquantiles of an n-element set are the k/NUL1order statistics that divide the
sorted set into kequal-sized sets (to within 1). Give an O.n lgk/-time algorithm
to list the kth quantiles of a set.
9.3-7
Describe an O.n/ -time algorithm that, given a set Sofndistinct numbers and
a positive integer k/DC4n, determines the knumbers in Sthat are closest to the
median of S.
9.3-8
LetXŒ1::n/c141 andYŒ 1::n /c141 be two arrays, each containing nnumbers already in
sorted order. Give an O.lgn/-time algorithm to ﬁnd the median of all
2nelements
in arrays XandY.
9.3-9
Professor Olay is consulting for an oil company, which is planning a large pipelinerunning east to west through an oil ﬁeld of nwells. The company wants to connect224 Chapter 9 Medians and Order Statistics
Figure 9.2 Professor Olay needs to determine the position of the east-west oil pipeline that mini-
mizes the total length of the north-south spurs.
a spur pipeline from each well directly to the main pipeline along a shortest route
(either north or south), as shown in Figure 9.2. Given the x-a n d y-coordinates of
the wells, how should the professor pick the optimal location of the main pipeline,which would be the one that minimizes the total length of the spurs? Show how todetermine the optimal location in linear time.
Problems
9-1 Largest inumbers in sorted order
Given a set of nnumbers, we wish to ﬁnd the ilargest in sorted order using a
comparison-based algorithm. Find the algorithm that implements each of the fol-lowing methods with the best asymptotic worst-case running time, and analyze therunning times of the algorithms in terms of nandi.
a.Sort the numbers, and list the ilargest.
b.Build a max-priority queue from the numbers, and call E
XTRACT -MAXitimes.
c.Use an order-statistic algorithm to ﬁnd the ith largest number, partition around
that number, and sort the ilargest numbers.Problems for Chapter 9 225
9-2 Weighted median
Forndistinct elements x1;x2;:::;x nwith positive weights w1;w2;:::;w nsuch
thatPn
iD1wiD1,t h eweighted (lower) median is the element xksatisfying
X
xi<xkwi<1
2
and
X
xi>xkwi/DC41
2:
For example, if the elements are 0:1; 0:35; 0:05; 0:1; 0:15; 0:05; 0:2 and each ele-
ment equals its weight (that is, wiDxiforiD1 ;2;:::;7 ), then the median is 0:1,
but the weighted median is 0:2.
a.Argue that the median of x1;x2;:::;x nis the weighted median of the xiwith
weights wiD1=nforiD1 ;2;:::;n .
b.Show how to compute the weighted median of nelements in O.n lgn/worst-
case time using sorting.
c.Show how to compute the weighted median in ‚.n/ worst-case time using a
linear-time median algorithm such as S ELECT from Section 9.3.
Thepost-ofﬁce location problem is deﬁned as follows. We are given npoints
p1;p2;:::;p nwith associated weights w1;w2;:::;w n. We wish to ﬁnd a point p
(not necessarily one of the input points) that minimizes the sumPn
iD1wid.p;p i/,
where d.a;b/ is the distance between points aandb.
d.Argue that the weighted median is a best solution for the 1-dimensional post-
ofﬁce location problem, in which points are simply real numbers and the dis-
tance between points aandbisd.a;b/Dja/NULbj.
e.Find the best solution for the 2-dimensional post-ofﬁce location problem, in
which the points are .x; y/ coordinate pairs and the distance between points
aD.x1;y1/andbD.x2;y2/is the Manhattan distance given by d.a;b/D
jx1/NULx2jCjy1/NULy2j.
9-3 Small order statistics
We showed that the worst-case number T .n/ of comparisons used by S ELECT
to select the ith order statistic from nnumbers satisﬁes T .n/D‚.n/ ,b u tt h e
constant hidden by the ‚-notation is rather large. When iis small relative to n,w e
can implement a different procedure that uses S ELECT as a subroutine but makes
fewer comparisons in the worst case.226 Chapter 9 Medians and Order Statistics
a.Describe an algorithm that uses Ui.n/comparisons to ﬁnd the ith smallest of n
elements, where
Ui.n/D(
T .n/ ifi/NAKn=2 ;
bn=2cCUi.dn=2e/CT. 2i/ otherwise :
(Hint: Begin withbn=2cdisjoint pairwise comparisons, and recurse on the set
containing the smaller element from each pair.)
b.Show that, if i<n = 2 ,t h e n Ui.n/DnCO.T.2i/ lg.n=i// .
c.Show that if iis a constant less than n=2,t h e n Ui.n/DnCO.lgn/.
d.Show that if iDn=k fork/NAK2,t h e n Ui.n/DnCO.T.2n=k/ lgk/.
9-4 Alternative analysis of randomized selection
In this problem, we use indicator random variables to analyze the R ANDOMIZED -
SELECT procedure in a manner akin to our analysis of R ANDOMIZED -QUICKSORT
in Section 7.4.2.
As in the quicksort analysis, we assume that all elements are distinct, and we
rename the elements of the input array Aas´1;´2;:::;´ n,w h e r e ´iis the ith
smallest element. Thus, the call R ANDOMIZED -SELECT . A ;1 ;n;k/ returns ´k.
For1/DC4i<j/DC4n,l e t
Xij kDIf´iis compared with ´jsometime during the execution of the algorithm
to ﬁnd ´kg:
a.Give an exact expression for E ŒXij k/c141.(Hint: Your expression may have differ-
ent values, depending on the values of i,j,a n d k.)
b.LetXkdenote the total number of comparisons between elements of array A
when ﬁnding ´k. Show that
EŒXk/c141/DC42 kX
iD1nX
jDk1
j/NULiC1CnX
jDkC1j/NULk/NUL1
j/NULkC1Ck/NUL2X
iD1k/NULi/NUL1
k/NULiC1!
:
c.Show that E ŒXk/c141/DC44n.
d.Conclude that, assuming all elements of array Aare distinct, R ANDOMIZED -
SELECT runs in expected time O.n/ .Notes for Chapter 9 227
Chapter notes
The worst-case linear-time median-ﬁnding algorithm was devised by Blum, Floyd,
Pratt, Rivest, and Tarjan [50]. The fast randomized version is due to Hoare [169].Floyd and Rivest [108] have developed an improved randomized version that parti-tions around an element recursively selected from a small sample of the elements.
It is still unknown exactly how many comparisons are needed to determine the
median. Bent and John [41] gave a lower bound of 2ncomparisons for median
ﬁnding, and Sch¨ onhage, Paterson, and Pippenger [302] gave an upper bound of 3n.
Dor and Zwick have improved on both of these bounds. Their upper bound [93]is slightly less than 2:95n , and their lower bound [94] is .2C/SI/n,f o ras m a l l
positive constant /SI, thereby improving slightly on related work by Dor et al. [92].
Paterson [272] describes some of these results along with other related work.III Data StructuresIntroduction
Sets are as fundamental to computer science as they are to mathematics. Whereas
mathematical sets are unchanging, the sets manipulated by algorithms can grow,shrink, or otherwise change over time. We call such sets dynamic . The next ﬁve
chapters present some basic techniques for representing ﬁnite dynamic sets andmanipulating them on a computer.
Algorithms may require several different types of operations to be performed on
sets. For example, many algorithms need only the ability to insert elements into,delete elements from, and test membership in a set. We call a dynamic set thatsupports these operations a dictionary . Other algorithms require more complicated
operations. For example, min-priority queues, which Chapter 6 introduced in thecontext of the heap data structure, support the operations of inserting an elementinto and extracting the smallest element from a set. The best way to implement a
dynamic set depends upon the operations that must be supported.
Elements of a dynamic set
In a typical implementation of a dynamic set, each element is represented by an
object whose attributes can be examined and manipulated if we have a pointer to
the object. (Section 10.3 discusses the implementation of objects and pointers in
programming environments that do not contain them as basic data types.) Some
kinds of dynamic sets assume that one of the object’s attributes is an identifying
key. If the keys are all different, we can think of the dynamic set as being a set
of key values. The object may contain satellite data , which are carried around in
other object attributes but are otherwise unused by the set implementation. It may230 Part III Data Structures
also have attributes that are manipulated by the set operations; these attributes may
contain data or pointers to other objects in the set.
Some dynamic sets presuppose that the keys are drawn from a totally ordered
set, such as the real numbers, or the set of all words under the usual alphabeticordering. A total ordering allows us to deﬁne the minimum element of the set, forexample, or to speak of the next element larger than a given element in a set.
Operations on dynamic sets
Operations on a dynamic set can be grouped into two categories: queries ,w h i c h
simply return information about the set, and modifying operations , which change
the set. Here is a list of typical operations. Any speciﬁc application will usuallyrequire only a few of these to be implemented.
S
EARCH .S; k/
A query that, given a set Sa n dak e yv a l u e k, returns a pointer xto an element
inSsuch that x:keyDk,o r NILif no such element belongs to S.
INSERT .S; x/
A modifying operation that augments the set Swith the element pointed to
byx. We usually assume that any attributes in element xneeded by the set
implementation have already been initialized.
DELETE .S; x/
A modifying operation that, given a pointer xto an element in the set S,r e -
moves xfrom S. (Note that this operation takes a pointer to an element x, not
ak e yv a l u e . )
MINIMUM .S/
A query on a totally ordered set Sthat returns a pointer to the element of S
with the smallest key.
MAXIMUM .S/
A query on a totally ordered set Sthat returns a pointer to the element of S
with the largest key.
SUCCESSOR .S; x/
A query that, given an element xwhose key is from a totally ordered set S,
returns a pointer to the next larger element in S,o r NILifxis the maximum
element.
PREDECESSOR .S; x/
A query that, given an element xwhose key is from a totally ordered set S,
returns a pointer to the next smaller element in S,o r NILifxis the minimum
element.Part III Data Structures 231
In some situations, we can extend the queries S UCCESSOR and P REDECESSOR
so that they apply to sets with nondistinct keys. For a set on nkeys, the normal
presumption is that a call to M INIMUM followed by n/NUL1calls to S UCCESSOR
enumerates the elements in the set in sorted order.
We usually measure the time taken to execute a set operation in terms of the size
of the set. For example, Chapter 13 describes a data structure that can support anyof the operations listed above on a set of size nin time O.lgn/.
Overview of Part III
Chapters 10–14 describe several data structures that we can use to implement
dynamic sets; we shall use many of these later to construct efﬁcient algorithmsfor a variety of problems. We already saw another important data structure—the
heap—in Chapter 6.
Chapter 10 presents the essentials of working with simple data structures such
as stacks, queues, linked lists, and rooted trees. It also shows how to implementobjects and pointers in programming environments that do not support them asprimitives. If you have taken an introductory programming course, then much ofthis material should be familiar to you.
Chapter 11 introduces hash tables, which support the dictionary operations I
N-
SERT ,DELETE ,a n dS EARCH . In the worst case, hashing requires ‚.n/ time to per-
form a S EARCH operation, but the expected time for hash-table operations is O.1/ .
The analysis of hashing relies on probability, but most of the chapter requires nobackground in the subject.
Binary search trees, which are covered in Chapter 12, support all the dynamic-
set operations listed above. In the worst case, each operation takes ‚.n/ time on a
tree with nelements, but on a randomly built binary search tree, the expected time
for each operation is O.lgn/. Binary search trees serve as the basis for many other
data structures.
Chapter 13 introduces red-black trees, which are a variant of binary search trees.
Unlike ordinary binary search trees, red-black trees are guaranteed to perform well:operations take O.lgn/time in the worst case. A red-black tree is a balanced search
tree; Chapter 18 in Part V presents another kind of balanced search tree, called aB-tree. Although the mechanics of red-black trees are somewhat intricate, you canglean most of their properties from the chapter without studying the mechanics indetail. Nevertheless, you probably will ﬁnd walking through the code to be quiteinstructive.
In Chapter 14, we show how to augment red-black trees to support operations
other than the basic ones listed above. First, we augment them so that we candynamically maintain order statistics for a set of keys. Then, we augment them ina different way to maintain intervals of real numbers.10 Elementary Data Structures
In this chapter, we examine the representation of dynamic sets by simple data struc-
tures that use pointers. Although we can construct many complex data structuresusing pointers, we present only the rudimentary ones: stacks, queues, linked lists,and rooted trees. We also show ways to synthesize objects and pointers from ar-rays.
10.1 Stacks and queues
Stacks and queues are dynamic sets in which the element removed from the setby the D
ELETE operation is prespeciﬁed. In a stack , the element deleted from
the set is the one most recently inserted: the stack implements a last-in, ﬁrst-out ,
orLIFO , policy. Similarly, in a queue , the element deleted is always the one that
has been in the set for the longest time: the queue implements a ﬁrst-in, ﬁrst-out ,
orFIFO , policy. There are several efﬁcient ways to implement stacks and queues
on a computer. In this section we show how to use a simple array to implementeach.
Stacks
The I
NSERT operation on a stack is often called P USH,a n dt h eD ELETE opera-
tion, which does not take an element argument, is often called P OP. These names
are allusions to physical stacks, such as the spring-loaded stacks of plates usedin cafeterias. The order in which plates are popped from the stack is the reverseof the order in which they were pushed onto the stack, since only the top plate isaccessible.
As Figure 10.1 shows, we can implement a stack of at most nelements with
an array SŒ 1::n /c141 . The array has an attribute S:topthat indexes the most recently10.1 Stacks and queues 233
1234567
S1 5 6291234567
S15 6 2 9 17 31234567
S15 6 2 9 17 3
(a) (b) (c)S:topD4S : topD6S : topD5
Figure 10.1 An array implementation of a stack S. Stack elements appear only in the lightly shaded
positions. (a)Stack Shas 4 elements. The top element is 9. (b)Stack Safter the calls P USH.S; 17/
and P USH.S; 3/ .(c)Stack Safter the call P OP.S/has returned the element 3, which is the one most
recently pushed. Although element 3 still appears in the array, it is no longer in the stack; the top is
element 17.
inserted element. The stack consists of elements SŒ 1::S: top/c141,w h e r e SŒ1/c141 is the
element at the bottom of the stack and SŒS: top/c141is the element at the top.
When S:topD0, the stack contains no elements and is empty . We can test to
see whether the stack is empty by query operation S TACK -EMPTY . If we attempt
to pop an empty stack, we say the stack underﬂows , which is normally an error.
IfS:topexceeds n, the stack overﬂows . (In our pseudocode implementation, we
don’t worry about stack overﬂow.)
We can implement each of the stack operations with just a few lines of code:
STACK -EMPTY .S/
1ifS:top==0
2 return TRUE
3else return FALSE
PUSH.S; x/
1S:topDS:topC1
2SŒS: top/c141Dx
POP.S/
1ifSTACK -EMPTY .S/
2 error “underﬂow”
3elseS:topDS:top/NUL1
4 return SŒS: topC1/c141
Figure 10.1 shows the effects of the modifying operations P USH and P OP. Each of
the three stack operations takes O.1/ time.234 Chapter 10 Elementary Data Structures
123456789 1 0 1 1 1 2
Q ( a ) 1 5 6984
123456789 1 0 1 1 1 2
Q ( b ) 1 5 6984 35 1 7
123456789 1 0 1 1 1 2
Q ( c ) 1 5 6984 35 1 7Q:headD7Q:headD7 Q:tailD12
Q:tailD3Q:tailD3
Q:headD8
Figure 10.2 A queue implemented using an array QŒ1 : : 12/c141 . Queue elements appear only in the
lightly shaded positions. (a)The queue has 5 elements, in locations QŒ7 : : 11/c141 .(b)The conﬁguration
of the queue after the calls E NQUEUE .Q; 17/ ,ENQUEUE .Q; 3/ ,a n dE NQUEUE .Q; 5/ .(c)The
conﬁguration of the queue after the call D EQUEUE .Q/ returns the key value 15 formerly at the
head of the queue. The new head has key 6.
Queues
We call the I NSERT operation on a queue E NQUEUE , and we call the D ELETE
operation D EQUEUE ; like the stack operation P OP,DEQUEUE takes no element ar-
gument. The FIFO property of a queue causes it to operate like a line of customerswaiting to pay a cashier. The queue has a head and atail. When an element is en-
queued, it takes its place at the tail of the queue, just as a newly arriving customertakes a place at the end of the line. The element dequeued is always the one atthe head of the queue, like the customer at the head of the line who has waited thelongest.
Figure 10.2 shows one way to implement a queue of at most n/NUL1elements
using an array QŒ1 : : n/c141 . The queue has an attribute Q:head that indexes, or points
to, its head. The attribute Q:tailindexes the next location at which a newly arriv-
ing element will be inserted into the queue. The elements in the queue reside inlocations Q:head;Q :headC1 ;:::;Q : tail/NUL1, where we “wrap around” in the
sense that location 1immediately follows location nin a circular order. When
Q:headDQ:tail, the queue is empty. Initially, we have Q:headDQ:tailD1.
If we attempt to dequeue an element from an empty queue, the queue underﬂows.10.1 Stacks and queues 235
When Q:headDQ:tailC1, the queue is full, and if we attempt to enqueue an
element, then the queue overﬂows.
In our procedures E NQUEUE and D EQUEUE , we have omitted the error checking
for underﬂow and overﬂow. (Exercise 10.1-4 asks you to supply code that checksfor these two error conditions.) The pseudocode assumes that nDQ:length .
E
NQUEUE .Q; x/
1QŒQ: tail/c141Dx
2ifQ:tail==Q:length
3 Q:tailD1
4elseQ:tailDQ:tailC1
DEQUEUE .Q/
1xDQŒQ: head/c141
2ifQ:head ==Q:length
3 Q:headD1
4elseQ:headDQ:headC1
5return x
Figure 10.2 shows the effects of the E NQUEUE and D EQUEUE operations. Each
operation takes O.1/ time.
Exercises
10.1-1
Using Figure 10.1 as a model, illustrate the result of each operation in the sequence
PUSH.S; 4/ ,PUSH.S; 1/ ,PUSH.S; 3/ ,POP.S/,PUSH.S; 8/ ,a n dP OP.S/on an
initially empty stack Sstored in array SŒ 1::6 /c141 .
10.1-2
Explain how to implement two stacks in one array AŒ1 : : n/c141 in such a way that
neither stack overﬂows unless the total number of elements in both stacks together
isn.T h eP USH and P OPoperations should run in O.1/ time.
10.1-3
Using Figure 10.2 as a model, illustrate the result of each operation in the
sequence E NQUEUE .Q; 4/ ,ENQUEUE .Q; 1/ ,ENQUEUE .Q; 3/ ,D EQUEUE .Q/,
ENQUEUE .Q; 8/ ,a n dD EQUEUE .Q/ on an initially empty queue Qstored in
array Q Œ 1::6 /c141 .
10.1-4
Rewrite E NQUEUE and D EQUEUE to detect underﬂow and overﬂow of a queue.236 Chapter 10 Elementary Data Structures
10.1-5
Whereas a stack allows insertion and deletion of elements at only one end, and aqueue allows insertion at one end and deletion at the other end, a deque (double-
ended queue) allows insertion and deletion at both ends. Write four O.1/ -time
procedures to insert elements into and delete elements from both ends of a dequeimplemented by an array.
10.1-6
Show how to implement a queue using two stacks. Analyze the running time of the
queue operations.
10.1-7
Show how to implement a stack using two queues. Analyze the running time of thestack operations.
10.2 Linked lists
Alinked list is a data structure in which the objects are arranged in a linear order.
Unlike an array, however, in which the linear order is determined by the array
indices, the order in a linked list is determined by a pointer in each object. Linkedlists provide a simple, ﬂexible representation for dynamic sets, supporting (thoughnot necessarily efﬁciently) all the operations listed on page 230.
As shown in Figure 10.3, each element of a doubly linked list Lis an object with
an attribute keyand two other pointer attributes: next andpre/ETB. The object may
also contain other satellite data. Given an element xin the list, x:next points to its
successor in the linked list, and x:pre/ETBpoints to its predecessor. If x:pre/ETBD
NIL,
the element xhas no predecessor and is therefore the ﬁrst element, or head ,o f
the list. If x:nextDNIL, the element xhas no successor and is therefore the last
element, or tail, of the list. An attribute L:head points to the ﬁrst element of the
list. If L:headDNIL, the list is empty.
A list may have one of several forms. It may be either singly linked or doubly
linked, it may be sorted or not, and it may be circular or not. If a list is singly
linked ,w eo m i tt h e pre/ETBpointer in each element. If a list is sorted , the linear order
of the list corresponds to the linear order of keys stored in elements of the list; the
minimum element is then the head of the list, and the maximum element is the
tail. If the list is unsorted , the elements can appear in any order. In a circular list ,
thepre/ETBpointer of the head of the list points to the tail, and the next pointer of
the tail of the list points to the head. We can think of a circular list as a ring of10.2 Linked lists 237
9 16 4 1prev key next
(a)
9 16 4 1 (b) 25
9 16 1 (c) 25 L:headL:headL:head
Figure 10.3 (a) A doubly linked list Lrepresenting the dynamic set f1; 4; 9; 16g. Each element in
the list is an object with attributes for the key and pointers (shown by arrows) to the next and previous
objects. The nextattribute of the tail and the pre/ETBattribute of the head are NIL, indicated by a diagonal
slash. The attribute L:head points to the head. (b)Following the execution of L IST-INSERT .L; x/ ,
where x:keyD25, the linked list has a new object with key 25as the new head. This new object
points to the old head with key 9.(c)The result of the subsequent call L IST-DELETE .L; x/ ,w h e r e x
points to the object with key 4.
elements. In the remainder of this section, we assume that the lists with which we
are working are unsorted and doubly linked.
Searching a linked list
The procedure L IST-SEARCH .L; k/ ﬁnds the ﬁrst element with key kin list L
by a simple linear search, returning a pointer to this element. If no object withkeykappears in the list, then the procedure returns
NIL. For the linked list in
Figure 10.3(a), the call L IST-SEARCH .L; 4/ returns a pointer to the third element,
and the call L IST-SEARCH .L; 7/ returns NIL.
LIST-SEARCH .L; k/
1xDL:head
2while x¤NILandx:key¤k
3 xDx:next
4return x
To search a list of nobjects, the L IST-SEARCH procedure takes ‚.n/ time in the
worst case, since it may have to search the entire list.
Inserting into a linked list
G i v e na ne l e m e n t xwhose keyattribute has already been set, the L IST-INSERT
procedure “splices” xonto the front of the linked list, as shown in Figure 10.3(b).238 Chapter 10 Elementary Data Structures
LIST-INSERT .L; x/
1x:nextDL:head
2ifL:head¤NIL
3 L:head:pre/ETBDx
4L:headDx
5x:pre/ETBDNIL
(Recall that our attribute notation can cascade, so that L:head:pre/ETBdenotes the
pre/ETBattribute of the object that L:head points to.) The running time for L IST-
INSERT on a list of nelements is O.1/ .
Deleting from a linked list
The procedure L IST-DELETE removes an element xfrom a linked list L.I t m u s t
be given a pointer to x, and it then “splices” xout of the list by updating pointers.
If we wish to delete an element with a given key, we must ﬁrst call L IST-SEARCH
to retrieve a pointer to the element.
LIST-DELETE .L; x/
1ifx:pre/ETB¤NIL
2 x:pre/ETB:nextDx:next
3elseL:headDx:next
4ifx:next¤NIL
5 x:next:pre/ETBDx:pre/ETB
Figure 10.3(c) shows how an element is deleted from a linked list. L IST-DELETE
runs in O.1/ time, but if we wish to delete an element with a given key, ‚.n/ time
is required in the worst case because we must ﬁrst call L IST-SEARCH to ﬁnd the
element.
Sentinels
The code for L IST-DELETE would be simpler if we could ignore the boundary
conditions at the head and tail of the list:
LIST-DELETE0.L; x/
1x:pre/ETB:nextDx:next
2x:next:pre/ETBDx:pre/ETB
Asentinel is a dummy object that allows us to simplify boundary conditions. For
example, suppose that we provide with list Lan object L:nilthat represents NIL10.2 Linked lists 239
9 16 4 1
9 16 4 1 25
9 16 4 25(a)
(b)
(c)
(d) L:nilL:nilL:nilL:nil
Figure 10.4 A circular, doubly linked list with a sentinel. The sentinel L:nilappears between the
head and tail. The attribute L:head is no longer needed, since we can access the head of the list
byL:nil:next.(a)An empty list. (b)The linked list from Figure 10.3(a), with key 9 at the head and
key 1 at the tail. (c)The list after executing L IST-INSERT0.L; x/ ,w h e r e x:keyD25. The new object
becomes the head of the list. (d)The list after deleting the object with key 1. The new tail is the
object with key 4.
but has all the attributes of the other objects in the list. Wherever we have a ref-
erence to NILin list code, we replace it by a reference to the sentinel L:nil.A s
shown in Figure 10.4, this change turns a regular doubly linked list into a circu-
lar, doubly linked list with a sentinel , in which the sentinel L:nillies between the
head and tail. The attribute L:nil:next points to the head of the list, and L:nil:pre/ETB
points to the tail. Similarly, both the next attribute of the tail and the pre/ETBat-
tribute of the head point to L:nil.S i n c e L:nil:next points to the head, we can
eliminate the attribute L:head altogether, replacing references to it by references
toL:nil:next. Figure 10.4(a) shows that an empty list consists of just the sentinel,
and both L:nil:next andL:nil:pre/ETBpoint to L:nil.
The code for L IST-SEARCH remains the same as before, but with the references
toNILandL:head changed as speciﬁed above:
LIST-SEARCH0.L; k/
1xDL:nil:next
2while x¤L:nilandx:key¤k
3 xDx:next
4return x
We use the two-line procedure L IST-DELETE0from before to delete an element
from the list. The following procedure inserts an element into the list:240 Chapter 10 Elementary Data Structures
LIST-INSERT0.L; x/
1x:nextDL:nil:next
2L:nil:next:pre/ETBDx
3L:nil:nextDx
4x:pre/ETBDL:nil
Figure 10.4 shows the effects of L IST-INSERT0and L IST-DELETE0on a sample list.
Sentinels rarely reduce the asymptotic time bounds of data structure operations,
but they can reduce constant factors. The gain from using sentinels within loopsis usually a matter of clarity of code rather than speed; the linked list code, forexample, becomes simpler when we use sentinels, but we save only O.1/ time in
the L
IST-INSERT0and L IST-DELETE0procedures. In other situations, however, the
use of sentinels helps to tighten the code in a loop, thus reducing the coefﬁcient of,
say,norn2in the running time.
We should use sentinels judiciously. When there are many small lists, the extra
storage used by their sentinels can represent signiﬁcant wasted memory. In thisbook, we use sentinels only when they truly simplify the code.
Exercises
10.2-1
Can you implement the dynamic-set operation I
NSERT on a singly linked list
inO.1/ time? How about D ELETE ?
10.2-2
Implement a stack using a singly linked list L. The operations P USH and P OP
should still take O.1/ time.
10.2-3
Implement a queue by a singly linked list L. The operations E NQUEUE and D E-
QUEUE should still take O.1/ time.
10.2-4
As written, each loop iteration in the L IST-SEARCH0procedure requires two tests:
one for x¤L:niland one for x:key¤k. Show how to eliminate the test for
x¤L:nilin each iteration.
10.2-5
Implement the dictionary operations I NSERT ,DELETE ,a n dS EARCH using singly
linked, circular lists. What are the running times of your procedures?10.3 Implementing pointers and objects 241
10.2-6
The dynamic-set operation U NION takes two disjoint sets S1andS2as input, and
it returns a set SDS1[S2consisting of all the elements of S1andS2.T h e
setsS1andS2are usually destroyed by the operation. Show how to support U NION
inO.1/ time using a suitable list data structure.
10.2-7
Give a ‚.n/ -time nonrecursive procedure that reverses a singly linked list of n
elements. The procedure should use no more than constant storage beyond that
needed for the list itself.
10.2-8 ?
Explain how to implement doubly linked lists using only one pointer value x:npper
item instead of the usual two ( next andpre/ETB). Assume that all pointer values can be
interpreted as k-bit integers, and deﬁne x:npto be x:npDx:next XOR x:pre/ETB,
thek-bit “exclusive-or” of x:next andx:pre/ETB.( T h ev a l u e NILis represented by 0.)
Be sure to describe what information you need to access the head of the list. Showhow to implement the S
EARCH ,INSERT ,a n dD ELETE operations on such a list.
Also show how to reverse such a list in O.1/ time.
10.3 Implementing pointers and objects
How do we implement pointers and objects in languages that do not provide them?
In this section, we shall see two ways of implementing linked data structures with-out an explicit pointer data type. We shall synthesize objects and pointers fromarrays and array indices.
A multiple-array representation of objects
We can represent a collection of objects that have the same attributes by using an
array for each attribute. As an example, Figure 10.5 shows how we can implementthe linked list of Figure 10.3(a) with three arrays. The array keyholds the values
of the keys currently in the dynamic set, and the pointers reside in the arrays next
andpre/ETB. For a given array index x, the array entries keyŒx/c141,nextŒx/c141,a n d pre/ETBŒx/c141
represent an object in the linked list. Under this interpretation, a pointer xis simply
a common index into the key,next,a n d pre/ETBarrays.
In Figure 10.3(a), the object with key 4follows the object with key 16in the
linked list. In Figure 10.5, key 4appears in keyŒ2/c141,a n dk e y 16appears in keyŒ5/c141,
and so nextŒ5/c141D2andpre/ETBŒ2/c141D5. Although the constant
NILappears in the next242 Chapter 10 Elementary Data Structures
12345678
keynext
prevL 7
4 1 16 932 5
52 7
Figure 10.5 The linked list of Figure 10.3(a) represented by the arrays key,next,a n d pre/ETB. Each
vertical slice of the arrays represents a single object. Stored pointers correspond to the array indices
shown at the top; the arrows show how to interpret them. Lightly shaded object positions contain list
elements. The variable Lkeeps the index of the head.
attribute of the tail and the pre/ETBattribute of the head, we usually use an integer
(such as 0or/NUL1) that cannot possibly represent an actual index into the arrays. A
variable Lholds the index of the head of the list.
A single-array representation of objects
The words in a computer memory are typically addressed by integers from 0
toM/NUL1,w h e r e Mis a suitably large integer. In many programming languages,
an object occupies a contiguous set of locations in the computer memory. A pointeris simply the address of the ﬁrst memory location of the object, and we can addressother memory locations within the object by adding an offset to the pointer.
We can use the same strategy for implementing objects in programming envi-
ronments that do not provide explicit pointer data types. For example, Figure 10.6shows how to use a single array Ato store the linked list from Figures 10.3(a)
and 10.5. An object occupies a contiguous subarray AŒj : : k/c141 . Each attribute of
the object corresponds to an offset in the range from 0tok/NULj, and a pointer to
the object is the index j. In Figure 10.6, the offsets corresponding to key,next,a n d
pre/ETBare 0, 1, and 2, respectively. To read the value of i:pre/ETB, given a pointer i,w e
add the value iof the pointer to the offset 2, thus reading AŒiC2/c141.
The single-array representation is ﬂexible in that it permits objects of different
lengths to be stored in the same array. The problem of managing such a heteroge-
neous collection of objects is more difﬁcult than the problem of managing a homo-
geneous collection, where all objects have the same attributes. Since most of the
data structures we shall consider are composed of homogeneous elements, it will
be sufﬁcient for our purposes to use the multiple-array representation of objects.10.3 Implementing pointers and objects 243
12345678
AL
4 1 16 974 49 1 01 11 21 31 41 51 61 71 81 92 02 12 22 32 4
prev
nextkey19
13 13 19
Figure 10.6 The linked list of Figures 10.3(a) and 10.5 represented in a single array A. Each list
element is an object that occupies a contiguous subarray of length 3 within the array. The threeattributes key,next,a n d pre/ETBcorrespond to the offsets 0, 1, and 2, respectively, within each object.
A pointer to an object is the index of the ﬁrst element of the object. Objects containing list elements
are lightly shaded, and arrows show the list ordering.
Allocating and freeing objects
To insert a key into a dynamic set represented by a doubly linked list, we must al-
locate a pointer to a currently unused object in the linked-list representation. Thus,it is useful to manage the storage of objects not currently used in the linked-listrepresentation so that one can be allocated. In some systems, a garbage collec-
toris responsible for determining which objects are unused. Many applications,
however, are simple enough that they can bear responsibility for returning an un-
used object to a storage manager. We shall now explore the problem of allocating
and freeing (or deallocating) homogeneous objects using the example of a doublylinked list represented by multiple arrays.
Suppose that the arrays in the multiple-array representation have length mand
that at some moment the dynamic set contains n/DC4melements. Then nobjects
represent elements currently in the dynamic set, and the remaining m/NULnobjects are
free; the free objects are available to represent elements inserted into the dynamic
set in the future.
We keep the free objects in a singly linked list, which we call the free list .T h e
free list uses only the next array, which stores the next pointers within the list.
The head of the free list is held in the global variable free. When the dynamic
set represented by linked list Lis nonempty, the free list may be intertwined with
listL, as shown in Figure 10.7. Note that each object in the representation is either
in list Lor in the free list, but not in both.
The free list acts like a stack: the next object allocated is the last one freed. We
can use a list implementation of the stack operations P
USH and P OPto implement
the procedures for allocating and freeing objects, respectively. We assume that theglobal variable freeused in the following procedures points to the ﬁrst element of
the free list.244 Chapter 10 Elementary Data Structures
12345678
keynext
prevL7
4 1 16 932 5
52 74
86 1free
(a)12345678
keynext
prevL4
4 1 16 932 5
52 78
76 1free
(b)425
12345678
keynext
prevL4
41 938 2
725
76 1free
(c)425
Figure 10.7 The effect of the A LLOCA TE -OBJECT and F REE-OBJECT procedures. (a)The list
of Figure 10.5 (lightly shaded) and a free list (heavily shaded). Arrows show the free-list structure.(b)The result of calling A
LLOCA TE -OBJECT ./(which returns index 4), setting keyŒ4/c141to 25, and
calling L IST-INSERT .L; 4/ . The new free-list head is object 8, which had been nextŒ4/c141on the free
list.(c)After executing L IST-DELETE .L; 5/ , we call F REE-OBJECT .5/. Object 5 becomes the new
free-list head, with object 8 following it on the free list.
ALLOCATE -OBJECT ./
1iffree ==NIL
2 error “out of space”
3elsexDfree
4 freeDx:next
5 return x
FREE-OBJECT .x/
1x:nextDfree
2freeDx
The free list initially contains all nunallocated objects. Once the free list has been
exhausted, running the A LLOCATE -OBJECT procedure signals an error. We can
even service several linked lists with just a single free list. Figure 10.8 shows twolinked lists and a free list intertwined through key,next,a n d pre/ETBarrays.
The two procedures run in O.1/ time, which makes them quite practical. We
can modify them to work for any homogeneous collection of objects by letting anyone of the attributes in the object act like a next attribute in the free list.10.3 Implementing pointers and objects 245
123456789 1 0
next
key
prevfree
362
637 1 5
79910
4 8
1L2
L1k1k2k3 k5k6k7 k9
Figure 10.8 Two linked lists, L1(lightly shaded) and L2(heavily shaded), and a free list (dark-
ened) intertwined.
Exercises
10.3-1
Draw a picture of the sequence h13; 4; 8; 19; 5; 11istored as a doubly linked list
using the multiple-array representation. Do the same for the single-array represen-
tation.
10.3-2
Write the procedures A LLOCATE -OBJECT and F REE-OBJECT for a homogeneous
collection of objects implemented by the single-array representation.
10.3-3
Why don’t we need to set or reset the pre/ETBattributes of objects in the implementa-
tion of the A LLOCATE -OBJECT and F REE-OBJECT procedures?
10.3-4
It is often desirable to keep all elements of a doubly linked list compact in storage,using, for example, the ﬁrst mindex locations in the multiple-array representation.
(This is the case in a paged, virtual-memory computing environment.) Explainhow to implement the procedures A
LLOCATE -OBJECT and F REE-OBJECT so that
the representation is compact. Assume that there are no pointers to elements of thelinked list outside the list itself. ( Hint: Use the array implementation of a stack.)
10.3-5
LetLbe a doubly linked list of length nstored in arrays key,pre/ETB,a n d next of
length m. Suppose that these arrays are managed by A
LLOCATE -OBJECT and
FREE-OBJECT procedures that keep a doubly linked free list F. Suppose further
that of the mitems, exactly nare on list Landm/NULnare on the free list. Write
a procedure C OMPACTIFY -LIST.L; F / that, given the list Land the free list F,
moves the items in Lso that they occupy array positions 1 ;2;:::;n and adjusts the
free list Fso that it remains correct, occupying array positions nC1; nC2;:::;m .
The running time of your procedure should be ‚.n/ , and it should use only a
constant amount of extra space. Argue that your procedure is correct.246 Chapter 10 Elementary Data Structures
10.4 Representing rooted trees
The methods for representing lists given in the previous section extend to any ho-
mogeneous data structure. In this section, we look speciﬁcally at the problem ofrepresenting rooted trees by linked data structures. We ﬁrst look at binary trees,and then we present a method for rooted trees in which nodes can have an arbitrary
number of children.
We represent each node of a tree by an object. As with linked lists, we assume
that each node contains a keyattribute. The remaining attributes of interest are
pointers to other nodes, and they vary according to the type of tree.
Binary trees
Figure 10.9 shows how we use the attributes p,left,a n d right to store pointers to
the parent, left child, and right child of each node in a binary tree T.I fx:pD
NIL,
thenxis the root. If node xhas no left child, then x:leftDNIL, and similarly for
the right child. The root of the entire tree Tis pointed to by the attribute T:root.I f
T:rootDNIL, then the tree is empty.
Rooted trees with unbounded branching
We can extend the scheme for representing a binary tree to any class of trees in
which the number of children of each node is at most some constant k: we replace
theleftandright attributes by child 1;child 2;:::; child k. This scheme no longer
works when the number of children of a node is unbounded, since we do not knowhow many attributes (arrays in the multiple-array representation) to allocate in ad-vance. Moreover, even if the number of children kis bounded by a large constant
but most nodes have a small number of children, we may waste a lot of memory.
Fortunately, there is a clever scheme to represent trees with arbitrary numbers of
children. It has the advantage of using only O.n/ space for any n-node rooted tree.
Theleft-child, right-sibling representation appears in Figure 10.10. As before,
each node contains a parent pointer p,a n d T:root points to the root of tree T.
Instead of having a pointer to each of its children, however, each node xhas only
two pointers:
1.x:left-child points to the leftmost child of node x,a n d
2.x:right -sibling points to the sibling of ximmediately to its right.
If node xhas no children, then x:left-childD
NIL, and if node xis the rightmost
child of its parent, then x:right -siblingDNIL.10.4 Representing rooted trees 247
T:root
Figure 10.9 The representation of a binary tree T. Each node xhas the attributes x:p(top), x:left
(lower left), and x:right (lower right). The keyattributes are not shown.
T:root
Figure 10.10 The left-child, right-sibling representation of a tree T. Each node xhas attributes x:p
(top), x:left-child (lower left), and x:right -sibling (lower right). The keyattributes are not shown.248 Chapter 10 Elementary Data Structures
Other tree representations
We sometimes represent rooted trees in other ways. In Chapter 6, for example,
we represented a heap, which is based on a complete binary tree, by a single arrayplus the index of the last node in the heap. The trees that appear in Chapter 21 aretraversed only toward the root, and so only the parent pointers are present; thereare no pointers to children. Many other schemes are possible. Which scheme isbest depends on the application.
Exercises
10.4-1
Draw the binary tree rooted at index 6that is represented by the following at-
tributes:
index key left right
11 2 7 3
21 5 8 NIL
34 1 0 NIL
41 0 5 9
52 NIL NIL
61 8 1 4
77 NIL NIL
81 4 6 2
92 1 NIL NIL
10 5 NIL NIL
10.4-2
Write an O.n/ -time recursive procedure that, given an n-node binary tree, prints
out the key of each node in the tree.
10.4-3
Write an O.n/ -time nonrecursive procedure that, given an n-node binary tree,
prints out the key of each node in the tree. Use a stack as an auxiliary data structure.
10.4-4
Write an O.n/ -time procedure that prints all the keys of an arbitrary rooted tree
withnnodes, where the tree is stored using the left-child, right-sibling representa-
tion.
10.4-5 ?
Write an O.n/ -time nonrecursive procedure that, given an n-node binary tree,
prints out the key of each node. Use no more than constant extra space outsideProblems for Chapter 10 249
of the tree itself and do not modify the tree, even temporarily, during the proce-
dure.
10.4-6 ?
The left-child, right-sibling representation of an arbitrary rooted tree uses threepointers in each node: left-child ,right -sibling ,a n d parent . From any node, its
parent can be reached and identiﬁed in constant time and all its children can bereached and identiﬁed in time linear in the number of children. Show how to useonly two pointers and one boolean value in each node so that the parent of a node
or all of its children can be reached and identiﬁed in time linear in the number of
children.
Problems
10-1 Comparisons among listsFor each of the four types of lists in the following table, what is the asymptoticworst-case running time for each dynamic-set operation listed?
unsorted,
 sorted,
 unsorted,
 sorted,
singly
 singly
 doubly
 doubly
linked
 linked
 linked
 linked
SEARCH .L; k/
INSERT .L; x/
DELETE .L; x/
SUCCESSOR .L; x/
PREDECESSOR .L; x/
MINIMUM .L/
MAXIMUM .L/
250 Chapter 10 Elementary Data Structures
10-2 Mergeable heaps using linked lists
Amergeable heap supports the following operations: M AKE-HEAP(which creates
an empty mergeable heap), I NSERT ,M INIMUM ,EXTRACT -MIN,a n dU NION .1
Show how to implement mergeable heaps using linked lists in each of the followingcases. Try to make each operation as efﬁcient as possible. Analyze the runningtime of each operation in terms of the size of the dynamic set(s) being operated on.
a.Lists are sorted.
b.Lists are unsorted.
c.Lists are unsorted, and dynamic sets to be merged are disjoint.
10-3 Searching a sorted compact list
Exercise 10.3-4 asked how we might maintain an n-element list compactly in the
ﬁrstnpositions of an array. We shall assume that all keys are distinct and that the
compact list is also sorted, that is, keyŒi/c141 < keyŒnextŒi/c141/c141for all iD1 ;2;:::;n such
thatnextŒi/c141¤
NIL. We will also assume that we have a variable Lthat contains
the index of the ﬁrst element on the list. Under these assumptions, you will showthat we can use the following randomized algorithm to search the list in O.p
n/
expected time.
COMPACT -LIST-SEARCH . L;n;k/
1iDL
2while i¤NILandkeyŒi/c141 < k
3 jDRANDOM .1; n/
4 ifkeyŒi/c141 < keyŒj /c141andkeyŒj /c141/DC4k
5 iDj
6 ifkeyŒi/c141==k
7 return i
8 iDnextŒi/c141
9ifi==NILorkeyŒi/c141 > k
10 return NIL
11else return i
If we ignore lines 3–7 of the procedure, we have an ordinary algorithm for
searching a sorted linked list, in which index ipoints to each position of the list in
1Because we have deﬁned a mergeable heap to support M INIMUM and E XTRACT -MIN, we can also
refer to it as a mergeable min-heap . Alternatively, if it supported M AXIMUM and E XTRACT -MAX,
it would be a mergeable max-heap .Problems for Chapter 10 251
turn. The search terminates once the index i“falls off” the end of the list or once
keyŒi/c141/NAKk. In the latter case, if keyŒi/c141Dk, clearly we have found a key with the
value k. If, however, keyŒi/c141 > k , then we will never ﬁnd a key with the value k,
and so terminating the search was the right thing to do.
Lines 3–7 attempt to skip ahead to a randomly chosen position j.S u c h a s k i p
beneﬁts us if keyŒj /c141is larger than keyŒi/c141a n dn ol a r g e rt h a n k; in such a case, j
marks a position in the list that iwould have to reach during an ordinary list search.
Because the list is compact, we know that any choice of jbetween 1andnindexes
some object in the list rather than a slot on the free list.
Instead of analyzing the performance of C OMPACT -LIST-SEARCH directly, we
shall analyze a related algorithm, C OMPACT -LIST-SEARCH0, which executes two
separate loops. This algorithm takes an additional parameter twhich determines
an upper bound on the number of iterations of the ﬁrst loop.
COMPACT -LIST-SEARCH0. L;n;k;t/
1iDL
2forqD1tot
3 jDRANDOM .1; n/
4 ifkeyŒi/c141 < keyŒj /c141andkeyŒj /c141/DC4k
5 iDj
6 ifkeyŒi/c141==k
7 return i
8while i¤NILandkeyŒi/c141 < k
9 iDnextŒi/c141
10ifi==NILorkeyŒi/c141 > k
11 return NIL
12else return i
To compare the execution of the algorithms C OMPACT -LIST-SEARCH . L;n;k/
and C OMPACT -LIST-SEARCH0. L;n;k;t/ , assume that the sequence of integers re-
turned by the calls of R ANDOM .1; n/ is the same for both algorithms.
a.Suppose that C OMPACT -LIST-SEARCH . L;n;k/ takes titerations of the while
loop of lines 2–8. Argue that C OMPACT -LIST-SEARCH0. L;n;k;t/ returns the
same answer and that the total number of iterations of both the forandwhile
loops within C OMPACT -LIST-SEARCH0is at least t.
In the call C OMPACT -LIST-SEARCH0. L;n;k;t/ ,l e tXtbe the random variable that
describes the distance in the linked list (that is, through the chain of next pointers)
from position ito the desired key kafter titerations of the forloop of lines 2–7
have occurred.252 Chapter 10 Elementary Data Structures
b.Argue that the expected running time of C OMPACT -LIST-SEARCH0. L;n;k;t/
isO.tCEŒXt/c141/.
c.Show that E ŒXt/c141/DC4Pn
rD1.1/NULr=n/t.(Hint: Use equation (C.25).)
d.Show thatPn/NUL1
rD0rt/DC4ntC1=.tC1/.
e.Prove that E ŒXt/c141/DC4n=.tC1/.
f.Show that C OMPACT -LIST-SEARCH0. L;n;k;t/ runs in O.tCn=t/ expected
time.
g.Conclude that C OMPACT -LIST-SEARCH runs in O.p
n/expected time.
h.Why do we assume that all keys are distinct in C OMPACT -LIST-SEARCH ?A r -
gue that random skips do not necessarily help asymptotically when the list con-tains repeated key values.
Chapter notes
Aho, Hopcroft, and Ullman [6] and Knuth [209] are excellent references for ele-mentary data structures. Many other texts cover both basic data structures and theirimplementation in a particular programming language. Examples of these types of
textbooks include Goodrich and Tamassia [147], Main [241], Shaffer [311], and
Weiss [352, 353, 354]. Gonnet [145] provides experimental data on the perfor-mance of many data-structure operations.
The origin of stacks and queues as data structures in computer science is un-
clear, since corresponding notions already existed in mathematics and paper-basedbusiness practices before the introduction of digital computers. Knuth [209] citesA. M. Turing for the development of stacks for subroutine linkage in 1947.
Pointer-based data structures also seem to be a folk invention. According to
Knuth, pointers were apparently used in early computers with drum memories. TheA-1 language developed by G. M. Hopper in 1951 represented algebraic formulasas binary trees. Knuth credits the IPL-II language, developed in 1956 by A. Newell,J. C. Shaw, and H. A. Simon, for recognizing the importance and promoting theuse of pointers. Their IPL-III language, developed in 1957, included explicit stackoperations.11 Hash Tables
Many applications require a dynamic set that supports only the dictionary opera-
tions I NSERT ,SEARCH ,a n dD ELETE . For example, a compiler that translates a
programming language maintains a symbol table, in which the keys of elementsare arbitrary character strings corresponding to identiﬁers in the language. A hashtable is an effective data structure for implementing dictionaries. Although search-ing for an element in a hash table can take as long as searching for an element in alinked list— ‚.n/ time in the worst case—in practice, hashing performs extremely
well. Under reasonable assumptions, the average time to search for an element in
a hash table is O.1/ .
A hash table generalizes the simpler notion of an ordinary array. Directly ad-
dressing into an ordinary array makes effective use of our ability to examine an
arbitrary position in an array in O.1/ time. Section 11.1 discusses direct address-
ing in more detail. We can take advantage of direct addressing when we can affordto allocate an array that has one position for every possible key.
When the number of keys actually stored is small relative to the total number of
possible keys, hash tables become an effective alternative to directly addressing anarray, since a hash table typically uses an array of size proportional to the numberof keys actually stored. Instead of using the key as an array index directly, the arrayindex is computed from the key. Section 11.2 presents the main ideas, focusing on
“chaining” as a way to handle “collisions,” in which more than one key maps to thesame array index. Section 11.3 describes how we can compute array indices fromkeys using hash functions. We present and analyze several variations on the basictheme. Section 11.4 looks at “open addressing,” which is another way to deal withcollisions. The bottom line is that hashing is an extremely effective and practicaltechnique: the basic dictionary operations require only O.1/ time on the average.
Section 11.5 explains how “perfect hashing” can support searches in O.1/ worst-
case time, when the set of keys being stored is static (that is, when the set of keys
never changes once stored).254 Chapter 11 Hash Tables
11.1 Direct-address tables
Direct addressing is a simple technique that works well when the universe Uof
keys is reasonably small. Suppose that an application needs a dynamic set in whicheach element has a key drawn from the universe UDf0; 1; : : : ; m/NUL1g,w h e r e m
is not too large. We shall assume that no two elements have the same key.
To represent the dynamic set, we use an array, or direct-address table , denoted
byTŒ 0::m/NUL1/c141, in which each position, or slot, corresponds to a key in the uni-
verse U. Figure 11.1 illustrates the approach; slot kpoints to an element in the set
with key k. If the set contains no element with key k,t h e n TŒ k/c141D
NIL.
The dictionary operations are trivial to implement:
DIRECT -ADDRESS -SEARCH .T; k/
1return TŒ k/c141
DIRECT -ADDRESS -INSERT .T; x/
1TŒ x: key/c141Dx
DIRECT -ADDRESS -DELETE .T; x/
1TŒ x: key/c141DNIL
Each of these operations takes only O.1/ time.
T
U
(universe of keys)
K
(actual
keys)2
3
581940
762
3
5
8key satellite data
20
1
3
4
5
678
9
Figure 11.1 How to implement a dynamic set by a direct-address table T. Each key in the universe
UDf0; 1; : : : ; 9gcorresponds to an index in the table. The set KDf2; 3; 5; 8gof actual keys
determines the slots in the table that contain pointers to elements. The other slots, heavily shaded,
contain NIL.11.1 Direct-address tables 255
For some applications, the direct-address table itself can hold the elements in the
dynamic set. That is, rather than storing an element’s key and satellite data in anobject external to the direct-address table, with a pointer from a slot in the table tothe object, we can store the object in the slot itself, thus saving space. We woulduse a special key within an object to indicate an empty slot. Moreover, it is oftenunnecessary to store the key of the object, since if we have the index of an objectin the table, we have its key. If keys are not stored, however, we must have someway to tell whether the slot is empty.
Exercises
11.1-1
Suppose that a dynamic set Sis represented by a direct-address table Tof length m.
Describe a procedure that ﬁnds the maximum element of S. What is the worst-case
performance of your procedure?
11.1-2
Abit vector is simply an array of bits ( 0sa n d 1s). A bit vector of length mtakes
much less space than an array of mpointers. Describe how to use a bit vector
to represent a dynamic set of distinct elements with no satellite data. Dictionary
operations should run in O.1/ time.
11.1-3
Suggest how to implement a direct-address table in which the keys of stored el-ements do not need to be distinct and the elements can have satellite data. Allthree dictionary operations (I
NSERT ,DELETE ,a n dS EARCH ) should run in O.1/
time. (Don’t forget that D ELETE takes as an argument a pointer to an object to be
deleted, not a key.)
11.1-4 ?
We wish to implement a dictionary by using direct addressing on a huge array. At
the start, the array entries may contain garbage, and initializing the entire arrayis impractical because of its size. Describe a scheme for implementing a direct-address dictionary on a huge array. Each stored object should use O.1/ space;
the operations S
EARCH ,INSERT ,a n dD ELETE should take O.1/ time each; and
initializing the data structure should take O.1/ time. ( Hint: Use an additional array,
treated somewhat like a stack whose size is the number of keys actually stored in
the dictionary, to help determine whether a given entry in the huge array is valid ornot.)256 Chapter 11 Hash Tables
11.2 Hash tables
The downside of direct addressing is obvious: if the universe Uis large, storing
at a b l e Tof sizejUjmay be impractical, or even impossible, given the memory
available on a typical computer. Furthermore, the set Kof keys actually stored
may be so small relative to Uthat most of the space allocated for Twould be
wasted.
When the set Kof keys stored in a dictionary is much smaller than the uni-
verse Uof all possible keys, a hash table requires much less storage than a direct-
address table. Speciﬁcally, we can reduce the storage requirement to ‚.jKj/while
we maintain the beneﬁt that searching for an element in the hash table still requiresonlyO.1/ time. The catch is that this bound is for the average-case time , whereas
for direct addressing it holds for the worst-case time .
With direct addressing, an element with key kis stored in slot k. With hashing,
this element is stored in slot h.k/ ; that is, we use a hash function hto compute the
slot from the key k. Here, hmaps the universe Uof keys into the slots of a hash
table TŒ 0::m/NUL1/c141:
hWU!f0; 1; : : : ; m/NUL1g;
where the size mof the hash table is typically much less than jUj. We say that an
element with key khashes to slot h.k/ ; we also say that h.k/ is thehash value of
keyk. Figure 11.2 illustrates the basic idea. The hash function reduces the range
of array indices and hence the size of the array. Instead of a size of jUj, the array
can have size
m.
T
U
(universe of keys)
K
(actual
keys)0
m–1k1
k2k3k4 k5h(k1)
h(k4)
h(k3)h(k2) = h(k5)
Figure 11.2 Using a hash function hto map keys to hash-table slots. Because keys k2andk5map
to the same slot, they collide.11.2 Hash tables 257
T
U
(universe of keys)
K
(actual
keys)k1
k2 k3k4 k5
k6k7
k8k1
k2
k3k4
k5
k6k7
k8
Figure 11.3 Collision resolution by chaining. Each hash-table slot TŒ j/c141 contains a linked list of
all the keys whose hash value is j. For example, h.k1/Dh.k4/andh.k5/Dh.k7/Dh.k2/.
The linked list can be either singly or doubly linked; we show it as doubly linked because deletion isfaster that way.
There is one hitch: two keys may hash to the same slot. We call this situation
acollision . Fortunately, we have effective techniques for resolving the conﬂict
created by collisions.
Of course, the ideal solution would be to avoid collisions altogether. We might
try to achieve this goal by choosing a suitable hash function h. One idea is to
make happear to be “random,” thus avoiding collisions or at least minimizing
their number. The very term “to hash,” evoking images of random mixing andchopping, captures the spirit of this approach. (Of course, a hash function hmust be
deterministic in that a given input kshould always produce the same output h.k/ .)
BecausejUj>m, however, there must be at least two keys that have the same hash
value; avoiding collisions altogether is therefore impossible. Thus, while a well-
designed, “random”-looking hash function can minimize the number of collisions,
we still need a method for resolving the collisions that do occur.
The remainder of this section presents the simplest collision resolution tech-
nique, called chaining. Section 11.4 introduces an alternative method for resolvingcollisions, called open addressing.
Collision resolution by chaining
Inchaining , we place all the elements that hash to the same slot into the same
linked list, as Figure 11.3 shows. Slot jcontains a pointer to the head of the list of
all stored elements that hash to j; if there are no such elements, slot jcontains
NIL.258 Chapter 11 Hash Tables
The dictionary operations on a hash table Tare easy to implement when colli-
sions are resolved by chaining:
CHAINED -HASH-INSERT .T; x/
1 insert xat the head of list TŒ h . x: key//c141
CHAINED -HASH-SEARCH .T; k/
1 search for an element with key kin list TŒ h . k / /c141
CHAINED -HASH-DELETE .T; x/
1 delete xfrom the list TŒ h . x: key//c141
The worst-case running time for insertion is O.1/ . The insertion procedure is fast
in part because it assumes that the element xbeing inserted is not already present in
the table; if necessary, we can check this assumption (at additional cost) by search-ing for an element whose key is x:keybefore we insert. For searching, the worst-
case running time is proportional to the length of the list; we shall analyze thisoperation more closely below. We can delete an element in O.1/ time if the lists
are doubly linked, as Figure 11.3 depicts. (Note that C
HAINED -HASH-DELETE
takes as input an element xand not its key k, so that we don’t have to search for x
ﬁrst. If the hash table supports deletion, then its linked lists should be doubly linkedso that we can delete an item quickly. If the lists were only singly linked, then to
delete element x, we would ﬁrst have to ﬁnd xin the list TŒ h . x: key//c141so that we
could update the next attribute of x’s predecessor. With singly linked lists, both
deletion and searching would have the same asymptotic running times.)
Analysis of hashing with chaining
How well does hashing with chaining perform? In particular, how long does it take
to search for an element with a given key?
Given a hash table Twith mslots that stores nelements, we deﬁne the load
factor ˛forTasn=m , that is, the average number of elements stored in a chain.
Our analysis will be in terms of ˛, which can be less than, equal to, or greater
than1.
The worst-case behavior of hashing with chaining is terrible: all nkeys hash
to the same slot, creating a list of length n. The worst-case time for searching is
thus‚.n/ plus the time to compute the hash function—no better than if we used
one linked list for all the elements. Clearly, we do not use hash tables for theirworst-case performance. (Perfect hashing, described in Section 11.5, does providegood worst-case performance when the set of keys is static, however.)
The average-case performance of hashing depends on how well the hash func-
tionhdistributes the set of keys to be stored among the mslots, on the average.11.2 Hash tables 259
Section 11.3 discusses these issues, but for now we shall assume that any given
element is equally likely to hash into any of the mslots, independently of where
any other element has hashed to. We call this the assumption of simple uniform
hashing .
ForjD0; 1; : : : ; m/NUL1, let us denote the length of the list TŒ j/c141 bynj,s ot h a t
nDn0Cn1C/SOH/SOH/SOHC nm/NUL1; (11.1)
and the expected value of njis EŒnj/c141D˛Dn=m .
We assume that O.1/ time sufﬁces to compute the hash value h.k/ ,s ot h a t
the time required to search for an element with key kdepends linearly on the
length nh.k/of the list TŒ h . k/ /c141 . Setting aside the O.1/ time required to compute
the hash function and to access slot h.k/ , let us consider the expected number of
elements examined by the search algorithm, that is, the number of elements in thelistTŒ h . k/ /c141 that the algorithm checks to see whether any have a key equal to k.W e
shall consider two cases. In the ﬁrst, the search is unsuccessful: no element in thetable has key k. In the second, the search successfully ﬁnds an element with key k.
Theorem 11.1
In a hash table in which collisions are resolved by chaining, an unsuccessful searchtakes average-case time ‚.1C˛/, under the assumption of simple uniform hashing.
Proof Under the assumption of simple uniform hashing, any key knot already
stored in the table is equally likely to hash to any of the mslots. The expected time
to search unsuccessfully for a key kis the expected time to search to the end of
listTŒ h . k/ /c141 , which has expected length E Œn
h.k//c141D˛. Thus, the expected number
of elements examined in an unsuccessful search is ˛, and the total time required
(including the time for computing h.k/ )i s‚.1C˛/.
The situation for a successful search is slightly different, since each list is not
equally likely to be searched. Instead, the probability that a list is searched is pro-portional to the number of elements it contains. Nonetheless, the expected searchtime still turns out to be ‚.1C˛/.
Theorem 11.2
In a hash table in which collisions are resolved by chaining, a successful searchtakes average-case time ‚.1C˛/, under the assumption of simple uniform hashing.
Proof We assume that the element being searched for is equally likely to be any
of the nelements stored in the table. The number of elements examined during a
successful search for an element xis one more than the number of elements that260 Chapter 11 Hash Tables
appear before xinx’s list. Because new elements are placed at the front of the
list, elements before xin the list were all inserted after xwas inserted. To ﬁnd
the expected number of elements examined, we take the average, over the nele-
ments xin the table, of 1plus the expected number of elements added to x’s list
after xwas added to the list. Let xidenote the ith element inserted into the ta-
ble, for iD1 ;2;:::;n ,a n dl e t kiDxi:key. For keys kiandkj,w ed e ﬁ n et h e
indicator random variable XijDIfh.k i/Dh.k j/g. Under the assumption of sim-
ple uniform hashing, we have Pr fh.k i/Dh.k j/gD1=m, and so by Lemma 5.1,
EŒXij/c141D1=m. Thus, the expected number of elements examined in a successful
search is
E"
1
nnX
iD1 
1CnX
jDiC1Xij!#
D1
nnX
iD1 
1CnX
jDiC1EŒXij/c141!
(by linearity of expectation)
D1
nnX
iD1 
1CnX
jDiC11
m!
D1C1
nmnX
iD1.n/NULi/
D1C1
nm nX
iD1n/NULnX
iD1i!
D1C1
nm/DC2
n2/NULn.nC1/
2/DC3
(by equation (A.1))
D1Cn/NUL1
2m
D1C˛
2/NUL˛
2n:
Thus, the total time required for a successful search (including the time for com-
puting the hash function) is ‚.2C˛=2/NUL˛=2n/D‚.1C˛/.
What does this analysis mean? If the number of hash-table slots is at least pro-
portional to the number of elements in the table, we have nDO.m/ and, con-
sequently, ˛Dn=mDO.m/=mDO.1/ . Thus, searching takes constant time
on average. Since insertion takes O.1/ worst-case time and deletion takes O.1/
worst-case time when the lists are doubly linked, we can support all dictionaryoperations in O.1/ time on average.11.2 Hash tables 261
Exercises
11.2-1
Suppose we use a hash function hto hash ndistinct keys into an array Tof
length m. Assuming simple uniform hashing, what is the expected number of
collisions? More precisely, what is the expected cardinality of ffk;lgWk¤land
h.k/Dh.l/g?
11.2-2
Demonstrate what happens when we insert the keys 5; 28; 19; 15; 20; 33; 12; 17; 10
into a hash table with collisions resolved by chaining. Let the table have 9slots,
and let the hash function be h.k/Dkmod9.
11.2-3
Professor Marley hypothesizes that he can obtain substantial performance gains bymodifying the chaining scheme to keep each list in sorted order. How does the pro-fessor’s modiﬁcation affect the running time for successful searches, unsuccessfulsearches, insertions, and deletions?
11.2-4
Suggest how to allocate and deallocate storage for elements within the hash tableitself by linking all unused slots into a free list. Assume that one slot can storea ﬂag and either one element plus a pointer or two pointers. All dictionary and
free-list operations should run in O.1/ expected time. Does the free list need to be
doubly linked, or does a singly linked free list sufﬁce?
11.2-5
Suppose that we are storing a set of nkeys into a hash table of size m. Show that if
the keys are drawn from a universe UwithjUj>n m ,t h e n Uhas a subset of size n
consisting of keys that all hash to the same slot, so that the worst-case searchingtime for hashing with chaining is ‚.n/ .
11.2-6
Suppose we have stored nkeys in a hash table of size m, with collisions resolved by
chaining, and that we know the length of each chain, including the length Lof the
longest chain. Describe a procedure that selects a key uniformly at random fromamong the keys in the hash table and returns it in expected time O.L/SOH.1C1=˛// .262 Chapter 11 Hash Tables
11.3 Hash functions
In this section, we discuss some issues regarding the design of good hash functions
and then present three schemes for their creation. Two of the schemes, hashing bydivision and hashing by multiplication, are heuristic in nature, whereas the thirdscheme, universal hashing, uses randomization to provide provably good perfor-
mance.
What makes a good hash function?
A good hash function satisﬁes (approximately) the assumption of simple uniform
hashing: each key is equally likely to hash to any of the mslots, independently of
where any other key has hashed to. Unfortunately, we typically have no way tocheck this condition, since we rarely know the probability distribution from whichthe keys are drawn. Moreover, the keys might not be drawn independently.
Occasionally we do know the distribution. For example, if we know that the
keys are random real numbers kindependently and uniformly distributed in the
range 0/DC4k<1 , then the hash function
h.k/Dbkmc
satisﬁes the condition of simple uniform hashing.
In practice, we can often employ heuristic techniques to create a hash function
that performs well. Qualitative information about the distribution of keys may be
useful in this design process. For example, consider a compiler’s symbol table, in
which the keys are character strings representing identiﬁers in a program. Closely
related symbols, such as ptandpts, often occur in the same program. A good
hash function would minimize the chance that such variants hash to the same slot.
A good approach derives the hash value in a way that we expect to be indepen-
dent of any patterns that might exist in the data. For example, the “division method”(discussed in Section 11.3.1) computes the hash value as the remainder when thekey is divided by a speciﬁed prime number. This method frequently gives goodresults, assuming that we choose a prime number that is unrelated to any patternsin the distribution of keys.
Finally, we note that some applications of hash functions might require stronger
properties than are provided by simple uniform hashing. For example, we mightwant keys that are “close” in some sense to yield hash values that are far apart.(This property is especially desirable when we are using linear probing, deﬁned inSection 11.4.) Universal hashing, described in Section 11.3.3, often provides the
desired properties.11.3 Hash functions 263
Interpreting keys as natural numbers
Most hash functions assume that the universe of keys is the set NDf0; 1; 2; : : :g
of natural numbers. Thus, if the keys are not natural numbers, we ﬁnd a way to
interpret them as natural numbers. For example, we can interpret a character stringas an integer expressed in suitable radix notation. Thus, we might interpret theidentiﬁer ptas the pair of decimal integers .112; 116/ ,s i n c e pD112andtD116
in the ASCII character set; then, expressed as a radix-128 integer, ptbecomes
.112/SOH128/C116D14452 . In the context of a given application, we can usually
devise some such method for interpreting each key as a (possibly large) naturalnumber. In what follows, we assume that the keys are natural numbers.
11.3.1 The division method
In the division method for creating hash functions, we map a key kinto one of m
slots by taking the remainder of kdivided by m. That is, the hash function is
h.k/Dkmodm:
For example, if the hash table has size mD12and the key is kD100,t h e n
h.k/D4. Since it requires only a single division operation, hashing by division is
quite fast.
When using the division method, we usually avoid certain values of m.F o r
example, mshould not be a power of 2, since if mD2
p,t h e n h.k/ is just the p
lowest-order bits of k. Unless we know that all low-order p-bit patterns are equally
likely, we are better off designing the hash function to depend on all the bits of thekey. As Exercise 11.3-3 asks you to show, choosing mD2
p/NUL1when kis a
character string interpreted in radix 2pmay be a poor choice, because permuting
the characters of kdoes not change its hash value.
A prime not too close to an exact power of 2 is often a good choice for m.F o r
example, suppose we wish to allocate a hash table, with collisions resolved bychaining, to hold roughly nD2000 character strings, where a character has 8bits.
We don’t mind examining an average of 3elements in an unsuccessful search, and
so we allocate a hash table of size mD701. We could choose mD701because
it is a prime near 2000=3 but not near any power of 2. Treating each key kas an
integer, our hash function would be
h.k/Dkmod701 :
11.3.2 The multiplication method
Themultiplication method for creating hash functions operates in two steps. First,
we multiply the key kby a constant Ain the range 0<A<1 and extract the264 Chapter 11 Hash Tables
× sDA/SOH2wwbits
k
r0 r1
h.k/extract pbits
Figure 11.4 The multiplication method of hashing. The w-bit representation of the key kis multi-
plied by the w-bit value sDA/SOH2w.T h e phighest-order bits of the lower w-bit half of the product
form the desired hash value h.k/ .
fractional part of kA. Then, we multiply this value by mand take the ﬂoor of the
result. In short, the hash function is
h.k/Dbm. k A mod1/c;
where “ kAmod1” means the fractional part of kA,t h a ti s , kA/NULbkAc.
An advantage of the multiplication method is that the value of mis not critical.
We typically choose it to be a power of 2(mD2pfor some integer p), since we
can then easily implement the function on most computers as follows. Suppose
that the word size of the machine is wbits and that kﬁts into a single word. We
restrict Ato be a fraction of the form s=2w,w h e r e sis an integer in the range
0<s<2w. Referring to Figure 11.4, we ﬁrst multiply kby the w-bit integer
sDA/SOH2w. The result is a 2w-bit value r12wCr0,w h e r e r1is the high-order word
of the product and r0is the low-order word of the product. The desired p-bit hash
value consists of the pmost signiﬁcant bits of r0.
Although this method works with any value of the constant A, it works better
with some values than with others. The optimal choice depends on the character-istics of the data being hashed. Knuth [211] suggests that
A/EM.p
5/NUL1/=2D0:6180339887 : : : (11.2)
is likely to work reasonably well.
As an example, suppose we have kD123456 ,pD14,mD214D16384 ,
andwD32. Adapting Knuth’s suggestion, we choose Ato be the fraction of the
form s=232that is closest to .p
5/NUL1/=2 ,s ot h a t AD2654435769=232.T h e n
k/SOHsD327706022297664 D.76300/SOH232/C17612864 ,a n ds o r1D76300
andr0D17612864 .T h e 14most signiﬁcant bits of r0yield the value h.k/D67.11.3 Hash functions 265
?11.3.3 Universal hashing
If a malicious adversary chooses the keys to be hashed by some ﬁxed hash function,
then the adversary can choose nkeys that all hash to the same slot, yielding an av-
erage retrieval time of ‚.n/ . Any ﬁxed hash function is vulnerable to such terrible
worst-case behavior; the only effective way to improve the situation is to choosethe hash function randomly in a way that is independent of the keys that are actually
going to be stored. This approach, called universal hashing , can yield provably
good performance on average, no matter which keys the adversary chooses.
In universal hashing, at the beginning of execution we select the hash function
at random from a carefully designed class of functions. As in the case of quick-sort, randomization guarantees that no single input will always evoke worst-casebehavior. Because we randomly select the hash function, the algorithm can be-have differently on each execution, even for the same input, guaranteeing goodaverage-case performance for any input. Returning to the example of a compiler’s
symbol table, we ﬁnd that the programmer’s choice of identiﬁers cannot now cause
consistently poor hashing performance. Poor performance occurs only when thecompiler chooses a random hash function that causes the set of identiﬁers to hashpoorly, but the probability of this situation occurring is small and is the same forany set of identiﬁers of the same size.
LetHbe a ﬁnite collection of hash functions that map a given universe Uof
keys into the range f0; 1; : : : ; m/NUL1g. Such a collection is said to be universal
if for each pair of distinct keys k;l2U, the number of hash functions h2H
for which h.k/Dh.l/ is at mostjHj=m. In other words, with a hash function
randomly chosen from H, the chance of a collision between distinct keys kandl
is no more than the chance 1=m of a collision if h.k/ andh.l/ were randomly and
independently chosen from the set f0; 1; : : : ; m/NUL1g.
The following theorem shows that a universal class of hash functions gives good
average-case behavior. Recall that n
idenotes the length of list TŒ i/c141.
Theorem 11.3
Suppose that a hash function his chosen randomly from a universal collection of
hash functions and has been used to hash nkeys into a table Tof size m,u s -
ing chaining to resolve collisions. If key kis not in the table, then the expected
length E Œnh.k//c141of the list that key khashes to is at most the load factor ˛Dn=m .
If key kis in the table, then the expected length E Œnh.k//c141of the list containing key k
is at most 1C˛.
Proof We note that the expectations here are over the choice of the hash func-
tion and do not depend on any assumptions about the distribution of the keys.For each pair kandlof distinct keys, deﬁne the indicator random variable266 Chapter 11 Hash Tables
XklDIfh.k/Dh.l/g. Since by the deﬁnition of a universal collection of hash
functions, a single pair of keys collides with probability at most 1=m,w eh a v e
Prfh.k/Dh.l/g/DC41=m. By Lemma 5.1, therefore, we have E ŒXkl/c141/DC41=m.
Next we deﬁne, for each key k, the random variable Ykthat equals the number
of keys other than kthat hash to the same slot as k,s ot h a t
YkDX
l2T
l¤kXkl:
Thus we have
EŒYk/c141DE2
4X
l2T
l¤kXkl3
5
DX
l2T
l¤kEŒXkl/c141 (by linearity of expectation)
/DC4X
l2T
l¤k1
m:
The remainder of the proof depends on whether key kis in table T.
/SIIfk62T,t h e n nh.k/DYkandjflWl2Tandl¤kgjDn. Thus E Œnh.k//c141D
EŒYk/c141/DC4n=mD˛.
/SIIfk2T, then because key kappears in list TŒ h . k/ /c141 and the count Ykdoes not
include key k,w eh a v e nh.k/DYkC1andjflWl2Tandl¤kgjDn/NUL1.
Thus E Œnh.k//c141DEŒYk/c141C1/DC4.n/NUL1/=mC1D1C˛/NUL1=m < 1C˛.
The following corollary says universal hashing provides the desired payoff: it
has now become impossible for an adversary to pick a sequence of operations thatforces the worst-case running time. By cleverly randomizing the choice of hashfunction at run time, we guarantee that we can process every sequence of operationswith a good average-case running time.
Corollary 11.4
Using universal hashing and collision resolution by chaining in an initially emptytable with mslots, it takes expected time ‚.n/ to handle any sequence of nI
NSERT ,
SEARCH ,a n dD ELETE operations containing O.m/ INSERT operations.
Proof Since the number of insertions is O.m/ ,w eh a v e nDO.m/ and so
˛DO.1/ .T h e I NSERT and D ELETE operations take constant time and, by The-
orem 11.3, the expected time for each S EARCH operation is O.1/ . By linearity of11.3 Hash functions 267
expectation, therefore, the expected time for the entire sequence of noperations
isO.n/ . Since each operation takes /DEL.1/ time, the ‚.n/ bound follows.
Designing a universal class of hash functions
It is quite easy to design a universal class of hash functions, as a little number
theory will help us prove. You may wish to consult Chapter 31 ﬁrst if you areunfamiliar with number theory.
We begin by choosing a prime number plarge enough so that every possible
keykis in the range 0top/NUL1, inclusive. Let Z
pdenote the setf0; 1; : : : ; p/NUL1g,
and let Z/ETX
pdenote the setf1 ;2;:::;p/NUL1g.S i n c e pis prime, we can solve equa-
tions modulo pwith the methods given in Chapter 31. Because we assume that the
size of the universe of keys is greater than the number of slots in the hash table, we
have p>m .
We now deﬁne the hash function habfor any a2Z/ETX
pand any b2Zpusing a
linear transformation followed by reductions modulo pand then modulo m:
hab.k/D..akCb/modp/modm: (11.3)
For example, with pD17andmD6,w eh a v e h3;4.8/D5. The family of all
such hash functions is
HpmD˚
habWa2Z/ETX
pandb2Zp/TAB
: (11.4)
Each hash function habmaps ZptoZm. This class of hash functions has the nice
property that the size mof the output range is arbitrary—not necessarily prime—a
feature which we shall use in Section 11.5. Since we have p/NUL1choices for a
andpchoices for b, the collection Hpmcontains p.p/NUL1/hash functions.
Theorem 11.5
The class Hpmof hash functions deﬁned by equations (11.3) and (11.4) is universal.
Proof Consider two distinct keys kandlfrom Zp,s ot h a t k¤l.F o r a g i v e n
hash function habwe let
rD.akCb/modp;
sD.alCb/modp:
We ﬁrst note that r¤s. Why? Observe that
r/NULs/DC1a.k/NULl/ .mod p/ :
It follows that r¤sbecause pis prime and both aand.k/NULl/are nonzero
modulo p, and so their product must also be nonzero modulo pby Theorem 31.6.
Therefore, when computing any hab2Hpm, distinct inputs kandlmap to distinct268 Chapter 11 Hash Tables
values randsmodulo p; there are no collisions yet at the “mod plevel.” Moreover,
each of the possible p.p/NUL1/choices for the pair .a; b/ witha¤0yields a different
resulting pair .r; s/ withr¤s, since we can solve for aandbgiven rands:
aD/NUL
.r/NULs/..k/NULl//NUL1modp//SOH
modp;
bD.r/NULak/modp;
where ..k/NULl//NUL1modp/denotes the unique multiplicative inverse, modulo p,
ofk/NULl. Since there are only p.p/NUL1/possible pairs .r; s/ with r¤s,t h e r e
is a one-to-one correspondence between pairs .a; b/ with a¤0and pairs .r; s/
withr¤s. Thus, for any given pair of inputs kandl,i fw ep i c k .a; b/ uniformly
at random from Z/ETX
p/STXZp, the resulting pair .r; s/ is equally likely to be any pair of
distinct values modulo p.
Therefore, the probability that distinct keys kandlcollide is equal to the prob-
ability that r/DC1s.mod m/when randsare randomly chosen as distinct values
modulo p.F o rag i v e nv a l u eo f r,o ft h e p/NUL1possible remaining values for s,t h e
number of values ssuch that s¤rands/DC1r.mod m/is at most
dp=me/NUL1/DC4..pCm/NUL1/=m//NUL1(by inequality (3.6))
D.p/NUL1/=m :
The probability that scollides with rwhen reduced modulo mis at most
..p/NUL1/=m/=.p/NUL1/D1=m.
Therefore, for any pair of distinct values k;l2Zp,
Prfhab.k/Dhab.l/g/DC41=m ;
so that Hpmis indeed universal.
Exercises
11.3-1
Suppose we wish to search a linked list of length n, where each element contains
ak e y kalong with a hash value h.k/ . Each key is a long character string. How
might we take advantage of the hash values when searching the list for an elementwith a given key?
11.3-2
Suppose that we hash a string of rcharacters into mslots by treating it as a
radix-128 number and then using the division method. We can easily representthe number mas a 32-bit computer word, but the string of rcharacters, treated as
a radix-128 number, takes many words. How can we apply the division method tocompute the hash value of the character string without using more than a constantnumber of words of storag eoutside the string itself?11.4 Open addressing 269
11.3-3
Consider a version of the division method in which h.k/Dkmodm,w h e r e
mD2p/NUL1andkis a character string interpreted in radix 2p. Show that if we
can derive string xfrom string yby permuting its characters, then xandyhash to
the same value. Give an example of an application in which this property would beundesirable in a hash function.
11.3-4
Consider a hash table of size mD1000 and a corresponding hash function h.k/D
bm. k A mod1/cforAD.p
5/NUL1/=2 . Compute the locations to which the keys
61,62,63,64,a n d 65are mapped.
11.3-5 ?
D e ﬁ n eaf a m i l y Hof hash functions from a ﬁnite set Uto a ﬁnite set Bto be
/SI-universal if for all pairs of distinct elements kandlinU,
Prfh.k/Dh.l/g/DC4/SI;
where the probability is over the choice of the hash function hdrawn at random
from the family H. Show that an /SI-universal family of hash functions must have
/SI/NAK1
jBj/NUL1
jUj:
11.3-6 ?
LetUbe the set of n-tuples of values drawn from Zp,a n dl e t BDZp,w h e r e p
is prime. Deﬁne the hash function hbWU!Bforb2Zpon an input n-tuple
ha0;a1;:::;a n/NUL1ifrom Uas
hb.ha0;a1;:::;a n/NUL1i/D n/NUL1X
jD0ajbj!
modp;
and let HDfhbWb2Zpg. Argue that His..n/NUL1/=p/ -universal according to
the deﬁnition of /SI-universal in Exercise 11.3-5. ( Hint: See Exercise 31.4-4.)
11.4 Open addressing
Inopen addressing , all elements occupy the hash table itself. That is, each table
entry contains either an element of the dynamic set or NIL. When searching for
an element, we systematically examine table slots until either we ﬁnd the desiredelement or we have ascertained that the element is not in the table. No lists and270 Chapter 11 Hash Tables
no elements are stored outside the table, unlike in chaining. Thus, in open ad-
dressing, the hash table can “ﬁll up” so that no further insertions can be made; oneconsequence is that the load factor ˛can never exceed 1.
Of course, we could store the linked lists for chaining inside the hash table, in
the otherwise unused hash-table slots (see Exercise 11.2-4), but the advantage ofopen addressing is that it avoids pointers altogether. Instead of following pointers,wecompute the sequence of slots to be examined. The extra memory freed by not
storing pointers provides the hash table with a larger number of slots for the same
amount of memory, potentially yielding fewer collisions and faster retrieval.
To perform insertion using open addressing, we successively examine, or probe ,
the hash table until we ﬁnd an empty slot in which to put the key. Instead of beingﬁxed in the order 0; 1; : : : ; m/NUL1(which requires ‚.n/ search time), the sequence
of positions probed depends upon the key being inserted . To determine which slots
to probe, we extend the hash function to include the probe number (starting from 0)
as a second input. Thus, the hash function becomes
hWU/STXf0; 1; : : : ; m/NUL1g!f0; 1; : : : ; m/NUL1g:
With open addressing, we require that for every key k,t h eprobe sequence
hh . k;0 / ;h . k;1 / ;:::;h . k;m /NUL1/i
be a permutation of h0;1;: : : ;m/NUL1i, so that every hash-table position is eventually
considered as a slot for a new key as the table ﬁlls up. In the following pseudocode,we assume that the elements in the hash table Tare keys with no satellite infor-
mation; the key kis identical to the element containing key k. Each slot contains
either a key or
NIL(if the slot is empty). The H ASH-INSERT procedure takes as
input a hash table Tand a key k. It either returns the slot number where it stores
keykor ﬂags an error because the hash table is already full.
HASH-INSERT .T; k/
1iD0
2repeat
3 jDh.k; i/
4 ifTŒ j/c141 ==NIL
5 TŒ j/c141Dk
6 return j
7 elseiDiC1
8until i==m
9error “hash table overﬂow”
The algorithm for searching for key kprobes the same sequence of slots that the
insertion algorithm examined when key kwas inserted. Therefore, the search can11.4 Open addressing 271
terminate (unsuccessfully) when it ﬁnds an empty slot, since kwould have been
inserted there and not later in its probe sequence. (This argument assumes that keysare not deleted from the hash table.) The procedure H
ASH-SEARCH takes as input
a hash table Tand a key k, returning jif it ﬁnds that slot jcontains key k,o r NIL
if key kis not present in table T.
HASH-SEARCH .T; k/
1iD0
2repeat
3 jDh.k; i/
4 ifTŒ j/c141 ==k
5 return j
6 iDiC1
7until TŒ j/c141 ==NILori==m
8return NIL
Deletion from an open-address hash table is difﬁcult. When we delete a key
from slot i, we cannot simply mark that slot as empty by storing NILin it. If
we did, we might be unable to retrieve any key kduring whose insertion we had
probed slot iand found it occupied. We can solve this problem by marking the
slot, storing in it the special value DELETED instead of NIL. We would then modify
the procedure H ASH-INSERT to treat such a slot as if it were empty so that we can
insert a new key there. We do not need to modify H ASH-SEARCH , since it will pass
over DELETED values while searching. When we use the special value DELETED ,
however, search times no longer depend on the load factor ˛, and for this reason
chaining is more commonly selected as a collision resolution technique when keysmust be deleted.
In our analysis, we assume uniform hashing : the probe sequence of each key
is equally likely to be any of the mŠpermutations ofh0 ; 1 ; :::; m/NUL1i.U n i -
form hashing generalizes the notion of simple uniform hashing deﬁned earlier to a
hash function that produces not just a single number, but a whole probe sequence.
True uniform hashing is difﬁcult to implement, however, and in practice suitable
approximations (such as double hashing, deﬁned below) are used.
We will examine three commonly used techniques to compute the probe se-
quences required for open addressing: linear probing, quadratic probing, and dou-
ble hashing. These techniques all guarantee that hh .k;0 /;h .k;1 /;:::;h .k;m /NUL1/i
is a permutation ofh0 ;1 ;:::;m/NUL1ifor each key k. None of these techniques ful-
ﬁlls the assumption of uniform hashing, however, since none of them is capable ofgenerating more than m
2different probe sequences (instead of the mŠthat uniform
hashing requires). Double hashing has the greatest number of probe sequences and,as one might expect, seems to give the best results.272 Chapter 11 Hash Tables
Linear probing
Given an ordinary hash function h0WU!f0; 1; : : : ; m/NUL1g, which we refer to as
anauxiliary hash function , the method of linear probing uses the hash function
h.k; i/D.h0.k/Ci/modm
foriD0; 1; : : : ; m/NUL1.G i v e n k e y k, we ﬁrst probe TŒ h0.k//c141, i.e., the slot given
by the auxiliary hash function. We next probe slot TŒ h0.k/C1/c141, and so on up to
slotTŒ m/NUL1/c141. Then we wrap around to slots T Œ0/c141; T Œ1/c141; : : : until we ﬁnally probe
slotTŒ h0.k//NUL1/c141. Because the initial probe determines the entire probe sequence,
there are only mdistinct probe sequences.
Linear probing is easy to implement, but it suffers from a problem known as
primary clustering . Long runs of occupied slots build up, increasing the average
search time. Clusters arise because an empty slot preceded by ifull slots gets ﬁlled
next with probability .iC1/=m . Long runs of occupied slots tend to get longer,
and the average search time increases.
Quadratic probing
Quadratic probing uses a hash function of the form
h.k; i/D.h0.k/Cc1iCc2i2/modm; (11.5)
where h0is an auxiliary hash function, c1andc2are positive auxiliary constants,
andiD0; 1; : : : ; m/NUL1. The initial position probed is TŒ h0.k//c141; later positions
probed are offset by amounts that depend in a quadratic manner on the probe num-beri. This method works much better than linear probing, but to make full use of
the hash table, the values of c
1,c2,a n d mare constrained. Problem 11-3 shows
one way to select these parameters. Also, if two keys have the same initial probeposition, then their probe sequences are the same, since h.k
1;0 /Dh.k 2;0 /im-
plies h.k 1;i/Dh.k 2;i/. This property leads to a milder form of clustering, called
secondary clustering . As in linear probing, the initial probe determines the entire
sequence, and so only mdistinct probe sequences are used.
Double hashing
Double hashing offers one of the best methods available for open addressing be-
cause the permutations produced have many of the characteristics of randomlychosen permutations. Double hashing uses a hash function of the form
h.k; i/D.h
1.k/Cih2.k// modm;
where both h1andh2are auxiliary hash functions. The initial probe goes to posi-
tionTŒ h 1.k//c141; successive probe positions are offset from previous positions by the11.4 Open addressing 273
0
123456789
10111279
69
98
721450
Figure 11.5 Insertion by double hashing. Here we have a hash table of size 13with h1.k/D
kmod13andh2.k/D1C.kmod11/.S i n c e 14/DC11.mod 13/and14/DC13.mod 11/,w ei n s e r t
the key 14into empty slot 9, after examining slots 1and5and ﬁnding them to be occupied.
amount h2.k/, modulo m. Thus, unlike the case of linear or quadratic probing, the
probe sequence here depends in two ways upon the key k, since the initial probe
position, the offset, or both, may vary. Figure 11.5 gives an example of insertionby double hashing.
The value h
2.k/must be relatively prime to the hash-table size mfor the entire
hash table to be searched. (See Exercise 11.4-4.) A convenient way to ensure thiscondition is to let mbe a power of 2and to design h
2so that it always produces an
odd number. Another way is to let mbe prime and to design h2so that it always
returns a positive integer less than m. For example, we could choose mprime and
let
h1.k/Dkmodm;
h2.k/D1C.kmodm0/;
where m0is chosen to be slightly less than m(say, m/NUL1). For example, if
kD123456 ,mD701,a n d m0D700,w eh a v e h1.k/D80andh2.k/D257,s o
that we ﬁrst probe position 80, and then we examine every 257th slot (modulo m)
until we ﬁnd the key or have examined every slot.
When mis prime or a power of 2, double hashing improves over linear or qua-
dratic probing in that ‚.m2/probe sequences are used, rather than ‚.m/ ,s i n c e
each possible .h1.k/; h 2.k// pair yields a distinct probe sequence. As a result, for274 Chapter 11 Hash Tables
such values of m, the performance of double hashing appears to be very close to
the performance of the “ideal” scheme of uniform hashing.
Although values of mother than primes or powers of 2could in principle be
used with double hashing, in practice it becomes more difﬁcult to efﬁciently gen-erate h
2.k/in a way that ensures that it is relatively prime to m, in part because the
relative density /RS.m/=m of such numbers may be small (see equation (31.24)).
Analysis of open-address hashing
As in our analysis of chaining, we express our analysis of open addressing in terms
of the load factor ˛Dn=m of the hash table. Of course, with open addressing, at
most one element occupies each slot, and thus n/DC4m, which implies ˛/DC41.
We assume that we are using uniform hashing. In this idealized scheme, the
probe sequencehh.k; 0/; h.k; 1/; : : : ; h.k; m /NUL1/iused to insert or search for
each key kis equally likely to be any permutation of h0; 1; : : : ; m/NUL1i. Of course,
a given key has a unique ﬁxed probe sequence associated with it; what we meanhere is that, considering the probability distribution on the space of keys and theoperation of the hash function on the keys, each possible probe sequence is equallylikely.
We now analyze the expected number of probes for hashing with open address-
ing under the assumption of uniform hashing, beginning with an analysis of the
number of probes made in an unsuccessful search.
Theorem 11.6
Given an open-address hash table with load factor ˛Dn=m < 1 , the expected
number of probes in an unsuccessful search is at most 1=.1/NUL˛/, assuming uniform
hashing.
Proof In an unsuccessful search, every probe but the last accesses an occupied
slot that does not contain the desired key, and the last slot probed is empty. Let usdeﬁne the random variable Xto be the number of probes made in an unsuccessful
search, and let us also deﬁne the event A
i,f o riD1 ;2;::: , to be the event that
anith probe occurs and it is to an occupied slot. Then the event fX/NAKigis the
intersection of events A1\A2\/SOH/SOH/SOH\ Ai/NUL1. We will bound PrfX/NAKigby bounding
PrfA1\A2\/SOH/SOH/SOH\ Ai/NUL1g. By Exercise C.2-5,
PrfA1\A2\/SOH/SOH/SOH\ Ai/NUL1gDPrfA1g/SOHPrfA2jA1g/SOHPrfA3jA1\A2g/SOH/SOH/SOH
PrfAi/NUL1jA1\A2\/SOH/SOH/SOH\ Ai/NUL2g:
Since there are nelements and mslots, PrfA1gDn=m .F o r j> 1 , the probability
that there is a jth probe and it is to an occupied slot, given that the ﬁrst j/NUL1
probes were to occupied slots, is .n/NULjC1/=.m/NULjC1/. This probability follows11.4 Open addressing 275
because we would be ﬁnding one of the remaining .n/NUL.j/NUL1//elements in one
of the .m/NUL.j/NUL1//unexamined slots, and by the assumption of uniform hashing,
the probability is the ratio of these quantities. Observing that n<m implies that
.n/NULj/ = . m/NULj//DC4n=m for all jsuch that 0/DC4j< m ,w eh a v ef o ra l l isuch that
1/DC4i/DC4m,
PrfX/NAKigDn
m/SOHn/NUL1
m/NUL1/SOHn/NUL2
m/NUL2/SOH/SOH/SOHn/NULiC2
m/NULiC2
/DC4/DLEn
m/DC1i/NUL1
D˛i/NUL1:
Now, we use equation (C.25) to bound the expected number of probes:
EŒX/c141D1X
iD1PrfX/NAKig
/DC41X
iD1˛i/NUL1
D1X
iD0˛i
D1
1/NUL˛:
This bound of 1=.1/NUL˛/D1C˛C˛2C˛3C/SOH/SOH/SOH has an intuitive interpretation.
We always make the ﬁrst probe. With probability approximately ˛, the ﬁrst probe
ﬁnds an occupied slot, so that we need to probe a second time. With probabilityapproximately ˛
2, the ﬁrst two slots are occupied so that we make a third probe,
and so on.
If˛is a constant, Theorem 11.6 predicts that an unsuccessful search runs in O.1/
time. For example, if the hash table is half full, the average number of probes in anunsuccessful search is at most 1=.1/NUL:5/D2.I fi ti s 90percent full, the average
number of probes is at most 1=.1/NUL:9/D10.
Theorem 11.6 gives us the performance of the H
ASH-INSERT procedure almost
immediately.
Corollary 11.7
Inserting an element into an open-address hash table with load factor ˛requires at
most 1=.1/NUL˛/probes on average, assuming uniform hashing.276 Chapter 11 Hash Tables
Proof An element is inserted only if there is room in the table, and thus ˛<1 .
Inserting a key requires an unsuccessful search followed by placing the key into theﬁrst empty slot found. Thus, the expected number of probes is at most 1=.1/NUL˛/.
We have to do a little more work to compute the expected number of probes for
a successful search.
Theorem 11.8
Given an open-address hash table with load factor ˛<1 , the expected number of
probes in a successful search is at most
1
˛ln1
1/NUL˛;
assuming uniform hashing and assuming that each key in the table is equally likely
to be searched for.
Proof A search for a key kreproduces the same probe sequence as when the
element with key kwas inserted. By Corollary 11.7, if kwas the .iC1/st key
inserted into the hash table, the expected number of probes made in a search for k
is at most 1=.1/NULi=m/Dm=.m/NULi/. Averaging over all nkeys in the hash table
gives us the expected number of probes in a successful search:
1
nn/NUL1X
iD0m
m/NULiDm
nn/NUL1X
iD01
m/NULi
D1
˛mX
kDm/NULnC11
k
/DC41
˛Zm
m/NULn.1=x/ dx (by inequality (A.12))
D1
˛lnm
m/NULn
D1
˛ln1
1/NUL˛:
If the hash table is half full, the expected number of probes in a successful search
is less than 1:387 . If the hash table is 90percent full, the expected number of probes
is less than 2:559 .11.5 Perfect hashing 277
Exercises
11.4-1
Consider inserting the keys 10; 22; 31; 4; 15; 28; 17; 88; 59 into a hash table of
length mD11using open addressing with the auxiliary hash function h0.k/Dk.
Illustrate the result of inserting these keys using linear probing, using quadratic
probing with c1D1andc2D3, and using double hashing with h1.k/Dkand
h2.k/D1C.kmod.m/NUL1//.
11.4-2
Write pseudocode for H ASH-DELETE as outlined in the text, and modify H ASH-
INSERT to handle the special value DELETED .
11.4-3
Consider an open-address hash table with uniform hashing. Give upper boundson the expected number of probes in an unsuccessful search and on the expectednumber of probes in a successful search when the load factor is 3=4and when it
is7=8.
11.4-4 ?
Suppose that we use double hashing to resolve collisions—that is, we use the hashfunction h.k; i/D.h
1.k/Cih2.k// modm. Show that if mandh2.k/have
greatest common divisor d/NAK1for some key k, then an unsuccessful search for
keykexamines .1=d/ th of the hash table before returning to slot h1.k/. Thus,
when dD1,s ot h a t mandh2.k/are relatively prime, the search may examine the
entire hash table. ( Hint: See Chapter 31.)
11.4-5 ?
Consider an open-address hash table with a load factor ˛. Find the nonzero value ˛
for which the expected number of probes in an unsuccessful search equals twicethe expected number of probes in a successful search. Use the upper bounds givenby Theorems 11.6 and 11.8 for these expected numbers of probes.
?11.5 Perfect hashing
Although hashing is often a good choice for its excellent average-case perfor-mance, hashing can also provide excellent worst-case performance when the set of
keys is static : once the keys are stored in the table, the set of keys never changes.
Some applications naturally have static sets of keys: consider the set of reservedwords in a programming language, or the set of ﬁle names on a CD-ROM. We278 Chapter 11 Hash Tables
0
1
2
3
4
5
6
7
8100 1 0
91 0 1 8 6 0 75
0123
100 7 00
0
16 23 88 40 37
0123 4567852m2S2a2b2m0S0a0b0
m5S5 a5b5
m7S7 a7b7T
4567872
9 1 01 11 21 31 41 522
Figure 11.6 Using perfect hashing to store the set KDf10; 22; 37; 40; 52; 60; 70; 72; 75 g.T h e
outer hash function is h.k/D..akCb/modp/modm,w h e r e aD3,bD42,pD101,a n d
mD9. For example, h.75/D2,a n ds ok e y 75hashes to slot 2 of table T. A secondary hash
table Sjstores all keys hashing to slot j. The size of hash table SjismjDn2
j, and the associated
hash function is hj.k/D..ajkCbj/modp/modmj.S i n c e h2.75/D7,k e y 75is stored in slot 7
of secondary hash table S2. No collisions occur in any of the secondary hash tables, and so searching
takes constant time in the worst case.
call a hashing technique perfect hashing ifO.1/ memory accesses are required to
perform a search in the worst case.
To create a perfect hashing scheme, we use two levels of hashing, with universal
hashing at each level. Figure 11.6 illustrates the approach.
The ﬁrst level is essentially the same as for hashing with chaining: we hash
thenkeys into mslots using a hash function hcarefully selected from a family of
universal hash functions.
Instead of making a linked list of the keys hashing to slot j, however, we use a
small secondary hash table Sjwith an associated hash function hj. By choosing
the hash functions hjcarefully, we can guarantee that there are no collisions at the
secondary level.
In order to guarantee that there are no collisions at the secondary level, however,
we will need to let the size mjof hash table Sjbe the square of the number njof
keys hashing to slot j. Although you might think that the quadratic dependence
ofmjonnjmay seem likely to cause the overall storage requirement to be exces-
sive, we shall show that by choosing the ﬁrst-level hash function well, we can limitthe expected total amount of space used to O.n/ .
We use hash functions chosen from the universal classes of hash functions of
Section 11.3.3. The ﬁrst-level hash function comes from the class H
pm,w h e r ea s
in Section 11.3.3, pis a prime number greater than any key value. Those keys11.5 Perfect hashing 279
hashing to slot jare re-hashed into a secondary hash table Sjof size mjusing a
hash function hjchosen from the class Hp;m j.1
We shall proceed in two steps. First, we shall determine how to ensure that
the secondary tables have no collisions. Second, we shall show that the expectedamount of memory used overall—for the primary hash table and all the secondaryhash tables—is O.n/ .
Theorem 11.9
Suppose that we store nkeys in a hash table of size mDn
2using a hash function h
randomly chosen from a universal class of hash functions. Then, the probability isless than 1=2that there are any collisions.
Proof There are/NUL
n
2/SOH
pairs of keys that may collide; each pair collides with prob-
ability 1=m ifhis chosen at random from a universal family Hof hash functions.
LetXbe a random variable that counts the number of collisions. When mDn2,
the expected number of collisions is
EŒX/c141D 
n
2!
/SOH1
n2
Dn2/NULn
2/SOH1
n2
<1 = 2 :
(This analysis is similar to the analysis of the birthday paradox in Section 5.4.1.)
Applying Markov’s inequality (C.30), Pr fX/NAKtg/DC4EŒX/c141 =t , with tD1,c o m -
pletes the proof.
In the situation described in Theorem 11.9, where mDn2, it follows that a hash
function hchosen at random from His more likely than not to have nocollisions.
Given the set Kofnkeys to be hashed (remember that Kis static), it is thus easy
to ﬁnd a collision-free hash function hwith a few random trials.
When nis large, however, a hash table of size mDn2is excessive. Therefore,
we adopt the two-level hashing approach, and we use the approach of Theorem 11.9only to hash the entries within each slot. We use an outer, or ﬁrst-level, hashfunction hto hash the keys into mDnslots. Then, if n
jkeys hash to slot j,w e
use a secondary hash table Sjof size mjDn2
jto provide collision-free constant-
time lookup.
1When njDmjD1, we don’t really need a hash function for slot j; when we choose a hash
function hab.k/D..akCb/modp/modmjfor such a slot, we just use aDbD0.280 Chapter 11 Hash Tables
We now turn to the issue of ensuring that the overall memory used is O.n/ .
Since the size mjof the jth secondary hash table grows quadratically with the
number njof keys stored, we run the risk that the overall amount of storage could
be excessive.
If the ﬁrst-level table size is mDn, then the amount of memory used is O.n/
for the primary hash table, for the storage of the sizes mjof the secondary hash
tables, and for the storage of the parameters ajandbjdeﬁning the secondary hash
functions hjdrawn from the class Hp;m jof Section 11.3.3 (except when njD1
and we use aDbD0). The following theorem and a corollary provide a bound on
the expected combined sizes of all the secondary hash tables. A second corollarybounds the probability that the combined size of all the secondary hash tables issuperlinear (actually, that it equals or exceeds 4n).
Theorem 11.10
Suppose that we store nkeys in a hash table of size mDnusing a hash function h
randomly chosen from a universal class of hash functions. Then, we have
E"
m/NUL1X
jD0n2
j#
<2 n;
where njis the number of keys hashing to slot j.
Proof We start with the following identity, which holds for any nonnegative inte-
gera:
a2DaC2 
a
2!
: (11.6)
We have
E"m/NUL1X
jD0n2
j#
DE"m/NUL1X
jD0 
njC2 
nj
2!!#
(by equation (11.6))
DE"m/NUL1X
jD0nj#
C2E"m/NUL1X
jD0 
nj
2!#
(by linearity of expectation)
DEŒn/c141C2E"m/NUL1X
jD0 
nj
2!#
(by equation (11.1))11.5 Perfect hashing 281
DnC2E"m/NUL1X
jD0 
nj
2!#
(since nis not a random variable) .
To evaluate the summationPm/NUL1
jD0/NULnj
2/SOH
, we observe that it is just the total number
of pairs of keys in the hash table that collide. By the properties of universal hashing,the expected value of this summation is at most
 
n
2!
1
mDn.n/NUL1/
2m
Dn/NUL1
2;
since mDn. Thus,
E"m/NUL1X
jD0n2
j#
/DC4nC2n/NUL1
2
D2n/NUL1
<2 n :
Corollary 11.11
Suppose that we store nkeys in a hash table of size mDnusing a hash func-
tionhrandomly chosen from a universal class of hash functions, and we set the
size of each secondary hash table to mjDn2
jforjD0; 1; : : : ; m/NUL1. Then,
the expected amount of storage required for all secondary hash tables in a perfecthashing scheme is less than 2n.
Proof Since m
jDn2
jforjD0; 1; : : : ; m/NUL1, Theorem 11.10 gives
E"m/NUL1X
jD0mj#
DE"m/NUL1X
jD0n2
j#
<2 n ; (11.7)
which completes the proof.
Corollary 11.12
Suppose that we store nkeys in a hash table of size mDnusing a hash function h
randomly chosen from a universal class of hash functions, and we set the size
of each secondary hash table to mjDn2
jforjD0; 1; : : : ; m/NUL1. Then, the
probability is less than 1=2that the total storage used for secondary hash tables
equals or exceeds 4n.282 Chapter 11 Hash Tables
Proof Again we apply Markov’s inequality (C.30), Pr fX/NAKtg/DC4EŒX/c141 =t ,t h i s
time to inequality (11.7), with XDPm/NUL1
jD0mjandtD4n:
Pr(m/NUL1X
jD0mj/NAK4n)
/DC4E/STXPm/NUL1
jD0mj/ETX
4n
<2n
4n
D1=2 :
From Corollary 11.12, we see that if we test a few randomly chosen hash func-
tions from the universal family, we will quickly ﬁnd one that uses a reasonableamount of storage.
Exercises
11.5-1 ?
Suppose that we insert nkeys into a hash table of size musing open addressing
and uniform hashing. Let p.n;m/ be the probability that no collisions occur. Show
thatp.n;m//DC4e
/NULn.n/NUL1/=2m.(Hint: See equation (3.12).) Argue that when nex-
ceedsp
m, the probability of avoiding collisions goes rapidly to zero.
Problems
11-1 Longest-probe bound for hashing
Suppose that we use an open-addressed hash table of size mto store n/DC4m=2
items.
a.Assuming uniform hashing, show that for iD1 ;2;:::;n , the probability is at
most 2/NULkthat the ith insertion requires strictly more than kprobes.
b.Show that for iD1 ;2;:::;n , the probability is O.1=n2/that the ith insertion
requires more than 2lgnprobes.
Let the random variable Xidenote the number of probes required by the ith inser-
tion. You have shown in part (b) that Pr fXi>2lgngDO.1=n2/. Let the random
variable XDmax 1/DC4i/DC4nXidenote the maximum number of probes required by
any of the ninsertions.
c.Show that PrfX>2 lgngDO.1=n/ .
d.Show that the expected length E ŒX/c141of the longest probe sequence is O.lgn/.Problems for Chapter 11 283
11-2 Slot-size bound for chaining
Suppose that we have a hash table with nslots, with collisions resolved by chain-
ing, and suppose that nkeys are inserted into the table. Each key is equally likely
to be hashed to each slot. Let Mbe the maximum number of keys in any slot after
all the keys have been inserted. Your mission is to prove an O.lgn=lg lgn/upper
bound on E ŒM /c141, the expected value of M.
a.Argue that the probability Qkthat exactly kkeys hash to a particular slot is
given by
QkD/DC21
n/DC3k/DC2
1/NUL1
n/DC3n/NULk 
n
k!
:
b.LetPkbe the probability that MDk, that is, the probability that the slot
containing the most keys contains kkeys. Show that Pk/DC4nQ k.
c.Use Stirling’s approximation, equation (3.18), to show that Qk<ek=kk.
d.Show that there exists a constant c>1 such that Qk0<1 = n3fork0D
clgn=lg lgn. Conclude that Pk<1 = n2fork/NAKk0Dclgn=lg lgn.
e.Argue that
EŒM /c141/DC4Pr/SUB
M>clgn
lg lgn/ESC
/SOHnCPr/SUB
M/DC4clgn
lg lgn/ESC
/SOHclgn
lg lgn:
Conclude that E ŒM /c141DO.lgn=lg lgn/.
11-3 Quadratic probing
Suppose that we are given a key kto search for in a hash table with positions
0; 1; : : : ; m/NUL1, and suppose that we have a hash function hmapping the key space
into the setf0; 1; : : : ; m/NUL1g. The search scheme is as follows:
1. Compute the value jDh.k/ , and set iD0.
2. Probe in position jfor the desired key k. If you ﬁnd it, or if this position is
empty, terminate the search.
3. Set iDiC1.I finow equals m, the table is full, so terminate the search.
Otherwise, set jD.iCj/modm, and return to step 2.
Assume that mi sap o w e ro f 2.
a.Show that this scheme is an instance of the general “quadratic probing” scheme
by exhibiting the appropriate constants c1andc2for equation (11.5).
b.Prove that this algorithm examines every table position in the worst case.284 Chapter 11 Hash Tables
11-4 Hashing and authentication
LetHbe a class of hash functions in which each hash function h2Hmaps the
universe Uof keys tof0; 1; : : : ; m/NUL1g. We say that Hisk-universal if, for every
ﬁxed sequence of kdistinct keyshx.1/;x.2/;:::;x.k/iand for any hchosen at
random from H, the sequencehh.x.1//;h.x.2//;: : : ;h.x.k//iis equally likely to be
any of the mksequences of length kwith elements drawn from f0; 1; : : : ; m/NUL1g.
a.Show that if the family Hof hash functions is 2-universal, then it is universal.
b.Suppose that the universe Uis the set of n-tuples of values drawn from
ZpDf0; 1; : : : ; p/NUL1g,w h e r e pis prime. Consider an element xD
hx0;x1;:::;x n/NUL1i2U.F o r a n y n-tuple aDha0;a1;:::;a n/NUL1i2U,d e -
ﬁne the hash function haby
ha.x/D n/NUL1X
jD0ajxj!
modp:
LetHDfhag. Show that His universal, but not 2-universal. ( Hint: Find a key
for which all hash functions in Hproduce the same value.)
c.Suppose that we modify Hslightly from part (b): for any a2Uand for any
b2Zp,d e ﬁ n e
h0
ab.x/D n/NUL1X
jD0ajxjCb!
modp
andH0Dfh0
abg. Argue that H0is2-universal. ( Hint: Consider ﬁxed n-tuples
x2Uandy2U, with xi¤yifor some i. What happens to h0
ab.x/
andh0
ab.y/asaiandbrange over Zp?)
d.Suppose that Alice and Bob secretly agree on a hash function hfrom a
2-universal family Hof hash functions. Each h2Hmaps from a universe of
keysUtoZp,w h e r e pis prime. Later, Alice sends a message mto Bob over the
Internet, where m2U. She authenticates this message to Bob by also sending
an authentication tag tDh.m/ , and Bob checks that the pair .m; t/ he receives
indeed satisﬁes tDh.m/ . Suppose that an adversary intercepts .m; t/ en route
and tries to fool Bob by replacing the pair .m; t/ with a different pair .m0;t0/.
Argue that the probability that the adversary succeeds in fooling Bob into ac-cepting .m
0;t0/is at most 1=p, no matter how much computing power the ad-
versary has, and even if the adversary knows the family Hof hash functions
used.Notes for Chapter 11 285
Chapter notes
Knuth [211] and Gonnet [145] are excellent references for the analysis of hash-
ing algorithms. Knuth credits H. P. Luhn (1953) for inventing hash tables, alongwith the chaining method for resolving collisions. At about the same time, G. M.Amdahl originated the idea of open addressing.
Carter and Wegman introduced the notion of universal classes of hash functions
in 1979 [58].
Fredman, Koml´ os, and Szemer´ edi [112] developed the perfect hashing scheme
for static sets presented in Section 11.5. An extension of their method to dynamicsets, handling insertions and deletions in amortized expected time O.1/ , has been
given by Dietzfelbinger et al. [86].12 Binary Search Trees
The search tree data structure supports many dynamic-set operations, including
SEARCH ,M INIMUM ,M AXIMUM ,PREDECESSOR ,SUCCESSOR ,INSERT ,a n d
DELETE . Thus, we can use a search tree both as a dictionary and as a priority
queue.
Basic operations on a binary search tree take time proportional to the height of
the tree. For a complete binary tree with nnodes, such operations run in ‚.lgn/
worst-case time. If the tree is a linear chain of nnodes, however, the same oper-
ations take ‚.n/ worst-case time. We shall see in Section 12.4 that the expected
height of a randomly built binary search tree is O.lgn/, so that basic dynamic-set
operations on such a tree take ‚.lgn/time on average.
In practice, we can’t always guarantee that binary search trees are built ran-
domly, but we can design variations of binary search trees with good guaranteedworst-case performance on basic operations. Chapter 13 presents one such vari-ation, red-black trees, which have height O.lgn/. Chapter 18 introduces B-trees,
which are particularly good for maintaining databases on secondary (disk) storage.
After presenting the basic properties of binary search trees, the following sec-
tions show how to walk a binary search tree to print its values in sorted order, howto search for a value in a binary search tree, how to ﬁnd the minimum or maximumelement, how to ﬁnd the predecessor or successor of an element, and how to insertinto or delete from a binary search tree. The basic mathematical properties of treesappear in Appendix B.
12.1 What is a binary search tree?
A binary search tree is organized, as the name suggests, in a binary tree, as shownin Figure 12.1. We can represent such a tree by a linked data structure in whicheach node is an object. In addition to a keyand satellite data, each node contains
attributes left,right ,a n d pthat point to the nodes corresponding to its left child,12.1 What is a binary search tree? 287
52 55
876
(a)6 8752
(b)
Figure 12.1 Binary search trees. For any node x, the keys in the left subtree of xare at most x:key,
and the keys in the right subtree of xare at least x:key. Different binary search trees can represent
the same set of values. The worst-case running time for most search-tree operations is proportional
to the height of the tree. (a)A binary search tree on 6nodes with height 2.(b)A less efﬁcient binary
search tree with height 4that contains the same keys.
its right child, and its parent, respectively. If a child or the parent is missing, the
appropriate attribute contains the value NIL. The root node is the only node in the
t r e ew h o s ep a r e n ti s NIL.
The keys in a binary search tree are always stored in such a way as to satisfy the
binary-search-tree property :
Letxbe a node in a binary search tree. If yis a node in the left subtree
ofx,t h e n y:key/DC4x:key.I fyis a node in the right subtree of x,t h e n
y:key/NAKx:key.
Thus, in Figure 12.1(a), the key of the root is 6, the keys 2,5,a n d 5in its left
subtree are no larger than 6,a n dt h ek e y s 7and8in its right subtree are no smaller
than6. The same property holds for every node in the tree. For example, the key 5
in the root’s left child is no smaller than the key 2in that node’s left subtree and no
larger than the key 5in the right subtree.
The binary-search-tree property allows us to print out all the keys in a binary
search tree in sorted order by a simple recursive algorithm, called an inorder tree
walk . This algorithm is so named because it prints the key of the root of a subtree
between printing the values in its left subtree and printing those in its right subtree.
(Similarly, a preorder tree walk prints the root before the values in either subtree,
and a postorder tree walk prints the root after the values in its subtrees.) To use
the following procedure to print all the elements in a binary search tree T, we call
INORDER -TREE-WALK.T:root/.288 Chapter 12 Binary Search Trees
INORDER -TREE-WALK.x/
1ifx¤NIL
2I NORDER -TREE-WALK.x:left/
3 print x:key
4I NORDER -TREE-WALK.x:right/
As an example, the inorder tree walk prints the keys in each of the two binary
search trees from Figure 12.1 in the order 2; 5; 5; 6; 7; 8 . The correctness of the
algorithm follows by induction directly from the binary-search-tree property.
It takes ‚.n/ t i m et ow a l ka n n-node binary search tree, since after the ini-
tial call, the procedure calls itself recursively exactly twice for each node in thetree—once for its left child and once for its right child. The following theoremgives a formal proof that it takes linear time to perform an inorder tree walk.
Theorem 12.1
Ifxis the root of an n-node subtree, then the call I
NORDER -TREE-WALK.x/
takes ‚.n/ time.
Proof LetT .n/ denote the time taken by I NORDER -TREE-WALK when it is
called on the root of an n-node subtree. Since I NORDER -TREE-WALK visits all n
nodes of the subtree, we have T .n/D/DEL.n/ . It remains to show that T .n/DO.n/ .
Since I NORDER -TREE-WALK takes a small, constant amount of time on an
empty subtree (for the test x¤NIL), we have T. 0 /Dcfor some constant c>0 .
Forn>0 , suppose that I NORDER -TREE-WALK is called on a node xwhose
left subtree has knodes and whose right subtree has n/NULk/NUL1nodes. The time to
perform I NORDER -TREE-WALK.x/is bounded by T .n//DC4T. k/CT. n/NULk/NUL1/Cd
for some constant d>0 that reﬂects an upper bound on the time to execute the
body of I NORDER -TREE-WALK.x/, exclusive of the time spent in recursive calls.
We use the substitution method to show that T .n/DO.n/ by proving that
T .n//DC4.cCd/nCc.F o r nD0,w eh a v e .cCd//SOH0CcDcDT. 0 / .F o r n>0 ,
we have
T .n//DC4T. k/CT. n/NULk/NUL1/Cd
D..cCd/kCc/C..cCd/.n/NULk/NUL1/Cc/Cd
D.cCd/nCc/NUL.cCd/CcCd
D.cCd/nCc;
which completes the proof.
12.2 Querying a binary search tree 289
Exercises
12.1-1
For the set off1; 4; 5; 10; 16; 17; 21 gof keys, draw binary search trees of heights 2,
3,4,5,a n d 6.
12.1-2
What is the difference between the binary-search-tree property and the min-heapproperty (see page 153)? Can the min-heap property be used to print out the keysof an n-node tree in sorted order in O.n/ time? Show how, or explain why not.
12.1-3
Give a nonrecursive algorithm that performs an inorder tree walk. ( Hint: An easy
solution uses a stack as an auxiliary data structure. A more complicated, but ele-gant, solution uses no stack but assumes that we can test two pointers for equality.)
12.1-4
Give recursive algorithms that perform preorder and postorder tree walks in ‚.n/
time on a tree of nnodes.
12.1-5
Argue that since sorting nelements takes /DEL.n lgn/time in the worst case in
the comparison model, any comparison-based algorithm for constructing a binarysearch tree from an arbitrary list of nelements takes /DEL.n lgn/time in the worst
case.
12.2 Querying a binary search tree
We often need to search for a key stored in a binary search tree. Besides the
SEARCH operation, binary search trees can support such queries as M INIMUM ,
MAXIMUM ,SUCCESSOR ,a n dP REDECESSOR . In this section, we shall examine
these operations and show how to support each one in time O.h/ on any binary
search tree of height h.
Searching
We use the following procedure to search for a node with a given key in a binary
search tree. Given a pointer to the root of the tree and a key k,TREE-SEARCH
returns a pointer to a node with key kif one exists; otherwise, it returns NIL.290 Chapter 12 Binary Search Trees
2 43
1376
17 201815
9
Figure 12.2 Queries on a binary search tree. To search for the key 13in the tree, we follow the path
15!6!7!13from the root. The minimum key in the tree is 2, which is found by following
leftpointers from the root. The maximum key 20is found by following right pointers from the root.
The successor of the node with key 15is the node with key 17, since it is the minimum key in the
right subtree of 15. The node with key 13has no right subtree, and thus its successor is its lowest
ancestor whose left child is also an ancestor. In this case, the node with key 15is its successor.
TREE-SEARCH .x; k/
1ifx==NILork==x:key
2 return x
3ifk<x : key
4 return TREE-SEARCH .x:left;k/
5else return TREE-SEARCH .x:right;k/
The procedure begins its search at the root and traces a simple path downward in
the tree, as shown in Figure 12.2. For each node xit encounters, it compares the
keykwithx:key. If the two keys are equal, the search terminates. If kis smaller
thanx:key, the search continues in the left subtree of x, since the binary-search-
tree property implies that kcould not be stored in the right subtree. Symmetrically,
ifkis larger than x:key, the search continues in the right subtree. The nodes
encountered during the recursion form a simple path downward from the root of
the tree, and thus the running time of T REE-SEARCH isO.h/ ,w h e r e his the height
of the tree.
We can rewrite this procedure in an iterative fashion by “unrolling” the recursion
into a while loop. On most computers, the iterative version is more efﬁcient.12.2 Querying a binary search tree 291
ITERATIVE -TREE-SEARCH .x; k/
1while x¤NILandk¤x:key
2 ifk<x : key
3 xDx:left
4 elsexDx:right
5return x
Minimum and maximum
We can always ﬁnd an element in a binary search tree whose key is a minimum by
following leftchild pointers from the root until we encounter a NIL,a ss h o w ni n
Figure 12.2. The following procedure returns a pointer to the minimum element in
the subtree rooted at a given node x, which we assume to be non- NIL:
TREE-MINIMUM .x/
1while x:left¤NIL
2 xDx:left
3return x
The binary-search-tree property guarantees that T REE-MINIMUM is correct. If a
node xhas no left subtree, then since every key in the right subtree of xis at least as
large as x:key, the minimum key in the subtree rooted at xisx:key. If node xhas
a left subtree, then since no key in the right subtree is smaller than x:keyand every
key in the left subtree is not larger than x:key, the minimum key in the subtree
rooted at xresides in the subtree rooted at x:left.
The pseudocode for T REE-MAXIMUM is symmetric:
TREE-MAXIMUM .x/
1while x:right¤NIL
2 xDx:right
3return x
Both of these procedures run in O.h/ time on a tree of height hsince, as in T REE-
SEARCH , the sequence of nodes encountered forms a simple path downward from
the root.
Successor and predecessor
Given a node in a binary search tree, sometimes we need to ﬁnd its successor in
the sorted order determined by an inorder tree walk. If all keys are distinct, the292 Chapter 12 Binary Search Trees
successor of a node xis the node with the smallest key greater than x:key.T h e
structure of a binary search tree allows us to determine the successor of a nodewithout ever comparing keys. The following procedure returns the successor of anode xin a binary search tree if it exists, and
NILifxhas the largest key in the
tree:
TREE-SUCCESSOR .x/
1ifx:right¤NIL
2 return TREE-MINIMUM .x:right/
3yDx:p
4while y¤NILandx==y:right
5 xDy
6 yDy:p
7return y
We break the code for T REE-SUCCESSOR into two cases. If the right subtree
of node xis nonempty, then the successor of xis just the leftmost node in x’s
right subtree, which we ﬁnd in line 2 by calling T REE-MINIMUM .x:right/.F o r
example, the successor of the node with key 15in Figure 12.2 is the node with
key17.
On the other hand, as Exercise 12.2-6 asks you to show, if the right subtree of
node xis empty and xhas a successor y,t h e n yis the lowest ancestor of xwhose
left child is also an ancestor of x. In Figure 12.2, the successor of the node with
key13is the node with key 15.T oﬁ n d y, we simply go up the tree from xuntil we
encounter a node that is the left child of its parent; lines 3–7 of T REE-SUCCESSOR
handle this case.
The running time of T REE-SUCCESSOR on a tree of height hisO.h/ ,s i n c ew e
either follow a simple path up the tree or follow a simple path down the tree. Theprocedure T
REE-PREDECESSOR , which is symmetric to T REE-SUCCESSOR ,a l s o
runs in time O.h/ .
Even if keys are not distinct, we deﬁne the successor and predecessor of any
node xas the node returned by calls made to T REE-SUCCESSOR .x/and T REE-
PREDECESSOR .x/, respectively.
In summary, we have proved the following theorem.
Theorem 12.2
We can implement the dynamic-set operations S EARCH ,M INIMUM ,M AXIMUM ,
SUCCESSOR ,a n dP REDECESSOR so that each one runs in O.h/ time on a binary
search tree of height h.
12.2 Querying a binary search tree 293
Exercises
12.2-1
Suppose that we have numbers between 1 and 1000 in a binary search tree, and wewant to search for the number 363. Which of the following sequences could notbe
the sequence of nodes examined?
a.2, 252, 401, 398, 330, 344, 397, 363.
b.924, 220, 911, 244, 898, 258, 362, 363.
c.925, 202, 911, 240, 912, 245, 363.
d.2, 399, 387, 219, 266, 382, 381, 278, 363.
e.935, 278, 347, 621, 299, 392, 358, 363.
12.2-2
Write recursive versions of T
REE-MINIMUM and T REE-MAXIMUM .
12.2-3
Write the T REE-PREDECESSOR procedure.
12.2-4
Professor Bunyan thinks he has discovered a remarkable property of binary searchtrees. Suppose that the search for key kin a binary search tree ends up in a leaf.
Consider three sets: A, the keys to the left of the search path; B, the keys on the
search path; and C, the keys to the right of the search path. Professor Bunyan
claims that any three keys a2A,b2B,a n d c2Cmust satisfy a/DC4b/DC4c.G i v e
a smallest possible counterexample to the professor’s claim.
12.2-5
Show that if a node in a binary search tree has two children, then its successor hasno left child and its predecessor has no right child.
12.2-6
Consider a binary search tree Twhose keys are distinct. Show that if the right
subtree of a node xinTis empty and xhas a successor y,t h e n yis the lowest
ancestor of xwhose left child is also an ancestor of x. (Recall that every node is
its own ancestor.)
12.2-7
An alternative method of performing an inorder tree walk of an n-node binary
search tree ﬁnds the minimum element in the tree by calling T
REE-MINIMUM and
then making n/NUL1calls to T REE-SUCCESSOR . Prove that this algorithm runs
in‚.n/ time.294 Chapter 12 Binary Search Trees
12.2-8
Prove that no matter what node we start at in a height- hbinary search tree, k
successive calls to T REE-SUCCESSOR takeO.kCh/time.
12.2-9
LetTbe a binary search tree whose keys are distinct, let xbe a leaf node, and let y
be its parent. Show that y:keyis either the smallest key in Tlarger than x:keyor
the largest key in Tsmaller than x:key.
12.3 Insertion and deletion
The operations of insertion and deletion cause the dynamic set represented by a
binary search tree to change. The data structure must be modiﬁed to reﬂect thischange, but in such a way that the binary-search-tree property continues to hold.As we shall see, modifying the tree to insert a new element is relatively straight-forward, but handling deletion is somewhat more intricate.
Insertion
To insert a new value /ETBinto a binary search tree T, we use the procedure T
REE-
INSERT . The procedure takes a node ´for which ´:keyD/ETB,´:leftDNIL,
and´:rightDNIL. It modiﬁes Tand some of the attributes of ´in such a way that
it inserts ´into an appropriate position in the tree.
TREE-INSERT .T; ´/
1yDNIL
2xDT:root
3while x¤NIL
4 yDx
5 if´:key<x : key
6 xDx:left
7 elsexDx:right
8´:pDy
9ify==NIL
10 T:rootD´ //treeTwas empty
11elseif ´:key<y : key
12 y:leftD´
13elsey:rightD´12.3 Insertion and deletion 295
2 95
13 1715 191812
Figure 12.3 Inserting an item with key 13into a binary search tree. Lightly shaded nodes indicate
the simple path from the root down to the position where the item is inserted. The dashed lineindicates the link in the tree that is added to insert the item.
Figure 12.3 shows how T REE-INSERT works. Just like the procedures T REE-
SEARCH and I TERATIVE -TREE-SEARCH ,TREE-INSERT begins at the root of the
tree and the pointer xtraces a simple path downward looking for a NILto replace
with the input item ´. The procedure maintains the trailing pointer yas the parent
ofx. After initialization, the while loop in lines 3–7 causes these two pointers
to move down the tree, going left or right depending on the comparison of ´:key
withx:key, until xbecomes NIL.T h i s NILoccupies the position where we wish to
place the input item ´. We need the trailing pointer y, because by the time we ﬁnd
the NILwhere ´belongs, the search has proceeded one step beyond the node that
needs to be changed. Lines 8–13 set the pointers that cause ´to be inserted.
Like the other primitive operations on search trees, the procedure T REE-INSERT
runs in O.h/ time on a tree of height h.
Deletion
The overall strategy for deleting a node ´from a binary search tree Thas three
basic cases but, as we shall see, one of the cases is a bit tricky.
/SIIf´has no children, then we simply remove it by modifying its parent to re-
place ´with NILas its child.
/SIIf´has just one child, then we elevate that child to take ´’s position in the tree
by modifying ´’s parent to replace ´by´’s child.
/SIIf´has two children, then we ﬁnd ´’s successor y—which must be in ´’s right
subtree—and have ytake´’s position in the tree. The rest of ´’s original right
subtree becomes y’s new right subtree, and ´’s left subtree becomes y’s new
left subtree. This case is the tricky one because, as we shall see, it matterswhether yis´’s right child.296 Chapter 12 Binary Search Trees
The procedure for deleting a given node ´from a binary search tree Ttakes as
arguments pointers to Tand´. It organizes its cases a bit differently from the three
cases outlined previously by considering the four cases shown in Figure 12.4.
/SIIf´has no left child (part (a) of the ﬁgure), then we replace ´by its right child,
which may or may not be NIL.W h e n ´’s right child is NIL, this case deals with
the situation in which ´has no children. When ´’s right child is non- NIL,t h i s
case handles the situation in which ´has just one child, which is its right child.
/SIIf´has just one child, which is its left child (part (b) of the ﬁgure), then we
replace ´by its left child.
/SIOtherwise, ´has both a left and a right child. We ﬁnd ´’s successor y,w h i c h
lies in ´’s right subtree and has no left child (see Exercise 12.2-5). We want to
splice yout of its current location and have it replace ´in the tree.
/SIIfyis´’s right child (part (c)), then we replace ´byy, leaving y’s right
child alone.
/SIOtherwise, ylies within ´’s right subtree but is not ´’s right child (part (d)).
In this case, we ﬁrst replace yby its own right child, and then we replace ´
byy.
In order to move subtrees around within the binary search tree, we deﬁne a
subroutine T RANSPLANT , which replaces one subtree as a child of its parent with
another subtree. When T RANSPLANT replaces the subtree rooted at node uwith
the subtree rooted at node /ETB, node u’s parent becomes node /ETB’s parent, and u’s
parent ends up having /ETBas its appropriate child.
TRANSPLANT . T;u ;/ETB/
1ifu:p==NIL
2 T:rootD/ETB
3elseif u==u:p:left
4 u:p:leftD/ETB
5elseu:p:rightD/ETB
6if/ETB¤NIL
7 /ETB:pDu:p
Lines 1–2 handle the case in which uis the root of T. Otherwise, uis either a left
child or a right child of its parent. Lines 3–4 take care of updating u:p:leftifu
is a left child, and line 5 updates u:p:right ifuis a right child. We allow /ETBto be
NIL, and lines 6–7 update /ETB:pif/ETBis non- NIL. Note that T RANSPLANT does not
attempt to update /ETB:leftand/ETB:right ; doing so, or not doing so, is the responsibility
of T RANSPLANT ’s caller.12.3 Insertion and deletion 297
q q
z (a) r
q q
z
l(b)
q
z
l(c)q
y
l y
q
z
l(d)
rq
z
l ryq
l ryr
l
x
x
x y
xx
NILNILNILNILNIL
Figure 12.4 Deleting a node ´from a binary search tree. Node ´may be the root, a left child of
node q, or a right child of q.(a)Node ´has no left child. We replace ´by its right child r,w h i c h
may or may not be NIL.(b)Node ´has a left child lbut no right child. We replace ´byl.(c)Node ´
has two children; its left child is node l, its right child is its successor y,a n d y’s right child is node x.
We replace ´byy, updating y’s left child to become l,b u tl e a v i n g xasy’s right child. (d)Node ´
has two children (left child land right child r), and its successor y¤rlies within the subtree rooted
atr. We replace yby its own right child x,a n dw es e t yto be r’s parent. Then, we set yto be q’s
child and the parent of l.298 Chapter 12 Binary Search Trees
With the T RANSPLANT procedure in hand, here is the procedure that deletes
node ´from binary search tree T:
TREE-DELETE .T; ´/
1if´:left==NIL
2T RANSPLANT . T;´ ;´ : right/
3elseif ´:right ==NIL
4T RANSPLANT . T;´ ;´ : left/
5elseyDTREE-MINIMUM .´:right/
6 ify:p¤´
7T RANSPLANT . T;y;y: right/
8 y:rightD´:right
9 y:right:pDy
10 T RANSPLANT . T;´ ;y/
11 y:leftD´:left
12 y:left:pDy
The T REE-DELETE procedure executes the four cases as follows. Lines 1–2
handle the case in which node ´has no left child, and lines 3–4 handle the case in
which ´has a left child but no right child. Lines 5–12 deal with the remaining two
cases, in which ´has two children. Line 5 ﬁnds node y, which is the successor
of´. Because ´has a nonempty right subtree, its successor must be the node in
that subtree with the smallest key; hence the call to T REE-MINIMUM .´:right/.A s
we noted before, yhas no left child. We want to splice yout of its current location,
and it should replace ´in the tree. If yis´’s right child, then lines 10–12 replace ´
as a child of its parent by yand replace y’s left child by ´’s left child. If yis
not´’s left child, lines 7–9 replace yas a child of its parent by y’s right child and
turn´’s right child into y’s right child, and then lines 10–12 replace ´as a child of
its parent by yand replace y’s left child by ´’s left child.
Each line of T REE-DELETE , including the calls to T RANSPLANT , takes constant
time, except for the call to T REE-MINIMUM in line 5. Thus, T REE-DELETE runs
inO.h/ time on a tree of height h.
In summary, we have proved the following theorem.
Theorem 12.3
We can implement the dynamic-set operations I NSERT and D ELETE so that each
one runs in O.h/ time on a binary search tree of height h.
12.4 Randomly built binary search trees 299
Exercises
12.3-1
Give a recursive version of the T REE-INSERT procedure.
12.3-2
Suppose that we construct a binary search tree by repeatedly inserting distinct val-
ues into the tree. Argue that the number of nodes examined in searching for avalue in the tree is one plus the number of nodes examined when the value wasﬁrst inserted into the tree.
12.3-3
We can sort a given set of nnumbers by ﬁrst building a binary search tree contain-
ing these numbers (using T
REE-INSERT repeatedly to insert the numbers one by
one) and then printing the numbers by an inorder tree walk. What are the worst-case and best-case running times for this sorting algorithm?
12.3-4
Is the operation of deletion “commutative” in the sense that deleting xand then y
from a binary search tree leaves the same tree as deleting yand then x? Argue why
it is or give a counterexample.
12.3-5
Suppose that instead of each node xkeeping the attribute x:p, pointing to x’s
parent, it keeps x:succ, pointing to x’s successor. Give pseudocode for S
EARCH ,
INSERT ,a n dD ELETE on a binary search tree Tusing this representation. These
procedures should operate in time O.h/ ,w h e r e his the height of the tree T.(Hint:
You may wish to implement a subroutine that returns the parent of a node.)
12.3-6
When node ´in T REE-DELETE has two children, we could choose node yas
its predecessor rather than its successor. What other changes to T REE-DELETE
would be necessary if we did so? Some have argued that a fair strategy, givingequal priority to predecessor and successor, yields better empirical performance.How might T
REE-DELETE be changed to implement such a fair strategy?
?12.4 Randomly built binary search trees
We have shown that each of the basic operations on a binary search tree runsinO.h/ time, where his the height of the tree. The height of a binary search300 Chapter 12 Binary Search Trees
tree varies, however, as items are inserted and deleted. If, for example, the nitems
are inserted in strictly increasing order, the tree will be a chain with height n/NUL1.
On the other hand, Exercise B.5-4 shows that h/NAKblgnc. As with quicksort, we
can show that the behavior of the average case is much closer to the best case thanto the worst case.
Unfortunately, little is known about the average height of a binary search tree
when both insertion and deletion are used to create it. When the tree is createdby insertion alone, the analysis becomes more tractable. Let us therefore deﬁne a
randomly built binary search tree onnkeys as one that arises from inserting the
keys in random order into an initially empty tree, where each of the nŠpermutations
of the input keys is equally likely. (Exercise 12.4-3 asks you to show that this notionis different from assuming that every binary search tree on nkeys is equally likely.)
In this section, we shall prove the following theorem.
Theorem 12.4
The expected height of a randomly built binary search tree on ndistinct keys is
O.lgn/.
Proof We start by deﬁning three random variables that help measure the height
of a randomly built binary search tree. We denote the height of a randomly built
binary search on nkeys by X
n, and we deﬁne the exponential height YnD2Xn.
When we build a binary search tree on nkeys, we choose one key as that of the
root, and we let Rndenote the random variable that holds this key’s rank within
the set of nkeys; that is, Rnholds the position that this key would occupy if the
set of keys were sorted. The value of Rnis equally likely to be any element of the
setf1 ;2;:::;ng.I fRnDi, then the left subtree of the root is a randomly built
binary search tree on i/NUL1keys, and the right subtree is a randomly built binary
search tree on n/NULikeys. Because the height of a binary tree is 1more than the
larger of the heights of the two subtrees of the root, the exponential height of abinary tree is twice the larger of the exponential heights of the two subtrees of theroot. If we know that R
nDi, it follows that
YnD2/SOHmax.Yi/NUL1;Yn/NULi/:
As base cases, we have that Y1D1, because the exponential height of a tree with 1
node is 20D1and, for convenience, we deﬁne Y0D0.
Next, deﬁne indicator random variables Zn;1;Zn;2;:::;Z n;n,w h e r e
Zn;iDIfRnDig:
Because Rnis equally likely to be any element of f1 ;2;:::;ng, it follows that
PrfRnDigD1=nforiD1 ;2;:::;n , and hence, by Lemma 5.1, we have
EŒZn;i/c141D1=n ; (12.1)12.4 Randomly built binary search trees 301
foriD1 ;2;:::;n . Because exactly one value of Zn;iis1and all others are 0,w e
also have
YnDnX
iD1Zn;i.2/SOHmax.Yi/NUL1;Yn/NULi// :
We shall show that E ŒYn/c141is polynomial in n, which will ultimately imply that
EŒXn/c141DO.lgn/.
We claim that the indicator random variable Zn;iDIfRnDigis independent
of the values of Yi/NUL1andYn/NULi. Having chosen RnDi, the left subtree (whose
exponential height is Yi/NUL1) is randomly built on the i/NUL1keys whose ranks are
less than i. This subtree is just like any other randomly built binary search tree
oni/NUL1keys. Other than the number of keys it contains, this subtree’s structure
is not affected at all by the choice of RnDi, and hence the random variables
Yi/NUL1andZn;iare independent. Likewise, the right subtree, whose exponential
height is Yn/NULi, is randomly built on the n/NULikeys whose ranks are greater than i.
Its structure is independent of the value of Rn, and so the random variables Yn/NULi
andZn;iare independent. Hence, we have
EŒYn/c141DE"nX
iD1Zn;i.2/SOHmax.Yi/NUL1;Yn/NULi//#
DnX
iD1EŒZn;i.2/SOHmax.Yi/NUL1;Yn/NULi///c141 (by linearity of expectation)
DnX
iD1EŒZn;i/c141EŒ2/SOHmax.Yi/NUL1;Yn/NULi//c141(by independence)
DnX
iD11
n/SOHEŒ2/SOHmax.Yi/NUL1;Yn/NULi//c141 (by equation (12.1))
D2
nnX
iD1EŒmax.Yi/NUL1;Yn/NULi//c141 (by equation (C.22))
/DC42
nnX
iD1.EŒYi/NUL1/c141CEŒYn/NULi/c141/ (by Exercise C.3-4) .
Since each term E ŒY0/c141;EŒY1/c141;:::; EŒYn/NUL1/c141appears twice in the last summation,
o n c ea sE ŒYi/NUL1/c141and once as E ŒYn/NULi/c141, we have the recurrence
EŒYn/c141/DC44
nn/NUL1X
iD0EŒYi/c141: (12.2)302 Chapter 12 Binary Search Trees
Using the substitution method, we shall show that for all positive integers n,t h e
recurrence (12.2) has the solution
EŒYn/c141/DC41
4 
nC3
3!
:
In doing so, we shall use the identity
n/NUL1X
iD0 
iC3
3!
D 
nC3
4!
: (12.3)
(Exercise 12.4-1 asks you to prove this identity.)
For the base cases, we note that the bounds 0DY0DEŒY0/c141/DC4.1=4//NUL3
3/SOH
D1=4
and1DY1DEŒY1/c141/DC4.1=4//NUL1C3
3/SOH
D1hold. For the inductive case, we have that
EŒYn/c141/DC44
nn/NUL1X
iD0EŒYi/c141
/DC44
nn/NUL1X
iD01
4 
iC3
3!
(by the inductive hypothesis)
D1
nn/NUL1X
iD0 
iC3
3!
D1
n 
nC3
4!
(by equation (12.3))
D1
n/SOH.nC3/Š
4Š .n/NUL1/Š
D1
4/SOH.nC3/Š
3Š nŠ
D1
4 
nC3
3!
:
We have bounded E ŒYn/c141, but our ultimate goal is to bound E ŒXn/c141.A s E x e r -
cise 12.4-4 asks you to show, the function f. x/D2xis convex (see page 1199).
Therefore, we can employ Jensen’s inequality (C.26), which says that
2EŒXn/c141/DC4E/STX
2Xn/ETX
DEŒYn/c141;
as follows:
2EŒXn/c141/DC41
4 
nC3
3!Problems for Chapter 12 303
D1
4/SOH.nC3/.nC2/.nC1/
6
Dn3C6n2C11nC6
24:
Taking logarithms of both sides gives E ŒXn/c141DO.lgn/.
Exercises
12.4-1
Prove equation (12.3).
12.4-2
Describe a binary search tree on nnodes such that the average depth of a node in
the tree is ‚.lgn/but the height of the tree is !.lgn/. Give an asymptotic upper
bound on the height of an n-node binary search tree in which the average depth of
a node is ‚.lgn/.
12.4-3
Show that the notion of a randomly chosen binary search tree on nkeys, where
each binary search tree of nkeys is equally likely to be chosen, is different from
the notion of a randomly built binary search tree given in this section. ( Hint: List
the possibilities when nD3.)
12.4-4
Show that the function f. x/D2xis convex.
12.4-5 ?
Consider R ANDOMIZED -QUICKSORT operating on a sequence of ndistinct input
numbers. Prove that for any constant k>0 ,a l lb u t O.1=nk/of the nŠinput
permutations yield an O.n lgn/running time.
Problems
12-1 Binary search trees with equal keys
Equal keys pose a problem for the implementation of binary search trees.
a.What is the asymptotic performance of T REE-INSERT when used to insert n
items with identical keys into an initially empty binary search tree?
We propose to improve T REE-INSERT by testing before line 5 to determine whether
´:keyDx:keyand by testing before line 11 to determine whether ´:keyDy:key.304 Chapter 12 Binary Search Trees
If equality holds, we implement one of the following strategies. For each strategy,
ﬁnd the asymptotic performance of inserting nitems with identical keys into an
initially empty binary search tree. (The strategies are described for line 5, in whichwe compare the keys of ´andx. Substitute yforxto arrive at the strategies for
line 11.)
b.Keep a boolean ﬂag x:bat node x, and set xto either x:leftorx:right based
on the value of x:b, which alternates between
FALSE and TRUE each time we
visitxwhile inserting a node with the same key as x.
c.Keep a list of nodes with equal keys at x, and insert ´into the list.
d.Randomly set xto either x:leftorx:right . (Give the worst-case performance
and informally derive the expected running time.)
12-2 Radix trees
Given two strings aDa0a1:::a pandbDb0b1:::b q, where each aiand each bj
is in some ordered set of characters, we say that string aislexicographically less
than string bif either
1. there exists an integer j,w h e r e 0/DC4j/DC4min.p; q/ , such that aiDbifor all
iD0; 1; : : : ; j/NUL1andaj<b j,o r
2.p<q andaiDbifor all iD0; 1; : : : ; p .
For example, if aandbare bit strings, then 10100 < 10110 by rule 1 (letting
jD3)a n d 10100 < 101000 by rule 2. This ordering is similar to that used in
English-language dictionaries.
Theradix tree data structure shown in Figure 12.5 stores the bit strings 1011,
10, 011, 100, and 0. When searching for a key aDa0a1:::a p,w eg ol e f ta ta
node of depth iifaiD0and right if aiD1.L e t Sbe a set of distinct bit strings
whose lengths sum to n. Show how to use a radix tree to sort Slexicographically
in‚.n/ time. For the example in Figure 12.5, the output of the sort should be the
sequence 0, 011, 10, 100, 1011.
12-3 Average node depth in a randomly built binary search tree
In this problem, we prove that the average depth of a node in a randomly built
binary search tree with nnodes is O.lgn/. Although this result is weaker than
that of Theorem 12.4, the technique we shall use reveals a surprising similarity
between the building of a binary search tree and the execution of R ANDOMIZED -
QUICKSORT from Section 7.3.
We deﬁne the total path length P.T/ of a binary tree Tas the sum, over all
nodes xinT, of the depth of node x, which we denote by d.x;T/ .Problems for Chapter 12 305
0110
10010
101101
10
10 1
1
Figure 12.5 A radix tree storing the bit strings 1011, 10, 011, 100, and 0. We can determine each
node’s key by traversing the simple path from the root to that node. There is no need, therefore, to
store the keys in the nodes; the keys appear here for illustrative purposes only. Nodes are heavily
shaded if the keys corresponding to them are not in the tree; such nodes are present only to establisha path to other nodes.
a.Argue that the average depth of a node in Tis
1
nX
x2Td.x;T/D1
nP.T/:
Thus, we wish to show that the expected value of P.T/ isO.n lgn/.
b.LetTLandTRdenote the left and right subtrees of tree T, respectively. Argue
that if Thasnnodes, then
P.T/DP.T L/CP.T R/Cn/NUL1:
c.LetP.n/ denote the average total path length of a randomly built binary search
tree with nnodes. Show that
P.n/D1
nn/NUL1X
iD0.P.i/CP.n/NULi/NUL1/Cn/NUL1/ :
d.Show how to rewrite P.n/ as
P.n/D2
nn/NUL1X
kD1P.k/C‚.n/ :
e.Recalling the alternative analysis of the randomized version of quicksort given
in Problem 7-3, conclude that P.n/DO.n lgn/.306 Chapter 12 Binary Search Trees
At each recursive invocation of quicksort, we choose a random pivot element to
partition the set of elements being sorted. Each node of a binary search tree parti-tions the set of elements that fall into the subtree rooted at that node.
f.Describe an implementation of quicksort in which the comparisons to sort a set
of elements are exactly the same as the comparisons to insert the elements intoa binary search tree. (The order in which comparisons are made may differ, butthe same comparisons must occur.)
12-4 Number of different binary trees
Letb
ndenote the number of different binary trees with nnodes. In this problem,
you will ﬁnd a formula for bn, as well as an asymptotic estimate.
a.Show that b0D1and that, for n/NAK1,
bnDn/NUL1X
kD0bkbn/NUL1/NULk:
b.Referring to Problem 4-4 for the deﬁnition of a generating function, let B.x/
be the generating function
B.x/D1X
nD0bnxn:
Show that B.x/DxB.x/2C1, and hence one way to express B.x/ in closed
form is
B.x/D1
2x/NUL
1/NULp
1/NUL4x/SOH
:
TheTaylor expansion off. x/ around the point xDais given by
f. x/D1X
kD0f.k/.a/
kŠ.x/NULa/k;
where f.k/.x/is the kth derivative of fevaluated at x.
c.Show that
bnD1
nC1 
2n
n!Notes for Chapter 12 307
(thenthCatalan number ) by using the Taylor expansion ofp
1/NUL4xaround
xD0. (If you wish, instead of using the Taylor expansion, you may use
the generalization of the binomial expansion (C.4) to nonintegral exponents n,
where for any real number nand for any integer k, we interpret/NULn
k/SOH
to be
n.n/NUL1//SOH/SOH/SOH.n/NULkC1/=kŠ ifk/NAK0,a n d 0otherwise.)
d.Show that
bnD4n
p
/EMn3=2.1CO.1=n// :
Chapter notes
Knuth [211] contains a good discussion of simple binary search trees as well as
many variations. Binary search trees seem to have been independently discoveredby a number of people in the late 1950s. Radix trees are often called “tries,” whichcomes from the middle letters in the word retrieval . Knuth [211] also discusses
them.
Many texts, including the ﬁrst two editions of this book, have a somewhat sim-
pler method of deleting a node from a binary search tree when both of its children
are present. Instead of replacing node ´by its successor y, we delete node ybut
copy its key and satellite data into node ´. The downside of this approach is that
the node actually deleted might not be the node passed to the delete procedure. Ifother components of a program maintain pointers to nodes in the tree, they couldmistakenly end up with “stale” pointers to nodes that have been deleted. Althoughthe deletion method presented in this edition of this book is a bit more complicated,it guarantees that a call to delete node ´deletes node ´and only node ´.
Section 15.5 will show how to construct an optimal binary search tree when
we know the search frequencies before constructing the tree. That is, given the
frequencies of searching for each key and the frequencies of searching for values
that fall between keys in the tree, we construct a binary search tree for which a
set of searches that follows these frequencies examines the minimum number of
nodes.
The proof in Section 12.4 that bounds the expected height of a randomly built
binary search tree is due to Aslam [24]. Mart´ ınez and Roura [243] give randomized
algorithms for insertion into and deletion from binary search trees in which theresult of either operation is a random binary search tree. Their deﬁnition of arandom binary search tree differs—only slightly—from that of a randomly builtbinary search tree in this chapter, however.13 Red-Black Trees
Chapter 12 showed that a binary search tree of height hcan support any of the basic
dynamic-set operations—such as S EARCH ,PREDECESSOR ,SUCCESSOR ,M INI-
MUM ,MAXIMUM ,INSERT ,a n dD ELETE —inO.h/ time. Thus, the set operations
are fast if the height of the search tree is small. If its height is large, however, theset operations may run no faster than with a linked list. Red-black trees are oneof many search-tree schemes that are “balanced” in order to guarantee that basicdynamic-set operations take O.lgn/time in the worst case.
13.1 Properties of red-black trees
Ared-black tree is a binary search tree with one extra bit of storage per node: its
color , which can be either RED orBLACK . By constraining the node colors on any
simple path from the root to a leaf, red-black trees ensure that no such path is morethan twice as long as any other, so that the tree is approximately balanced .
Each node of the tree now contains the attributes color ,key,left,right ,a n d p.I f
a child or the parent of a node does not exist, the corresponding pointer attributeof the node contains the value
NIL. We shall regard these NILs as being pointers to
leaves (external nodes) of the binary search tree and the normal, key-bearing nodesas being internal nodes of the tree.
A red-black tree is a binary tree that satisﬁes the following red-black properties :
1. Every node is either red or black.
2. The root is black.
3. Every leaf (
NIL) is black.
4. If a node is red, then both its children are black.
5. For each node, all simple paths from the node to descendant leaves contain the
same number of black nodes.13.1 Properties of red-black trees 309
Figure 13.1(a) shows an example of a red-black tree.
As a matter of convenience in dealing with boundary conditions in red-black
tree code, we use a single sentinel to represent NIL(see page 238). For a red-black
treeT, the sentinel T:nilis an object with the same attributes as an ordinary node
in the tree. Its color attribute is BLACK , and its other attributes— p,left,right ,
andkey—can take on arbitrary values. As Figure 13.1(b) shows, all pointers to NIL
are replaced by pointers to the sentinel T:nil.
We use the sentinel so that we can treat a NILchild of a node xas an ordinary
node whose parent is x. Although we instead could add a distinct sentinel node
for each NILin the tree, so that the parent of each NILis well deﬁned, that ap-
proach would waste space. Instead, we use the one sentinel T:nilto represent all
theNILs—all leaves and the root’s parent. The values of the attributes p,left,right ,
andkeyof the sentinel are immaterial, although we may set them during the course
of a procedure for our convenience.
We generally conﬁne our interest to the internal nodes of a red-black tree, since
they hold the key values. In the remainder of this chapter, we omit the leaves when
we draw red-black trees, as shown in Figure 13.1(c).
We call the number of black nodes on any simple path from, but not including, a
node xdown to a leaf the black-height of the node, denoted bh .x/. By property 5,
the notion of black-height is well deﬁned, since all descending simple paths fromthe node have the same number of black nodes. We deﬁne the black-height of ared-black tree to be the black-height of its root.
The following lemma shows why red-black trees make good search trees.
Lemma 13.1
A red-black tree with ninternal nodes has height at most 2lg.nC1/.
Proof We start by showing that the subtree rooted at any node xcontains at least
2
bh.x//NUL1internal nodes. We prove this claim by induction on the height of x.I f
the height of xis0,t h e n xmust be a leaf ( T:nil), and the subtree rooted at xindeed
contains at least 2bh.x//NUL1D20/NUL1D0internal nodes. For the inductive step,
consider a node xthat has positive height and is an internal node with two children.
Each child has a black-height of either bh .x/or bh.x//NUL1, depending on whether
its color is red or black, respectively. Since the height of a child of xis less than
the height of xitself, we can apply the inductive hypothesis to conclude that each
child has at least 2bh.x//NUL1/NUL1internal nodes. Thus, the subtree rooted at xcontains
at least .2bh.x//NUL1/NUL1/C.2bh.x//NUL1/NUL1/C1D2bh.x//NUL1internal nodes, which proves
the claim.
To complete the proof of the lemma, let hbe the height of the tree. According
to property 4, at least half the nodes on any simple path from the root to a leaf, not310 Chapter 13 Red-Black Trees
NIL NILNIL NIL NIL NIL NILNIL NIL
NIL NILNIL NIL NIL NIL
NIL NIL NIL NILNIL NIL26
41
47 30
28 38
35 3917
21
23 19
2014
16
1510
12 7
3 1112
112
11123
111 12123
(a)
26
41
47 30
28 38
35 3917
21
23 19
2014
16
1510
12 7
3
(b)
26
41
47 30
28 38
35 3917
21
23 19
2014
16
1510
12 7
3(c)T:nil
Figure 13.1 A red-black tree with black nodes darkened and red nodes shaded. Every node in a
red-black tree is either red or black, the children of a red node are both black, and every simple pathfrom a node to a descendant leaf contains the same number of black nodes. (a)Every leaf, shown
as a
NIL, is black. Each non- NILnode is marked with its black-height; NILs have black-height 0.
(b)The same red-black tree but with each NILreplaced by the single sentinel T:nil, which is always
black, and with black-heights omitted. The root’s parent is also the sentinel. (c)The same red-black
tree but with leaves and the root’s parent omitted entirely. We shall use this drawing style in the
remainder of this chapter.13.1 Properties of red-black trees 311
including the root, must be black. Consequently, the black-height of the root must
be at least h=2; thus,
n/NAK2h=2/NUL1:
Moving the 1to the left-hand side and taking logarithms on both sides yields
lg.nC1//NAKh=2,o rh/DC42lg.nC1/.
As an immediate consequence of this lemma, we can implement the dynamic-set
operations S EARCH ,M INIMUM ,M AXIMUM ,SUCCESSOR ,a n dP REDECESSOR
inO.lgn/time on red-black trees, since each can run in O.h/ time on a binary
search tree of height h(as shown in Chapter 12) and any red-black tree on nnodes
is a binary search tree with height O.lgn/. (Of course, references to NILin the
algorithms of Chapter 12 would have to be replaced by T:nil.) Although the al-
gorithms T REE-INSERT and T REE-DELETE from Chapter 12 run in O.lgn/time
when given a red-black tree as input, they do not directly support the dynamic-setoperations I
NSERT and D ELETE , since they do not guarantee that the modiﬁed bi-
nary search tree will be a red-black tree. We shall see in Sections 13.3 and 13.4,however, how to support these two operations in O.lgn/time.
Exercises
13.1-1
In the style of Figure 13.1(a), draw the complete binary search tree of height 3on
the keysf1 ;2;:::;1 5g.A d d t h e
NILleaves and color the nodes in three different
ways such that the black-heights of the resulting red-black trees are 2,3,a n d 4.
13.1-2
Draw the red-black tree that results after T REE-INSERT is called on the tree in
Figure 13.1 with key 36. If the inserted node is colored red, is the resulting tree a
red-black tree? What if it is colored black?
13.1-3
Let us deﬁne a relaxed red-black tree as a binary search tree that satisﬁes red-
black properties 1, 3, 4, and 5. In other words, the root may be either red or black.Consider a relaxed red-black tree Twhose root is red. If we color the root of T
black but make no other changes to T, is the resulting tree a red-black tree?
13.1-4
Suppose that we “absorb” every red node in a red-black tree into its black parent,so that the children of the red node become children of the black parent. (Ignore
what happens to the keys.) What are the possible degrees of a black node after all312 Chapter 13 Red-Black Trees
its red children are absorbed? What can you say about the depths of the leaves of
the resulting tree?
13.1-5
Show that the longest simple path from a node xin a red-black tree to a descendant
leaf has length at most twice that of the shortest simple path from node xto a
descendant leaf.
13.1-6
What is the largest possible number of internal nodes in a red-black tree with black-
height k? What is the smallest possible number?
13.1-7
Describe a red-black tree on nkeys that realizes the largest possible ratio of red in-
ternal nodes to black internal nodes. What is this ratio? What tree has the smallestpossible ratio, and what is the ratio?
13.2 Rotations
The search-tree operations T REE-INSERT and T REE-DELETE , when run on a red-
black tree with nkeys, take O.lgn/time. Because they modify the tree, the result
may violate the red-black properties enumerated in Section 13.1. To restore theseproperties, we must change the colors of some of the nodes in the tree and alsochange the pointer structure.
We change the pointer structure through rotation , which is a local operation in
a search tree that preserves the binary-search-tree property. Figure 13.2 shows thetwo kinds of rotations: left rotations and right rotations. When we do a left rotationon a node x, we assume that its right child yis not T:nil;xmay be any node in
the tree whose right child is not T:nil. The left rotation “pivots” around the link
from xtoy. It makes ythe new root of the subtree, with xasy’s left child and y’s
left child as x’s right child.
The pseudocode for L
EFT-ROTATE assumes that x:right¤T:niland that the
root’s parent is T:nil.13.2 Rotations 313
y
x
αβγx
y α
βγLEFT-ROTATE (T,x)
RIGHT-ROTATE (T,y)
Figure 13.2 The rotation operations on a binary search tree. The operation L EFT-ROTATE .T; x/
transforms the conﬁguration of the two nodes on the right into the conﬁguration on the left by chang-
ing a constant number of pointers. The inverse operation R IGHT -ROTATE .T; y/ transforms the con-
ﬁguration on the left into the conﬁguration on the right. The letters ˛,ˇ,a n d /CRrepresent arbitrary
subtrees. A rotation operation preserves the binary-search-tree property: the keys in ˛precede x:key,
which precedes the keys in ˇ, which precede y:key, which precedes the keys in /CR.
LEFT-ROTATE .T; x/
1yDx:right //sety
2x:rightDy:left //turny’s left subtree into x’s right subtree
3ify:left¤T:nil
4 y:left:pDx
5y:pDx:p //linkx’s parent to y
6ifx:p==T:nil
7 T:rootDy
8elseif x==x:p:left
9 x:p:leftDy
10elsex:p:rightDy
11y:leftDx //putxony’s left
12x:pDy
Figure 13.3 shows an example of how L EFT-ROTATE modiﬁes a binary search
tree. The code for R IGHT -ROTATE is symmetric. Both L EFT-ROTATE and R IGHT -
ROTATE run in O.1/ time. Only pointers are changed by a rotation; all other
attributes in a node remain the same.
Exercises
13.2-1
Write pseudocode for R IGHT -ROTATE .
13.2-2
Argue that in every n-node binary search tree, there are exactly n/NUL1possible
rotations.314 Chapter 13 Red-Black Trees
234
67
11
91 8
14
12 1719
22
20x
y
234
67
18
19
14
12 1722
20xy
11
9LEFT-ROTATE (T,x)
Figure 13.3 An example of how the procedure L EFT-ROTATE .T; x/ modiﬁes a binary search tree.
Inorder tree walks of the input tree and the modiﬁed tree produce the same listing of key values.
13.2-3
Leta,b,a n d cbe arbitrary nodes in subtrees ˛,ˇ,a n d /CR, respectively, in the left
tree of Figure 13.2. How do the depths of a,b,a n d cchange when a left rotation
is performed on node xin the ﬁgure?
13.2-4
Show that any arbitrary n-node binary search tree can be transformed into any other
arbitrary n-node binary search tree using O.n/ rotations. ( Hint: First show that at
most n/NUL1right rotations sufﬁce to transform the tree into a right-going chain.)
13.2-5 ?
We say that a binary search tree T1can be right-converted to binary search tree T2
if it is possible to obtain T2from T1via a series of calls to R IGHT -ROTATE .G i v e
an example of two trees T1andT2such that T1cannot be right-converted to T2.
Then, show that if a tree T1can be right-converted to T2, it can be right-converted
using O.n2/calls to R IGHT -ROTATE .13.3 Insertion 315
13.3 Insertion
We can insert a node into an n-node red-black tree in O.lgn/time. To do so, we
use a slightly modiﬁed version of the T REE-INSERT procedure (Section 12.3) to
insert node ´into the tree Tas if it were an ordinary binary search tree, and then we
color ´red. (Exercise 13.3-1 asks you to explain why we choose to make node ´
red rather than black.) To guarantee that the red-black properties are preserved, we
then call an auxiliary procedure RB-I NSERT -FIXUP to recolor nodes and perform
rotations. The call RB-I NSERT .T; ´/ inserts node ´, whose keyis assumed to have
already been ﬁlled in, into the red-black tree T.
RB-I NSERT .T; ´/
1yDT:nil
2xDT:root
3while x¤T:nil
4 yDx
5 if´:key<x : key
6 xDx:left
7 elsexDx:right
8´:pDy
9ify==T:nil
10 T:rootD´
11elseif ´:key<y : key
12 y:leftD´
13elsey:rightD´
14´:leftDT:nil
15´:rightDT:nil
16´:colorDRED
17 RB-I NSERT -FIXUP.T; ´/
The procedures T REE-INSERT and RB-I NSERT differ in four ways. First, all
instances of NILin T REE-INSERT are replaced by T:nil. Second, we set ´:left
and´:right toT:nilin lines 14–15 of RB-I NSERT , in order to maintain the
proper tree structure. Third, we color ´red in line 16. Fourth, because col-
oring ´red may cause a violation of one of the red-black properties, we call
RB-I NSERT -FIXUP.T; ´/ in line 17 of RB-I NSERT to restore the red-black prop-
erties.316 Chapter 13 Red-Black Trees
RB-I NSERT -FIXUP .T; ´/
1while ´:p:color ==RED
2 if´:p==´:p:p:left
3 yD´:p:p:right
4 ify:color ==RED
5 ´:p:colorDBLACK //case 1
6 y:colorDBLACK //case 1
7 ´:p:p:colorDRED //case 1
8 ´D´:p:p //case 1
9 else if ´==´:p:right
10 ´D´:p //case 2
11 L EFT-ROTATE .T; ´/ //case 2
12 ´:p:colorDBLACK //case 3
13 ´:p:p:colorDRED //case 3
14 R IGHT -ROTATE .T; ´: p:p/ //case 3
15 else(same as then clause
with “right” and “left” exchanged)
16T:root:colorDBLACK
To understand how RB-I NSERT -FIXUP works, we shall break our examination
of the code into three major steps. First, we shall determine what violations ofthe red-black properties are introduced in RB-I
NSERT when node ´is inserted
and colored red. Second, we shall examine the overall goal of the while loop in
lines 1–15. Finally, we shall explore each of the three cases1within the while
loop’s body and see how they accomplish the goal. Figure 13.4 shows how RB-
INSERT -FIXUP operates on a sample red-black tree.
Which of the red-black properties might be violated upon the call to RB-
INSERT -FIXUP ? Property 1 certainly continues to hold, as does property 3, since
both children of the newly inserted red node are the sentinel T:nil. Property 5,
which says that the number of black nodes is the same on every simple path froma given node, is satisﬁed as well, because node ´replaces the (black) sentinel, and
node ´is red with sentinel children. Thus, the only properties that might be vi-
olated are property 2, which requires the root to be black, and property 4, whichsays that a red node cannot have a red child. Both possible violations are due to ´
being colored red. Property 2 is violated if ´is the root, and property 4 is violated
if´’s parent is red. Figure 13.4(a) shows a violation of property 4 after the node ´
has been inserted.
1Case 2 falls through into case 3, and so these two cases are not mutually exclusive.13.3 Insertion 317
zy11
2
17
5
4814
15
zy11
2
1 7
5
4814
15(a)
(b)Case 1
zy11
7
2 8
414
15 (c)Case 2
154
z7
2
1511
14 (d)Case 3
48
15
Figure 13.4 The operation of RB-I NSERT -FIXUP .(a)A node ´after insertion. Because both ´
and its parent ´:pare red, a violation of property 4 occurs. Since ´’s uncle yis red, case 1 in the
code applies. We recolor nodes and move the pointer ´up the tree, resulting in the tree shown in (b).
Once again, ´and its parent are both red, but ´’s uncle yis black. Since ´is the right child of ´:p,
case2applies. We perform a left rotation, and the tree that results is shown in (c).N o w , ´is the left
child of its parent, and case 3 applies. Recoloring and right rotation yield the tree in (d),w h i c hi sa
legal red-black tree.318 Chapter 13 Red-Black Trees
Thewhile loop in lines 1–15 maintains the following three-part invariant at the
start of each iteration of the loop:
a. Node ´is red.
b. If´:pis the root, then ´:pis black.
c. If the tree violates any of the red-black properties, then it violates at most
one of them, and the violation is of either property 2 or property 4. If thetree violates property 2, it is because ´is the root and is red. If the tree
violates property 4, it is because both ´and´:pare red.
Part (c), which deals with violations of red-black properties, is more central to
showing that RB-I
NSERT -FIXUP restores the red-black properties than parts (a)
and (b), which we use along the way to understand situations in the code. Becausewe’ll be focusing on node ´and nodes near it in the tree, it helps to know from
part (a) that ´is red. We shall use part (b) to show that the node ´:p:pexists when
we reference it in lines 2, 3, 7, 8, 13, and 14.
Recall that we need to show that a loop invariant is true prior to the ﬁrst itera-
tion of the loop, that each iteration maintains the loop invariant, and that the loop
invariant gives us a useful property at loop termination.
We start with the initialization and termination arguments. Then, as we exam-
ine how the body of the loop works in more detail, we shall argue that the loop
maintains the invariant upon each iteration. Along the way, we shall also demon-strate that each iteration of the loop has two possible outcomes: either the pointer ´
moves up the tree, or we perform some rotations and then the loop terminates.
Initialization: Prior to the ﬁrst iteration of the loop, we started with a red-black
tree with no violations, and we added a red node ´. We show that each part of
the invariant holds at the time RB-I
NSERT -FIXUP is called:
a. When RB-I NSERT -FIXUP is called, ´is the red node that was added.
b. If´:pis the root, then ´:pstarted out black and did not change prior to the
call of RB-I NSERT -FIXUP .
c. We have already seen that properties 1, 3, and 5 hold when RB-I NSERT -
FIXUP is called.
If the tree violates property 2, then the red root must be the newly added
node ´, which is the only internal node in the tree. Because the parent and
both children of ´are the sentinel, which is black, the tree does not also
violate property 4. Thus, this violation of property 2 is the only violation ofred-black properties in the entire tree.
If the tree violates property 4, then, because the children of node ´are black
sentinels and the tree had no other violations prior to ´being added, the13.3 Insertion 319
violation must be because both ´and´:pare red. Moreover, the tree violates
no other red-black properties.
Termination: When the loop terminates, it does so because ´:pis black. (If ´is
the root, then ´:pis the sentinel T:nil, which is black.) Thus, the tree does not
violate property 4 at loop termination. By the loop invariant, the only propertythat might fail to hold is property 2. Line 16 restores this property, too, so thatwhen RB-I
NSERT -FIXUP terminates, all the red-black properties hold.
Maintenance: We actually need to consider six cases in the while loop, but three
of them are symmetric to the other three, depending on whether line 2 deter-mines ´’s parent ´:pto be a left child or a right child of ´’s grandparent ´:p:p.
We have given the code only for the situation in which ´:pis a left child. The
node ´:p:pexists, since by part (b) of the loop invariant, if ´:pis the root,
then´:pis black. Since we enter a loop iteration only if ´:pis red, we know
that´:pcannot be the root. Hence, ´:p:pexists.
We distinguish case 1 from cases 2 and 3 by the color of ´’s parent’s sibling,
or “uncle.” Line 3 makes ypoint to ´’s uncle ´:p:p:right , and line 4 tests y’s
color. If yis red, then we execute case 1. Otherwise, control passes to cases 2
and 3. In all three cases, ´’s grandparent ´:p:pis black, since its parent ´:pis
red, and property 4 is violated only between ´and´:p.
Case 1: ´’s uncle yis red
Figure 13.5 shows the situation for case 1 (lines 5–8), which occurs when
both´:pandyare red. Because ´:p:pis black, we can color both
´:pandy
black, thereby ﬁxing the problem of ´and´:pboth being red, and we can
color ´:p:pred, thereby maintaining property 5. We then repeat the while loop
with´:p:pas the new node ´. The pointer ´moves up two levels in the tree.
Now, we show that case 1 maintains the loop invariant at the start of the next
iteration. We use ´to denote node ´in the current iteration, and ´0D´:p:p
to denote the node that will be called node ´at the test in line 1 upon the next
iteration.
a. Because this iteration colors ´:p:pred, node ´0is red at the start of the next
iteration.
b. The node ´0:pis´:p:p:pin this iteration, and the color of this node does not
change. If this node is the root, it was black prior to this iteration, and itremains black at the start of the next iteration.
c. We have already argued that case 1 maintains property 5, and it does not
introduce a violation of properties 1 or 3.320 Chapter 13 Red-Black Trees
zyC
D A
B α
βγδε(a)C
D A
B α
βγδεnew z
yC
D B
δεC
D B
A
αβγδ εnew z
(b)
A
αβγ z
Figure 13.5 Case 1 of the procedure RB-I NSERT -FIXUP . Property 4 is violated, since ´and its
parent ´:pare both red. We take the same action whether (a)´is a right child or (b)´is a left
child. Each of the subtrees ˛,ˇ,/CR,ı,a n d "has a black root, and each has the same black-height.
The code for case 1 changes the colors of some nodes, preserving property 5: all downward simple
paths from a node to a leaf have the same number of blacks. The while loop continues with node ´’s
grandparent ´:p:pas the new ´. Any violation of property 4 can now occur only between the new ´,
which is red, and its parent, if it is red as well.
If node ´0is the root at the start of the next iteration, then case 1 corrected
the lone violation of property 4 in this iteration. Since ´0is red and it is the
root, property 2 becomes the only one that is violated, and this violation isdue to ´
0.
If node ´0is not the root at the start of the next iteration, then case 1 has
not created a violation of property 2. Case 1 corrected the lone violationof property 4 that existed at the start of this iteration. It then made ´
0red
and left ´0:palone. If ´0:pwas black, there is no violation of property 4.
If´0:pwas red, coloring ´0red created one violation of property 4 between ´0
and´0:p.
Case 2: ´’s uncle yis black and ´is a right child
Case 3: ´’s uncle yis black and ´is a left child
In cases 2 and 3, the color of ´’s uncle yis black. We distinguish the two cases
according to whether ´is a right or left child of ´:p. Lines 10–11 constitute
case 2, which is shown in Figure 13.6 together with case 3. In case 2, node ´
is a right child of its parent. We immediately use a left rotation to transformthe situation into case 3 (lines 12–14), in which node ´is a left child. Because13.3 Insertion 321
C
A
B α
βγδ
Case 2zy B
A
αβγδ
Case 3zy zAB
C
αβ γ δC
Figure 13.6 Cases 2 and 3 of the procedure RB-I NSERT -FIXUP . As in case 1, property 4 is violated
in either case 2 or case 3 because ´and its parent ´:pare both red. Each of the subtrees ˛,ˇ,/CR,a n d ı
has a black root ( ˛,ˇ,a n d /CRfrom property 4, and ıbecause otherwise we would be in case 1), and
each has the same black-height. We transform case 2 into case 3 by a left rotation, which preserves
property 5: all downward simple paths from a node to a leaf have the same number of blacks. Case 3
causes some color changes and a right rotation, which also preserve property 5. The while loop then
terminates, because property 4 is satisﬁed: there are no longer two red nodes in a row.
both ´and´:pare red, the rotation affects neither the black-height of nodes
nor property 5. Whether we enter case 3 directly or through case 2, ´’s uncle y
is black, since otherwise we would have executed case 1. Additionally, thenode ´:p:pexists, since we have argued that this node existed at the time that
lines 2 and 3 were executed, and after moving ´up one level in line 10 and then
down one level in line 11, the identity of ´:p:premains unchanged. In case 3,
we execute some color changes and a right rotation, which preserve property 5,
and then, since we no longer have two red nodes in a row, we are done. Thewhile loop does not iterate another time, since ´:pis now black.
We now show that cases 2 and 3 maintain the loop invariant. (As we have just
argued, ´:pwill be black upon the next test in line 1, and the loop body will not
execute again.)
a. Case 2 makes ´point to ´:p, which is red. No further change to ´or its color
occurs in cases 2 and 3.
b. Case 3 makes ´:pblack, so that if ´:pis the root at the start of the next
iteration, it is black.
c. As in case 1, properties 1, 3, and 5 are maintained in cases 2 and 3.
Since node ´is not the root in cases 2 and 3, we know that there is no viola-
tion of property 2. Cases 2 and 3 do not introduce a violation of property 2,
since the only node that is made red becomes a child of a black node by the
rotation in case 3.
Cases 2 and 3 correct the lone violation of property 4, and they do not intro-
duce another violation.322 Chapter 13 Red-Black Trees
Having shown that each iteration of the loop maintains the invariant, we have
shown that RB-I NSERT -FIXUP correctly restores the red-black properties.
Analysis
What is the running time of RB-I NSERT ? Since the height of a red-black tree on n
nodes is O.lgn/, lines 1–16 of RB-I NSERT takeO.lgn/time. In RB-I NSERT -
FIXUP ,t h ewhile loop repeats only if case 1 occurs, and then the pointer ´moves
two levels up the tree. The total number of times the while loop can be executed
is therefore O.lgn/. Thus, RB-I NSERT takes a total of O.lgn/time. Moreover, it
never performs more than two rotations, since the while loop terminates if case 2
or case 3 is executed.
Exercises
13.3-1
In line 16 of RB-I NSERT , we set the color of the newly inserted node ´to red.
Observe that if we had chosen to set ´’s color to black, then property 4 of a red-
black tree would not be violated. Why didn’t we choose to set ´’s color to black?
13.3-2
Show the red-black trees that result after successively inserting the keys 41; 38; 31;
12; 19; 8 into an initially empty red-black tree.
13.3-3
Suppose that the black-height of each of the subtrees ˛;ˇ;/CR;ı;" in Figures 13.5
and 13.6 is k. Label each node in each ﬁgure with its black-height to verify that
the indicated transformation preserves property 5.
13.3-4
Professor Teach is concerned that RB-I NSERT -FIXUP might set T:nil:color to
RED, in which case the test in line 1 would not cause the loop to terminate when ´
is the root. Show that the professor’s concern is unfounded by arguing that RB-
INSERT -FIXUP never sets T:nil:color toRED.
13.3-5
Consider a red-black tree formed by inserting nnodes with RB-I NSERT . Argue
that if n>1 , the tree has at least one red node.
13.3-6
Suggest how to implement RB-I NSERT efﬁciently if the representation for red-
black trees includes no storage for parent pointers.13.4 Deletion 323
13.4 Deletion
Like the other basic operations on an n-node red-black tree, deletion of a node takes
timeO.lgn/. Deleting a node from a red-black tree is a bit more complicated than
inserting a node.
The procedure for deleting a node from a red-black tree is based on the T REE-
DELETE procedure (Section 12.3). First, we need to customize the T RANSPLANT
subroutine that T REE-DELETE calls so that it applies to a red-black tree:
RB-T RANSPLANT . T;u ;/ETB/
1ifu:p==T:nil
2 T:rootD/ETB
3elseif u==u:p:left
4 u:p:leftD/ETB
5elseu:p:rightD/ETB
6/ETB:pDu:p
The procedure RB-T RANSPLANT differs from T RANSPLANT in two ways. First,
line 1 references the sentinel T:nilinstead of NIL. Second, the assignment to /ETB:pin
line 6 occurs unconditionally: we can assign to /ETB:peven if /ETBpoints to the sentinel.
In fact, we shall exploit the ability to assign to /ETB:pwhen /ETBDT:nil.
The procedure RB-D ELETE is like the T REE-DELETE procedure, but with ad-
ditional lines of pseudocode. Some of the additional lines keep track of a node y
that might cause violations of the red-black properties. When we want to deletenode ´and´has fewer than two children, then ´is removed from the tree, and we
want yto be ´.W h e n ´has two children, then yshould be ´’s successor, and y
moves into ´’s position in the tree. We also remember y’s color before it is re-
moved from or moved within the tree, and we keep track of the node xthat moves
intoy’s original position in the tree, because node xmight also cause violations
of the red-black properties. After deleting node ´, RB-D
ELETE calls an auxiliary
procedure RB-D ELETE -FIXUP , which changes colors and performs rotations to
restore the red-black properties.324 Chapter 13 Red-Black Trees
RB-D ELETE .T; ´/
1yD´
2y-original -colorDy:color
3if´:left==T:nil
4 xD´:right
5 RB-T RANSPLANT . T;´ ;´ : right/
6elseif ´:right ==T:nil
7 xD´:left
8 RB-T RANSPLANT . T;´ ;´ : left/
9elseyDTREE-MINIMUM .´:right/
10 y-original -colorDy:color
11 xDy:right
12 ify:p==´
13 x:pDy
14 elseRB-T RANSPLANT . T;y;y: right/
15 y:rightD´:right
16 y:right:pDy
17 RB-T RANSPLANT . T;´ ;y/
18 y:leftD´:left
19 y:left:pDy
20 y:colorD´:color
21ify-original -color ==BLACK
22 RB-D ELETE -FIXUP .T; x/
Although RB-D ELETE contains almost twice as many lines of pseudocode as
TREE-DELETE , the two procedures have the same basic structure. You can ﬁnd
each line of T REE-DELETE within RB-D ELETE (with the changes of replacing
NILbyT:niland replacing calls to T RANSPLANT by calls to RB-T RANSPLANT ),
executed under the same conditions.
Here are the other differences between the two procedures:
/SIWe maintain node yas the node either removed from the tree or moved within
the tree. Line 1 sets yto point to node ´when ´has fewer than two children
and is therefore removed. When ´has two children, line 9 sets yto point to ´’s
successor, just as in T REE-DELETE ,a n d ywill move into ´’s position in the
tree.
/SIBecause node y’s color might change, the variable y-original -color stores y’s
color before any changes occur. Lines 2 and 10 set this variable immediately
after assignments to y.W h e n ´has two children, then y¤´and node y
moves into node ´’s original position in the red-black tree; line 20 gives ythe
same color as ´. We need to save y’s original color in order to test it at the13.4 Deletion 325
end of RB-D ELETE ; if it was black, then removing or moving ycould cause
violations of the red-black properties.
/SIAs discussed, we keep track of the node xthat moves into node y’s original
position. The assignments in lines 4, 7, and 11 set xto point to either y’s only
child or, if yhas no children, the sentinel T:nil. (Recall from Section 12.3
thatyhas no left child.)
/SISince node xmoves into node y’s original position, the attribute x:pis always
set to point to the original position in the tree of y’s parent, even if xis, in fact,
the sentinel T:nil. Unless ´isy’s original parent (which occurs only when ´has
two children and its successor yis´’s right child), the assignment to x:ptakes
place in line 6 of RB-T RANSPLANT . (Observe that when RB-T RANSPLANT
is called in lines 5, 8, or 14, the second parameter passed is the same as x.)
When y’s original parent is ´, however, we do not want x:pto point to y’s orig-
inal parent, since we are removing that node from the tree. Because node ywill
move up to take ´’s position in the tree, setting x:ptoyin line 13 causes x:p
to point to the original position of y’s parent, even if xDT:nil.
/SIFinally, if node ywas black, we might have introduced one or more violations
of the red-black properties, and so we call RB-D ELETE -FIXUP in line 22 to
restore the red-black properties. If ywas red, the red-black properties still hold
when yis removed or moved, for the following reasons:
1. No black-heights in the tree have changed.
2. No red nodes have been made adjacent. Because ytakes ´’s place in the
tree, along with ´’s color, we cannot have two adjacent red nodes at y’s new
position in the tree. In addition, if ywas not ´’s right child, then y’s original
right child xreplaces yin the tree. If yis red, then xmust be black, and so
replacing ybyxcannot cause two red nodes to become adjacent.
3. Since ycould not have been the root if it was red, the root remains black.
If node ywas black, three problems may arise, which the call of RB-D ELETE -
FIXUP will remedy. First, if yhad been the root and a red child of ybecomes the
new root, we have violated property 2. Second, if both xandx:pare red, then
we have violated property 4. Third, moving ywithin the tree causes any simple
path that previously contained yto have one fewer black node. Thus, property 5
is now violated by any ancestor of yin the tree. We can correct the violation
of property 5 by saying that node x, now occupying y’s original position, has an
“extra” black. That is, if we add 1to the count of black nodes on any simple path
that contains x, then under this interpretation, property 5 holds. When we remove
or move the black node y, we “push” its blackness onto node x. The problem is
that now node xis neither red nor black, thereby violating property 1. Instead,326 Chapter 13 Red-Black Trees
node xis either “doubly black” or “red-and-black,” and it contributes either 2or1,
respectively, to the count of black nodes on simple paths containing x.T h e color
attribute of xwill still be either RED (ifxis red-and-black) or BLACK (ifxis
doubly black). In other words, the extra black on a node is reﬂected in x’s pointing
to the node rather than in the color attribute.
We can now see the procedure RB-D ELETE -FIXUP and examine how it restores
the red-black properties to the search tree.
RB-D ELETE -FIXUP .T; x/
1while x¤T:root andx:color ==BLACK
2 ifx==x:p:left
3 wDx:p:right
4 ifw:color ==RED
5 w:colorDBLACK //case 1
6 x:p:colorDRED //case 1
7L EFT-ROTATE .T; x: p/ //case 1
8 wDx:p:right //case 1
9 ifw:left:color ==BLACK andw:right:color ==BLACK
10 w:colorDRED //case 2
11 xDx:p //case 2
12 else if w:right:color ==BLACK
13 w:left:colorDBLACK //case 3
14 w:colorDRED //case 3
15 R IGHT -ROTATE .T; w/ //case 3
16 wDx:p:right //case 3
17 w:colorDx:p:color //case 4
18 x:p:colorDBLACK //case 4
19 w:right:colorDBLACK //case 4
20 L EFT-ROTATE .T; x: p/ //case 4
21 xDT:root //case 4
22 else(same as then clause with “right” and “left” exchanged)
23x:colorDBLACK
The procedure RB-D ELETE -FIXUP restores properties 1, 2, and 4. Exercises
13.4-1 and 13.4-2 ask you to show that the procedure restores properties 2 and 4,
and so in the remainder of this section, we shall focus on property 1. The goal of
thewhile loop in lines 1–22 is to move the extra black up the tree until
1.xpoints to a red-and-black node, in which case we color x(singly) black in
line 23;
2.xpoints to the root, in which case we simply “remove” the extra black; or
3. having performed suitable rotations and recolorings, we exit the loop.13.4 Deletion 327
Within the while loop, xalways points to a nonroot doubly black node. We
determine in line 2 whether xis a left child or a right child of its parent x:p.( W e
have given the code for the situation in which xis a left child; the situation in
which xis a right child—line 22—is symmetric.) We maintain a pointer wto
the sibling of x. Since node xis doubly black, node wcannot be T:nil, because
otherwise, the number of blacks on the simple path from x:pto the (singly black)
leafwwould be smaller than the number on the simple path from x:ptox.
The four cases2in the code appear in Figure 13.7. Before examining each case
in detail, let’s look more generally at how we can verify that the transformation
in each of the cases preserves property 5. The key idea is that in each case, thetransformation applied preserves the number of black nodes (including x’s extra
black) from (and including) the root of the subtree shown to each of the subtrees˛ ;ˇ ;:::;/DLE . Thus, if property 5 holds prior to the transformation, it continues to
hold afterward. For example, in Figure 13.7(a), which illustrates case 1, the num-ber of black nodes from the root to either subtree ˛orˇis 3, both before and after
the transformation. (Again, remember that node xadds an extra black.) Similarly,
the number of black nodes from the root to any of /CR,ı,",a n d /DLEis 2, both be-
fore and after the transformation. In Figure 13.7(b), the counting must involve the
value cof the color attribute of the root of the subtree shown, which can be either
RED orBLACK . If we deﬁne count .RED/D0and count .BLACK /D1, then the
number of black nodes from the root to ˛is2Ccount .c/, both before and after
the transformation. In this case, after the transformation, the new node xhascolor
attribute c, but this node is really either red-and-black (if cDRED) or doubly black
(ifcDBLACK ). You can verify the other cases similarly (see Exercise 13.4-5).
Case 1: x’s sibling wis red
Case 1 (lines 5–8 of RB-D ELETE -FIXUP and Figure 13.7(a)) occurs when node w,
the sibling of node x, is red. Since wmust have black children, we can switch the
colors of wandx:pand then perform a left-rotation on x:pwithout violating any
of the red-black properties. The new sibling of x, which is one of w’s children
prior to the rotation, is now black, and thus we have converted case 1 into case 2,3, or 4.
Cases 2, 3, and 4 occur when node wis black; they are distinguished by the
colors of w’s children.
2As in RB-I NSERT -FIXUP , the cases in RB-D ELETE -FIXUP are not mutually exclusive.328 Chapter 13 Red-Black Trees
Case 2: x’s sibling wis black, and both of w’s children are black
In case 2 (lines 10–11 of RB-D ELETE -FIXUP and Figure 13.7(b)), both of w’s
children are black. Since wis also black, we take one black off both xandw,
leaving xwith only one black and leaving wred. To compensate for removing
one black from xandw, we would like to add an extra black to x:p,w h i c hw a s
originally either red or black. We do so by repeating the while loop with x:pas
the new node x. Observe that if we enter case 2 through case 1, the new node x
is red-and-black, since the original x:pwas red. Hence, the value cof the color
attribute of the new node xisRED, and the loop terminates when it tests the loop
condition. We then color the new node x(singly) black in line 23.
Case 3: x’s sibling wis black, w’s left child is red, and w’s right child is black
Case 3 (lines 13–16 and Figure 13.7(c)) occurs when wis black, its left child
is red, and its right child is black. We can switch the colors of wand its left
child w:leftand then perform a right rotation on wwithout violating any of the
red-black properties. The new sibling wofxis now a black node with a red right
child, and thus we have transformed case 3 into case 4.
Case 4: x’s sibling wis black, and w’s right child is red
Case 4 (lines 17–21 and Figure 13.7(d)) occurs when node x’s sibling wis black
andw’s right child is red. By making some color changes and performing a left ro-
tation on x:p, we can remove the extra black on x, making it singly black, without
violating any of the red-black properties. Setting xto be the root causes the while
loop to terminate when it tests the loop condition.
Analysis
What is the running time of RB-D ELETE ? Since the height of a red-black tree of n
nodes is O.lgn/, the total cost of the procedure without the call to RB-D ELETE -
FIXUP takes O.lgn/time. Within RB-D ELETE -FIXUP , each of cases 1, 3, and 4
lead to termination after performing a constant number of color changes and atmost three rotations. Case 2 is the only case in which the while loop can be re-
peated, and then the pointer xmoves up the tree at most O.lgn/times, performing
no rotations. Thus, the procedure RB-D
ELETE -FIXUP takes O.lgn/time and per-
forms at most three rotations, and the overall time for RB-D ELETE is therefore
alsoO.lgn/.13.4 Deletion 329
AB
D
CE αβ
γδ εζxw
AB
CD
E
x new w
αβ γ δεζ
AB
D
CE αβ
γδ εζxwc
AB
D
CE αβ
γδ εζc new x
AB
D
C E αβ
γδ εζxwc
AB
C
D αβ γ
δ
εζxc
new w
AB
D
CE αβ
γδεζxwc c
αβAB
CD
E (d)(c)(b)(a)
γδ εζCase 4Case 3Case 2Case 1 E
c′ c′
newxDT:root
Figure 13.7 The cases in the while loop of the procedure RB-D ELETE -FIXUP . Darkened nodes
have color attributes BLACK , heavily shaded nodes have color attributes RED, and lightly shaded
nodes have color attributes represented by candc0, which may be either RED orBLACK . The letters
˛ ;ˇ ;:::;/DLE represent arbitrary subtrees. Each case transforms the conﬁguration on the left into the
conﬁguration on the right by changing some colors and/or performing a rotation. Any node pointed
to by xhas an extra black and is either doubly black or red-and-black. Only case 2 causes the loop to
repeat. (a)Case 1 is transformed to case 2, 3, or 4 by exchanging the colors of nodes BandDand
performing a left rotation. (b)In case 2, the extra black represented by the pointer xmoves up the
tree by coloring node Dred and setting xto point to node B. If we enter case 2 through case 1, the
while loop terminates because the new node xis red-and-black, and therefore the value cof its color
attribute is RED.(c)Case 3 is transformed to case 4 by exchanging the colors of nodes CandDand
performing a right rotation. (d)Case 4 removes the extra black represented by xby changing some
colors and performing a left rotation (without violating the red-black properties), and then the loop
terminates.330 Chapter 13 Red-Black Trees
Exercises
13.4-1
Argue that after executing RB-D ELETE -FIXUP , the root of the tree must be black.
13.4-2
Argue that if in RB-D ELETE bothxandx:pare red, then property 4 is restored by
the call to RB-D ELETE -FIXUP .T; x/ .
13.4-3
In Exercise 13.3-2, you found the red-black tree that results from successivelyinserting the keys 41; 38; 31; 12; 19; 8 into an initially empty tree. Now show the
red-black trees that result from the successive deletion of the keys in the order8; 12; 19; 31; 38; 41 .
13.4-4
In which lines of the code for RB-D
ELETE -FIXUP might we examine or modify
the sentinel T:nil?
13.4-5
In each of the cases of Figure 13.7, give the count of black nodes from the root ofthe subtree shown to each of the subtrees ˛ ;ˇ ;:::;/DLE , and verify that each count
remains the same after the transformation. When a node has a color attribute c
orc
0, use the notation count .c/or count .c0/symbolically in your count.
13.4-6
Professors Skelton and Baron are concerned that at the start of case 1 of RB-
DELETE -FIXUP , the node x:pmight not be black. If the professors are correct,
then lines 5–6 are wrong. Show that x:pmust be black at the start of case 1, so that
the professors have nothing to worry about.
13.4-7
Suppose that a node xis inserted into a red-black tree with RB-I NSERT and then
is immediately deleted with RB-D ELETE . Is the resulting red-black tree the same
as the initial red-black tree? Justify your answer.Problems for Chapter 13 331
Problems
13-1 Persistent dynamic sets
During the course of an algorithm, we sometimes ﬁnd that we need to maintain pastversions of a dynamic set as it is updated. We call such a set persistent .O n ew a yt o
implement a persistent set is to copy the entire set whenever it is modiﬁed, but this
approach can slow down a program and also consume much space. Sometimes, we
can do much better.
Consider a persistent set Swith the operations I
NSERT ,DELETE ,a n dS EARCH ,
which we implement using binary search trees as shown in Figure 13.8(a). Wemaintain a separate root for every version of the set. In order to insert the key 5
into the set, we create a new node with key 5. This node becomes the left child
of a new node with key 7, since we cannot modify the existing node with key 7.
Similarly, the new node with key 7becomes the left child of a new node with
key8whose right child is the existing node with key 10. The new node with key 8
becomes, in turn, the right child of a new root r
0with key 4whose left child is the
existing node with key 3. We thus copy only part of the tree and share some of the
nodes with the original tree, as shown in Figure 13.8(b).
Assume that each tree node has the attributes key,left,a n d right but no parent.
(See also Exercise 13.3-6.)
4
3
28
71 04
3
28
71 04
8
7
5
(b) (a)rr r ′
Figure 13.8 (a) A binary search tree with keys 2; 3; 4; 7; 8; 10 .(b)The persistent binary search
tree that results from the insertion of key 5. The most recent version of the set consists of the nodes
reachable from the root r0, and the previous version consists of the nodes reachable from r. Heavily
shaded nodes are added when key 5is inserted.332 Chapter 13 Red-Black Trees
a.For a general persistent binary search tree, identify the nodes that we need to
change to insert a key kor delete a node y.
b.Write a procedure P ERSISTENT -TREE-INSERT that, given a persistent tree T
and a key kto insert, returns a new persistent tree T0that is the result of insert-
ingkintoT.
c.If the height of the persistent binary search tree Tish, what are the time and
space requirements of your implementation of P ERSISTENT -TREE-INSERT ?
(The space requirement is proportional to the number of new nodes allocated.)
d.Suppose that we had included the parent attribute in each node. In this case,
PERSISTENT -TREE-INSERT would need to perform additional copying. Prove
that P ERSISTENT -TREE-INSERT would then require /DEL.n/ time and space,
where nis the number of nodes in the tree.
e.Show how to use red-black trees to guarantee that the worst-case running time
and space are O.lgn/per insertion or deletion.
13-2 Join operation on red-black trees
Thejoin operation takes two dynamic sets S1andS2a n da ne l e m e n t xsuch that
for any x12S1andx22S2,w eh a v e x1:key/DC4x:key/DC4x2:key. It returns a set
SDS1[fxg[S2. In this problem, we investigate how to implement the join
operation on red-black trees.
a.Given a red-black tree T, let us store its black-height as the new attribute T:bh.
Argue that RB-I NSERT and RB-D ELETE can maintain the bhattribute with-
out requiring extra storage in the nodes of the tree and without increasing theasymptotic running times. Show that while descending through T, we can de-
termine the black-height of each node we visit in O.1/ time per node visited.
We wish to implement the operation RB-J
OIN.T1;x;T 2/, which destroys T1andT2
and returns a red-black tree TDT1[fxg[T2.L e t nbe the total number of nodes
inT1andT2.
b.Assume that T1:bh/NAKT2:bh. Describe an O.lgn/-time algorithm that ﬁnds a
black node yinT1with the largest key from among those nodes whose black-
height is T2:bh.
c.LetTybe the subtree rooted at y. Describe how Ty[fxg[T2can replace Ty
inO.1/ time without destroying the binary-search-tree property.
d.What color should we make xso that red-black properties 1, 3, and 5 are main-
tained? Describe how to enforce properties 2 and 4 in O.lgn/time.Problems for Chapter 13 333
e.Argue that no generality is lost by making the assumption in part (b). Describe
the symmetric situation that arises when T1:bh/DC4T2:bh.
f.Argue that the running time of RB-J OINisO.lgn/.
13-3 AVL trees
AnAVL tree is a binary search tree that is height balanced : for each node x,t h e
heights of the left and right subtrees of xdiffer by at most 1. To implement an A VL
tree, we maintain an extra attribute in each node: x:his the height of node x.A s
for any other binary search tree T, we assume that T:root points to the root node.
a.Prove that an AVL tree with nnodes has height O.lgn/.(Hint: Prove that
an AVL tree of height hhas at least Fhnodes, where Fhis the hth Fibonacci
number.)
b.To insert into an AVL tree, we ﬁrst place a node into the appropriate place in bi-
nary search tree order. Afterward, the tree might no longer be height balanced.Speciﬁcally, the heights of the left and right children of some node might differ
by2. Describe a procedure B
ALANCE .x/, which takes a subtree rooted at x
whose left and right children are height balanced and have heights that differby at most 2, i.e.,jx:right:h/NULx:left:hj/DC42, and alters the subtree rooted at x
to be height balanced. ( Hint: Use rotations.)
c.Using part (b), describe a recursive procedure AVL-I
NSERT .x; ´/ that takes
a node xwithin an AVL tree and a newly created node ´(whose key has al-
ready been ﬁlled in), and adds ´to the subtree rooted at x, maintaining the
property that xis the root of an AVL tree. As in T REE-INSERT from Sec-
tion 12.3, assume that ´:keyhas already been ﬁlled in and that ´:leftDNIL
and´:rightDNIL; also assume that ´:hD0. Thus, to insert the node ´into
the AVL tree T, we call AVL-I NSERT .T:root;´ /.
d.Show that AVL-I NSERT , run on an n-node AVL tree, takes O.lgn/time and
performs O.1/ rotations.
13-4 Treaps
If we insert a set of nitems into a binary search tree, the resulting tree may be
horribly unbalanced, leading to long search times. As we saw in Section 12.4,however, randomly built binary search trees tend to be balanced. Therefore, onestrategy that, on average, builds a balanced tree for a ﬁxed set of items would be torandomly permute the items and then insert them in that order into the tree.
What if we do not have all the items at once? If we receive the items one at a
time, can we still randomly build a binary search tree out of them?334 Chapter 13 Red-Black Trees
G: 4
B: 7 H: 5
A: 10 E: 23 K: 65
I: 73
Figure 13.9 A treap. Each node xis labeled with x:key:x:priority . For example, the root has
keyGand priority 4.
We will examine a data structure that answers this question in the afﬁrmative. A
treap is a binary search tree with a modiﬁed way of ordering the nodes. Figure 13.9
shows an example. As usual, each node xin the tree has a key value x:key.I n
addition, we assign x:priority , which is a random number chosen independently
for each node. We assume that all priorities are distinct and also that all keys aredistinct. The nodes of the treap are ordered so that the keys obey the binary-search-
tree property and the priorities obey the min-heap order property:
/SIIf/ETBis a left child of u,t h e n /ETB:key<u : key.
/SIIf/ETBis a right child of u,t h e n /ETB:key>u : key.
/SIIf/ETBis a child of u,t h e n /ETB:priority >u : priority .
(This combination of properties is why the tree is called a “treap”: it has features
of both a binary search tree and a heap.)
It helps to think of treaps in the following way. Suppose that we insert nodes
x1;x2;:::;x n, with associated keys, into a treap. Then the resulting treap is the
tree that would have been formed if the nodes had been inserted into a normal
binary search tree in the order given by their (randomly chosen) priorities, i.e.,
xi:priority <x j:priority means that we had inserted xibefore xj.
a.Show that given a set of nodes x1;x2;:::;x n, with associated keys and priori-
ties, all distinct, the treap associated with these nodes is unique.
b.Show that the expected height of a treap is ‚.lgn/, and hence the expected time
to search for a value in the treap is ‚.lgn/.
Let us see how to insert a new node into an existing treap. The ﬁrst thing we do
is assign to the new node a random priority. Then we call the insertion algorithm,
which we call T REAP -INSERT , whose operation is illustrated in Figure 13.10.Problems for Chapter 13 335
G: 4
B: 7 H: 5
A: 10 E: 23 K: 65
I: 73G: 4
B: 7 H: 5
A: 10 E: 23 K: 65
I: 73C: 25
C: 25
(a) (b)
G: 4
B: 7 H: 5
A: 10 E: 23 K: 65
I: 73 C: 25
(c)D: 9
D: 9G: 4
B: 7 H: 5
A: 10 E: 23 K: 65
I: 73
(d)D: 9
C: 25
G: 4
B: 7 H: 5
A: 10 K: 65
I: 73
(e)D: 9
C: 25 E: 23B: 7
A: 10
(f)D: 9
C: 25 E: 23F: 2
I: 73K: 65H: 5G: 4F: 2
…
Figure 13.10 The operation of T REAP -INSERT .(a)The original treap, prior to insertion. (b)The
treap after inserting a node with key Cand priority 25. (c)–(d) Intermediate stages when inserting a
node with key Dand priority 9. (e)The treap after the insertion of parts (c) and (d) is done. (f)The
treap after inserting a node with key Fand priority 2.336 Chapter 13 Red-Black Trees
15
9 18
3 12 25
21 6
(a)15
9 18
3 12 25
21 6
(b)
Figure 13.11 Spines of a binary search tree. The left spine is shaded in (a), and the right spine is
shaded in (b).
c.Explain how T REAP -INSERT works. Explain the idea in English and give pseu-
docode. ( Hint: Execute the usual binary-search-tree insertion procedure and
then perform rotations to restore the min-heap order property.)
d.Show that the expected running time of T REAP -INSERT is‚.lgn/.
TREAP -INSERT performs a search and then a sequence of rotations. Although
these two operations have the same expected running time, they have differentcosts in practice. A search reads information from the treap without modifying it.In contrast, a rotation changes parent and child pointers within the treap. On mostcomputers, read operations are much faster than write operations. Thus we wouldlike T
REAP -INSERT to perform few rotations. We will show that the expected
number of rotations performed is bounded by a constant.
In order to do so, we will need some deﬁnitions, which Figure 13.11 depicts.
Theleft spine of a binary search tree Tis the simple path from the root to the node
with the smallest key. In other words, the left spine is the simple path from the
root that consists of only left edges. Symmetrically, the right spine ofTis the
simple path from the root consisting of only right edges. The length of a spine is
the number of nodes it contains.
e.Consider the treap Timmediately after T REAP -INSERT has inserted node x.
LetCbe the length of the right spine of the left subtree of x.L e t Dbe the
length of the left spine of the right subtree of x. Prove that the total number of
rotations that were performed during the insertion of xis equal to CCD.
We will now calculate the expected values of CandD. Without loss of generality,
we assume that the keys are 1 ;2;:::;n , since we are comparing them only to one
another.Notes for Chapter 13 337
For nodes xandyin treap T,w h e r e y¤x,l e tkDx:keyandiDy:key.W e
deﬁne indicator random variables
XikDIfyis in the right spine of the left subtree of xg:
f.Show that XikD1if and only if y:priority >x : priority ,y:key<x : key, and,
for every ´such that y:key<´ : key<x : key,w eh a v e y:priority <´ : priority .
g.Show that
PrfXikD1gD.k/NULi/NUL1/Š
.k/NULiC1/Š
D1
.k/NULiC1/.k/NULi/:
h.Show that
EŒC /c141Dk/NUL1X
jD11
j.jC1/
D1/NUL1
k:
i.Use a symmetry argument to show that
EŒD/c141D1/NUL1
n/NULkC1:
j.Conclude that the expected number of rotations performed when inserting a
node into a treap is less than 2.
Chapter notes
The idea of balancing a search tree is due to Adel’son-Vel’ski˘ ı and Landis [2], who
introduced a class of balanced search trees called “AVL trees” in 1962, described inProblem 13-3. Another class of search trees, called “2-3 trees,” was introduced byJ. E. Hopcroft (unpublished) in 1970. A 2-3 tree maintains balance by manipulatingthe degrees of nodes in the tree. Chapter 18 covers a generalization of 2-3 treesintroduced by Bayer and McCreight [35], called “B-trees.”
Red-black trees were invented by Bayer [34] under the name “symmetric binary
B-trees.” Guibas and Sedgewick [155] studied their properties at length and in-troduced the red/black color convention. Andersson [15] gives a simpler-to-code338 Chapter 13 Red-Black Trees
variant of red-black trees. Weiss [351] calls this variant AA-trees. An AA-tree is
similar to a red-black tree except that left children may never be red.
Treaps, the subject of Problem 13-4, were proposed by Seidel and Aragon [309].
They are the default implementation of a dictionary in LEDA [253], which is awell-implemented collection of data structures and algorithms.
There are many other variations on balanced binary trees, including weight-
balanced trees [264], k-neighbor trees [245], and scapegoat trees [127]. Perhaps
the most intriguing are the “splay trees” introduced by Sleator and Tarjan [320],
which are “self-adjusting.” (See Tarjan [330] for a good description of splay trees.)
Splay trees maintain balance without any explicit balance condition such as color.Instead, “splay operations” (which involve rotations) are performed within the treeevery time an access is made. The amortized cost (see Chapter 17) of each opera-t i o no na n n-node tree is O.lgn/.
Skip lists [286] provide an alternative to balanced binary trees. A skip list is a
linked list that is augmented with a number of additional pointers. Each dictionary
operation runs in expected time O.lgn/on a skip list of nitems.14 Augmenting Data Structures
Some engineering situations require no more than a “textbook” data struc-
ture—such as a doubly linked list, a hash table, or a binary search tree—but manyothers require a dash of creativity. Only in rare situations will you need to cre-ate an entirely new type of data structure, though. More often, it will sufﬁce toaugment a textbook data structure by storing additional information in it. You canthen program new operations for the data structure to support the desired applica-tion. Augmenting a data structure is not always straightforward, however, since theadded information must be updated and maintained by the ordinary operations onthe data structure.
This chapter discusses two data structures that we construct by augmenting red-
black trees. Section 14.1 describes a data structure that supports general order-
statistic operations on a dynamic set. We can then quickly ﬁnd the ith smallest
number in a set or the rank of a given element in the total ordering of the set.Section 14.2 abstracts the process of augmenting a data structure and provides a
theorem that can simplify the process of augmenting red-black trees. Section 14.3
uses this theorem to help design a data structure for maintaining a dynamic set ofintervals, such as time intervals. Given a query interval, we can then quickly ﬁndan interval in the set that overlaps it.
14.1 Dynamic order statistics
Chapter 9 introduced the notion of an order statistic. Speciﬁcally, the ith order
statistic of a set of nelements, where i2f1 ;2;:::;ng, is simply the element in the
set with the ith smallest key. We saw how to determine any order statistic in O.n/
time from an unordered set. In this section, we shall see how to modify red-black
trees so that we can determine any order statistic for a dynamic set in O.lgn/time.
We shall also see how to compute the rank of an element—its position in the linear
order of the set—in O.lgn/time.340 Chapter 14 Augmenting Data Structures
1371 210
141614
21 12 47
2019 212117
28
35 393847 304126
121412
1
1135 1720
key
size
Figure 14.1 An order-statistic tree, which is an augmented red-black tree. Shaded nodes are red,
and darkened nodes are black. In addition to its usual attributes, each node xhas an attribute x:size,
which is the number of nodes, other than the sentinel, in the subtree rooted at x.
Figure 14.1 shows a data structure that can support fast order-statistic operations.
Anorder-statistic tree Tis simply a red-black tree with additional information
stored in each node. Besides the usual red-black tree attributes x:key,x:color ,x:p,
x:left,a n d x:right in a node x, we have another attribute, x:size. This attribute
contains the number of (internal) nodes in the subtree rooted at x(including x
itself), that is, the size of the subtree. If we deﬁne the sentinel’s size to be 0—that
is, we set T:nil:sizeto be 0—then we have the identity
x:sizeDx:left:sizeCx:right:sizeC1:
We do not require keys to be distinct in an order-statistic tree. (For example, the
tree in Figure 14.1 has two keys with value 14 and two keys with value 21.) In thepresence of equal keys, the above notion of rank is not well deﬁned. We removethis ambiguity for an order-statistic tree by deﬁning the rank of an element as theposition at which it would be printed in an inorder walk of the tree. In Figure 14.1,for example, the key 14 stored in a black node has rank 5, and the key 14 stored ina red node has rank 6.
Retrieving an element with a given rank
Before we show how to maintain this size information during insertion and dele-
tion, let us examine the implementation of two order-statistic queries that use thisadditional information. We begin with an operation that retrieves an element witha given rank. The procedure OS-S
ELECT .x; i/ returns a pointer to the node con-
taining the ith smallest key in the subtree rooted at x. To ﬁnd the node with the ith
smallest key in an order-statistic tree T, we call OS-S ELECT .T:root;i/.14.1 Dynamic order statistics 341
OS-S ELECT .x; i/
1rDx:left:sizeC1
2ifi==r
3 return x
4elseif i<r
5 return OS-S ELECT .x:left;i/
6else return OS-S ELECT .x:right;i/NULr/
In line 1 of OS-S ELECT , we compute r, the rank of node xwithin the subtree
rooted at x.T h e v a l u e o f x:left:sizeis the number of nodes that come before x
in an inorder tree walk of the subtree rooted at x. Thus, x:left:sizeC1is the
rank of xwithin the subtree rooted at x.I fiDr, then node xis the ith smallest
element, and so we return xin line 3. If i<r , then the ith smallest element
resides in x’s left subtree, and so we recurse on x:leftin line 5. If i>r ,t h e n
theith smallest element resides in x’s right subtree. Since the subtree rooted at x
contains relements that come before x’s right subtree in an inorder tree walk, the
ith smallest element in the subtree rooted at xis the .i/NULr/th smallest element in
the subtree rooted at x:right . Line 6 determines this element recursively.
To see how OS-S ELECT operates, consider a search for the 17th smallest ele-
ment in the order-statistic tree of Figure 14.1. We begin with xas the root, whose
key is 26, and with iD17. Since the size of 26’s left subtree is 12, its rank is 13.
Thus, we know that the node with rank 17 is the 17/NUL13D4th smallest element
in 26’s right subtree. After the recursive call, xis the node with key 41, and iD4.
Since the size of 41’s left subtree is 5, its rank within its subtree is 6. Thus, weknow that the node with rank 4 is the 4th smallest element in 41’s left subtree. Af-ter the recursive call, xis the node with key 30, and its rank within its subtree is 2.
Thus, we recurse once again to ﬁnd the 4/NUL2D2nd smallest element in the subtree
rooted at the node with key 38. We now ﬁnd that its left subtree has size 1, which
means it is the second smallest element. Thus, the procedure returns a pointer tothe node with key 38.
Because each recursive call goes down one level in the order-statistic tree, the
total time for OS-S
ELECT is at worst proportional to the height of the tree. Since
the tree is a red-black tree, its height is O.lgn/,w h e r e nis the number of nodes.
Thus, the running time of OS-S ELECT isO.lgn/for a dynamic set of nelements.
Determining the rank of an element
Given a pointer to a node xin an order-statistic tree T, the procedure OS-R ANK
returns the position of xin the linear order determined by an inorder tree walk
ofT.342 Chapter 14 Augmenting Data Structures
OS-R ANK.T; x/
1rDx:left:sizeC1
2yDx
3while y¤T:root
4 ify==y:p:right
5 rDrCy:p:left:sizeC1
6 yDy:p
7return r
The procedure works as follows. We can think of node x’s rank as the number of
nodes preceding xin an inorder tree walk, plus 1 for xitself. OS-R ANK maintains
the following loop invariant:
At the start of each iteration of the while loop of lines 3–6, ris the rank
ofx:keyin the subtree rooted at node y.
We use this loop invariant to show that OS-R ANK works correctly as follows:
Initialization: Prior to the ﬁrst iteration, line 1 sets rto be the rank of x:keywithin
the subtree rooted at x. Setting yDxin line 2 makes the invariant true the
ﬁrst time the test in line 3 executes.
Maintenance: At the end of each iteration of the while loop, we set yDy:p.
Thus we must show that if ris the rank of x:keyin the subtree rooted at yat the
start of the loop body, then ris the rank of x:keyin the subtree rooted at y:p
at the end of the loop body. In each iteration of the while loop, we consider
the subtree rooted at y:p. We have already counted the number of nodes in the
subtree rooted at node ythat precede xin an inorder walk, and so we must add
the nodes in the subtree rooted at y’s sibling that precede xin an inorder walk,
plus1fory:pif it, too, precedes x.I fyis a left child, then neither y:pnor any
node in y:p’s right subtree precedes x,a n ds ow el e a v e ralone. Otherwise, yis
a right child and all the nodes in y:p’s left subtree precede x, as does y:pitself.
Thus, in line 5, we add y:p:left:sizeC1to the current value of r.
Termination: The loop terminates when yDT:root, so that the subtree rooted
atyis the entire tree. Thus, the value of ris the rank of x:keyin the entire tree.
As an example, when we run OS-R ANK on the order-statistic tree of Figure 14.1
to ﬁnd the rank of the node with key 38, we get the following sequence of valuesofy:keyandrat the top of the while loop:
iteration y:key r
13 8 2
23 0 4
34 1 442 6 1 714.1 Dynamic order statistics 343
The procedure returns the rank 17.
Since each iteration of the while loop takes O.1/ time, and ygoes up one level in
the tree with each iteration, the running time of OS-R ANK is at worst proportional
to the height of the tree: O.lgn/on an n-node order-statistic tree.
Maintaining subtree sizes
Given the size attribute in each node, OS-S ELECT and OS-R ANK can quickly
compute order-statistic information. But unless we can efﬁciently maintain theseattributes within the basic modifying operations on red-black trees, our work willhave been for naught. We shall now show how to maintain subtree sizes for bothinsertion and deletion without affecting the asymptotic running time of either op-eration.
We noted in Section 13.3 that insertion into a red-black tree consists of two
phases. The ﬁrst phase goes down the tree from the root, inserting the new nodeas a child of an existing node. The second phase goes up the tree, changing colorsand performing rotations to maintain the red-black properties.
To maintain the subtree sizes in the ﬁrst phase, we simply increment x:sizefor
each node xon the simple path traversed from the root down toward the leaves. The
new node added gets a sizeof 1. Since there are O.lgn/nodes on the traversed
path, the additional cost of maintaining the sizeattributes is O.lgn/.
In the second phase, the only structural changes to the underlying red-black tree
are caused by rotations, of which there are at most two. Moreover, a rotation isa local operation: only two nodes have their sizeattributes invalidated. The link
around which the rotation is performed is incident on these two nodes. Referringto the code for L
EFT-ROTATE .T; x/ in Section 13.2, we add the following lines:
13y:sizeDx:size
14x:sizeDx:left:sizeCx:right:sizeC1
Figure 14.2 illustrates how the attributes are updated. The change to R IGHT -
ROTATE is symmetric.
Since at most two rotations are performed during insertion into a red-black tree,
we spend only O.1/ additional time updating sizeattributes in the second phase.
Thus, the total time for insertion into an n-node order-statistic tree is O.lgn/,
which is asymptotically the same as for an ordinary red-black tree.
Deletion from a red-black tree also consists of two phases: the ﬁrst operates
on the underlying search tree, and the second causes at most three rotations andotherwise performs no structural changes. (See Section 13.4.) The ﬁrst phaseeither removes one node yfrom the tree or moves upward it within the tree. To
update the subtree sizes, we simply traverse a simple path from node y(starting
from its original position within the tree) up to the root, decrementing the size344 Chapter 14 Augmenting Data Structures
LEFT-ROTATE (T,x)
RIGHT-ROTATE (T,y)93
19y
42
11x
6 479342
19
12
6
4 7x
y
Figure 14.2 Updating subtree sizes during rotations. The link around which we rotate is incident
on the two nodes whose sizeattributes need to be updated. The updates are local, requiring only the
sizeinformation stored in x,y, and the roots of the subtrees shown as triangles.
attribute of each node on the path. Since this path has length O.lgn/in an n-
node red-black tree, the additional time spent maintaining sizeattributes in the ﬁrst
phase is O.lgn/. We handle the O.1/ rotations in the second phase of deletion
in the same manner as for insertion. Thus, both insertion and deletion, includingmaintaining the sizeattributes, take O.lgn/time for an n-node order-statistic tree.
Exercises
14.1-1
Show how OS-S
ELECT .T:root; 10/ operates on the red-black tree Tof Fig-
ure 14.1.
14.1-2
Show how OS-R ANK.T; x/ operates on the red-black tree Tof Figure 14.1 and
the node xwithx:keyD35.
14.1-3
Write a nonrecursive version of OS-S ELECT .
14.1-4
Write a recursive procedure OS-K EY-RANK.T; k/ that takes as input an order-
statistic tree Tand a key kand returns the rank of kin the dynamic set represented
byT. Assume that the keys of Tare distinct.
14.1-5
Given an element xin an n-node order-statistic tree and a natural number i,h o w
can we determine the ith successor of xin the linear order of the tree in O.lgn/
time?14.2 How to augment a data structure 345
14.1-6
Observe that whenever we reference the size attribute of a node in either OS-
SELECT or OS-R ANK, we use it only to compute a rank. Accordingly, suppose
we store in each node its rank in the subtree of which it is the root. Show how tomaintain this information during insertion and deletion. (Remember that these twooperations can cause rotations.)
14.1-7
Show how to use an order-statistic tree to count the number of inversions (see
Problem 2-4) in an array of size nin time O.n lgn/.
14.1-8 ?
Consider nchords on a circle, each deﬁned by its endpoints. Describe an O.n lgn/-
time algorithm to determine the number of pairs of chords that intersect inside thecircle. (For example, if the nchords are all diameters that meet at the center, then
the correct answer is/NUL
n
2/SOH
.) Assume that no two chords share an endpoint.
14.2 How to augment a data structure
The process of augmenting a basic data structure to support additional functionality
occurs quite frequently in algorithm design. We shall use it again in the next sectionto design a data structure that supports operations on intervals. In this section, weexamine the steps involved in such augmentation. We shall also prove a theoremthat allows us to augment red-black trees easily in many cases.
We can break the process of augmenting a data structure into four steps:
1. Choose an underlying data structure.
2. Determine additional information to maintain in the underlying data structure.3. Verify that we can maintain the additional information for the basic modifying
operations on the underlying data structure.
4. Develop new operations.
As with any prescriptive design method, you should not blindly follow the steps
in the order given. Most design work contains an element of trial and error, and
progress on all steps usually proceeds in parallel. There is no point, for example, in
determining additional information and developing new operations (steps 2 and 4)
if we will not be able to maintain the additional information efﬁciently. Neverthe-
less, this four-step method provides a good focus for your efforts in augmentinga data structure, and it is also a good way to organize the documentation of anaugmented data structure.346 Chapter 14 Augmenting Data Structures
We followed these steps in Section 14.1 to design our order-statistic trees. For
step 1, we chose red-black trees as the underlying data structure. A clue to thesuitability of red-black trees comes from their efﬁcient support of other dynamic-set operations on a total order, such as M
INIMUM ,M AXIMUM ,SUCCESSOR ,a n d
PREDECESSOR .
For step 2, we added the sizeattribute, in which each node xstores the size of the
subtree rooted at x. Generally, the additional information makes operations more
efﬁcient. For example, we could have implemented OS-S ELECT and OS-R ANK
using just the keys stored in the tree, but they would not have run in O.lgn/time.
Sometimes, the additional information is pointer information rather than data, asin Exercise 14.2-1.
For step 3, we ensured that insertion and deletion could maintain the sizeat-
tributes while still running in O.lgn/time. Ideally, we should need to update only
a few elements of the data structure in order to maintain the additional information.For example, if we simply stored in each node its rank in the tree, the OS-S
ELECT
and OS-R ANK procedures would run quickly, but inserting a new minimum ele-
ment would cause a change to this information in every node of the tree. When we
store subtree sizes instead, inserting a new element causes information to change
in only O.lgn/nodes.
For step 4, we developed the operations OS-S ELECT and OS-R ANK. After all,
the need for new operations is why we bother to augment a data structure in the ﬁrstplace. Occasionally, rather than developing new operations, we use the additional
information to expedite existing ones, as in Exercise 14.2-1.
Augmenting red-black trees
When red-black trees underlie an augmented data structure, we can prove that in-
sertion and deletion can always efﬁciently maintain certain kinds of additional in-
formation, thereby making step 3 very easy. The proof of the following theorem issimilar to the argument from Section 14.1 that we can maintain the sizeattribute
for order-statistic trees.
Theorem 14.1 (Augmenting a red-black tree)
Letfbe an attribute that augments a red-black tree Tofnnodes, and suppose that
the value of ffor each node xdepends on only the information in nodes x,x:left,
andx:right , possibly including x:left:fandx:right:f. Then, we can maintain the
values of fin all nodes of Tduring insertion and deletion without asymptotically
affecting the O.lgn/performance of these operations.
Proof The main idea of the proof is that a change to an fattribute in a node x
propagates only to ancestors of xin the tree. That is, changing x:fmay re-14.2 How to augment a data structure 347
quire x:p:fto be updated, but nothing else; updating x:p:fmay require x:p:p:f
to be updated, but nothing else; and so on up the tree. Once we have updatedT:root:f, no other node will depend on the new value, and so the process termi-
nates. Since the height of a red-black tree is O.lgn/, changing an fattribute in a
node costs O.lgn/time in updating all nodes that depend on the change.
Insertion of a node xintoTconsists of two phases. (See Section 13.3.) The
ﬁrst phase inserts xas a child of an existing node x:p. We can compute the value
ofx:finO.1/ time since, by supposition, it depends only on information in the
other attributes of xitself and the information in x’s children, but x’s children are
both the sentinel T:nil. Once we have computed x:f, the change propagates up
the tree. Thus, the total time for the ﬁrst phase of insertion is O.lgn/. During the
second phase, the only structural changes to the tree come from rotations. Sinceonly two nodes change in a rotation, the total time for updating the fattributes
isO.lgn/per rotation. Since the number of rotations during insertion is at most
two, the total time for insertion is O.lgn/.
Like insertion, deletion has two phases. (See Section 13.4.) In the ﬁrst phase,
changes to the tree occur when the deleted node is removed from the tree. If the
deleted node had two children at the time, then its successor moves into the position
of the deleted node. Propagating the updates to fcaused by these changes costs
at most O.lgn/, since the changes modify the tree locally. Fixing up the red-black
tree during the second phase requires at most three rotations, and each rotationrequires at most O.lgn/time to propagate the updates to f. Thus, like insertion,
the total time for deletion is O.lgn/.
In many cases, such as maintaining the sizeattributes in order-statistic trees, the
cost of updating after a rotation is O.1/ , rather than the O.lgn/derived in the proof
of Theorem 14.1. Exercise 14.2-3 gives an example.
Exercises
14.2-1
Show, by adding pointers to the nodes, how to support each of the dynamic-setqueries M
INIMUM ,MAXIMUM ,SUCCESSOR ,a n dP REDECESSOR inO.1/ worst-
case time on an augmented order-statistic tree. The asymptotic performance ofother operations on order-statistic trees should not be affected.
14.2-2
Can we maintain the black-heights of nodes in a red-black tree as attributes in the
nodes of the tree without affecting the asymptotic performance of any of the red-
black tree operations? Show how, or argue why not. How about maintaining thedepths of nodes?348 Chapter 14 Augmenting Data Structures
14.2-3 ?
Let˝be an associative binary operator, and let abe an attribute maintained in each
node of a red-black tree. Suppose that we want to include in each node xan addi-
tional attribute fsuch that x:fDx1:a˝x2:a˝/SOH/SOH/SOH˝ xm:a,w h e r e x1;x2;:::;x m
is the inorder listing of nodes in the subtree rooted at x. Show how to update the f
attributes in O.1/ time after a rotation. Modify your argument slightly to apply it
to the sizeattributes in order-statistic trees.
14.2-4 ?
We wish to augment red-black trees with an operation RB-E NUMERATE . x;a;b/
that outputs all the keys ksuch that a/DC4k/DC4bin a red-black tree rooted at x.
Describe how to implement RB-E NUMERATE in‚.mClgn/time, where mis the
number of keys that are output and nis the number of internal nodes in the tree.
(Hint: You do not need to add new attributes to the red-black tree.)
14.3 Interval trees
In this section, we shall augment red-black trees to support operations on dynamic
sets of intervals. A closed interval is an ordered pair of real numbers Œt1;t2/c141, with
t1/DC4t2. The interval Œt1;t2/c141represents the setft2RWt1/DC4t/DC4t2g.Open and
half-open intervals omit both or one of the endpoints from the set, respectively. In
this section, we shall assume that intervals are closed; extending the results to openand half-open intervals is conceptually straightforward.
Intervals are convenient for representing events that each occupy a continuous
period of time. We might, for example, wish to query a database of time intervalsto ﬁnd out what events occurred during a given interval. The data structure in thissection provides an efﬁcient means for maintaining such an interval database.
We can represent an interval Œt
1;t2/c141as an object i, with attributes i:lowDt1
(thelow endpoint )a n d i:highDt2(thehigh endpoint ). We say that intervals i
andi0overlap ifi\i0¤;,t h a ti s ,i f i:low/DC4i0:high andi0:low/DC4i:high.A s
Figure 14.3 shows, any two intervals iandi0satisfy the interval trichotomy ;t h a t
is, exactly one of the following three properties holds:
a.iandi0overlap,
b.iis to the left of i0(i.e.,i:high<i0:low),
c.iis to the right of i0(i.e.,i0:high<i :low).
Aninterval tree is a red-black tree that maintains a dynamic set of elements, with
each element xcontaining an interval x:int. Interval trees support the following
operations:14.3 Interval trees 349
iii i
(a)
i
(b)i
(c)i′ i′ i′ i′
i′i′
Figure 14.3 The interval trichotomy for two closed intervals iandi0.(a)Ifiandi0overlap, there
are four situations; in each, i:low/DC4i0:high andi0:low/DC4i:high.(b)The intervals do not overlap,
andi:high<i0:low.(c)The intervals do not overlap, and i0:high<i :low.
INTERVAL -INSERT .T; x/ adds the element x, whose intattribute is assumed to
contain an interval, to the interval tree T.
INTERVAL -DELETE .T; x/ removes the element xfrom the interval tree T.
INTERVAL -SEARCH .T; i/ returns a pointer to an element xin the interval tree T
such that x:intoverlaps interval i, or a pointer to the sentinel T:nilif no such
element is in the set.
Figure 14.4 shows how an interval tree represents a set of intervals. We shall track
the four-step method from Section 14.2 as we review the design of an interval treeand the operations that run on it.
Step 1: Underlying data structure
We choose a red-black tree in which each node xcontains an interval x:intand the
key of xis the low endpoint, x:int:low, of the interval. Thus, an inorder tree walk
of the data structure lists the intervals in sorted order by low endpoint.
Step 2: Additional information
In addition to the intervals themselves, each node xcontains a value x:max,w h i c h
is the maximum value of any interval endpoint stored in the subtree rooted at x.
Step 3: Maintaining the information
We must verify that insertion and deletion take O.lgn/time on an interval tree
ofnnodes. We can determine x:max given interval x:intand the max values of
node x’s children:350 Chapter 14 Augmenting Data Structures
0 5 10 15 20 25 300568151617192526 26
30
20
19
21
23
9
10
8
3(a)
[0,3]
3[6,10]
10[5,8]
10[8,9]
23
[15,23]
23[16,21]
30
[17,19]
20[26,26]
26
[19,20]
20(b)[25,30]
30int
max
Figure 14.4 An interval tree. (a)A set of 10 intervals, shown sorted bottom to top by left endpoint.
(b)The interval tree that represents them. Each node xcontains an interval, shown above the dashed
line, and the maximum value of any interval endpoint in the subtree rooted at x, shown below the
dashed line. An inorder tree walk of the tree lists the nodes in sorted order by left endpoint.
x:maxDmax.x:int:high;x:left:max;x:right:max/:
Thus, by Theorem 14.1, insertion and deletion run in O.lgn/time. In fact, we
can update the max attributes after a rotation in O.1/ time, as Exercises 14.2-3
and 14.3-1 show.
Step 4: Developing new operations
The only new operation we need is I NTERVAL -SEARCH .T; i/ , which ﬁnds a node
in tree Twhose interval overlaps interval i. If there is no interval that overlaps iin
the tree, the procedure returns a pointer to the sentinel T:nil.14.3 Interval trees 351
INTERVAL -SEARCH .T; i/
1xDT:root
2while x¤T:nilandidoes not overlap x:int
3 ifx:left¤T:nilandx:left:max/NAKi:low
4 xDx:left
5 elsexDx:right
6return x
The search for an interval that overlaps istarts with xat the root of the tree and
proceeds downward. It terminates when either it ﬁnds an overlapping interval or x
points to the sentinel T:nil. Since each iteration of the basic loop takes O.1/ time,
and since the height of an n-node red-black tree is O.lgn/,t h eI NTERVAL -SEARCH
procedure takes O.lgn/time.
Before we see why I NTERVAL -SEARCH is correct, let’s examine how it works
on the interval tree in Figure 14.4. Suppose we wish to ﬁnd an interval that overlapsthe interval iDŒ22; 25/c141 . We begin with xas the root, which contains Œ16; 21/c141 and
does not overlap i.S i n c e x:left:maxD23is greater than i:lowD22, the loop
continues with xas the left child of the root—the node containing Œ8; 9/c141 , which also
does not overlap i. This time, x:left:maxD10is less than i:lowD22,a n ds ot h e
loop continues with the right child of xas the new x. Because the interval Œ15; 23/c141
stored in this node overlaps i, the procedure returns this node.
As an example of an unsuccessful search, suppose we wish to ﬁnd an interval
that overlaps iDŒ11; 14/c141 in the interval tree of Figure 14.4. We once again be-
gin with xas the root. Since the root’s interval Œ16; 21/c141 does not overlap i,a n d
since x:left:maxD23is greater than i:lowD11, we go left to the node con-
taining Œ8; 9/c141 .I n t e r v a l Œ8; 9/c141 does not overlap i,a n d x:left:maxD10is less than
i:lowD11, and so we go right. (Note that no interval in the left subtree over-
lapsi
.) Interval Œ15; 23/c141 does not overlap i, and its left child is T:nil,s oa g a i nw e
go right, the loop terminates, and we return the sentinel T:nil.
To see why I NTERVAL -SEARCH is correct, we must understand why it sufﬁces
to examine a single path from the root. The basic idea is that at any node x,
ifx:intdoes not overlap i, the search always proceeds in a safe direction: the
search will deﬁnitely ﬁnd an overlapping interval if the tree contains one. Thefollowing theorem states this property more precisely.
Theorem 14.2
Any execution of I
NTERVAL -SEARCH .T; i/ either returns a node whose interval
overlaps i, or it returns T:niland the tree Tcontains no node whose interval over-
lapsi.352 Chapter 14 Augmenting Data Structures
i
(a) (b)i′
i′ ii ′i′′i′′i′′
Figure 14.5 Intervals in the proof of Theorem 14.2. The value of x:left:max is shown in each case
as a dashed line. (a)The search goes right. No interval i0inx’s left subtree can overlap i.(b)The
search goes left. The left subtree of xcontains an interval that overlaps i(situation not shown),
orx’s left subtree contains an interval i0such that i0:highDx:left:max.S i n c e idoes not overlap i0,
neither does it overlap any interval i00inx’s right subtree, since i0:low/DC4i00:low.
Proof Thewhile loop of lines 2–5 terminates either when xDT:niloriover-
lapsx:int. In the latter case, it is certainly correct to return x. Therefore, we focus
on the former case, in which the while loop terminates because xDT:nil.
We use the following invariant for the while loop of lines 2–5:
If tree Tcontains an interval that overlaps i, then the subtree rooted at x
contains such an interval.
We use this loop invariant as follows:
Initialization: Prior to the ﬁrst iteration, line 1 sets xto be the root of T,s ot h a t
the invariant holds.
Maintenance: Each iteration of the while loop executes either line 4 or line 5. We
shall show that both cases maintain the loop invariant.
If line 5 is executed, then because of the branch condition in line 3, we
have x:leftDT:nil,o rx:left:max <i : low.I fx:leftDT:nil, the subtree
rooted at x:leftclearly contains no interval that overlaps i, and so setting x
tox:right maintains the invariant. Suppose, therefore, that x:left¤T:niland
x:left:max<i : low. As Figure 14.5(a) shows, for each interval i0inx’s left
subtree, we have
i0:high/DC4x:left:max
<i : low:
By the interval trichotomy, therefore, i0andido not overlap. Thus, the left
subtree of xcontains no intervals that overlap i, so that setting xtox:right
maintains the invariant.14.3 Interval trees 353
If, on the other hand, line 4 is executed, then we will show that the contrapos-
itive of the loop invariant holds. That is, if the subtree rooted at x:leftcon-
tains no interval overlapping i, then no interval anywhere in the tree overlaps i.
Since line 4 is executed, then because of the branch condition in line 3, wehave x:left:max/NAKi:low. Moreover, by deﬁnition of the max attribute, x’s left
subtree must contain some interval i
0such that
i0:highDx:left:max
/NAKi:low:
(Figure 14.5(b) illustrates the situation.) Since iandi0do not overlap, and
since it is not true that i0:high <i : low, it follows by the interval trichotomy
thati:high<i0:low. Interval trees are keyed on the low endpoints of intervals,
and thus the search-tree property implies that for any interval i00inx’s right
subtree,
i:high <i0:low
/DC4i00:low:
By the interval trichotomy, iandi00do not overlap. We conclude that whether
or not any interval in x’s left subtree overlaps i, setting xtox:leftmaintains
the invariant.
Termination: If the loop terminates when xDT:nil, then the subtree rooted at x
contains no interval overlapping i. The contrapositive of the loop invariant
implies that Tcontains no interval that overlaps i. Hence it is correct to return
xDT:nil.
Thus, the I NTERVAL -SEARCH procedure works correctly.
Exercises
14.3-1
Write pseudocode for L EFT-ROTATE that operates on nodes in an interval tree and
updates the max attributes in O.1/ time.
14.3-2
Rewrite the code for I NTERVAL -SEARCH so that it works properly when all inter-
vals are open.
14.3-3
Describe an efﬁcient algorithm that, given an interval i, returns an interval over-
lapping ithat has the minimum low endpoint, or T:nilif no such interval exists.354 Chapter 14 Augmenting Data Structures
14.3-4
G i v e na ni n t e r v a lt r e e Tand an interval i, describe how to list all intervals in T
that overlap iinO.min.n; k lgn//time, where kis the number of intervals in the
output list. ( Hint: One simple method makes several queries, modifying the tree
between queries. A slightly more complicated method does not modify the tree.)
14.3-5
Suggest modiﬁcations to the interval-tree procedures to support the new opera-tion I
NTERVAL -SEARCH -EXACTLY .T; i/ ,w h e r e Tis an interval tree and iis
an interval. The operation should return a pointer to a node xinTsuch that
x:int:lowDi:lowandx:int:highDi:high,o rT:nilifTcontains no such node.
All operations, including I NTERVAL -SEARCH -EXACTLY , should run in O.lgn/
time on an n-node interval tree.
14.3-6
Show how to maintain a dynamic set Qof numbers that supports the operation
MIN-GAP, which gives the magnitude of the difference of the two closest num-
bers in Q. For example, if QDf1; 5; 9; 15; 18; 22g,t h e nM IN-GAP.Q/ returns
18/NUL15D3,s i n c e 15and18are the two closest numbers in Q. Make the op-
erations I NSERT ,DELETE ,SEARCH ,a n dM IN-GAPas efﬁcient as possible, and
analyze their running times.
14.3-7 ?
VLSI databases commonly represent an integrated circuit as a list of rectan-gles. Assume that each rectangle is rectilinearly oriented (sides parallel to thex-a n d y-axes), so that we represent a rectangle by its minimum and maximum x-
andy-coordinates. Give an O.n lgn/-time algorithm to decide whether or not a set
ofnrectangles so represented contains two rectangles that overlap. Your algorithm
need not report all intersecting pairs, but it must report that an overlap exists if onerectangle entirely covers another, even if the boundary lines do not intersect. ( Hint:
Move a “sweep” line across the set of rectangles.)
Problems
14-1 Point of maximum overlapSuppose that we wish to keep track of a point of maximum overlap in a set of
intervals—a point with the largest number of intervals in the set that overlap it.
a.Show that there will always be a point of maximum overlap that is an endpoint
of one of the segments.Notes for Chapter 14 355
b.Design a data structure that efﬁciently supports the operations I NTERVAL -
INSERT ,INTERVAL -DELETE ,a n dF IND-POM, which returns a point of max-
imum overlap. ( Hint: Keep a red-black tree of all the endpoints. Associate
a value ofC1with each left endpoint, and associate a value of /NUL1with each
right endpoint. Augment each node of the tree with some extra information tomaintain the point of maximum overlap.)
14-2 Josephus permutation
We deﬁne the Josephus problem as follows. Suppose that npeople form a circle
and that we are given a positive integer m/DC4n. Beginning with a designated
ﬁrst person, we proceed around the circle, removing every mth person. After each
person is removed, counting continues around the circle that remains. This processcontinues until we have removed all npeople. The order in which the people are
removed from the circle deﬁnes the .n; m/ -Josephus permutation of the integers
1 ;2;:::;n . For example, the .7; 3/ -Josephus permutation is h3; 6; 2; 7; 5; 1; 4i.
a.Suppose that mis a constant. Describe an O.n/ -time algorithm that, given an
integer n, outputs the .n; m/ -Josephus permutation.
b.Suppose that mis not a constant. Describe an O.n lgn/-time algorithm that,
given integers nandm, outputs the .n; m/ -Josephus permutation.
Chapter notes
In their book, Preparata and Shamos [282] describe several of the interval trees
that appear in the literature, citing work by H. Edelsbrunner (1980) and E. M.McCreight (1981). The book details an interval tree that, given a static databaseofnintervals, allows us to enumerate all kintervals that overlap a given query
interval in O.kClgn/time.IV Advanced Design and Analysis TechniquesIntroduction
This part covers three important techniques used in designing and analyzing efﬁ-
cient algorithms: dynamic programming (Chapter 15), greedy algorithms (Chap-
ter 16), and amortized analysis (Chapter 17). Earlier parts have presented otherwidely applicable techniques, such as divide-and-conquer, randomization, and howto solve recurrences. The techniques in this part are somewhat more sophisticated,but they help us to attack many computational problems. The themes introduced inthis part will recur later in this book.
Dynamic programming typically applies to optimization problems in which we
make a set of choices in order to arrive at an optimal solution. As we make
each choice, subproblems of the same form often arise. Dynamic programming
is effective when a given subproblem may arise from more than one partial set of
choices; the key technique is to store the solution to each such subproblem in case itshould reappear. Chapter 15 shows how this simple idea can sometimes transformexponential-time algorithms into polynomial-time algorithms.
Like dynamic-programming algorithms, greedy algorithms typically apply to
optimization problems in which we make a set of choices in order to arrive at an
optimal solution. The idea of a greedy algorithm is to make each choice in a locallyoptimal manner. A simple example is coin-changing: to minimize the number ofU.S. coins needed to make change for a given amount, we can repeatedly selectthe largest-denomination coin that is not larger than the amount that remains. Agreedy approach provides an optimal solution for many such problems much morequickly than would a dynamic-programming approach. We cannot always easilytell whether a greedy approach will be effective, however. Chapter 16 introduces358 Part IV Advanced Design and Analysis Techniques
matroid theory, which provides a mathematical basis that can help us to show that
a greedy algorithm yields an optimal solution.
We use amortized analysis to analyze certain algorithms that perform a sequence
of similar operations. Instead of bounding the cost of the sequence of operationsby bounding the actual cost of each operation separately, an amortized analysisprovides a bound on the actual cost of the entire sequence. One advantage of thisapproach is that although some operations might be expensive, many others mightbe cheap. In other words, many of the operations might run in well under the worst-
case time. Amortized analysis is not just an analysis tool, however; it is also a way
of thinking about the design of algorithms, since the design of an algorithm and theanalysis of its running time are often closely intertwined. Chapter 17 introducesthree ways to perform an amortized analysis of an algorithm.15 Dynamic Programming
Dynamic programming, like the divide-and-conquer method, solves problems by
combining the solutions to subproblems. (“Programming” in this context refersto a tabular method, not to writing computer code.) As we saw in Chapters 2and 4, divide-and-conquer algorithms partition the problem into disjoint subprob-lems, solve the subproblems recursively, and then combine their solutions to solvethe original problem. In contrast, dynamic programming applies when the subprob-lems overlap—that is, when subproblems share subsubproblems. In this context,a divide-and-conquer algorithm does more work than necessary, repeatedly solv-ing the common subsubproblems. A dynamic-programming algorithm solves eachsubsubproblem just once and then saves its answer in a table, thereby avoiding thework of recomputing the answer every time it solves each subsubproblem.
We typically apply dynamic programming to optimization problems . Such prob-
lems can have many possible solutions. Each solution has a value, and we wish toﬁnd a solution with the optimal (minimum or maximum) value. We call such a
solution anoptimal solution to the problem, as opposed to theoptimal solution,
since there may be several solutions that achieve the optimal value.
When developing a dynamic-programming algorithm, we follow a sequence of
four steps:
1. Characterize the structure of an optimal solution.
2. Recursively deﬁne the value of an optimal solution.
3. Compute the value of an optimal solution, typically in a bottom-up fashion.
4. Construct an optimal solution from computed information.
Steps 1–3 form the basis of a dynamic-programming solution to a problem. If we
need only the value of an optimal solution, and not the solution itself, then wecan omit step 4. When we do perform step 4, we sometimes maintain additionalinformation during step 3 so that we can easily construct an optimal solution.
The sections that follow use the dynamic-programming method to solve some
optimization problems. Section 15.1 examines the problem of cutting a rod into360 Chapter 15 Dynamic Programming
rods of smaller length in way that maximizes their total value. Section 15.2 asks
how we can multiply a chain of matrices while performing the fewest total scalarmultiplications. Given these examples of dynamic programming, Section 15.3 dis-cusses two key characteristics that a problem must have for dynamic programmingto be a viable solution technique. Section 15.4 then shows how to ﬁnd the longestcommon subsequence of two sequences via dynamic programming. Finally, Sec-tion 15.5 uses dynamic programming to construct binary search trees that are opti-mal, given a known distribution of keys to be looked up.
15.1 Rod cutting
Our ﬁrst example uses dynamic programming to solve a simple problem in decid-ing where to cut steel rods. Serling Enterprises buys long steel rods and cuts theminto shorter rods, which it then sells. Each cut is free. The management of SerlingEnterprises wants to know the best way to cut up the rods.
We assume that we know, for iD1 ;2;::: , the price p
iin dollars that Serling
Enterprises charges for a rod of length iinches. Rod lengths are always an integral
number of inches. Figure 15.1 gives a sample price table.
Therod-cutting problem is the following. Given a rod of length ninches and a
table of prices piforiD1 ;2;:::;n , determine the maximum revenue rnobtain-
able by cutting up the rod and selling the pieces. Note that if the price pnfor a rod
of length nis large enough, an optimal solution may require no cutting at all.
Consider the case when nD4. Figure 15.2 shows all the ways to cut up a rod
of4inches in length, including the way with no cuts at all. We see that cutting a
4-inch rod into two 2-inch pieces produces revenue p2Cp2D5C5D10,w h i c h
is optimal.
We can cut up a rod of length nin2n/NUL1different ways, since we have an in-
dependent option of cutting, or not cutting, at distance iinches from the left end,
length i
1234 5 6 7 8 9 1 0
price pi
15891 01 71 72 02 43 0
Figure 15.1 A sample price table for rods. Each rod of length iinches earns the company pi
dollars of revenue.15.1 Rod cutting 361
9
(a)1
(b)8
(c) (d)
(e) (f) (g)1
(h)11155 1 8
5 1 15 1 15 1 1
Figure 15.2 The8possible ways of cutting up a rod of length 4. Above each piece is the
value of that piece, according to the sample price chart of Figure 15.1. The optimal strategy ispart (c)—cutting the rod into two pieces of length 2—which has total value 10.
foriD1 ;2;:::;n/NUL1.1We denote a decomposition into pieces using ordinary
additive notation, so that 7D2C2C3indicates that a rod of length 7is cut into
three pieces—two of length 2and one of length 3. If an optimal solution cuts the
rod into kpieces, for some 1/DC4k/DC4n, then an optimal decomposition
nDi1Ci2C/SOH/SOH/SOHC ik
of the rod into pieces of lengths i1,i2, ..., ikprovides maximum corresponding
revenue
rnDpi1Cpi2C/SOH/SOH/SOHC pik:
For our sample problem, we can determine the optimal revenue ﬁgures ri,f o r
iD1 ;2;:::;1 0 , by inspection, with the corresponding optimal decompositions
1If we required the pieces to be cut in order of nondecreasing size, there would be fewer ways
to consider. For nD4, we would consider only 5such ways: parts (a), (b), (c), (e), and (h)
in Figure 15.2. The number of ways is called the partition function ; it is approximately equal to
e/EMp
2n=3=4np
3. This quantity is less than 2n/NUL1, but still much greater than any polynomial in n.
We shall not pursue this line of inquiry further, however.362 Chapter 15 Dynamic Programming
r1D1from solution 1D1(no cuts) ;
r2D5from solution 2D2(no cuts) ;
r3D8from solution 3D3(no cuts) ;
r4D10from solution 4D2C2;
r5D13from solution 5D2C3;
r6D17from solution 6D6(no cuts) ;
r7D18from solution 7D1C6or7D2C2C3;
r8D22from solution 8D2C6;
r9D25from solution 9D3C6;
r10D30from solution 10D10 (no cuts) :
More generally, we can frame the values rnforn/NAK1in terms of optimal rev-
enues from shorter rods:
rnDmax.pn;r1Crn/NUL1;r2Crn/NUL2;:::;r n/NUL1Cr1/: (15.1)
The ﬁrst argument, pn, corresponds to making no cuts at all and selling the rod of
length nas is. The other n/NUL1arguments to max correspond to the maximum rev-
enue obtained by making an initial cut of the rod into two pieces of size iandn/NULi,
for each iD1 ;2;:::;n/NUL1, and then optimally cutting up those pieces further,
obtaining revenues riandrn/NULifrom those two pieces. Since we don’t know ahead
of time which value of ioptimizes revenue, we have to consider all possible values
foriand pick the one that maximizes revenue. We also have the option of picking
noiat all if we can obtain more revenue by selling the rod uncut.
Note that to solve the original problem of size n, we solve smaller problems of
the same type, but of smaller sizes. Once we make the ﬁrst cut, we may considerthe two pieces as independent instances of the rod-cutting problem. The overalloptimal solution incorporates optimal solutions to the two related subproblems,maximizing revenue from each of those two pieces. We say that the rod-cuttingproblem exhibits optimal substructure : optimal solutions to a problem incorporate
optimal solutions to related subproblems, which we may solve independently.
In a related, but slightly simpler, way to arrange a recursive structure for the rod-
cutting problem, we view a decomposition as consisting of a ﬁrst piece of length i
cut off the left-hand end, and then a right-hand remainder of length n/NULi.O n l y
the remainder, and not the ﬁrst piece, may be further divided. We may view everydecomposition of a length- nrod in this way: as a ﬁrst piece followed by some
decomposition of the remainder. When doing so, we can couch the solution withno cuts at all as saying that the ﬁrst piece has size iDnand revenue p
nand that
the remainder has size 0with corresponding revenue r0D0. We thus obtain the
following simpler version of equation (15.1):
rnDmax
1/DC4i/DC4n.piCrn/NULi/: (15.2)15.1 Rod cutting 363
In this formulation, an optimal solution embodies the solution to only onerelated
subproblem—the remainder—rather than two.
Recursive top-down implementation
The following procedure implements the computation implicit in equation (15.2)
in a straightforward, top-down, recursive manner.
CUT-ROD.p; n/
1ifn==0
2 return 0
3qD/NUL1
4foriD1ton
5 qDmax.q; pŒi/c141CCUT-ROD.p; n/NULi//
6return q
Procedure C UT-RODtakes as input an array pŒ1::n/c141 of prices and an integer n,
and it returns the maximum revenue possible for a rod of length n.I fnD0,n o
revenue is possible, and so C UT-RODreturns 0in line 2. Line 3 initializes the
maximum revenue qto/NUL1, so that the forloop in lines 4–5 correctly computes
qDmax 1/DC4i/DC4n.piCCUT-ROD.p; n/NULi//; line 6 then returns this value. A simple
induction on nproves that this answer is equal to the desired answer rn,u s i n g
equation (15.2).
If you were to code up C UT-RODin your favorite programming language and run
it on your computer, you would ﬁnd that once the input size becomes moderatelylarge, your program would take a long time to run. For nD40, you would ﬁnd that
your program takes at least several minutes, and most likely more than an hour. Infact, you would ﬁnd that each time you increase nby1, your program’s running
time would approximately double.
Why is C
UT-RODso inefﬁcient? The problem is that C UT-RODcalls itself
recursively over and over again with the same parameter values; it solves thesame subproblems repeatedly. Figure 15.3 illustrates what happens for nD4:
C
UT-ROD.p; n/ calls C UT-ROD.p; n/NULi/foriD1 ;2;:::;n . Equivalently,
CUT-ROD.p; n/ calls C UT-ROD.p; j / for each jD0; 1; : : : ; n/NUL1. When this
process unfolds recursively, the amount of work done, as a function of n,g r o w s
explosively.
To analyze the running time of C UT-ROD,l e tT .n/ denote the total number of
calls made to C UT-RODwhen called with its second parameter equal to n.T h i s
expression equals the number of nodes in a subtree whose root is labeled nin the
recursion tree. The count includes the initial call at its root. Thus, T. 0 /D1and364 Chapter 15 Dynamic Programming
3
1 0
0
00 12 0
0120104
Figure 15.3 The recursion tree showing recursive calls resulting from a call C UT-ROD.p; n/ for
nD4. Each node label gives the size nof the corresponding subproblem, so that an edge from
a parent with label sto a child with label tcorresponds to cutting off an initial piece of size s/NULt
and leaving a remaining subproblem of size t. A path from the root to a leaf corresponds to one of
the2n/NUL1ways of cutting up a rod of length n. In general, this recursion tree has 2nnodes and 2n/NUL1
leaves.
T .n/D1Cn/NUL1X
jD0T. j/: (15.3)
The initial 1is for the call at the root, and the term T. j/ counts the number of calls
(including recursive calls) due to the call C UT-ROD.p; n/NULi/,w h e r e jDn/NULi.
As Exercise 15.1-1 asks you to show,
T .n/D2n; (15.4)
and so the running time of C UT-RODis exponential in n.
In retrospect, this exponential running time is not so surprising. C UT-RODex-
plicitly considers all the 2n/NUL1possible ways of cutting up a rod of length n.T h e
tree of recursive calls has 2n/NUL1leaves, one for each possible way of cutting up the
rod. The labels on the simple path from the root to a leaf give the sizes of eachremaining right-hand piece before making each cut. That is, the labels give thecorresponding cut points, measured from the right-hand end of the rod.
Using dynamic programming for optimal rod cutting
We now show how to convert C
UT-RODinto an efﬁcient algorithm, using dynamic
programming.
The dynamic-programming method works as follows. Having observed that a
naive recursive solution is inefﬁcient because it solves the same subproblems re-
peatedly, we arrange for each subproblem to be solved only once, saving its solu-
tion. If we need to refer to this subproblem’s solution again later, we can just look it15.1 Rod cutting 365
up, rather than recompute it. Dynamic programming thus uses additional memory
to save computation time; it serves an example of a time-memory trade-off .T h e
savings may be dramatic: an exponential-time solution may be transformed into apolynomial-time solution. A dynamic-programming approach runs in polynomialtime when the number of distinct subproblems involved is polynomial in the input
size and we can solve each such subproblem in polynomial time.
There are usually two equivalent ways to implement a dynamic-programming
approach. We shall illustrate both of them with our rod-cutting example.
The ﬁrst approach is top-down with memoization .
2In this approach, we write
the procedure recursively in a natural manner, but modiﬁed to save the result ofeach subproblem (usually in an array or hash table). The procedure now ﬁrst checksto see whether it has previously solved this subproblem. If so, it returns the savedvalue, saving further computation at this level; if not, the procedure computes thevalue in the usual manner. We say that the recursive procedure has been memoized ;
it “remembers” what results it has computed previously.
The second approach is the bottom-up method . This approach typically depends
on some natural notion of the “size” of a subproblem, such that solving any par-
ticular subproblem depends only on solving “smaller” subproblems. We sort the
subproblems by size and solve them in size order, smallest ﬁrst. When solving aparticular subproblem, we have already solved all of the smaller subproblems itssolution depends upon, and we have saved their solutions. We solve each sub-problem only once, and when we ﬁrst see it, we have already solved all of its
prerequisite subproblems.
These two approaches yield algorithms with the same asymptotic running time,
except in unusual circumstances where the top-down approach does not actuallyrecurse to examine all possible subproblems. The bottom-up approach often hasmuch better constant factors, since it has less overhead for procedure calls.
Here is the the pseudocode for the top-down C
UT-RODprocedure, with memo-
ization added:
MEMOIZED -CUT-ROD.p; n/
1l e t rŒ 0::n /c141 be a new array
2foriD0ton
3 rŒi/c141D/NUL1
4return MEMOIZED -CUT-ROD-AUX. p;n;r/
2This is not a misspelling. The word really is memoization , not memorization .Memoization comes
from memo , since the technique consists of recording a value so that we can look it up later.366 Chapter 15 Dynamic Programming
MEMOIZED -CUT-ROD-AUX. p;n;r/
1ifrŒn/c141/NAK0
2 return rŒn/c141
3ifn==0
4 qD0
5elseqD/NUL1
6 foriD1ton
7 qDmax.q; pŒi/c141CMEMOIZED -CUT-ROD-AUX.p; n/NULi;r//
8rŒn/c141Dq
9return q
Here, the main procedure M EMOIZED -CUT-RODinitializes a new auxiliary ar-
rayrŒ 0::n /c141 with the value/NUL1, a convenient choice with which to denote “un-
known.” (Known revenue values are always nonnegative.) It then calls its helper
routine, M EMOIZED -CUT-ROD-AUX.
The procedure M EMOIZED -CUT-ROD-AUXis just the memoized version of our
previous procedure, C UT-ROD. It ﬁrst checks in line 1 to see whether the desired
value is already known and, if it is, then line 2 returns it. Otherwise, lines 3–7compute the desired value qin the usual manner, line 8 saves it in rŒn/c141, and line 9
returns it.
The bottom-up version is even simpler:
B
OTTOM -UP-CUT-ROD.p; n/
1l e t rŒ 0::n /c141 b ean e wa r r a y
2rŒ0/c141D0
3forjD1ton
4 qD/NUL1
5 foriD1toj
6 qDmax.q; pŒi/c141CrŒj/NULi/c141/
7 rŒj/c141Dq
8return rŒn/c141
For the bottom-up dynamic-programming approach, B OTTOM -UP-CUT-ROD
uses the natural ordering of the subproblems: a problem of size iis “smaller”
than a subproblem of size jifi<j . Thus, the procedure solves subproblems of
sizes jD0; 1; : : : ; n ,i nt h a to r d e r .
Line 1 of procedure B OTTOM -UP-CUT-RODcreates a new array rŒ 0::n /c141 in
which to save the results of the subproblems, and line 2 initializes rŒ0/c141to0,s i n c e
a rod of length 0earns no revenue. Lines 3–6 solve each subproblem of size j,f o r
jD1 ;2;:::;n , in order of increasing size. The approach used to solve a problem
of a particular size jis the same as that used by C UT-ROD, except that line 6 now15.1 Rod cutting 367
3
0124
Figure 15.4 The subproblem graph for the rod-cutting problem with nD4. The vertex labels
give the sizes of the corresponding subproblems. A directed edge .x; y/ indicates that we need a
solution to subproblem ywhen solving subproblem x. This graph is a reduced version of the tree of
Figure 15.3, in which all nodes with the same label are collapsed into a single vertex and all edges
go from parent to child.
directly references array entry rŒj/NULi/c141instead of making a recursive call to solve
the subproblem of size j/NULi. Line 7 saves in rŒj/c141 the solution to the subproblem
of size j. Finally, line 8 returns rŒn/c141, which equals the optimal value rn.
The bottom-up and top-down versions have the same asymptotic running time.
The running time of procedure B OTTOM -UP-CUT-RODis‚.n2/, due to its
doubly-nested loop structure. The number of iterations of its inner forloop, in
lines 5–6, forms an arithmetic series. The running time of its top-down counterpart,
MEMOIZED -CUT-ROD,i sa l s o ‚.n2/, although this running time may be a little
harder to see. Because a recursive call to solve a previously solved subproblemreturns immediately, M
EMOIZED -CUT-RODsolves each subproblem just once. It
solves subproblems for sizes 0; 1; : : : ; n . To solve a subproblem of size n,t h efor
loop of lines 6–7 iterates ntimes. Thus, the total number of iterations of this for
loop, over all recursive calls of M EMOIZED -CUT-ROD, forms an arithmetic series,
giving a total of ‚.n2/iterations, just like the inner forloop of B OTTOM -UP-
CUT-ROD. (We actually are using a form of aggregate analysis here. We shall see
aggregate analysis in detail in Section 17.1.)
Subproblem graphs
When we think about a dynamic-programming problem, we should understand the
set of subproblems involved and how subproblems depend on one another.
Thesubproblem graph for the problem embodies exactly this information. Fig-
ure 15.4 shows the subproblem graph for the rod-cutting problem with nD4.I t
is a directed graph, containing one vertex for each distinct subproblem. The sub-368 Chapter 15 Dynamic Programming
problem graph has a directed edge from the vertex for subproblem xto the vertex
for subproblem yif determining an optimal solution for subproblem xinvolves
directly considering an optimal solution for subproblem y. For example, the sub-
problem graph contains an edge from xtoyif a top-down recursive procedure for
solving xdirectly calls itself to solve y. We can think of the subproblem graph
as a “reduced” or “collapsed” version of the recursion tree for the top-down recur-sive method, in which we coalesce all nodes for the same subproblem into a singlevertex and direct all edges from parent to child.
The bottom-up method for dynamic programming considers the vertices of the
subproblem graph in such an order that we solve the subproblems yadjacent to
a given subproblem xbefore we solve subproblem x. (Recall from Section B.4
that the adjacency relation is not necessarily symmetric.) Using the terminologyfrom Chapter 22, in a bottom-up dynamic-programming algorithm, we consider thevertices of the subproblem graph in an order that is a “reverse topological sort,” ora “topological sort of the transpose” (see Section 22.4) of the subproblem graph. Inother words, no subproblem is considered until all of the subproblems it dependsupon have been solved. Similarly, using notions from the same chapter, we canview the top-down method (with memoization) for dynamic programming as a“depth-ﬁrst search” of the subproblem graph (see Section 22.3).
The size of the subproblem graph GD.V; E/ can help us determine the running
time of the dynamic programming algorithm. Since we solve each subproblem justonce, the running time is the sum of the times needed to solve each subproblem.
Typically, the time to compute the solution to a subproblem is proportional to the
degree (number of outgoing edges) of the corresponding vertex in the subproblemgraph, and the number of subproblems is equal to the number of vertices in the sub-problem graph. In this common case, the running time of dynamic programmingis linear in the number of vertices and edges.
Reconstructing a solution
Our dynamic-programming solutions to the rod-cutting problem return the value of
an optimal solution, but they do not return an actual solution: a list of piece sizes.We can extend the dynamic-programming approach to record not only the optimalvalue computed for each subproblem, but also a choice that led to the optimal
value. With this information, we can readily print an optimal solution.
Here is an extended version of B
OTTOM -UP-CUT-RODthat computes, for each
rod size j, not only the maximum revenue rj, but also sj, the optimal size of the
ﬁrst piece to cut off:15.1 Rod cutting 369
EXTENDED -BOTTOM -UP-CUT-ROD.p; n/
1l e t rŒ 0::n /c141 andsŒ0:: n/c141 be new arrays
2rŒ0/c141D0
3forjD1ton
4 qD/NUL1
5 foriD1toj
6 ifq<p Œ i /c141CrŒj/NULi/c141
7 qDpŒi/c141CrŒj/NULi/c141
8 sŒj/c141Di
9 rŒj/c141Dq
10return rands
This procedure is similar to B OTTOM -UP-CUT-ROD, except that it creates the ar-
raysin line 1, and it updates sŒj/c141 in line 8 to hold the optimal size iof the ﬁrst
piece to cut off when solving a subproblem of size j.
The following procedure takes a price table pand a rod size n, and it calls
EXTENDED -BOTTOM -UP-CUT-RODto compute the array sŒ1:: n/c141 of optimal
ﬁrst-piece sizes and then prints out the complete list of piece sizes in an optimaldecomposition of a rod of length n:
P
RINT -CUT-ROD-SOLUTION .p; n/
1.r; s/DEXTENDED -BOTTOM -UP-CUT-ROD.p; n/
2while n>0
3 print sŒn/c141
4 nDn/NULsŒn/c141
In our rod-cutting example, the call E XTENDED -BOTTOM -UP-CUT-ROD.p; 10/
would return the following arrays:
i
0 1 2 3 456789 1 0
rŒi/c141
 0 1 5 8 10 13 17 18 22 25 30
sŒi/c141
 0 1 2 3 226123 1 0
A call to P RINT -CUT-ROD-SOLUTION .p; 10/ would print just 10, but a call with
nD7would print the cuts 1and6, corresponding to the ﬁrst optimal decomposi-
tion for r7given earlier.
Exercises
15.1-1
Show that equation (15.4) follows from equation (15.3) and the initial conditionT. 0 /D1.370 Chapter 15 Dynamic Programming
15.1-2
Show, by means of a counterexample, that the following “greedy” strategy doesnot always determine an optimal way to cut rods. Deﬁne the density of a rod of
length ito be p
i=i, that is, its value per inch. The greedy strategy for a rod of
length ncuts off a ﬁrst piece of length i,w h e r e 1/DC4i/DC4n, having maximum
density. It then continues by applying the greedy strategy to the remaining piece oflength n/NULi.
15.1-3
Consider a modiﬁcation of the rod-cutting problem in which, in addition to a
price p
ifor each rod, each cut incurs a ﬁxed cost of c. The revenue associated with
a solution is now the sum of the prices of the pieces minus the costs of making thecuts. Give a dynamic-programming algorithm to solve this modiﬁed problem.
15.1-4
Modify M
EMOIZED -CUT-RODto return not only the value but the actual solution,
too.
15.1-5
The Fibonacci numbers are deﬁned by recurrence (3.22). Give an O.n/ -time
dynamic-programming algorithm to compute the nth Fibonacci number. Draw the
subproblem graph. How many vertices and edges are in the graph?
15.2 Matrix-chain multiplication
Our next example of dynamic programming is an algorithm that solves the problemof matrix-chain multiplication. We are given a sequence (chain) hA
1;A2;:::;A ni
ofnmatrices to be multiplied, and we wish to compute the product
A1A2/SOH/SOH/SOHAn: (15.5)
We can evaluate the expression (15.5) using the standard algorithm for multiply-
ing pairs of matrices as a subroutine once we have parenthesized it to resolve all
ambiguities in how the matrices are multiplied together. Matrix multiplication is
associative, and so all parenthesizations yield the same product. A product of ma-trices is fully parenthesized if it is either a single matrix or the product of two fully
parenthesized matrix products, surrounded by parentheses. For example, if thechain of matrices is hA
1;A2;A3;A4i, then we can fully parenthesize the product
A1A2A3A4in ﬁve distinct ways:15.2 Matrix-chain multiplication 371
.A1.A2.A3A4/// ;
.A1..A 2A3/A4// ;
..A 1A2/.A 3A4// ;
..A 1.A2A3//A 4/;
...A 1A2/A3/A4/:
How we parenthesize a chain of matrices can have a dramatic impact on the cost
of evaluating the product. Consider ﬁrst the cost of multiplying two matrices. Thestandard algorithm is given by the following pseudocode, which generalizes the
S
QUARE -MATRIX -MULTIPLY procedure from Section 4.2. The attributes rows
andcolumns are the numbers of rows and columns in a matrix.
MATRIX -MULTIPLY .A; B/
1ifA:columns¤B:rows
2 error “incompatible dimensions”
3elseletCb ean e w A:rows/STXB:columns matrix
4 foriD1toA:rows
5 forjD1toB:columns
6 cijD0
7 forkD1toA:columns
8 cijDcijCaik/SOHbkj
9 return C
We can multiply two matrices AandBonly if they are compatible : the number of
columns of Amust equal the number of rows of B.I fAis ap/STXqmatrix and Bis
aq/STXrmatrix, the resulting matrix Cis ap/STXrmatrix. The time to compute Cis
dominated by the number of scalar multiplications in line 8, which is pqr.I nw h a t
follows, we shall express costs in terms of the number of scalar multiplications.
To illustrate the different costs incurred by different parenthesizations of a matrix
product, consider the problem of a chain hA1;A2;A3iof three matrices. Suppose
that the dimensions of the matrices are 10/STX100,100/STX5,a n d 5/STX50, respec-
tively. If we multiply according to the parenthesization ..A 1A2/A3/, we perform
10/SOH100/SOH5D5000 scalar multiplications to compute the 10/STX5matrix prod-
uctA1A2, plus another 10/SOH5/SOH50D2500 scalar multiplications to multiply this
matrix by A3, for a total of 7500 scalar multiplications. If instead we multiply
according to the parenthesization .A1.A2A3//, we perform 100/SOH5/SOH50D25,000
scalar multiplications to compute the 100/STX50matrix product A2A3, plus another
10/SOH100/SOH50D50,000 scalar multiplications to multiply A1by this matrix, for a
total of 75,000 scalar multiplications. Thus, computing the product according tothe ﬁrst parenthesization is 10times faster.
We state the matrix-chain multiplication problem as follows: given a chain
hA
1;A2;:::;A niofnmatrices, where for iD1 ;2;:::;n , matrix Aihas dimension372 Chapter 15 Dynamic Programming
pi/NUL1/STXpi, fully parenthesize the product A1A2/SOH/SOH/SOHAnin a way that minimizes the
number of scalar multiplications.
Note that in the matrix-chain multiplication problem, we are not actually multi-
plying matrices. Our goal is only to determine an order for multiplying matricesthat has the lowest cost. Typically, the time invested in determining this optimalorder is more than paid for by the time saved later on when actually performing thematrix multiplications (such as performing only 7500 scalar multiplications insteadof 75,000).
Counting the number of parenthesizations
Before solving the matrix-chain multiplication problem by dynamic programming,
let us convince ourselves that exhaustively checking all possible parenthesizations
does not yield an efﬁcient algorithm. Denote the number of alternative parenthe-
sizations of a sequence of nmatrices by P.n/ .W h e n nD1, we have just one
matrix and therefore only one way to fully parenthesize the matrix product. Whenn/NAK2, a fully parenthesized matrix product is the product of two fully parenthe-
sized matrix subproducts, and the split between the two subproducts may occurbetween the kth and .kC1/st matrices for any kD1 ;2;:::;n/NUL1. Thus, we
obtain the recurrence
P.n/D
/c128
1 ifnD1;
n/NUL1X
kD1P.k/P.n/NULk/ifn/NAK2:(15.6)
Problem 12-4 asked you to show that the solution to a similar recurrence is the
sequence of Catalan numbers , which grows as /DEL.4n=n3=2/. A simpler exercise
(see Exercise 15.2-3) is to show that the solution to the recurrence (15.6) is /DEL.2n/.
The number of solutions is thus exponential in n, and the brute-force method of
exhaustive search makes for a poor strategy when determining how to optimally
parenthesize a matrix chain.
Applying dynamic programming
We shall use the dynamic-programming method to determine how to optimally
parenthesize a matrix chain. In so doing, we shall follow the four-step sequence
that we stated at the beginning of this chapter:
1. Characterize the structure of an optimal solution.
2. Recursively deﬁne the value of an optimal solution.
3. Compute the value of an optimal solution.15.2 Matrix-chain multiplication 373
4. Construct an optimal solution from computed information.
We shall go through these steps in order, demonstrating clearly how we apply each
step to the problem.
Step 1: The structure of an optimal parenthesization
For our ﬁrst step in the dynamic-programming paradigm, we ﬁnd the optimal sub-
structure and then use it to construct an optimal solution to the problem from opti-mal solutions to subproblems. In the matrix-chain multiplication problem, we canperform this step as follows. For convenience, let us adopt the notation A
i::j,w h e r e
i/DC4j, for the matrix that results from evaluating the product AiAiC1/SOH/SOH/SOHAj.O b -
serve that if the problem is nontrivial, i.e., i<j , then to parenthesize the product
AiAiC1/SOH/SOH/SOHAj, we must split the product between AkandAkC1for some integer k
in the range i/DC4k<j . That is, for some value of k, we ﬁrst compute the matrices
Ai:: kandAkC1::jand then multiply them together to produce the ﬁnal product Ai::j.
The cost of parenthesizing this way is the cost of computing the matrix Ai:: k,p l u s
the cost of computing AkC1::j, plus the cost of multiplying them together.
The optimal substructure of this problem is as follows. Suppose that to op-
timally parenthesize AiAiC1/SOH/SOH/SOHAj, we split the product between AkandAkC1.
Then the way we parenthesize the “preﬁx” subchain AiAiC1/SOH/SOH/SOHAkwithin this
optimal parenthesization of AiAiC1/SOH/SOH/SOHAjmust be an optimal parenthesization of
AiAiC1/SOH/SOH/SOHAk. Why? If there were a less costly way to parenthesize AiAiC1/SOH/SOH/SOHAk,
then we could substitute that parenthesization in the optimal parenthesizationofA
iAiC1/SOH/SOH/SOHAjto produce another way to parenthesize AiAiC1/SOH/SOH/SOHAjwhose cost
was lower than the optimum: a contradiction. A similar observation holds for howwe parenthesize the subchain A
kC1AkC2/SOH/SOH/SOHAjin the optimal parenthesization of
AiAiC1/SOH/SOH/SOHAj: it must be an optimal parenthesization of AkC1AkC2/SOH/SOH/SOHAj.
Now we use our optimal substructure to show that we can construct an optimal
solution to the problem from optimal solutions to subproblems. We have seen thatany solution to a nontrivial instance of the matrix-chain multiplication problemrequires us to split the product, and that any optimal solution contains within it op-timal solutions to subproblem instances. Thus, we can build an optimal solution toan instance of the matrix-chain multiplication problem by splitting the problem into
two subproblems (optimally parenthesizing A
iAiC1/SOH/SOH/SOHAkandAkC1AkC2/SOH/SOH/SOHAj),
ﬁnding optimal solutions to subproblem instances, and then combining these op-timal subproblem solutions. We must ensure that when we search for the correctplace to split the product, we have considered all possible places, so that we aresure of having examined the optimal one.374 Chapter 15 Dynamic Programming
Step 2: A recursive solution
Next, we deﬁne the cost of an optimal solution recursively in terms of the optimal
solutions to subproblems. For the matrix-chain multiplication problem, we pick asour subproblems the problems of determining the minimum cost of parenthesizingA
iAiC1/SOH/SOH/SOHAjfor1/DC4i/DC4j/DC4n.L e t mŒi; j /c141 be the minimum number of scalar
multiplications needed to compute the matrix Ai::j; for the full problem, the lowest-
cost way to compute A1::nwould thus be mŒ1; n/c141 .
We can deﬁne mŒi; j /c141 recursively as follows. If iDj, the problem is trivial;
the chain consists of just one matrix Ai::iDAi, so that no scalar multiplications
are necessary to compute the product. Thus, mŒi; i/c141D0foriD1 ;2;:::;n .T o
compute mŒi; j /c141 when i<j , we take advantage of the structure of an optimal
solution from step 1. Let us assume that to optimally parenthesize, we split theproduct A
iAiC1/SOH/SOH/SOHAjbetween AkandAkC1,w h e r e i/DC4k<j . Then, mŒi; j /c141
equals the minimum cost for computing the subproducts Ai:: kandAkC1::j,p l u st h e
cost of multiplying these two matrices together. Recalling that each matrix Aiis
pi/NUL1/STXpi, we see that computing the matrix product Ai::kAkC1::jtakes pi/NUL1pkpj
scalar multiplications. Thus, we obtain
mŒi; j /c141DmŒi; k/c141CmŒkC1; j /c141Cpi/NUL1pkpj:
This recursive equation assumes that we know the value of k, which we do not.
There are only j/NULipossible values for k, however, namely kDi;iC1 ;:::;j/NUL1.
Since the optimal parenthesization must use one of these values for k, we need only
check them all to ﬁnd the best. Thus, our recursive deﬁnition for the minimum costof parenthesizing the product A
iAiC1/SOH/SOH/SOHAjbecomes
mŒi; j /c141D(0 ifiDj;
min
i/DC4k<jfmŒi; k/c141CmŒkC1; j /c141Cpi/NUL1pkpjgifi<j:(15.7)
ThemŒi; j /c141 values give the costs of optimal solutions to subproblems, but they
do not provide all the information we need to construct an optimal solution. Tohelp us do so, we deﬁne sŒi;j/c141 to be a value of kat which we split the product
A
iAiC1/SOH/SOH/SOHAjin an optimal parenthesization. That is, sŒi;j/c141 equals a value ksuch
thatmŒi; j /c141DmŒi; k/c141CmŒkC1; j /c141Cpi/NUL1pkpj.
Step 3: Computing the optimal costs
At this point, we could easily write a recursive algorithm based on recurrence (15.7)
to compute the minimum cost mŒ1; n/c141 for multiplying A1A2/SOH/SOH/SOHAn.A sw es a wf o r
the rod-cutting problem, and as we shall see in Section 15.3, this recursive algo-rithm takes exponential time, which is no better than the brute-force method ofchecking each way of parenthesizing the product.15.2 Matrix-chain multiplication 375
Observe that we have relatively few distinct subproblems: one subproblem for
each choice of iandjsatisfying 1/DC4i/DC4j/DC4n,o r/NULn
2/SOH
CnD‚.n2/in all.
A recursive algorithm may encounter each subproblem many times in differentbranches of its recursion tree. This property of overlapping subproblems is thesecond hallmark of when dynamic programming applies (the ﬁrst hallmark beingoptimal substructure).
Instead of computing the solution to recurrence (15.7) recursively, we compute
the optimal cost by using a tabular, bottom-up approach. (We present the corre-
sponding top-down approach using memoization in Section 15.3.)
We shall implement the tabular, bottom-up method in the procedure M
ATRIX -
CHAIN -ORDER , which appears below. This procedure assumes that matrix Ai
has dimensions pi/NUL1/STXpiforiD1 ;2;:::;n . Its input is a sequence pD
hp0;p1;:::;p ni,w h e r e p:lengthDnC1. The procedure uses an auxiliary
table m Œ 1::n ;1::n /c141 for storing the mŒi; j /c141 costs and another auxiliary table
sŒ 1::n/NUL1 ;2::n /c141 that records which index of kachieved the optimal cost in com-
puting mŒi; j /c141 . We shall use the table sto construct an optimal solution.
In order to implement the bottom-up approach, we must determine which entries
of the table we refer to when computing mŒi; j /c141 . Equation (15.7) shows that the
costmŒi; j /c141 of computing a matrix-chain product of j/NULiC1matrices depends only
on the costs of computing matrix-chain products of fewer than j/NULiC1matrices.
That is, for kDi;iC1 ;:::;j/NUL1, the matrix Ai::kis a product of k/NULiC1<
j/NULiC1matrices and the matrix AkC1::jis a product of j/NULk<j/NULiC1
matrices. Thus, the algorithm should ﬁll in the table min a manner that corresponds
to solving the parenthesization problem on matrix chains of increasing length. Forthe subproblem of optimally parenthesizing the chain A
iAiC1/SOH/SOH/SOHAj, we consider
the subproblem size to be the length j/NULiC1of the chain.
MATRIX -CHAIN -ORDER .p/
1nDp:length/NUL1
2l e t m Œ 1::n ;1::n /c141 andsŒ 1::n/NUL1 ;2::n /c141 be new tables
3foriD1ton
4 mŒi; i/c141D0
5forlD2ton //lis the chain length
6 foriD1ton/NULlC1
7 jDiCl/NUL1
8 mŒi; j /c141D1
9 forkDitoj/NUL1
10 qDmŒi; k/c141CmŒkC1; j /c141Cpi/NUL1pkpj
11 ifq<m Œ i ; j /c141
12 mŒi; j /c141Dq
13 sŒi;j/c141Dk
14return mands376 Chapter 15 Dynamic Programming
A6 A5 A4 A3 A2 A100000015,750 2,625 750 1,000 5,0007,875 4,375 2,500 3,5009,375 7,125 5,37511,875 10,50015,125
1234561
2
3
4
5
6jim
123451335333333
234561
2
3
4
5jis
Figure 15.5 Themandstables computed by M AT RIX -CHAIN -ORDER fornD6and the follow-
i n gm a t r i xd i m e n s i o n s :
matrix
 A1 A2 A3 A4 A5 A6
dimension
 30/STX35 35/STX15 15/STX55/STX10 10/STX20 20/STX25
The tables are rotated so that the main diagonal runs horizontally. The mtable uses only the main
diagonal and upper triangle, and the stable uses only the upper triangle. The minimum number of
scalar multiplications to multiply the 6 matrices is mŒ1; 6/c141D15,125. Of the darker entries, the pairs
that have the same shading are taken together in line 10 when computing
mŒ2; 5/c141Dmin8
ˆ<
ˆ:mŒ2; 2/c141CmŒ3; 5/c141Cp1p2p5D0C2500C35/SOH15/SOH20D13,000 ;
mŒ2; 3/c141CmŒ4; 5/c141Cp1p3p5D2625C1000C35/SOH5/SOH20D7125 ;
mŒ2; 4/c141CmŒ5; 5/c141Cp1p4p5D4375C0C35/SOH10/SOH20D11,375
D7125 :
The algorithm ﬁrst computes mŒi; i/c141D0foriD1 ;2;:::;n (the minimum
costs for chains of length 1) in lines 3–4. It then uses recurrence (15.7) to compute
mŒi; iC1/c141foriD1 ;2;:::;n/NUL1(the minimum costs for chains of length lD2)
during the ﬁrst execution of the forloop in lines 5–13. The second time through the
loop, it computes mŒi; iC2/c141foriD1 ;2;:::;n/NUL2(the minimum costs for chains of
length lD3), and so forth. At each step, the mŒi; j /c141 cost computed in lines 10–13
depends only on table entries mŒi; k/c141 andmŒkC1; j /c141 already computed.
Figure 15.5 illustrates this procedure on a chain of nD6matrices. Since
we have deﬁned mŒi; j /c141 only for i/DC4j, only the portion of the table mstrictly
above the main diagonal is used. The ﬁgure shows the table rotated to make themain diagonal run horizontally. The matrix chain is listed along the bottom. Us-ing this layout, we can ﬁnd the minimum cost mŒi; j /c141 for multiplying a subchain
A
iAiC1/SOH/SOH/SOHAjof matrices at the intersection of lines running northeast from Aiand15.2 Matrix-chain multiplication 377
northwest from Aj. Each horizontal row in the table contains the entries for matrix
chains of the same length. M ATRIX -CHAIN -ORDER computes the rows from bot-
tom to top and from left to right within each row. It computes each entry mŒi; j /c141
using the products pi/NUL1pkpjforkDi;iC1 ;:::;j/NUL1and all entries southwest
and southeast from mŒi; j /c141 .
A simple inspection of the nested loop structure of M ATRIX -CHAIN -ORDER
yields a running time of O.n3/for the algorithm. The loops are nested three deep,
and each loop index ( l,i,a n d k) takes on at most n/NUL1values. Exercise 15.2-5 asks
you to show that the running time of this algorithm is in fact also /DEL.n3/.T h e a l -
gorithm requires ‚.n2/space to store the mandstables. Thus, M ATRIX -CHAIN -
ORDER is much more efﬁcient than the exponential-time method of enumerating
all possible parenthesizations and checking each one.
Step 4: Constructing an optimal solution
Although M ATRIX -CHAIN -ORDER determines the optimal number of scalar mul-
tiplications needed to compute a matrix-chain product, it does not directly showhow to multiply the matrices. The table sŒ1:: n/NUL1 ;2::n /c141 gives us the informa-
tion we need to do so. Each entry sŒi;j/c141 records a value of ksuch that an op-
timal parenthesization of A
iAiC1/SOH/SOH/SOHAjsplits the product between AkandAkC1.
Thus, we know that the ﬁnal matrix multiplication in computing A1::noptimally
isA1::sŒ1;n/c141 AsŒ1;n/c141 C1::n. We can determine the earlier matrix multiplications recur-
sively, since sŒ1;sŒ1;n/c141/c141 determines the last matrix multiplication when computing
A1::sŒ1;n/c141 andsŒsŒ1;n/c141C1; n/c141 determines the last matrix multiplication when com-
puting AsŒ1;n/c141 C1::n. The following recursive procedure prints an optimal parenthe-
sization ofhAi;AiC1;:::;A ji,g i v e nt h e stable computed by M ATRIX -CHAIN -
ORDER and the indices iandj. The initial call P RINT -OPTIMAL -PARENS . s ;1 ;n /
prints an optimal parenthesization of hA1;A2;:::;A ni.
PRINT -OPTIMAL -PARENS . s ;i;j/
1ifi==j
2 print “ A”i
3elseprint “(”
4P RINT -OPTIMAL -PARENS . s ;i;sŒ i;j/c141 /
5P RINT -OPTIMAL -PARENS . s ;sŒ i;j/c141C1; j /
6 print “)”
In the example of Figure 15.5, the call P RINT -OPTIMAL -PARENS . s ;1 ;6 / prints
the parenthesization ..A 1.A2A3//..A 4A5/A6//.378 Chapter 15 Dynamic Programming
Exercises
15.2-1
Find an optimal parenthesization of a matrix-chain product whose sequence ofdimensions ish5; 10; 3; 12; 5; 50; 6i.
15.2-2
Give a recursive algorithm M
ATRIX -CHAIN -MULTIPLY . A ;s ;i;j/ that actually
performs the optimal matrix-chain multiplication, given the sequence of matriceshA
1;A2;:::;A ni,t h estable computed by M ATRIX -CHAIN -ORDER , and the in-
dices iandj. (The initial call would be M ATRIX -CHAIN -MULTIPLY . A ;s ;1 ;n / .)
15.2-3
Use the substitution method to show that the solution to the recurrence (15.6)is/DEL.2
n/.
15.2-4
Describe the subproblem graph for matrix-chain multiplication with an input chainof length n. How many vertices does it have? How many edges does it have, and
which edges are they?
15.2-5
LetR.i;j/ be the number of times that table entry mŒi; j /c141 is referenced while
computing other table entries in a call of M
ATRIX -CHAIN -ORDER . Show that the
total number of references for the entire table is
nX
iD1nX
jDiR.i;j/Dn3/NULn
3:
(Hint: You may ﬁnd equation (A.3) useful.)
15.2-6
Show that a full parenthesization of an n-element expression has exactly n/NUL1pairs
of parentheses.
15.3 Elements of dynamic programming
Although we have just worked through two examples of the dynamic-programming
method, you might still be wondering just when the method applies. From an en-
gineering perspective, when should we look for a dynamic-programming solutionto a problem? In this section, we examine the two key ingredients that an opti-15.3 Elements of dynamic programming 379
mization problem must have in order for dynamic programming to apply: optimal
substructure and overlapping subproblems. We also revisit and discuss more fullyhow memoization might help us take advantage of the overlapping-subproblemsproperty in a top-down recursive approach.
Optimal substructure
The ﬁrst step in solving an optimization problem by dynamic programming is to
characterize the structure of an optimal solution. Recall that a problem exhibitsoptimal substructure if an optimal solution to the problem contains within it opti-
mal solutions to subproblems. Whenever a problem exhibits optimal substructure,we have a good clue that dynamic programming might apply. (As Chapter 16 dis-cusses, it also might mean that a greedy strategy applies, however.) In dynamic
programming, we build an optimal solution to the problem from optimal solutions
to subproblems. Consequently, we must take care to ensure that the range of sub-problems we consider includes those used in an optimal solution.
We discovered optimal substructure in both of the problems we have examined
in this chapter so far. In Section 15.1, we observed that the optimal way of cut-ting up a rod of length n(if we make any cuts at all) involves optimally cutting
up the two pieces resulting from the ﬁrst cut. In Section 15.2, we observed that
an optimal parenthesization of A
iAiC1/SOH/SOH/SOHAjthat splits the product between Ak
andAkC1contains within it optimal solutions to the problems of parenthesizing
AiAiC1/SOH/SOH/SOHAkandAkC1AkC2/SOH/SOH/SOHAj.
You will ﬁnd yourself following a common pattern in discovering optimal sub-
structure:
1. You show that a solution to the problem consists of making a choice, such as
choosing an initial cut in a rod or choosing an index at which to split the matrixchain. Making this choice leaves one or more subproblems to be solved.
2. You suppose that for a given problem, you are given the choice that leads to an
optimal solution. You do not concern yourself yet with how to determine thischoice. You just assume that it has been given to you.
3. Given this choice, you determine which subproblems ensue and how to best
characterize the resulting space of subproblems.
4. You show that the solutions to the subproblems used within an optimal solution
to the problem must themselves be optimal by using a “cut-and-paste” tech-nique. You do so by supposing that each of the subproblem solutions is notoptimal and then deriving a contradiction. In particular, by “cutting out” thenonoptimal solution to each subproblem and “pasting in” the optimal one, youshow that you can get a better solution to the original problem, thus contradict-ing your supposition that you already had an optimal solution. If an optimal380 Chapter 15 Dynamic Programming
solution gives rise to more than one subproblem, they are typically so similar
that you can modify the cut-and-paste argument for one to apply to the otherswith little effort.
To characterize the space of subproblems, a good rule of thumb says to try to
keep the space as simple as possible and then expand it as necessary. For example,the space of subproblems that we considered for the rod-cutting problem containedthe problems of optimally cutting up a rod of length ifor each size i. This sub-
problem space worked well, and we had no need to try a more general space ofsubproblems.
Conversely, suppose that we had tried to constrain our subproblem space for
matrix-chain multiplication to matrix products of the form A
1A2/SOH/SOH/SOHAj. As before,
an optimal parenthesization must split this product between AkandAkC1for some
1/DC4k<j . Unless we could guarantee that kalways equals j/NUL1, we would ﬁnd
that we had subproblems of the form A1A2/SOH/SOH/SOHAkandAkC1AkC2/SOH/SOH/SOHAj,a n dt h a t
the latter subproblem is not of the form A1A2/SOH/SOH/SOHAj. For this problem, we needed
to allow our subproblems to vary at “both ends,” that is, to allow both iandjto
vary in the subproblem AiAiC1/SOH/SOH/SOHAj.
Optimal substructure varies across problem domains in two ways:
1. how many subproblems an optimal solution to the original problem uses, and
2. how many choices we have in determining which subproblem(s) to use in an
optimal solution.
In the rod-cutting problem, an optimal solution for cutting up a rod of size n
uses just one subproblem (of size n/NULi), but we must consider nchoices for i
in order to determine which one yields an optimal solution. Matrix-chain mul-tiplication for the subchain A
iAiC1/SOH/SOH/SOHAjserves as an example with two sub-
problems and j/NULichoices. For a given matrix Akat which we split the prod-
uct, we have two subproblems—parenthesizing AiAiC1/SOH/SOH/SOHAkand parenthesizing
AkC1AkC2/SOH/SOH/SOHAj—and we must solve both of them optimally. Once we determine
the optimal solutions to subproblems, we choose from among j/NULicandidates for
the index k.
Informally, the running time of a dynamic-programming algorithm depends on
the product of two factors: the number of subproblems overall and how manychoices we look at for each subproblem. In rod cutting, we had ‚.n/ subproblems
overall, and at most nchoices to examine for each, yielding an O.n
2/running time.
Matrix-chain multiplication had ‚.n2/subproblems overall, and in each we had at
most n/NUL1choices, giving an O.n3/running time (actually, a ‚.n3/running time,
by Exercise 15.2-5).
Usually, the subproblem graph gives an alternative way to perform the same
analysis. Each vertex corresponds to a subproblem, and the choices for a sub-15.3 Elements of dynamic programming 381
problem are the edges incident to that subproblem. Recall that in rod cutting,
the subproblem graph had nvertices and at most nedges per vertex, yielding an
O.n2/running time. For matrix-chain multiplication, if we were to draw the sub-
problem graph, it would have ‚.n2/vertices and each vertex would have degree at
most n/NUL1, giving a total of O.n3/vertices and edges.
Dynamic programming often uses optimal substructure in a bottom-up fashion.
That is, we ﬁrst ﬁnd optimal solutions to subproblems and, having solved the sub-problems, we ﬁnd an optimal solution to the problem. Finding an optimal solu-
tion to the problem entails making a choice among subproblems as to which we
will use in solving the problem. The cost of the problem solution is usually thesubproblem costs plus a cost that is directly attributable to the choice itself. Inrod cutting, for example, ﬁrst we solved the subproblems of determining optimalways to cut up rods of length iforiD0; 1; : : : ; n/NUL1, and then we determined
which such subproblem yielded an optimal solution for a rod of length n,u s i n g
equation (15.2). The cost attributable to the choice itself is the term p
iin equa-
tion (15.2). In matrix-chain multiplication, we determined optimal parenthesiza-
tions of subchains of AiAiC1/SOH/SOH/SOHAj, and then we chose the matrix Akat which to
split the product. The cost attributable to the choice itself is the term pi/NUL1pkpj.
In Chapter 16, we shall examine “greedy algorithms,” which have many similar-
ities to dynamic programming. In particular, problems to which greedy algorithmsapply have optimal substructure. One major difference between greedy algorithmsand dynamic programming is that instead of ﬁrst ﬁnding optimal solutions to sub-
problems and then making an informed choice, greedy algorithms ﬁrst make a
“greedy” choice—the choice that looks best at the time—and then solve a resultingsubproblem, without bothering to solve all possible related smaller subproblems.Surprisingly, in some cases this strategy works!
Subtleties
You should be careful not to assume that optimal substructure applies when it doesnot. Consider the following two problems in which we are given a directed graphGD.V; E/ and vertices u; /ETB2V.
Unweighted shortest path:
3Find a path from uto/ETBconsisting of the fewest
edges. Such a path must be simple, since removing a cycle from a path pro-duces a path with fewer edges.
3We use the term “unweighted” to distinguish this problem from that of ﬁnding shortest paths with
weighted edges, which we shall see in Chapters 24 and 25. We can use the breadth-ﬁrst searchtechnique of Chapter 22 to solve the unweighted problem.382 Chapter 15 Dynamic Programming
q r
s t
Figure 15.6 A directed graph showing that the problem of ﬁnding a longest simple path in an
unweighted directed graph does not have optimal substructure. The path q!r!tis a longest
simple path from qtot, but the subpath q!ris not a longest simple path from qtor, nor is the
subpath r!ta longest simple path from rtot.
Unweighted longest simple path: Find a simple path from uto/ETBconsisting of
the most edges. We need to include the requirement of simplicity because other-
wise we can traverse a cycle as many times as we like to create paths with anarbitrarily large number of edges.
The unweighted shortest-path problem exhibits optimal substructure, as follows.
Suppose that u¤/ETB, so that the problem is nontrivial. Then, any path pfrom u
to/ETBmust contain an intermediate vertex, say w. (Note that wmay be uor/ETB.)
Thus, we can decompose the path up;/ETBinto subpaths up1;wp2;/ETB. Clearly, the
number of edges in pequals the number of edges in p1plus the number of edges
inp2. We claim that if pis an optimal (i.e., shortest) path from uto/ETB,t h e n p1
must be a shortest path from utow. Why? We use a “cut-and-paste” argument:
if there were another path, say p0
1, from utowwith fewer edges than p1,t h e nw e
could cut out p1and paste in p0
1to produce a path up0
1;wp2;/ETBwith fewer edges
thanp, thus contradicting p’s optimality. Symmetrically, p2must be a shortest
path from wto/ETB. Thus, we can ﬁnd a shortest path from uto/ETBby considering
all intermediate vertices w, ﬁnding a shortest path from utowand a shortest path
from wto/ETB, and choosing an intermediate vertex wthat yields the overall shortest
path. In Section 25.2, we use a variant of this observation of optimal substructureto ﬁnd a shortest path between every pair of vertices on a weighted, directed graph.
You might be tempted to assume that the problem of ﬁnding an unweighted
longest simple path exhibits optimal substructure as well. After all, if we decom-
pose a longest simple path up;/ETBinto subpaths up1;wp2;/ETB, then mustn’t p1
be a longest simple path from utow, and mustn’t p2be a longest simple path
from wto/ETB? The answer is no! Figure 15.6 supplies an example. Consider the
pathq!r!t, which is a longest simple path from qtot.I sq!ra longest
simple path from qtor? No, for the path q!s!t!ri sas i m p l ep a t h
that is longer. Is r!ta longest simple path from rtot? No again, for the path
r!q!s!tis a simple path that is longer.15.3 Elements of dynamic programming 383
This example shows that for longest simple paths, not only does the problem
lack optimal substructure, but we cannot necessarily assemble a “legal” solutionto the problem from solutions to subproblems. If we combine the longest simplepaths q!s!t!randr!q!s!t, we get the path q!s!t!r!
q!s!t, which is not simple. Indeed, the problem of ﬁnding an unweighted
longest simple path does not appear to have any sort of optimal substructure. Noefﬁcient dynamic-programming algorithm for this problem has ever been found. Infact, this problem is NP-complete, which—as we shall see in Chapter 34—means
that we are unlikely to ﬁnd a way to solve it in polynomial time.
Why is the substructure of a longest simple path so different from that of a short-
est path? Although a solution to a problem for both longest and shortest paths usestwo subproblems, the subproblems in ﬁnding the longest simple path are not inde-
pendent , whereas for shortest paths they are. What do we mean by subproblems
being independent? We mean that the solution to one subproblem does not affectthe solution to another subproblem of the same problem. For the example of Fig-ure 15.6, we have the problem of ﬁnding a longest simple path from qtotwith two
subproblems: ﬁnding longest simple paths from qtorand from rtot. For the ﬁrst
of these subproblems, we choose the path q!s!t!r, and so we have also
used the vertices sandt. We can no longer use these vertices in the second sub-
problem, since the combination of the two solutions to subproblems would yield apath that is not simple. If we cannot use vertex tin the second problem, then we
cannot solve it at all, since tis required to be on the path that we ﬁnd, and it is
not the vertex at which we are “splicing” together the subproblem solutions (that
vertex being r). Because we use vertices sandtin one subproblem solution, we
cannot use them in the other subproblem solution. We must use at least one of themto solve the other subproblem, however, and we must use both of them to solve itoptimally. Thus, we say that these subproblems are not independent. Looked atanother way, using resources in solving one subproblem (those resources beingvertices) renders them unavailable for the other subproblem.
Why, then, are the subproblems independent for ﬁnding a shortest path? The
answer is that by nature, the subproblems do not share resources. We claim thatif a vertex wis on a shortest path pfrom uto/ETB, then we can splice together any
shortest path u
p1;wandanyshortest path wp2;/ETBto produce a shortest path from u
to/ETB. We are assured that, other than w, no vertex can appear in both paths p1
andp2. Why? Suppose that some vertex x¤wappears in both p1andp2,s ot h a t
we can decompose p1asupux;x;wandp2asw;xpx/ETB;/ETB. By the optimal
substructure of this problem, path phas as many edges as p1andp2together; let’s
say that phaseedges. Now let us construct a path p0Dupux;xpx/ETB;/ETBfrom uto/ETB.
Because we have excised the paths from xtowand from wtox, each of which
contains at least one edge, path p0contains at most e/NUL2edges, which contradicts384 Chapter 15 Dynamic Programming
the assumption that pis a shortest path. Thus, we are assured that the subproblems
for the shortest-path problem are independent.
Both problems examined in Sections 15.1 and 15.2 have independent subprob-
lems. In matrix-chain multiplication, the subproblems are multiplying subchainsA
iAiC1/SOH/SOH/SOHAkandAkC1AkC2/SOH/SOH/SOHAj. These subchains are disjoint, so that no ma-
trix could possibly be included in both of them. In rod cutting, to determine thebest way to cut up a rod of length n, we look at the best ways of cutting up rods
of length iforiD0; 1; : : : ; n/NUL1. Because an optimal solution to the length- n
problem includes just one of these subproblem solutions (after we have cut off the
ﬁrst piece), independence of subproblems is not an issue.
Overlapping subproblems
The second ingredient that an optimization problem must have for dynamic pro-
gramming to apply is that the space of subproblems must be “small” in the sensethat a recursive algorithm for the problem solves the same subproblems over andover, rather than always generating new subproblems. Typically, the total numberof distinct subproblems is a polynomial in the input size. When a recursive algo-rithm revisits the same problem repeatedly, we say that the optimization problemhasoverlapping subproblems .
4In contrast, a problem for which a divide-and-
conquer approach is suitable usually generates brand-new problems at each step
of the recursion. Dynamic-programming algorithms typically take advantage of
overlapping subproblems by solving each subproblem once and then storing thesolution in a table where it can be looked up when needed, using constant time perlookup.
In Section 15.1, we brieﬂy examined how a recursive solution to rod cut-
ting makes exponentially many calls to ﬁnd solutions of smaller subproblems.
Our dynamic-programming solution takes an exponential-time recursive algorithm
down to quadratic time.
To illustrate the overlapping-subproblems property in greater detail, let us re-
examine the matrix-chain multiplication problem. Referring back to Figure 15.5,observe that M
ATRIX -CHAIN -ORDER repeatedly looks up the solution to subprob-
lems in lower rows when solving subproblems in higher rows. For example, itreferences entry mŒ3; 4/c141 four times: during the computations of mŒ2; 4/c141 ,mŒ1; 4/c141 ,
4It may seem strange that dynamic programming relies on subproblems being both independent
and overlapping. Although these requirements may sound contradictory, they describe two differentnotions, rather than two points on the same axis. Two subproblems of the same problem are inde-
pendent if they do not share resources. Two subproblems are overlapping if they are really the same
subproblem that occurs as a subproblem of different problems.15.3 Elements of dynamic programming 385
1..4
1..1 2..4 1..2 3..4 1..3 4..4
2..2 3..4 2..3 4..4 1..1 2..2 3..3 4..4 1..1 2..3 1..2 3..33..3 4..4 2..2 3..3 2..2 3..3 1..1 2..2
Figure 15.7 The recursion tree for the computation of R ECURSIVE -MAT RIX -CHAIN. p ;1 ;4 / .
Each node contains the parameters iandj. The computations performed in a shaded subtree are
replaced by a single table lookup in M EMOIZED -MAT RIX -CHAIN .
mŒ3; 5/c141 ,a n d mŒ3; 6/c141 . If we were to recompute mŒ3; 4/c141 each time, rather than just
looking it up, the running time would increase dramatically. To see how, considerthe following (inefﬁcient) recursive procedure that determines mŒi; j /c141 , the mini-
mum number of scalar multiplications needed to compute the matrix-chain productA
i:: jDAiAiC1/SOH/SOH/SOHAj. The procedure is based directly on the recurrence (15.7).
RECURSIVE -MATRIX -CHAIN . p;i;j/
1ifi==j
2 return 0
3mŒi; j /c141D1
4forkDitoj/NUL1
5 qDRECURSIVE -MATRIX -CHAIN . p;i;k/
CRECURSIVE -MATRIX -CHAIN .p; kC1; j /
Cpi/NUL1pkpj
6 ifq<m Œ i ; j /c141
7 mŒi; j /c141Dq
8return mŒi; j /c141
Figure 15.7 shows the recursion tree produced by the call R ECURSIVE -MATRIX -
CHAIN . p;1 ;4 / . Each node is labeled by the values of the parameters iandj.
Observe that some pairs of values occur many times.
In fact, we can show that the time to compute mŒ1; n/c141 by this recursive proce-
dure is at least exponential in n.L e t T .n/ denote the time taken by R ECURSIVE -
MATRIX -CHAIN to compute an optimal parenthesization of a chain of nmatrices.
Because the execution of lines 1–2 and of lines 6–7 each take at least unit time, as386 Chapter 15 Dynamic Programming
does the multiplication in line 5, inspection of the procedure yields the recurrence
T. 1 //NAK1;
T .n//NAK1Cn/NUL1X
kD1.T .k/CT. n/NULk/C1/ forn>1:
Noting that for iD1 ;2;:::;n/NUL1, each term T. i/ appears once as T. k/ and once
asT. n/NULk/, and collecting the n/NUL11s in the summation together with the 1out
front, we can rewrite the recurrence as
T .n//NAK2n/NUL1X
iD1T. i/Cn: (15.8)
We shall prove that T .n/D/DEL.2n/using the substitution method. Speciﬁ-
cally, we shall show that T .n//NAK2n/NUL1for all n/NAK1. The basis is easy, since
T. 1 //NAK1D20. Inductively, for n/NAK2we have
T .n//NAK2n/NUL1X
iD12i/NUL1Cn
D2n/NUL2X
iD02iCn
D2.2n/NUL1/NUL1/Cn(by equation (A.5))
D2n/NUL2Cn
/NAK2n/NUL1;
which completes the proof. Thus, the total amount of work performed by the call
RECURSIVE -MATRIX -CHAIN . p;1 ;n / is at least exponential in n.
Compare this top-down, recursive algorithm (without memoization) with the
bottom-up dynamic-programming algorithm. The latter is more efﬁcient because
it takes advantage of the overlapping-subproblems property. Matrix-chain mul-
tiplication has only ‚.n2/distinct subproblems, and the dynamic-programming
algorithm solves each exactly once. The recursive algorithm, on the other hand,
must again solve each subproblem every time it reappears in the recursion tree.Whenever a recursion tree for the natural recursive solution to a problem contains
the same subproblem repeatedly, and the total number of distinct subproblems is
small, dynamic programming can improve efﬁciency, sometimes dramatically.15.3 Elements of dynamic programming 387
Reconstructing an optimal solution
As a practical matter, we often store which choice we made in each subproblem in
a table so that we do not have to reconstruct this information from the costs that westored.
For matrix-chain multiplication, the table sŒi;j/c141 saves us a signiﬁcant amount of
work when reconstructing an optimal solution. Suppose that we did not maintainthesŒi;j/c141 table, having ﬁlled in only the table mŒi; j /c141 containing optimal subprob-
lem costs. We choose from among j/NULipossibilities when we determine which
subproblems to use in an optimal solution to parenthesizing A
iAiC1/SOH/SOH/SOHAj,a n d
j/NULiis not a constant. Therefore, it would take ‚.j/NULi/D!.1/ time to recon-
struct which subproblems we chose for a solution to a given problem. By storinginsŒi;j/c141 the index of the matrix at which we split the product A
iAiC1/SOH/SOH/SOHAj,w e
can reconstruct each choice in O.1/ time.
Memoization
As we saw for the rod-cutting problem, there is an alternative approach to dy-
namic programming that often offers the efﬁciency of the bottom-up dynamic-programming approach while maintaining a top-down strategy. The idea is to
memoize the natural, but inefﬁcient, recursive algorithm. As in the bottom-up ap-
proach, we maintain a table with subproblem solutions, but the control structurefor ﬁlling in the table is more like the recursive algorithm.
A memoized recursive algorithm maintains an entry in a table for the solution to
each subproblem. Each table entry initially contains a special value to indicate thatthe entry has yet to be ﬁlled in. When the subproblem is ﬁrst encountered as therecursive algorithm unfolds, its solution is computed and then stored in the table.Each subsequent time that we encounter this subproblem, we simply look up thevalue stored in the table and return it.
5
Here is a memoized version of R ECURSIVE -MATRIX -CHAIN . Note where it
resembles the memoized top-down method for the rod-cutting problem.
5This approach presupposes that we know the set of all possible subproblem parameters and that we
have established the relationship between table positions and subproblems. Another, more general,
approach is to memoize by using hashing with the subproblem parameters as keys.388 Chapter 15 Dynamic Programming
MEMOIZED -MATRIX -CHAIN .p/
1nDp:length/NUL1
2l e t m Œ 1::n;1::n /c141 be a new table
3foriD1ton
4 forjDiton
5 mŒi; j /c141D1
6return LOOKUP -CHAIN . m;p;1 ;n /
LOOKUP -CHAIN . m;p;i;j/
1ifmŒi; j /c141 <1
2 return mŒi; j /c141
3ifi==j
4 mŒi; j /c141D0
5else for kDitoj/NUL1
6 qDLOOKUP -CHAIN . m;p;i;k/
CLOOKUP -CHAIN . m;p;kC1; j /Cpi/NUL1pkpj
7 ifq<m Œ i ; j /c141
8 mŒi; j /c141Dq
9return mŒi; j /c141
The M EMOIZED -MATRIX -CHAIN procedure, like M ATRIX -CHAIN -ORDER ,
maintains a table m Œ 1::n ;1::n /c141 of computed values of mŒi; j /c141 , the minimum num-
ber of scalar multiplications needed to compute the matrix Ai::j. Each table entry
initially contains the value 1to indicate that the entry has yet to be ﬁlled in. Upon
calling L OOKUP -CHAIN . m;p;i;j/ , if line 1 ﬁnds that mŒi; j /c141 <1, then the pro-
cedure simply returns the previously computed cost mŒi; j /c141 in line 2. Otherwise,
the cost is computed as in R ECURSIVE -MATRIX -CHAIN , stored in mŒi; j /c141 ,a n d
returned. Thus, L OOKUP -CHAIN . m;p;i;j/ always returns the value of mŒi; j /c141 ,
but it computes it only upon the ﬁrst call of L OOKUP -CHAIN with these speciﬁc
values of iandj.
Figure 15.7 illustrates how M EMOIZED -MATRIX -CHAIN saves time compared
with R ECURSIVE -MATRIX -CHAIN . Shaded subtrees represent values that it looks
up rather than recomputes.
Like the bottom-up dynamic-programming algorithm M ATRIX -CHAIN -ORDER ,
the procedure M EMOIZED -MATRIX -CHAIN runs in O.n3/time. Line 5 of
MEMOIZED -MATRIX -CHAIN executes ‚.n2/times. We can categorize the calls
of L OOKUP -CHAIN into two types:
1. calls in which mŒi; j /c141D1 , so that lines 3–9 execute, and
2. calls in which mŒi; j /c141 <1,s ot h a tL OOKUP -CHAIN simply returns in line 2.15.3 Elements of dynamic programming 389
There are ‚.n2/calls of the ﬁrst type, one per table entry. All calls of the sec-
ond type are made as recursive calls by calls of the ﬁrst type. Whenever a givencall of L
OOKUP -CHAIN makes recursive calls, it makes O.n/ of them. There-
fore, there are O.n3/calls of the second type in all. Each call of the second type
takes O.1/ time, and each call of the ﬁrst type takes O.n/ time plus the time spent
in its recursive calls. The total time, therefore, is O.n3/. Memoization thus turns
an/DEL.2n/-time algorithm into an O.n3/-time algorithm.
In summary, we can solve the matrix-chain multiplication problem by either a
top-down, memoized dynamic-programming algorithm or a bottom-up dynamic-
programming algorithm in O.n3/time. Both methods take advantage of the
overlapping-subproblems property. There are only ‚.n2/distinct subproblems in
total, and either of these methods computes the solution to each subproblem onlyonce. Without memoization, the natural recursive algorithm runs in exponentialtime, since solved subproblems are repeatedly solved.
In general practice, if all subproblems must be solved at least once, a bottom-up
dynamic-programming algorithm usually outperforms the corresponding top-down
memoized algorithm by a constant factor, because the bottom-up algorithm has no
overhead for recursion and less overhead for maintaining the table. Moreover, for
some problems we can exploit the regular pattern of table accesses in the dynamic-programming algorithm to reduce time or space requirements even further. Alter-natively, if some subproblems in the subproblem space need not be solved at all,the memoized solution has the advantage of solving only those subproblems that
are deﬁnitely required.
Exercises
15.3-1
Which is a more efﬁcient way to determine the optimal number of multiplicationsin a matrix-chain multiplication problem: enumerating all the ways of parenthesiz-ing the product and computing the number of multiplications for each, or running
R
ECURSIVE -MATRIX -CHAIN ? Justify your answer.
15.3-2
Draw the recursion tree for the M ERGE -SORT procedure from Section 2.3.1 on an
array of 16elements. Explain why memoization fails to speed up a good divide-
and-conquer algorithm such as M ERGE -SORT.
15.3-3
Consider a variant of the matrix-chain multiplication problem in which the goal isto parenthesize the sequence of matrices so as to maximize, rather than minimize,390 Chapter 15 Dynamic Programming
the number of scalar multiplications. Does this problem exhibit optimal substruc-
ture?
15.3-4
As stated, in dynamic programming we ﬁrst solve the subproblems and then choosewhich of them to use in an optimal solution to the problem. Professor Capuletclaims that we do not always need to solve all the subproblems in order to ﬁnd anoptimal solution. She suggests that we can ﬁnd an optimal solution to the matrix-chain multiplication problem by always choosing the matrix A
kat which to split
the subproduct AiAiC1/SOH/SOH/SOHAj(by selecting kto minimize the quantity pi/NUL1pkpj)
before solving the subproblems. Find an instance of the matrix-chain multiplica-
tion problem for which this greedy approach yields a suboptimal solution.
15.3-5
Suppose that in the rod-cutting problem of Section 15.1, we also had limit lion the
number of pieces of length ithat we are allowed to produce, for iD1 ;2;:::;n .
Show that the optimal-substructure property described in Section 15.1 no longerholds.
15.3-6
Imagine that you wish to exchange one currency for another. You realize thatinstead of directly exchanging one currency for another, you might be better offmaking a series of trades through other currencies, winding up with the currencyyou want. Suppose that you can trade ndifferent currencies, numbered 1 ;2;:::;n ,
where you start with currency 1and wish to wind up with currency n.Y o u a r e
given, for each pair of currencies iandj, an exchange rate r
ij, meaning that if
you start with dunits of currency i, you can trade for drijunits of currency j.
A sequence of trades may entail a commission, which depends on the number oftrades you make. Let c
kbe the commission that you are charged when you make k
trades. Show that, if ckD0for all kD1 ;2;:::;n , then the problem of ﬁnding the
best sequence of exchanges from currency 1to currency nexhibits optimal sub-
structure. Then show that if commissions ckare arbitrary values, then the problem
of ﬁnding the best sequence of exchanges from currency 1to currency ndoes not
necessarily exhibit optimal substructure.
15.4 Longest common subsequence
Biological applications often need to compare the DNA of two (or more) dif-ferent organisms. A strand of DNA consists of a string of molecules called15.4 Longest common subsequence 391
bases , where the possible bases are adenine, guanine, cytosine, and thymine.
Representing each of these bases by its initial letter, we can express a strandof DNA as a string over the ﬁnite set fA;C;G;Tg. (See Appendix C for
the deﬁnition of a string.) For example, the DNA of one organism may beS
1DACCGGTCGAGTGCGCGGAAGCCGGCCGAA , and the DNA of another organ-
ism may be S2DGTCGTTCGGAATGCCGTTGCTCTGTAAA . One reason to com-
pare two strands of DNA is to determine how “similar” the two strands are, as somemeasure of how closely related the two organisms are. We can, and do, deﬁne sim-
ilarity in many different ways. For example, we can say that two DNA strands are
similar if one is a substring of the other. (Chapter 32 explores algorithms to solvethis problem.) In our example, neither S
1norS2is a substring of the other. Alter-
natively, we could say that two strands are similar if the number of changes neededto turn one into the other is small. (Problem 15-5 looks at this notion.) Yet anotherway to measure the similarity of strands S
1andS2is by ﬁnding a third strand S3
in which the bases in S3appear in each of S1andS2; these bases must appear
in the same order, but not necessarily consecutively. The longer the strand S3we
can ﬁnd, the more similar S1andS2are. In our example, the longest strand S3is
GTCGTCGGAAGCCGGCCGAA .
We formalize this last notion of similarity as the longest-common-subsequence
problem. A subsequence of a given sequence is just the given sequence with zero or
more elements left out. Formally, given a sequence XDhx1;x2;:::;x mi, another
sequence ZDh´1;´2;:::;´ kiis asubsequence ofXif there exists a strictly
increasing sequence hi1;i2;:::;i kiof indices of Xsuch that for all jD1 ;2;:::;k ,
we have xijD´j. For example, ZDhB; C; D; Biis a subsequence of XD
hA;B;C;B;D;A;B iwith corresponding index sequence h2; 3; 5; 7i.
Given two sequences XandY, we say that a sequence Zis acommon sub-
sequence ofXandYifZis a subsequence of both XandY. For example, if
XDhA;B;C;B;D;A;B iandYDhB;D;C;A;B;Ai, the sequencehB;C;Aiis
a common subsequence of both XandY. The sequencehB;C;Aiis not a longest
common subsequence (LCS) of XandY, however, since it has length 3and the
sequencehB;C;B;Ai, which is also common to both XandY, has length 4.T h e
sequencehB; C; B; Aiis an LCS of XandY, as is the sequence hB; D; A; Bi,
since XandYhave no common subsequence of length 5or greater.
In the longest-common-subsequence problem , we are given two sequences
XDhx1;x2;:::;x miandYDhy1;y2;:::;y nia n dw i s ht oﬁ n dam a x i m u m -
length common subsequence of XandY. This section shows how to efﬁciently
solve the LCS problem using dynamic programming.392 Chapter 15 Dynamic Programming
Step 1: Characterizing a longest common subsequence
In a brute-force approach to solving the LCS problem, we would enumerate all
subsequences of Xand check each subsequence to see whether it is also a subse-
quence of Y, keeping track of the longest subsequence we ﬁnd. Each subsequence
ofXcorresponds to a subset of the indices f1 ;2;:::;mgofX. Because Xhas2m
subsequences, this approach requires exponential time, making it impractical for
long sequences.
The LCS problem has an optimal-substructure property, however, as the follow-
ing theorem shows. As we shall see, the natural classes of subproblems corre-spond to pairs of “preﬁxes” of the two input sequences. To be precise, given asequence XDhx
1;x2;:::;x mi,w ed e ﬁ n et h e ithpreﬁx ofX,f o riD0; 1; : : : ; m ,
asXiDhx1;x2;:::;x ii. For example, if XDhA; B; C; B; D; A; B i,t h e n
X4DhA; B; C; BiandX0is the empty sequence.
Theorem 15.1 (Optimal substructure of an LCS)
LetXDhx1;x2;:::;x miandYDhy1;y2;:::;y nibe sequences, and let ZD
h´1;´2;:::;´ kibe any LCS of XandY.
1. If xmDyn,t h e n ´kDxmDynandZk/NUL1is an LCS of Xm/NUL1andYn/NUL1.
2. If xm¤yn,t h e n ´k¤xmimplies that Zis an LCS of Xm/NUL1andY.
3. If xm¤yn,t h e n ´k¤ynimplies that Zis an LCS of XandYn/NUL1.
Proof (1) If ´k¤xm, then we could append xmDyntoZto obtain a common
subsequence of XandYof length kC1, contradicting the supposition that Zis
alongest common subsequence of XandY. Thus, we must have ´kDxmDyn.
Now, the preﬁx Zk/NUL1is a length- .k/NUL1/common subsequence of Xm/NUL1andYn/NUL1.
We wish to show that it is an LCS. Suppose for the purpose of contradictionthat there exists a common subsequence WofX
m/NUL1andYn/NUL1with length greater
thank/NUL1. Then, appending xmDyntoWproduces a common subsequence of
XandYwhose length is greater than k, which is a contradiction.
(2) If ´k¤xm,t h e n Zis a common subsequence of Xm/NUL1andY. If there were a
common subsequence WofXm/NUL1andYwith length greater than k,t h e n Wwould
also be a common subsequence of XmandY, contradicting the assumption that Z
is an LCS of XandY.
(3) The proof is symmetric to (2).
The way that Theorem 15.1 characterizes longest common subsequences tells
us that an LCS of two sequences contains within it an LCS of preﬁxes of the two
sequences. Thus, the LCS problem has an optimal-substructure property. A recur-15.4 Longest common subsequence 393
sive solution also has the overlapping-subproblems property, as we shall see in a
moment.
Step 2: A recursive solution
Theorem 15.1 implies that we should examine either one or two subproblems when
ﬁnding an LCS of XDhx1;x2;:::;x miandYDhy1;y2;:::;y ni.I fxmDyn,
we must ﬁnd an LCS of Xm/NUL1andYn/NUL1. Appending xmDynto this LCS yields
an LCS of XandY.I fxm¤yn, then we must solve two subproblems: ﬁnding an
LCS of Xm/NUL1andYand ﬁnding an LCS of XandYn/NUL1. Whichever of these two
LCSs is longer is an LCS of XandY. Because these cases exhaust all possibilities,
we know that one of the optimal subproblem solutions must appear within an LCSofXandY.
We can readily see the overlapping-subproblems property in the LCS problem.
To ﬁnd an LCS of XandY, we may need to ﬁnd the LCSs of XandY
n/NUL1and
ofXm/NUL1andY. But each of these subproblems has the subsubproblem of ﬁnding
an LCS of Xm/NUL1andYn/NUL1. Many other subproblems share subsubproblems.
As in the matrix-chain multiplication problem, our recursive solution to the LCS
problem involves establishing a recurrence for the value of an optimal solution.
Let us deﬁne cŒi;j/c141 to be the length of an LCS of the sequences XiandYj.I f
either iD0orjD0, one of the sequences has length 0, and so the LCS has
length 0. The optimal substructure of the LCS problem gives the recursive formula
cŒi;j/c141D/c128
0 ifiD0orjD0;
cŒi/NUL1; j/NUL1/c141C1 ifi;j > 0 andxiDyj;
max.cŒi; j/NUL1/c141; cŒi/NUL1; j /c141/ ifi;j > 0 andxi¤yj:(15.9)
Observe that in this recursive formulation, a condition in the problem restricts
which subproblems we may consider. When xiDyj, we can and should consider
the subproblem of ﬁnding an LCS of Xi/NUL1andYj/NUL1. Otherwise, we instead con-
sider the two subproblems of ﬁnding an LCS of XiandYj/NUL1and of Xi/NUL1andYj.I n
the previous dynamic-programming algorithms we have examined—for rod cuttingand matrix-chain multiplication—we ruled out no subproblems due to conditionsin the problem. Finding an LCS is not the only dynamic-programming algorithmthat rules out subproblems based on conditions in the problem. For example, theedit-distance problem (see Problem 15-5) has this characteristic.
Step 3: Computing the length of an LCS
Based on equation (15.9), we could easily write an exponential-time recursive al-
gorithm to compute the length of an LCS of two sequences. Since the LCS problem394 Chapter 15 Dynamic Programming
has only ‚.mn/ distinct subproblems, however, we can use dynamic programming
to compute the solutions bottom up.
Procedure LCS-L ENGTH takes two sequences XDhx1;x2; :::; x miand
YDhy1;y2;:::;y nias inputs. It stores the cŒi;j/c141 values in a table cŒ 0::m;0::n /c141 ,
and it computes the entries in row-major order. (That is, the procedure ﬁlls in the
ﬁrst row of cfrom left to right, then the second row, and so on.) The procedure also
maintains the table bŒ 1::m;1::n /c141 to help us construct an optimal solution. Intu-
itively, bŒi;j/c141 points to the table entry corresponding to the optimal subproblem
solution chosen when computing cŒi;j/c141 . The procedure returns the bandctables;
cŒm;n/c141 contains the length of an LCS of XandY.
LCS-L ENGTH .X; Y /
1mDX:length
2nDY:length
3l e t bŒ 1::m;1::n /c141 andcŒ 0::m ;0::n /c141 be new tables
4foriD1tom
5 cŒi;0/c141D0
6forjD0ton
7 cŒ0;j/c141D0
8foriD1tom
9 forjD1ton
10 ifxi==yj
11 cŒi;j/c141DcŒi/NUL1; j/NUL1/c141C1
12 bŒi;j/c141D“-”
13 elseif cŒi/NUL1; j /c141/NAKcŒi;j/NUL1/c141
14 cŒi;j/c141DcŒi/NUL1; j /c141
15 bŒi;j/c141D“"”
16 elsecŒi;j/c141DcŒi;j/NUL1/c141
17 bŒi;j/c141D“ ”
18return candb
Figure 15.8 shows the tables produced by LCS-L ENGTH on the sequences XD
hA; B; C; B; D; A; B iandYDhB; D; C; A; B; Ai. The running time of the
procedure is ‚.mn/ , since each table entry takes ‚.1/ time to compute.
Step 4: Constructing an LCS
Thebtable returned by LCS-L ENGTH enables us to quickly construct an LCS of
XDhx1;x2;:::;x miandYDhy1;y2;:::;y ni. We simply begin at bŒm;n/c141 and
trace through the table by following the arrows. Whenever we encounter a “ -”i n
entry bŒi;j/c141 , it implies that xiDyjis an element of the LCS that LCS-L ENGTH15.4 Longest common subsequence 395
0 0 0 0 0 0 0
0 0 0 0 1 1 1
0 1 1 1 2 2
0 1 1 2 2 2
0 1 1 2 2 3
0 1 2 2 2 3 3
0 1 2 2 3 3
0 1 2 2 3 4 41
2
3
4BDCABA123456 0
A
B
C
B
D
AB1
23
4
5670j
i
xiyj
Figure 15.8 Thecandbtables computed by LCS-L ENGTH on the sequences XDhA; B; C; B;
D;A;BiandYDhB;D;C;A;B;Ai. The square in row iand column jcontains the value of cŒi; j/c141
and the appropriate arrow for the value of bŒi;j/c141 .T h ee n t r y 4incŒ7;6/c141 —the lower right-hand corner
of the table—is the length of an LCS hB; C; B; AiofXandY.F o r i;j > 0 ,e n t r y cŒi;j/c141 depends
only on whether xiDyjand the values in entries cŒi/NUL1; j /c141,cŒi;j/NUL1/c141,a n d cŒi/NUL1; j/NUL1/c141,w h i c h
are computed before cŒi;j/c141 . To reconstruct the elements of an LCS, follow the bŒi;j/c141 arrows from
the lower right-hand corner; the sequence is shaded. Each “ -” on the shaded sequence corresponds
to an entry (highlighted) for which xiDyji sam e m b e ro fa nL C S .
found. With this method, we encounter the elements of this LCS in reverse order.
The following recursive procedure prints out an LCS of XandYin the proper,
forward order. The initial call is P RINT -LCS . b;X;X: length ;Y:length /.
PRINT -LCS . b;X;i;j/
1ifi==0orj==0
2 return
3ifbŒi;j/c141 ==“-”
4P RINT -LCS . b;X;i/NUL1; j/NUL1/
5 print xi
6elseif bŒi;j/c141 ==“"”
7P RINT -LCS . b;X;i/NUL1; j /
8elsePRINT -LCS . b;X;i;j/NUL1/
For the btable in Figure 15.8, this procedure prints BCBA . The procedure takes
timeO.mCn/, since it decrements at least one of iandjin each recursive call.396 Chapter 15 Dynamic Programming
Improving the code
Once you have developed an algorithm, you will often ﬁnd that you can improve
on the time or space it uses. Some changes can simplify the code and improveconstant factors but otherwise yield no asymptotic improvement in performance.Others can yield substantial asymptotic savings in time and space.
In the LCS algorithm, for example, we can eliminate the btable altogether. Each
cŒi;j/c141 entry depends on only three other ctable entries: cŒi/NUL1; j/NUL1/c141,cŒi/NUL1; j /c141,
andcŒi;j/NUL1/c141. Given the value of cŒi;j/c141 , we can determine in O.1/ time which of
these three values was used to compute cŒi;j/c141 , without inspecting table b. Thus, we
can reconstruct an LCS in O.mCn/time using a procedure similar to P
RINT -LCS.
(Exercise 15.4-2 asks you to give the pseudocode.) Although we save ‚.mn/ space
by this method, the auxiliary space requirement for computing an LCS does notasymptotically decrease, since we need ‚.mn/ space for the ctable anyway.
We can, however, reduce the asymptotic space requirements for LCS-L
ENGTH ,
since it needs only two rows of table cat a time: the row being computed and the
previous row. (In fact, as Exercise 15.4-4 asks you to show, we can use only slightlymore than the space for one row of cto compute the length of an LCS.) This
improvement works if we need only the length of an LCS; if we need to reconstruct
the elements of an LCS, the smaller table does not keep enough information to
retrace our steps in O.mCn/time.
Exercises
15.4-1
Determine an LCS of h1; 0; 0; 1; 0; 1; 0; 1iandh0; 1; 0; 1; 1; 0; 1; 1; 0 i.
15.4-2
Give pseudocode to reconstruct an LCS from the completed ctable and the original
sequences XDhx
1;x2;:::;x miandYDhy1;y2;:::;y niinO.mCn/time,
without using the btable.
15.4-3
Give a memoized version of LCS-L ENGTH that runs in O.mn/ time.
15.4-4
Show how to compute the length of an LCS using only 2/SOHmin.m; n/ entries in the c
table plus O.1/ additional space. Then show how to do the same thing, but using
min.m; n/ entries plus O.1/ additional space.15.5 Optimal binary search trees 397
15.4-5
Give an O.n2/-time algorithm to ﬁnd the longest monotonically increasing subse-
quence of a sequence of nnumbers.
15.4-6 ?
Give an O.n lgn/-time algorithm to ﬁnd the longest monotonically increasing sub-
sequence of a sequence of nnumbers. ( Hint: Observe that the last element of a
candidate subsequence of length iis at least as large as the last element of a can-
didate subsequence of length i/NUL1. Maintain candidate subsequences by linking
them through the input sequence.)
15.5 Optimal binary search trees
Suppose that we are designing a program to translate text from English to French.
For each occurrence of each English word in the text, we need to look up its Frenchequivalent. We could perform these lookup operations by building a binary searchtree with nEnglish words as keys and their French equivalents as satellite data.
Because we will search the tree for each individual word in the text, we want the
total time spent searching to be as low as possible. We could ensure an O.lgn/
search time per occurrence by using a red-black tree or any other balanced binarysearch tree. Words appear with different frequencies, however, and a frequentlyused word such as themay appear far from the root while a rarely used word such
asmachicolation appears near the root. Such an organization would slow down the
translation, since the number of nodes visited when searching for a key in a binarysearch tree equals one plus the depth of the node containing the key. We wantwords that occur frequently in the text to be placed nearer the root.
6Moreover,
some words in the text might have no French translation,7and such words would
not appear in the binary search tree at all. How do we organize a binary search treeso as to minimize the number of nodes visited in all searches, given that we knowhow often each word occurs?
What we need is known as an optimal binary search tree . Formally, we are
given a sequence KDhk
1;k2;:::;k niofndistinct keys in sorted order (so that
k1<k 2</SOH/SOH/SOH<k n), and we wish to build a binary search tree from these keys.
For each key ki, we have a probability pithat a search will be for ki.S o m e
searches may be for values not in K, and so we also have nC1“dummy keys”
6If the subject of the text is castle architecture, we might want machicolation to appear near the root.
7Yes, machicolation has a French counterpart: mˆachicoulis .398 Chapter 15 Dynamic Programming
k2
k1 k4
k3 k5 d0 d1
d2 d3 d4 d5
(a)k2
k1
k4
k3k5
d0 d1
d2 d3d4d5
(b)
Figure 15.9 Two binary search trees for a set of nD5keys with the following probabilities:
i
 012345
pi
 0.15 0.10 0.05 0.10 0.20
qi
0.05 0.10 0.05 0.05 0.05 0.10
(a)A binary search tree with expected search cost 2.80. (b)A binary search tree with expected search
cost 2.75. This tree is optimal.
d0;d1;d2;:::;d nrepresenting values not in K. In particular, d0represents all val-
ues less than k1,dnrepresents all values greater than kn,a n df o r iD1 ;2;:::;n/NUL1,
the dummy key direpresents all values between kiandkiC1. For each dummy
keydi, we have a probability qithat a search will correspond to di. Figure 15.9
shows two binary search trees for a set of nD5keys. Each key kiis an internal
node, and each dummy key diis a leaf. Every search is either successful (ﬁnding
some key ki) or unsuccessful (ﬁnding some dummy key di) ,a n ds ow eh a v e
nX
iD1piCnX
iD0qiD1: (15.10)
Because we have probabilities of searches for each key and each dummy key,
we can determine the expected cost of a search in a given binary search tree T.L e t
us assume that the actual cost of a search equals the number of nodes examined,i.e., the depth of the node found by the search in T, plus 1. Then the expected cost
of a search in Tis
EŒsearch cost in T/c141D nX
iD1.depthT.ki/C1//SOHpiCnX
iD0.depthT.di/C1//SOHqi
D1CnX
iD1depthT.ki//SOHpiCnX
iD0depthT.di//SOHqi; (15.11)15.5 Optimal binary search trees 399
where depthTdenotes a node’s depth in the tree T. The last equality follows from
equation (15.10). In Figure 15.9(a), we can calculate the expected search cost nodeby node:
node depth probability contribution
k1 1 0.15 0.30
k2 0 0.10 0.10
k3 2 0.05 0.15
k4 1 0.10 0.20
k5 2 0.20 0.60
d0 2 0.05 0.15
d1 2 0.10 0.30
d2 3 0.05 0.20
d3 3 0.05 0.20
d4 3 0.05 0.20
d5 3 0.10 0.40
Total 2.80
For a given set of probabilities, we wish to construct a binary search tree whose
expected search cost is smallest. We call such a tree an optimal binary search tree .
Figure 15.9(b) shows an optimal binary search tree for the probabilities given inthe ﬁgure caption; its expected cost is 2.75. This example shows that an optimalbinary search tree is not necessarily a tree whose overall height is smallest. Norcan we necessarily construct an optimal binary search tree by always putting the
key with the greatest probability at the root. Here, key k
5has the greatest search
probability of any key, yet the root of the optimal binary search tree shown is k2.
(The lowest expected cost of any binary search tree with k5at the root is 2.85.)
As with matrix-chain multiplication, exhaustive checking of all possibilities fails
to yield an efﬁcient algorithm. We can label the nodes of any n-node binary tree
with the keys k1;k2;:::;k nto construct a binary search tree, and then add in the
dummy keys as leaves. In Problem 12-4, we saw that the number of binary treeswith nnodes is /DEL.4
n=n3=2/, and so we would have to examine an exponential
number of binary search trees in an exhaustive search. Not surprisingly, we shallsolve this problem with dynamic programming.
Step 1: The structure of an optimal binary search tree
To characterize the optimal substructure of optimal binary search trees, we start
with an observation about subtrees. Consider any subtree of a binary search tree.It must contain keys in a contiguous range k
i;:::;k j,f o rs o m e 1/DC4i/DC4j/DC4n.
In addition, a subtree that contains keys ki;:::;k jmust also have as its leaves the
dummy keys di/NUL1;:::;d j.
Now we can state the optimal substructure: if an optimal binary search tree T
has a subtree T0containing keys ki;:::;k j, then this subtree T0must be optimal as400 Chapter 15 Dynamic Programming
well for the subproblem with keys ki;:::;k jand dummy keys di/NUL1;:::;d j.T h e
usual cut-and-paste argument applies. If there were a subtree T00whose expected
cost is lower than that of T0, then we could cut T0out of Tand paste in T00,
resulting in a binary search tree of lower expected cost than T, thus contradicting
the optimality of T.
We need to use the optimal substructure to show that we can construct an opti-
mal solution to the problem from optimal solutions to subproblems. Given keysk
i;:::;k j, one of these keys, say kr(i/DC4r/DC4j), is the root of an optimal
subtree containing these keys. The left subtree of the root krcontains the keys
ki;:::;k r/NUL1(and dummy keys di/NUL1;:::;d r/NUL1), and the right subtree contains the
keyskrC1;:::;k j(and dummy keys dr;:::;d j). As long as we examine all candi-
date roots kr,w h e r e i/DC4r/DC4j, and we determine all optimal binary search trees
containing ki;:::;k r/NUL1and those containing krC1;:::;k j, we are guaranteed that
we will ﬁnd an optimal binary search tree.
There is one detail worth noting about “empty” subtrees. Suppose that in a
subtree with keys ki;:::;k j, we select kias the root. By the above argument, ki’s
left subtree contains the keys ki;:::;k i/NUL1. We interpret this sequence as containing
no keys. Bear in mind, however, that subtrees also contain dummy keys. We adopt
the convention that a subtree containing keys ki;:::;k i/NUL1has no actual keys but
does contain the single dummy key di/NUL1. Symmetrically, if we select kjas the root,
thenkj’s right subtree contains the keys kjC1;:::;k j; this right subtree contains
no actual keys, but it does contain the dummy key dj.
Step 2: A recursive solution
We are ready to deﬁne the value of an optimal solution recursively. We pick our
subproblem domain as ﬁnding an optimal binary search tree containing the keys
ki;:::;k j,w h e r e i/NAK1,j/DC4n,a n d j/NAKi/NUL1.( W h e n jDi/NUL1,t h e r e
are no actual keys; we have just the dummy key di/NUL1.) Let us deﬁne eŒi;j/c141 as
the expected cost of searching an optimal binary search tree containing the keysk
i;:::;k j. Ultimately, we wish to compute eŒ1;n/c141 .
The easy case occurs when jDi/NUL1. Then we have just the dummy key di/NUL1.
The expected search cost is eŒi;i/NUL1/c141Dqi/NUL1.
When j/NAKi, we need to select a root krfrom among ki;:::;k ja n dt h e nm a k ea n
optimal binary search tree with keys ki;:::;k r/NUL1as its left subtree and an optimal
binary search tree with keys krC1;:::;k jas its right subtree. What happens to the
expected search cost of a subtree when it becomes a subtree of a node? The depthof each node in the subtree increases by 1. By equation (15.11), the expected searchcost of this subtree increases by the sum of all the probabilities in the subtree. Fora subtree with keys k
i;:::;k j, let us denote this sum of probabilities as15.5 Optimal binary search trees 401
w.i;j/DjX
lDiplCjX
lDi/NUL1ql: (15.12)
Thus, if kris the root of an optimal subtree containing keys ki;:::;k j,w eh a v e
eŒi;j/c141DprC.eŒi; r/NUL1/c141Cw.i;r/NUL1//C.eŒrC1; j /c141Cw.rC1; j // :
Noting that
w.i;j/Dw.i;r/NUL1/CprCw.rC1; j / ;
we rewrite eŒi;j/c141 as
eŒi;j/c141DeŒi;r/NUL1/c141CeŒrC1; j /c141Cw.i;j/ : (15.13)
The recursive equation (15.13) assumes that we know which node krto use as
the root. We choose the root that gives the lowest expected search cost, giving usour ﬁnal recursive formulation:
eŒi;j/c141D(
q
i/NUL1 ifjDi/NUL1;
min
i/DC4r/DC4jfeŒi;r/NUL1/c141CeŒrC1; j /c141Cw.i;j/gifi/DC4j:(15.14)
TheeŒi;j/c141 values give the expected search costs in optimal binary search trees.
To help us keep track of the structure of optimal binary search trees, we deﬁnerootŒi; j /c141 ,f o r 1/DC4i/DC4j/DC4n, to be the index rfor which k
ris the root of an
optimal binary search tree containing keys ki;:::;k j. Although we will see how
to compute the values of rootŒi; j /c141 , we leave the construction of an optimal binary
search tree from these values as Exercise 15.5-1.
Step 3: Computing the expected search cost of an optimal binary search tree
At this point, you may have noticed some similarities between our characterizations
of optimal binary search trees and matrix-chain multiplication. For both problemdomains, our subproblems consist of contiguous index subranges. A direct, recur-sive implementation of equation (15.14) would be as inefﬁcient as a direct, recur-sive matrix-chain multiplication algorithm. Instead, we store the eŒi;j/c141 values in a
table eŒ1::nC1 ;0::n /c141 . The ﬁrst index needs to run to nC1rather than nbecause
in order to have a subtree containing only the dummy key d
n, we need to compute
and store eŒnC1; n/c141. The second index needs to start from 0because in order to
have a subtree containing only the dummy key d0, we need to compute and store
eŒ1; 0/c141 . We use only the entries eŒi;j/c141 for which j/NAKi/NUL1. We also use a table
rootŒi; j /c141 , for recording the root of the subtree containing keys ki;:::;k j.T h i s
table uses only the entries for which 1/DC4i/DC4j/DC4n.
We will need one other table for efﬁciency. Rather than compute the value
ofw.i;j/ from scratch every time we are computing eŒi;j/c141 —which would take402 Chapter 15 Dynamic Programming
‚.j/NULi/additions—we store these values in a table wŒ1::nC1 ;0::n /c141 .F o rt h e
base case, we compute wŒi;i/NUL1/c141Dqi/NUL1for1/DC4i/DC4nC1.F o r j/NAKi,w e
compute
wŒi;j/c141DwŒi;j/NUL1/c141CpjCqj: (15.15)
Thus, we can compute the ‚.n2/values of wŒi;j/c141 in‚.1/ time each.
The pseudocode that follows takes as inputs the probabilities p1;:::;p nand
q0;:::;q nand the size n, and it returns the tables eandroot.
OPTIMAL -BST . p;q;n /
1l e t eŒ1::nC1 ;0::n /c141 ,wŒ1::nC1 ;0::n /c141 ,
androotŒ 1::n ;1::n /c141 be new tables
2foriD1tonC1
3 eŒi;i/NUL1/c141Dqi/NUL1
4 wŒi;i/NUL1/c141Dqi/NUL1
5forlD1ton
6 foriD1ton/NULlC1
7 jDiCl/NUL1
8 eŒi;j/c141D1
9 wŒi;j/c141DwŒi;j/NUL1/c141CpjCqj
10 forrDitoj
11 tDeŒi;r/NUL1/c141CeŒrC1; j /c141CwŒi;j/c141
12 ift<e Œ i ; j /c141
13 eŒi;j/c141Dt
14 rootŒi; j /c141Dr
15return eandroot
From the description above and the similarity to the M ATRIX -CHAIN -ORDER pro-
cedure in Section 15.2, you should ﬁnd the operation of this procedure to be fairlystraightforward. The forloop of lines 2–4 initializes the values of eŒi;i/NUL1/c141
andwŒi;i/NUL1/c141.T h e forloop of lines 5–14 then uses the recurrences (15.14)
and (15.15) to compute eŒi;j/c141 andwŒi;j/c141 for all 1/DC4i/DC4j/DC4n. In the ﬁrst itera-
tion, when lD1, the loop computes eŒi;i/c141 andwŒi;i/c141 foriD1 ;2;:::;n . The sec-
ond iteration, with lD2, computes eŒi;iC1/c141andwŒi;iC1/c141foriD1 ;2;:::;n/NUL1,
and so forth. The innermost forloop, in lines 10–14, tries each candidate index r
to determine which key k
rto use as the root of an optimal binary search tree con-
taining keys ki;:::;k j.T h i s forloop saves the current value of the index rin
rootŒi; j /c141 whenever it ﬁnds a better key to use as the root.
Figure 15.10 shows the tables eŒi;j/c141 ,wŒi;j/c141 ,a n d rootŒi; j /c141 computed by the
procedure O PTIMAL -BST on the key distribution shown in Figure 15.9. As in the
matrix-chain multiplication example of Figure 15.5, the tables are rotated to make15.5 Optimal binary search trees 403
2.75
1.75
1.25
0.90
0.45
0.052.00
1.20
0.70
0.40
0.101.30
0.60
0.25
0.050.90
0.30
0.050.50
0.05 0.10e
012345
654321
ji 1.00
0.70
0.55
0.45
0.30
0.050.80
0.50
0.35
0.25
0.100.60
0.30
0.15
0.050.50
0.20
0.050.35
0.05 0.10w
012345
654321
ji
2
2
2
1
14
2
2
25
4
35
4 5root
12345
54321
ji
Figure 15.10 The tables eŒi;j/c141 ,wŒi;j/c141 ,a n d rootŒi; j /c141 computed by O PTIMAL -BST on the key
distribution shown in Figure 15.9. The tables are rotated so that the diagonals run horizontally.
the diagonals run horizontally. O PTIMAL -BST computes the rows from bottom to
top and from left to right within each row.
The O PTIMAL -BST procedure takes ‚.n3/time, just like M ATRIX -CHAIN -
ORDER . We can easily see that its running time is O.n3/, since its forloops are
nested three deep and each loop index takes on at most nvalues. The loop indices in
OPTIMAL -BST do not have exactly the same bounds as those in M ATRIX -CHAIN -
ORDER , but they are within at most 1 in all directions. Thus, like M ATRIX -CHAIN -
ORDER ,t h eO PTIMAL -BST procedure takes /DEL.n3/time.
Exercises
15.5-1
Write pseudocode for the procedure C ONSTRUCT -OPTIMAL -BST .root/which,
given the table root, outputs the structure of an optimal binary search tree. For the
example in Figure 15.10, your procedure should print out the structure404 Chapter 15 Dynamic Programming
k2is the root
k1is the left child of k2
d0is the left child of k1
d1is the right child of k1
k5is the right child of k2
k4is the left child of k5
k3is the left child of k4
d2is the left child of k3
d3is the right child of k3
d4is the right child of k4
d5is the right child of k5
corresponding to the optimal binary search tree shown in Figure 15.9(b).
15.5-2
Determine the cost and structure of an optimal binary search tree for a set of nD7
keys with the following probabilities:
i
 01234567
pi
 0.04 0.06 0.08 0.02 0.10 0.12 0.14
qi
0.06 0.06 0.06 0.06 0.05 0.05 0.05 0.05
15.5-3
Suppose that instead of maintaining the table wŒi;j/c141 , we computed the value
ofw.i;j/ directly from equation (15.12) in line 9 of O PTIMAL -BST and used this
computed value in line 11. How would this change affect the asymptotic runningtime of O
PTIMAL -BST?
15.5-4 ?
Knuth [212] has shown that there are always roots of optimal subtrees such thatrootŒi; j/NUL1/c141/DC4rootŒi; j /c141/DC4rootŒiC1; j /c141 for all 1/DC4i<j/DC4n. Use this fact to
modify the O
PTIMAL -BST procedure to run in ‚.n2/time.
Problems
15-1 Longest simple path in a directed acyclic graph
Suppose that we are given a directed acyclic graph GD.V; E/ with real-
valued edge weights and two distinguished vertices sandt. Describe a dynamic-
programming approach for ﬁnding a longest weighted simple path from stot.
What does the subproblem graph look like? What is the efﬁciency of your algo-rithm?Problems for Chapter 15 405
(a) (b)
Figure 15.11 Seven points in the plane, shown on a unit grid. (a)The shortest closed tour, with
length approximately 24:89 . This tour is not bitonic. (b)T h es h o r t e s tb i t o n i ct o u rf o rt h es a m es e to f
points. Its length is approximately 25:58 .
15-2 Longest palindrome subsequence
Apalindrome is a nonempty string over some alphabet that reads the same for-
ward and backward. Examples of palindromes are all strings of length 1,civic ,
racecar ,a n daibohphobia (fear of palindromes).
Give an efﬁcient algorithm to ﬁnd the longest palindrome that is a subsequence
of a given input string. For example, given the input character , your algorithm
should return carac . What is the running time of your algorithm?
15-3 Bitonic euclidean traveling-salesman problem
In the euclidean traveling-salesman problem ,w ea r eg i v e nas e to f npoints in
the plane, and we wish to ﬁnd the shortest closed tour that connects all npoints.
Figure 15.11(a) shows the solution to a 7-point problem. The general problem is
NP-hard, and its solution is therefore believed to require more than polynomialtime (see Chapter 34).
J. L. Bentley has suggested that we simplify the problem by restricting our at-
tention to bitonic tours , that is, tours that start at the leftmost point, go strictly
rightward to the rightmost point, and then go strictly leftward back to the startingpoint. Figure 15.11(b) shows the shortest bitonic tour of the same 7points. In this
case, a polynomial-time algorithm is possible.
Describe an O.n
2/-time algorithm for determining an optimal bitonic tour. You
may assume that no two points have the same x-coordinate and that all operations
on real numbers take unit time. ( Hint: Scan left to right, maintaining optimal pos-
sibilities for the two parts of the tour.)
15-4 Printing neatly
Consider the problem of neatly printing a paragraph with a monospaced font (allcharacters having the same width) on a printer. The input text is a sequence of n406 Chapter 15 Dynamic Programming
words of lengths l1;l2;:::;l n, measured in characters. We want to print this para-
graph neatly on a number of lines that hold a maximum of Mcharacters each. Our
criterion of “neatness” is as follows. If a given line contains words ithrough j,
where i/DC4j, and we leave exactly one space between words, the number of extra
space characters at the end of the line is M/NULjCi/NULPj
kDilk, which must be
nonnegative so that the words ﬁt on the line. We wish to minimize the sum, overall lines except the last, of the cubes of the numbers of extra space characters at theends of lines. Give a dynamic-programming algorithm to print a paragraph of n
words neatly on a printer. Analyze the running time and space requirements of
your algorithm.
15-5 Edit distance
In order to transform one source string of text xŒ 1::m /c141 to a target string yŒ 1::n /c141 ,
we can perform various transformation operations. Our goal is, given xandy,
to produce a series of transformations that change xtoy. We use an ar-
ray´—assumed to be large enough to hold all the characters it will need—to hold
the intermediate results. Initially, ´is empty, and at termination, we should have
´Œj /c141DyŒj/c141 forjD1 ;2;:::;n . We maintain current indices iintoxandjinto´,
and the operations are allowed to alter ´and these indices. Initially, iDjD1.
We are required to examine every character in xduring the transformation, which
means that at the end of the sequence of transformation operations, we must have
iDmC1.
We may choose from among six transformation operations:
Copy a character from xto´by setting ´Œj /c141DxŒi/c141and then incrementing both i
andj. This operation examines xŒi/c141.
Replace a character from xby another character c, by setting ´Œj /c141Dc,a n dt h e n
incrementing both iandj. This operation examines xŒi/c141.
Delete a character from xby incrementing i
but leaving jalone. This operation
examines xŒi/c141.
Insert the character cinto´by setting ´Œj /c141Dcand then incrementing j,b u t
leaving ialone. This operation examines no characters of x.
Twiddle (i.e., exchange) the next two characters by copying them from xto´but
in the opposite order; we do so by setting ´Œj /c141DxŒiC1/c141and´ŒjC1/c141DxŒi/c141
and then setting iDiC2andjDjC2. This operation examines xŒi/c141
andxŒiC1/c141.
Killthe remainder of xby setting iDmC1. This operation examines all char-
acters in xthat have not yet been examined. This operation, if performed, must
be the ﬁnal operation.Problems for Chapter 15 407
As an example, one way to transform the source string algorithm to the target
string altruistic is to use the following sequence of operations, where the
underlined characters are xŒi/c141and´Œj /c141 after the operation:
Operation x´
initial strings a
lgorithm
copy al
gorithm a
copy alg
orithm al
replace by t algo
 rithm alt
delete algor
 ithm alt
copy algori
 thm altr
insert u algori
 thm altru
insert i algori
 thm altrui
insert s algori
 thm altruis
twiddle algorith
 m altruisti
insert c algorith
 m altruistic
kill algorithm
 altruistic
Note that there are several other sequences of transformation operations that trans-
formalgorithm toaltruistic .
Each of the transformation operations has an associated cost. The cost of an
operation depends on the speciﬁc application, but we assume that each operation’scost is a constant that is known to us. We also assume that the individual costs ofthe copy and replace operations are less than the combined costs of the delete andinsert operations; otherwise, the copy and replace operations would not be used.
The cost of a given sequence of transformation operations is the sum of the costs
of the individual operations in the sequence. For the sequence above, the cost oftransforming algorithm toaltruistic is
.3/SOHcost.copy//Ccost.replace /Ccost.delete /C.4/SOHcost.insert //
Ccost.twiddle /Ccost.kill/:
a.Given two sequences xŒ1::m/c141 andyŒ 1::n /c141 and set of transformation-operation
costs, the edit distance from xtoyis the cost of the least expensive operation
sequence that transforms xtoy. Describe a dynamic-programming algorithm
that ﬁnds the edit distance from xŒ1::m/c141 toyŒ 1::n /c141 and prints an optimal op-
eration sequence. Analyze the running time and space requirements of your
algorithm.
The edit-distance problem generalizes the problem of aligning two DNA sequences
(see, for example, Setubal and Meidanis [310, Section 3.2]). There are severalmethods for measuring the similarity of two DNA sequences by aligning them.One such method to align two sequences xandyconsists of inserting spaces at408 Chapter 15 Dynamic Programming
arbitrary locations in the two sequences (including at either end) so that the result-
ing sequences x0andy0have the same length but do not have a space in the same
position (i.e., for no position jare both x0Œj /c141andy0Œj /c141a space). Then we assign a
“score” to each position. Position jreceives a score as follows:
/SIC1ifx0Œj /c141Dy0Œj /c141and neither is a space,
/SI/NUL1ifx0Œj /c141¤y0Œj /c141and neither is a space,
/SI/NUL2if either x0Œj /c141ory0Œj /c141is a space.
The score for the alignment is the sum of the scores of the individual positions. For
example, given the sequences xDGATCGGCAT andyDCAATGTGAATC , one
alignment is
G ATCG GCAT
CAAT GTGAATC-*++*+*+-++ *
A+under a position indicates a score of C1for that position, a -indicates a score
of/NUL1,a n da *indicates a score of /NUL2, so that this alignment has a total score of
6/SOH1/NUL2/SOH1/NUL4/SOH2D/NUL4.
b.Explain how to cast the problem of ﬁnding an optimal alignment as an edit
distance problem using a subset of the transformation operations copy, replace,
delete, insert, twiddle, and kill.
15-6 Planning a company party
Professor Stewart is consulting for the president of a corporation that is planninga company party. The company has a hierarchical structure; that is, the supervisorrelation forms a tree rooted at the president. The personnel ofﬁce has ranked eachemployee with a conviviality rating, which is a real number. In order to make the
party fun for all attendees, the president does not want both an employee and his
or her immediate supervisor to attend.
Professor Stewart is given the tree that describes the structure of the corporation,
using the left-child, right-sibling representation described in Section 10.4. Eachnode of the tree holds, in addition to the pointers, the name of an employee andthat employee’s conviviality ranking. Describe an algorithm to make up a guestlist that maximizes the sum of the conviviality ratings of the guests. Analyze therunning time of your algorithm.
15-7 Viterbi algorithm
We can use dynamic programming on a directed graph GD.V; E/ for speech
recognition. Each edge .u; /ETB/2Eis labeled with a sound /ESC.u;/ETB/ from a ﬁ-
nite set †of sounds. The labeled graph is a formal model of a person speakingProblems for Chapter 15 409
a restricted language. Each path in the graph starting from a distinguished ver-
tex/ETB02Vcorresponds to a possible sequence of sounds produced by the model.
We deﬁne the label of a directed path to be the concatenation of the labels of theedges on that path.
a.Describe an efﬁcient algorithm that, given an edge-labeled graph Gwith dis-
tinguished vertex /ETB
0and a sequence sDh/ESC1;/ESC2;:::;/ESC kiof sounds from †,
returns a path in Gthat begins at /ETB0and has sas its label, if any such path exists.
Otherwise, the algorithm should return NO-SUCH -PATH . Analyze the running
time of your algorithm. ( Hint: You may ﬁnd concepts from Chapter 22 useful.)
Now, suppose that every edge .u; /ETB/2Ehas an associated nonnegative proba-
bility p.u;/ETB/ of traversing the edge .u; /ETB/ from vertex uand thus producing the
corresponding sound. The sum of the probabilities of the edges leaving any vertexequals 1. The probability of a path is deﬁned to be the product of the probabil-
ities of its edges. We can view the probability of a path beginning at /ETB
0as the
probability that a “random walk” beginning at /ETB0will follow the speciﬁed path,
where we randomly choose which edge to take leaving a vertex uaccording to the
probabilities of the available edges leaving u.
b.Extend your answer to part (a) so that if a path is returned, it is a most prob-
able path starting at /ETB0and having label s. Analyze the running time of your
algorithm.
15-8 Image compression by seam carving
We are given a color picture consisting of an m/STXnarray AŒ1 : : m; 1 : : n/c141 of pixels,
where each pixel speciﬁes a triple of red, green, and blue (RGB) intensities. Sup-pose that we wish to compress this picture slightly. Speciﬁcally, we wish to remove
one pixel from each of the mrows, so that the whole picture becomes one pixel
narrower. To avoid disturbing visual effects, however, we require that the pixelsremoved in two adjacent rows be in the same or adjacent columns; the pixels re-moved form a “seam” from the top row to the bottom row where successive pixelsin the seam are adjacent vertically or diagonally.
a.Show that the number of such possible seams grows at least exponentially in m,
assuming that n>1 .
b.Suppose now that along with each pixel AŒi; j /c141 , we have calculated a real-
valued disruption measure dŒi;j/c141 , indicating how disruptive it would be to
remove pixel AŒi; j /c141 . Intuitively, the lower a pixel’s disruption measure, the
more similar the pixel is to its neighbors. Suppose further that we deﬁne thedisruption measure of a seam to be the sum of the disruption measures of itspixels.410 Chapter 15 Dynamic Programming
Give an algorithm to ﬁnd a seam with the lowest disruption measure. How
efﬁcient is your algorithm?
15-9 Breaking a string
A certain string-processing language allows a programmer to break a string intotwo pieces. Because this operation copies the string, it costs ntime units to break
a string of ncharacters into two pieces. Suppose a programmer wants to break
a string into many pieces. The order in which the breaks occur can affect thetotal amount of time used. For example, suppose that the programmer wants tobreak a 20-character string after characters 2,8,a n d 10(numbering the characters
in ascending order from the left-hand end, starting from 1). If she programs the
breaks to occur in left-to-right order, then the ﬁrst break costs 20time units, the
second break costs 18time units (breaking the string from characters 3to20at
character 8), and the third break costs 12time units, totaling 50time units. If she
programs the breaks to occur in right-to-left order, however, then the ﬁrst breakcosts 20time units, the second break costs 10time units, and the third break costs
8time units, totaling 38time units. In yet another order, she could break ﬁrst at 8
(costing 20), then break the left piece at 2(costing 8), and ﬁnally the right piece
at10(costing 12), for a total cost of 40.
Design an algorithm that, given the numbers of characters after which to break,
determines a least-cost way to sequence those breaks. More formally, given astring Swithncharacters and an array L Œ 1::m /c141 containing the break points, com-
pute the lowest cost for a sequence of breaks, along with a sequence of breaks thatachieves this cost.
15-10 Planning an investment strategy
Your knowledge of algorithms helps you obtain an exciting job with the AcmeComputer Company, along with a $10,000 signing bonus. You decide to investthis money with the goal of maximizing your return at the end of 10years. You
decide to use the Amalgamated Investment Company to manage your investments.
Amalgamated Investments requires you to observe the following rules. It offers n
different investments, numbered 1through n. In each year j, investment iprovides
a return rate of r
ij. In other words, if you invest ddollars in investment iin year j,
then at the end of year j, you have drijdollars. The return rates are guaranteed,
that is, you are given all the return rates for the next 10years for each investment.
You make investment decisions only once per year. At the end of each year, youcan leave the money made in the previous year in the same investments, or youcan shift money to other investments, by either shifting money between existinginvestments or moving money to a new investement. If you do not move yourmoney between two consecutive years, you pay a fee of f
1dollars, whereas if you
switch your money, you pay a fee of f2dollars, where f2>f 1.Problems for Chapter 15 411
a.The problem, as stated, allows you to invest your money in multiple investments
in each year. Prove that there exists an optimal investment strategy that, ineach year, puts all the money into a single investment. (Recall that an optimalinvestment strategy maximizes the amount of money after 10years and is not
concerned with any other objectives, such as minimizing risk.)
b.Prove that the problem of planning your optimal investment strategy exhibits
optimal substructure.
c.Design an algorithm that plans your optimal investment strategy. What is the
running time of your algorithm?
d.Suppose that Amalgamated Investments imposed the additional restriction that,
at any point, you can have no more than $15,000 in any one investment. Showthat the problem of maximizing your income at the end of 10years no longer
exhibits optimal substructure.
15-11 Inventory planning
The Rinky Dink Company makes machines that resurface ice rinks. The demandfor such products varies from month to month, and so the company needs to de-velop a strategy to plan its manufacturing given the ﬂuctuating, but predictable,demand. The company wishes to design a plan for the next nmonths. For each
month i, the company knows the demand d
i, that is, the number of machines that
it will sell. Let DDPn
iD1dibe the total demand over the next nmonths. The
company keeps a full-time staff who provide labor to manufacture up to mma-
chines per month. If the company needs to make more than mmachines in a given
month, it can hire additional, part-time labor, at a cost that works out to cdollars
per machine. Furthermore, if, at the end of a month, the company is holding anyunsold machines, it must pay inventory costs. The cost for holding jmachines is
given as a function h.j / forjD1 ;2;:::;D ,w h e r e h.j //NAK0for1/DC4j/DC4Dand
h.j //DC4h.jC1/for1/DC4j/DC4D/NUL1.
Give an algorithm that calculates a plan for the company that minimizes its costs
while fulﬁlling all the demand. The running time should be polyomial in nandD.
15-12 Signing free-agent baseball players
Suppose that you are the general manager for a major-league baseball team. During
the off-season, you need to sign some free-agent players for your team. The team
owner has given you a budget of $ Xto spend on free agents. You are allowed to
spend less than $ Xaltogether, but the owner will ﬁre you if you spend any more
than $ X.412 Chapter 15 Dynamic Programming
You are considering Ndifferent positions, and for each position, Pfree-agent
players who play that position are available.8Because you do not want to overload
your roster with too many players at any position, for each position you may signat most one free agent who plays that position. (If you do not sign any players at aparticular position, then you plan to stick with the players you already have at thatposition.)
To determine how valuable a player is going to be, you decide to use a sabermet-
ric statistic
9known as “VORP,” or “value over replacement player.” A player with
a higher VORP is more valuable than a player with a lower VORP. A player with a
higher VORP is not necessarily more expensive to sign than a player with a lowerVORP, because factors other than a player’s value determine how much it costs tosign him.
For each available free-agent player, you have three pieces of information:
/SIthe player’s position,
/SIthe amount of money it will cost to sign the player, and
/SIthe player’s VORP.
Devise an algorithm that maximizes the total VORP of the players you sign while
spending no more than $ Xaltogether. You may assume that each player signs for a
multiple of $100,000. Your algorithm should output the total VORP of the players
you sign, the total amount of money you spend, and a list of which players you
sign. Analyze the running time and space requirement of your algorithm.
Chapter notes
R. Bellman began the systematic study of dynamic programming in 1955. The
word “programming,” both here and in linear programming, refers to using a tab-ular solution method. Although optimization techniques incorporating elements ofdynamic programming were known earlier, Bellman provided the area with a solidmathematical basis [37].
8Although there are nine positions on a baseball team, Nis not necesarily equal to 9because some
general managers have particular ways of thinking about positions. For example, a general managermight consider right-handed pitchers and left-handed pitchers to be separate “positions,” as well as
starting pitchers, long relief pitchers (relief pitchers who can pitch several innings), and short relief
pitchers (relief pitchers who normally pitch at most only one inning).
9Sabermetrics is the application of statistical analysis to baseball records. It provides several ways
to compare the relative values of individual players.Notes for Chapter 15 413
Galil and Park [125] classify dynamic-programming algorithms according to the
size of the table and the number of other table entries each entry depends on. Theycall a dynamic-programming algorithm tD=eD if its table size is O.n
t/and each
entry depends on O.ne/other entries. For example, the matrix-chain multiplication
algorithm in Section 15.2 would be 2D=1D , and the longest-common-subsequence
algorithm in Section 15.4 would be 2D=0D .
Hu and Shing [182, 183] give an O.n lgn/-time algorithm for the matrix-chain
multiplication problem.
TheO.mn/ -time algorithm for the longest-common-subsequence problem ap-
pears to be a folk algorithm. Knuth [70] posed the question of whether subquadraticalgorithms for the LCS problem exist. Masek and Paterson [244] answered thisquestion in the afﬁrmative by giving an algorithm that runs in O.mn= lgn/time,
where n/DC4mand the sequences are drawn from a set of bounded size. For the
special case in which no element appears more than once in an input sequence,Szymanski [326] shows how to solve the problem in O..nCm/lg.nCm//time.
Many of these results extend to the problem of computing string edit distances
(Problem 15-5).
An early paper on variable-length binary encodings by Gilbert and Moore [133]
had applications to constructing optimal binary search trees for the case in which allprobabilities p
iare0; this paper contains an O.n3/-time algorithm. Aho, Hopcroft,
and Ullman [5] present the algorithm from Section 15.5. Exercise 15.5-4 is due toKnuth [212]. Hu and Tucker [184] devised an algorithm for the case in which all
probabilities p
iare0that uses O.n2/time and O.n/ space; subsequently, Knuth
[211] reduced the time to O.n lgn/.
Problem 15-8 is due to Avidan and Shamir [27], who have posted on the Web a
wonderful video illustrating this image-compression technique.16 Greedy Algorithms
Algorithms for optimization problems typically go through a sequence of steps,
with a set of choices at each step. For many optimization problems, using dynamicprogramming to determine the best choices is overkill; simpler, more efﬁcient al-gorithms will do. A greedy algorithm always makes the choice that looks best at
the moment. That is, it makes a locally optimal choice in the hope that this choicewill lead to a globally optimal solution. This chapter explores optimization prob-lems for which greedy algorithms provide optimal solutions. Before reading thischapter, you should read about dynamic programming in Chapter 15, particularlySection 15.3.
Greedy algorithms do not always yield optimal solutions, but for many problems
they do. We shall ﬁrst examine, in Section 16.1, a simple but nontrivial problem,
the activity-selection problem, for which a greedy algorithm efﬁciently computesan optimal solution. We shall arrive at the greedy algorithm by ﬁrst consider-ing a dynamic-programming approach and then showing that we can always make
greedy choices to arrive at an optimal solution. Section 16.2 reviews the basic
elements of the greedy approach, giving a direct approach for proving greedy al-gorithms correct. Section 16.3 presents an important application of greedy tech-niques: designing data-compression (Huffman) codes. In Section 16.4, we inves-tigate some of the theory underlying combinatorial structures called “matroids,”for which a greedy algorithm always produces an optimal solution. Finally, Sec-tion 16.5 applies matroids to solve a problem of scheduling unit-time tasks withdeadlines and penalties.
The greedy method is quite powerful and works well for a wide range of prob-
lems. Later chapters will present many algorithms that we can view as applica-tions of the greedy method, including minimum-spanning-tree algorithms (Chap-ter 23), Dijkstra’s algorithm for shortest paths from a single source (Chapter 24),and Chv´ atal’s greedy set-covering heuristic (Chapter 35). Minimum-spanning-tree
algorithms furnish a classic example of the greedy method. Although you can read16.1 An activity-selection problem 415
this chapter and Chapter 23 independently of each other, you might ﬁnd it useful
to read them together.
16.1 An activity-selection problem
Our ﬁrst example is the problem of scheduling several competing activities that re-quire exclusive use of a common resource, with a goal of selecting a maximum-sizeset of mutually compatible activities. Suppose we have a set SDfa
1;a2;:::;a ng
ofnproposed activities that wish to use a resource, such as a lecture hall, which
can serve only one activity at a time. Each activity aihas astart time siand aﬁnish
timefi,w h e r e 0/DC4si<f i<1. If selected, activity aitakes place during the
half-open time interval Œsi;fi/. Activities aiandajarecompatible if the intervals
Œsi;fi/andŒsj;fj/do not overlap. That is, aiandajare compatible if si/NAKfj
orsj/NAKfi.I n t h e activity-selection problem , we wish to select a maximum-size
subset of mutually compatible activities. We assume that the activities are sortedin monotonically increasing order of ﬁnish time:
f
1/DC4f2/DC4f3/DC4/SOH/SOH/SOH/DC4 fn/NUL1/DC4fn: (16.1)
(We shall see later the advantage that this assumption provides.) For example,
consider the following set Sof activities:
i
 123456 7 8 9 1 01 1
si
130535 6 8 8 2 1 2
fi
4567991 01 11 21 41 6
For this example, the subset fa3;a9;a11gconsists of mutually compatible activities.
It is not a maximum subset, however, since the subset fa1;a4;a8;a11gis larger. In
fact,fa1;a4;a8;a11gis a largest subset of mutually compatible activities; another
largest subset isfa2;a4;a9;a11g.
We shall solve this problem in several steps. We start by thinking about a
dynamic-programming solution, in which we consider several choices when deter-mining which subproblems to use in an optimal solution. We shall then observe thatwe need to consider only one choice—the greedy choice—and that when we makethe greedy choice, only one subproblem remains. Based on these observations, weshall develop a recursive greedy algorithm to solve the activity-scheduling prob-lem. We shall complete the process of developing a greedy solution by convertingthe recursive algorithm to an iterative one. Although the steps we shall go throughin this section are slightly more involved than is typical when developing a greedyalgorithm, they illustrate the relationship between greedy algorithms and dynamic
programming.416 Chapter 16 Greedy Algorithms
The optimal substructure of the activity-selection problem
We can easily verify that the activity-selection problem exhibits optimal substruc-
ture. Let us denote by Sijthe set of activities that start after activity aiﬁnishes and
that ﬁnish before activity ajstarts. Suppose that we wish to ﬁnd a maximum set of
mutually compatible activities in Sij, and suppose further that such a maximum set
isAij, which includes some activity ak. By including akin an optimal solution, we
are left with two subproblems: ﬁnding mutually compatible activities in the set Sik
(activities that start after activity aiﬁnishes and that ﬁnish before activity akstarts)
and ﬁnding mutually compatible activities in the set Skj(activities that start after
activity akﬁnishes and that ﬁnish before activity ajstarts). Let AikDAij\Sik
andAkjDAij\Skj,s ot h a t Aikcontains the activities in Aijthat ﬁnish before ak
starts and Akjcontains the activities in Aijthat start after akﬁnishes. Thus, we
have AijDAik[fakg[Akj, and so the maximum-size set Aijof mutually com-
patible activities in Sijconsists ofjAijjDjAikjCjAkjjC1activities.
The usual cut-and-paste argument shows that the optimal solution Aijmust also
include optimal solutions to the two subproblems for SikandSkj. If we could
ﬁnd a set A0
kjof mutually compatible activities in SkjwherejA0
kjj>jAkjj,t h e n
we could use A0
kj, rather than Akj, in a solution to the subproblem for Sij.W e
would have constructed a set of jAikjCjA0
kjjC1>jAikjCjAkjjC1DjAijj
mutually compatible activities, which contradicts the assumption that Aijis an
optimal solution. A symmetric argument applies to the activities in Sik.
This way of characterizing optimal substructure suggests that we might solve
the activity-selection problem by dynamic programming. If we denote the size ofan optimal solution for the set S
ijbycŒi;j/c141 , then we would have the recurrence
cŒi;j/c141DcŒi;k/c141CcŒk;j/c141C1:
Of course, if we did not know that an optimal solution for the set Sijincludes
activity ak, we would have to examine all activities in Sijto ﬁnd which one to
choose, so that
cŒi;j/c141D(0 ifSijD;;
max
ak2SijfcŒi;k/c141CcŒk;j/c141C1gifSij¤;:(16.2)
We could then develop a recursive algorithm and memoize it, or we could work
bottom-up and ﬁll in table entries as we go along. But we would be overlookinganother important characteristic of the activity-selection problem that we can use
to great advantage.16.1 An activity-selection problem 417
Making the greedy choice
What if we could choose an activity to add to our optimal solution without having
to ﬁrst solve all the subproblems? That could save us from having to consider allthe choices inherent in recurrence (16.2). In fact, for the activity-selection problem,we need consider only one choice: the greedy choice.
What do we mean by the greedy choice for the activity-selection problem? Intu-
ition suggests that we should choose an activity that leaves the resource availablefor as many other activities as possible. Now, of the activities we end up choos-ing, one of them must be the ﬁrst one to ﬁnish. Our intuition tells us, therefore,to choose the activity in Swith the earliest ﬁnish time, since that would leave the
resource available for as many of the activities that follow it as possible. (If morethan one activity in Shas the earliest ﬁnish time, then we can choose any such
activity.) In other words, since the activities are sorted in monotonically increasingorder by ﬁnish time, the greedy choice is activity a
1. Choosing the ﬁrst activity
to ﬁnish is not the only way to think of making a greedy choice for this problem;
Exercise 16.1-3 asks you to explore other possibilities.
If we make the greedy choice, we have only one remaining subproblem to solve:
ﬁnding activities that start after a1ﬁnishes. Why don’t we have to consider ac-
tivities that ﬁnish before a1starts? We have that s1<f 1,a n d f1is the earliest
ﬁnish time of any activity, and therefore no activity can have a ﬁnish time less than
or equal to s1. Thus, all activities that are compatible with activity a1must start
aftera1ﬁnishes.
Furthermore, we have already established that the activity-selection problem ex-
hibits optimal substructure. Let SkDfai2SWsi/NAKfkgbe the set of activities that
start after activity akﬁnishes. If we make the greedy choice of activity a1,t h e n S1
remains as the only subproblem to solve.1Optimal substructure tells us that if a1
is in the optimal solution, then an optimal solution to the original problem consists
of activity a1and all the activities in an optimal solution to the subproblem S1.
One big question remains: is our intuition correct? Is the greedy choice—in
which we choose the ﬁrst activity to ﬁnish—always part of some optimal solution?The following theorem shows that it is.
1We sometimes refer to the sets Skas subproblems rather than as just sets of activities. It will always
be clear from the context whether we are referring to Skas a set of activities or as a subproblem
whose input is that set.418 Chapter 16 Greedy Algorithms
Theorem 16.1
Consider any nonempty subproblem Sk,a n dl e t ambe an activity in Skwith the
earliest ﬁnish time. Then amis included in some maximum-size subset of mutually
compatible activities of Sk.
Proof LetAkbe a maximum-size subset of mutually compatible activities in Sk,
and let ajbe the activity in Akwith the earliest ﬁnish time. If ajDam,w ea r e
done, since we have shown that amis in some maximum-size subset of mutually
compatible activities of Sk.I faj¤am, let the set A0
kDAk/NULfajg[famgbeAk
but substituting amforaj. The activities in A0
kare disjoint, which follows because
the activities in Akare disjoint, ajis the ﬁrst activity in Akto ﬁnish, and fm/DC4fj.
SincejA0
kjDjAkj, we conclude that A0
kis a maximum-size subset of mutually
compatible activities of Sk, and it includes am.
Thus, we see that although we might be able to solve the activity-selection prob-
lem with dynamic programming, we don’t need to. (Besides, we have not yetexamined whether the activity-selection problem even has overlapping subprob-lems.) Instead, we can repeatedly choose the activity that ﬁnishes ﬁrst, keep onlythe activities compatible with this activity, and repeat until no activities remain.Moreover, because we always choose the activity with the earliest ﬁnish time, theﬁnish times of the activities we choose must strictly increase. We can considereach activity just once overall, in monotonically increasing order of ﬁnish times.
An algorithm to solve the activity-selection problem does not need to work
bottom-up, like a table-based dynamic-programming algorithm. Instead, it can
work top-down, choosing an activity to put into the optimal solution and then solv-ing the subproblem of choosing activities from those that are compatible with thosealready chosen. Greedy algorithms typically have this top-down design: make achoice and then solve a subproblem, rather than the bottom-up technique of solvingsubproblems before making a choice.
A recursive greedy algorithm
Now that we have seen how to bypass the dynamic-programming approach and in-
stead use a top-down, greedy algorithm, we can write a straightforward, recursiveprocedure to solve the activity-selection problem. The procedure R
ECURSIVE -
ACTIVITY -SELECTOR takes the start and ﬁnish times of the activities, represented
as arrays sandf,2the index kthat deﬁnes the subproblem Skit is to solve, and
2Because the pseudocode takes sandfas arrays, it indexes into them with square brackets rather
than subscripts.16.1 An activity-selection problem 419
the size nof the original problem. It returns a maximum-size set of mutually com-
patible activities in Sk. We assume that the ninput activities are already ordered
by monotonically increasing ﬁnish time, according to equation (16.1). If not, wecan sort them into this order in O.n lgn/time, breaking ties arbitrarily. In order
to start, we add the ﬁctitious activity a
0with f0D0, so that subproblem S0is
the entire set of activities S. The initial call, which solves the entire problem, is
RECURSIVE -ACTIVITY -SELECTOR . s ;f ;0 ;n / .
RECURSIVE -ACTIVITY -SELECTOR . s ;f ;k;n /
1mDkC1
2while m/DC4nandsŒm/c141 < f Œk/c141 //ﬁnd the ﬁrst activity in Skto ﬁnish
3 mDmC1
4ifm/DC4n
5 returnfamg[RECURSIVE -ACTIVITY -SELECTOR . s ;f ;m;n /
6else return;
Figure 16.1 shows the operation of the algorithm. In a given recursive call
RECURSIVE -ACTIVITY -SELECTOR . s ;f ;k;n / ,t h ewhile loop of lines 2–3 looks
for the ﬁrst activity in Skto ﬁnish. The loop examines akC1;akC2;:::;a n, un-
til it ﬁnds the ﬁrst activity amthat is compatible with ak; such an activity has
sm/NAKfk. If the loop terminates because it ﬁnds such an activity, line 5 returns
the union offamgand the maximum-size subset of Smreturned by the recursive
call R ECURSIVE -ACTIVITY -SELECTOR . s ;f ;m;n / . Alternatively, the loop may
terminate because m>n , in which case we have examined all activities in Sk
without ﬁnding one that is compatible with ak. In this case, SkD;,a n ds ot h e
procedure returns;in line 6.
Assuming that the activities have already been sorted by ﬁnish times, the running
time of the call R ECURSIVE -ACTIVITY -SELECTOR . s ;f ;0 ;n / is‚.n/ ,w h i c hw e
can see as follows. Over all recursive calls, each activity is examined exactly oncein the while loop test of line 2. In particular, activity a
iis examined in the last call
made in which k<i .
An iterative greedy algorithm
We easily can convert our recursive procedure to an iterative one. The procedure
RECURSIVE -ACTIVITY -SELECTOR is almost “tail recursive” (see Problem 7-4):
it ends with a recursive call to itself followed by a union operation. It is usually astraightforward task to transform a tail-recursive procedure to an iterative form; infact, some compilers for certain programming languages perform this task automat-ically. As written, R
ECURSIVE -ACTIVITY -SELECTOR works for subproblems Sk,
i.e., subproblems that consist of the last activities to ﬁnish.420 Chapter 16 Greedy Algorithms
0123456789 1 0 1 1 1 2 1 3 1 4time235
306
457
539
659
76 1 0
88 1 1
98 1 2
1 021 4
11 12 16114k skfk
a1a2
a1a3
a1a4
a1 a4a5
a1 a4a6
a1 a4a7
a1 a4a8
a1 a4 a8a9
a1 a4 a8a10
a1 a4 a8a11
a1 a4 a8 a110–0
a1
a0a0
RECURSIVE -ACTIVITY -SELECTOR (s,f, 0, 11)
RECURSIVE -ACTIVITY -SELECTOR (s,f, 1, 11)
RECURSIVE -ACTIVITY -SELECTOR (s,f, 4, 11)
RECURSIVE -ACTIVITY -SELECTOR (s,f, 8, 11)m = 1
m = 4
m = 8
m = 11
RECURSIVE -ACTIVITY -SELECTOR (s,f, 11, 11)
15 16
Figure 16.1 The operation of R ECURSIVE -ACTIVITY -SELECTOR on the 11activities given ear-
lier. Activities considered in each recursive call appear between horizontal lines. The ﬁctitious
activity a0ﬁnishes at time 0, and the initial call R ECURSIVE -ACTIVITY -SELECTOR . s;f ;0 ;1 1 / ,s e -
lects activity a1. In each recursive call, the activities that have already been selected are shaded,
and the activity shown in white is being considered. If the starting time of an activity occurs before
the ﬁnish time of the most recently added activity (t he arrow between them points left), it is re-
jected. Otherwise (the arro w points directly up or to the right), it is selected. The last recursive call,
RECURSIVE -ACTIVITY -SELECTOR .s; f; 11; 11/ , returns;. The resulting set of selected activities is
fa1;a4;a8;a11g.16.1 An activity-selection problem 421
The procedure G REEDY -ACTIVITY -SELECTOR is an iterative version of the pro-
cedure R ECURSIVE -ACTIVITY -SELECTOR . It also assumes that the input activi-
ties are ordered by monotonically increasing ﬁnish time. It collects selected activ-ities into a set Aand returns this set when it is done.
G
REEDY -ACTIVITY -SELECTOR .s; f /
1nDs:length
2ADfa1g
3kD1
4formD2ton
5 ifsŒm/c141/NAKfŒ k /c141
6 ADA[famg
7 kDm
8return A
The procedure works as follows. The variable kindexes the most recent addition
toA, corresponding to the activity akin the recursive version. Since we consider
the activities in order of monotonically increasing ﬁnish time, fkis always the
maximum ﬁnish time of any activity in A.T h a ti s ,
fkDmaxffiWai2Ag: (16.3)
Lines 2–3 select activity a1, initialize Ato contain just this activity, and initialize k
to index this activity. The forloop of lines 4–7 ﬁnds the earliest activity in Skto
ﬁnish. The loop considers each activity amin turn and adds amtoAif it is compat-
ible with all previously selected activities; such an activity is the earliest in Skto
ﬁnish. To see whether activity amis compatible with every activity currently in A,
it sufﬁces by equation (16.3) to check (in line 5) that its start time smis not earlier
than the ﬁnish time fkof the activity most recently added to A. If activity amis
compatible, then lines 6–7 add activity amtoAand set ktom. The set Areturned
by the call G REEDY -ACTIVITY -SELECTOR .s; f / is precisely the set returned by
the call R ECURSIVE -ACTIVITY -SELECTOR . s ;f ;0 ;n / .
Like the recursive version, G REEDY -ACTIVITY -SELECTOR schedules a set of n
activities in ‚.n/ time, assuming that the activities were already sorted initially by
their ﬁnish times.
Exercises
16.1-1
Give a dynamic-programming algorithm for the activity-selection problem, basedon recurrence (16.2). Have your algorithm compute the sizes cŒi;j/c141 as deﬁned
above and also produce the maximum-size subset of mutually compatible activities.422 Chapter 16 Greedy Algorithms
Assume that the inputs have been sorted as in equation (16.1). Compare the running
time of your solution to the running time of G REEDY -ACTIVITY -SELECTOR .
16.1-2
Suppose that instead of always selecting the ﬁrst activity to ﬁnish, we instead selectthe last activity to start that is compatible with all previously selected activities. De-scribe how this approach is a greedy algorithm, and prove that it yields an optimalsolution.
16.1-3
Not just any greedy approach to the activity-selection problem produces a max-
imum-size set of mutually compatible activities. Give an example to show thatthe approach of selecting the activity of least duration from among those that arecompatible with previously selected activities does not work. Do the same forthe approaches of always selecting the compatible activity that overlaps the fewestother remaining activities and always selecting the compatible remaining activitywith the earliest start time.
16.1-4
Suppose that we have a set of activities to schedule among a large number of lecturehalls, where any activity can take place in any lecture hall. We wish to scheduleall the activities using as few lecture halls as possible. Give an efﬁcient greedyalgorithm to determine which activity should use which lecture hall.
(This problem is also known as the interval-graph coloring problem . We can
create an interval graph whose vertices are the given activities and whose edgesconnect incompatible activities. The smallest number of colors required to color
every vertex so that no two adjacent vertices have the same color corresponds to
ﬁnding the fewest lecture halls needed to schedule all of the given activities.)
16.1-5
Consider a modiﬁcation to the activity-selection problem in which each activity a
i
has, in addition to a start and ﬁnish time, a value /ETBi. The objective is no longer
to maximize the number of activities scheduled, but instead to maximize the totalvalue of the activities scheduled. That is, we wish to choose a set Aof compatible
activities such thatP
ak2A/ETBkis maximized. Give a polynomial-time algorithm for
this problem.16.2 Elements of the greedy strategy 423
16.2 Elements of the greedy strategy
A greedy algorithm obtains an optimal solution to a problem by making a sequence
of choices. At each decision point, the algorithm makes choice that seems best atthe moment. This heuristic strategy does not always produce an optimal solution,but as we saw in the activity-selection problem, sometimes it does. This section
discusses some of the general properties of greedy methods.
The process that we followed in Section 16.1 to develop a greedy algorithm was
a bit more involved than is typical. We went through the following steps:
1. Determine the optimal substructure of the problem.
2. Develop a recursive solution. (For the activity-selection problem, we formu-
lated recurrence (16.2), but we bypassed developing a recursive algorithm basedon this recurrence.)
3. Show that if we make the greedy choice, then only one subproblem remains.
4. Prove that it is always safe to make the greedy choice. (Steps 3 and 4 can occur
in either order.)
5. Develop a recursive algorithm that implements the greedy strategy.
6. Convert the recursive algorithm to an iterative algorithm.
In going through these steps, we saw in great detail the dynamic-programming un-
derpinnings of a greedy algorithm. For example, in the activity-selection problem,we ﬁrst deﬁned the subproblems S
ij, where both iandjvaried. We then found
that if we always made the greedy choice, we could restrict the subproblems to beof the form S
k.
Alternatively, we could have fashioned our optimal substructure with a greedy
choice in mind, so that the choice leaves just one subproblem to solve. In the
activity-selection problem, we could have started by dropping the second subscript
and deﬁning subproblems of the form Sk. Then, we could have proven that a greedy
choice (the ﬁrst activity amto ﬁnish in Sk), combined with an optimal solution to
the remaining set Smof compatible activities, yields an optimal solution to Sk.
More generally, we design greedy algorithms according to the following sequenceof steps:
1. Cast the optimization problem as one in which we make a choice and are left
with one subproblem to solve.
2. Prove that there is always an optimal solution to the original problem that makes
the greedy choice, so that the greedy choice is always safe.424 Chapter 16 Greedy Algorithms
3. Demonstrate optimal substructure by showing that, having made the greedy
choice, what remains is a subproblem with the property that if we combine anoptimal solution to the subproblem with the greedy choice we have made, wearrive at an optimal solution to the original problem.
We shall use this more direct process in later sections of this chapter. Neverthe-
less, beneath every greedy algorithm, there is almost always a more cumbersomedynamic-programming solution.
How can we tell whether a greedy algorithm will solve a particular optimization
problem? No way works all the time, but the greedy-choice property and optimalsubstructure are the two key ingredients. If we can demonstrate that the problemhas these properties, then we are well on the way to developing a greedy algorithmfor it.
Greedy-choice property
The ﬁrst key ingredient is the greedy-choice property : we can assemble a globally
optimal solution by making locally optimal (greedy) choices. In other words, whenwe are considering which choice to make, we make the choice that looks best inthe current problem, without considering results from subproblems.
Here is where greedy algorithms differ from dynamic programming. In dynamic
programming, we make a choice at each step, but the choice usually depends on thesolutions to subproblems. Consequently, we typically solve dynamic-programmingproblems in a bottom-up manner, progressing from smaller subproblems to largersubproblems. (Alternatively, we can solve them top down, but memoizing. Ofcourse, even though the code works top down, we still must solve the subprob-lems before making a choice.) In a greedy algorithm, we make whatever choiceseems best at the moment and then solve the subproblem that remains. The choice
made by a greedy algorithm may depend on choices so far, but it cannot depend on
any future choices or on the solutions to subproblems. Thus, unlike dynamic pro-gramming, which solves the subproblems before making the ﬁrst choice, a greedyalgorithm makes its ﬁrst choice before solving any subproblems. A dynamic-programming algorithm proceeds bottom up, whereas a greedy strategy usuallyprogresses in a top-down fashion, making one greedy choice after another, reduc-ing each given problem instance to a smaller one.
Of course, we must prove that a greedy choice at each step yields a globally
optimal solution. Typically, as in the case of Theorem 16.1, the proof examinesa globally optimal solution to some subproblem. It then shows how to modifythe solution to substitute the greedy choice for some other choice, resulting in onesimilar, but smaller, subproblem.
We can usually make the greedy choice more efﬁciently than when we have to
consider a wider set of choices. For example, in the activity-selection problem, as-16.2 Elements of the greedy strategy 425
suming that we had already sorted the activities in monotonically increasing order
of ﬁnish times, we needed to examine each activity just once. By preprocessing theinput or by using an appropriate data structure (often a priority queue), we oftencan make greedy choices quickly, thus yielding an efﬁcient algorithm.
Optimal substructure
A problem exhibits optimal substructure if an optimal solution to the problem
contains within it optimal solutions to subproblems. This property is a key in-gredient of assessing the applicability of dynamic programming as well as greedyalgorithms. As an example of optimal substructure, recall how we demonstrated inSection 16.1 that if an optimal solution to subproblem S
ijincludes an activity ak,
then it must also contain optimal solutions to the subproblems SikandSkj.G i v e n
this optimal substructure, we argued that if we knew which activity to use as ak,w e
could construct an optimal solution to Sijby selecting akalong with all activities
in optimal solutions to the subproblems SikandSkj. Based on this observation of
optimal substructure, we were able to devise the recurrence (16.2) that describedthe value of an optimal solution.
We usually use a more direct approach regarding optimal substructure when
applying it to greedy algorithms. As mentioned above, we have the luxury of
assuming that we arrived at a subproblem by having made the greedy choice in
the original problem. All we really need to do is argue that an optimal solution to
the subproblem, combined with the greedy choice already made, yields an optimalsolution to the original problem. This scheme implicitly uses induction on thesubproblems to prove that making the greedy choice at every step produces anoptimal solution.
Greedy versus dynamic programming
Because both the greedy and dynamic-programming strategies exploit optimal sub-
structure, you might be tempted to generate a dynamic-programming solution to aproblem when a greedy solution sufﬁces or, conversely, you might mistakenly think
that a greedy solution works when in fact a dynamic-programming solution is re-
quired. To illustrate the subtleties between the two techniques, let us investigate
two variants of a classical optimization problem.
The0-1 knapsack problem is the following. A thief robbing a store ﬁnds n
items. The ith item is worth /ETB
idollars and weighs wipounds, where /ETBiandwiare
integers. The thief wants to take as valuable a load as possible, but he can carry atmost Wpounds in his knapsack, for some integer W. Which items should he take?
(We call this the 0-1 knapsack problem because for each item, the thief must either426 Chapter 16 Greedy Algorithms
take it or leave it behind; he cannot take a fractional amount of an item or take an
item more than once.)
In the fractional knapsack problem , the setup is the same, but the thief can take
fractions of items, rather than having to make a binary (0-1) choice for each item.You can think of an item in the 0-1 knapsack problem as being like a gold ingotand an item in the fractional knapsack problem as more like gold dust.
Both knapsack problems exhibit the optimal-substructure property. For the 0-1
problem, consider the most valuable load that weighs at most Wpounds. If we
remove item jfrom this load, the remaining load must be the most valuable load
weighing at most W/NULw
jthat the thief can take from the n/NUL1original items
excluding j. For the comparable fractional problem, consider that if we remove
a weight wof one item jfrom the optimal load, the remaining load must be the
most valuable load weighing at most W/NULwthat the thief can take from the n/NUL1
original items plus wj/NULwpounds of item j.
Although the problems are similar, we can solve the fractional knapsack problem
by a greedy strategy, but we cannot solve the 0-1 problem by such a strategy. To
solve the fractional problem, we ﬁrst compute the value per pound /ETBi=wifor each
item. Obeying a greedy strategy, the thief begins by taking as much as possible of
the item with the greatest value per pound. If the supply of that item is exhaustedand he can still carry more, he takes as much as possible of the item with the nextgreatest value per pound, and so forth, until he reaches his weight limit W. Thus,
by sorting the items by value per pound, the greedy algorithm runs in O.n lgn/
time. We leave the proof that the fractional knapsack problem has the greedy-
choice property as Exercise 16.2-1.
To see that this greedy strategy does not work for the 0-1 knapsack problem,
consider the problem instance illustrated in Figure 16.2(a). This example has 3
items and a knapsack that can hold 50pounds. Item 1weighs 10pounds and
is worth 60dollars. Item 2weighs 20pounds and is worth 100dollars. Item 3
weighs 30pounds and is worth 120dollars. Thus, the value per pound of item 1is
6dollars per pound, which is greater than the value per pound of either item 2(5
dollars per pound) or item 3(4dollars per pound). The greedy strategy, therefore,
would take item 1ﬁrst. As you can see from the case analysis in Figure 16.2(b),
however, the optimal solution takes items 2and3, leaving item 1behind. The two
possible solutions that take item 1are both suboptimal.
For the comparable fractional problem, however, the greedy strategy, which
takes item 1ﬁrst, does yield an optimal solution, as shown in Figure 16.2(c). Tak-
ing item 1doesn’t work in the 0-1 problem because the thief is unable to ﬁll his
knapsack to capacity, and the empty space lowers the effective value per pound ofhis load. In the 0-1 problem, when we consider whether to include an item in theknapsack, we must compare the solution to the subproblem that includes the itemwith the solution to the subproblem that excludes the item before we can make the16.2 Elements of the greedy strategy 427
10
$60item 120
$100item 2
30
$120item 3
50
knapsack
(a)+$120
$100
= $220+
$60$100
= $160+
$60$120
= $180
(b)+
$60$100
= $240$80
+
(c)2030
1020
1030
102020
30
Figure 16.2 An example showing that the greedy strategy does not work for the 0-1 knapsack
problem. (a)The thief must select a subset of the three items shown whose weight must not exceed
50pounds. (b)The optimal subset includes items 2and3. Any solution with item 1is suboptimal,
even though item 1has the greatest value per pound. (c)For the fractional knapsack problem, taking
the items in order of greatest value per pound yields an optimal solution.
choice. The problem formulated in this way gives rise to many overlapping sub-
problems—a hallmark of dynamic programming, and indeed, as Exercise 16.2-2
asks you to show, we can use dynamic programming to solve the 0-1 problem.
Exercises
16.2-1
Prove that the fractional knapsack problem has the greedy-choice property.
16.2-2
Give a dynamic-programming solution to the 0-1 knapsack problem that runs inO.nW / time, where nis the number of items and Wis the maximum weight of
items that the thief can put in his knapsack.
16.2-3
Suppose that in a 0-1 knapsack problem, the order of the items when sorted byincreasing weight is the same as their order when sorted by decreasing value. Givean efﬁcient algorithm to ﬁnd an optimal solution to this variant of the knapsackproblem, and argue that your algorithm is correct.
16.2-4
Professor Gekko has always dreamed of inline skating across North Dakota. Heplans to cross the state on highway U.S. 2, which runs from Grand Forks, on theeastern border with Minnesota, to Williston, near the western border with Montana.428 Chapter 16 Greedy Algorithms
The professor can carry two liters of water, and he can skate mmiles before running
out of water. (Because North Dakota is relatively ﬂat, the professor does not haveto worry about drinking water at a greater rate on uphill sections than on ﬂat ordownhill sections.) The professor will start in Grand Forks with two full liters ofwater. His ofﬁcial North Dakota state map shows all the places along U.S. 2 atwhich he can reﬁll his water and the distances between these locations.
The professor’s goal is to minimize the number of water stops along his route
across the state. Give an efﬁcient method by which he can determine which water
stops he should make. Prove that your strategy yields an optimal solution, and give
its running time.
16.2-5
Describe an efﬁcient algorithm that, given a set fx
1;x2;:::;x ngof points on the
real line, determines the smallest set of unit-length closed intervals that containsall of the given points. Argue that your algorithm is correct.
16.2-6 ?
Show how to solve the fractional knapsack problem in O.n/ time.
16.2-7
Suppose you are given two sets AandB, each containing npositive integers. You
can choose to reorder each set however you like. After reordering, let a
ibe the ith
element of set A,a n dl e t bibe the ith element of set B. You then receive a payoff
ofQn
iD1aibi. Give an algorithm that will maximize your payoff. Prove that your
algorithm maximizes the payoff, and state its running time.
16.3 Huffman codes
Huffman codes compress data very effectively: savings of 20% to 90% are typical,depending on the characteristics of the data being compressed. We consider thedata to be a sequence of characters. Huffman’s greedy algorithm uses a table givinghow often each character occurs (i.e., its frequency) to build up an optimal way ofrepresenting each character as a binary string.
Suppose we have a 100,000-character data ﬁle that we wish to store compactly.
We observe that the characters in the ﬁle occur with the frequencies given by Fig-
ure 16.3. That is, only 6different characters appear, and the character aoccurs
45,000 times.
We have many options for how to represent such a ﬁle of information. Here,
we consider the problem of designing a binary character code (orcode for short)16.3 Huffman codes 429
abcd e f
Frequency (in thousands) 45 13 12 16 9 5
Fixed-length codeword 000 001 010 011 100 101
Variable-length codeword 0 101 100 111 1101 1100
Figure 16.3 A character-coding problem. A data ﬁle of 100,000 characters contains only the char-
acters a–f, with the frequencies indicated. If we assign each character a 3-bit codeword, we can
encode the ﬁle in 300,000 bits. Using the variable-length code shown, we can encode the ﬁle in only
224,000 bits.
in which each character is represented by a unique binary string, which we call a
codeword .I fw eu s ea ﬁxed-length code , we need 3bits to represent 6characters:
a= 000, b= 001, ...,f= 101. This method requires 300,000 bits to code the
entire ﬁle. Can we do better?
Avariable-length code can do considerably better than a ﬁxed-length code, by
giving frequent characters short codewords and infrequent characters long code-words. Figure 16.3 shows such a code; here the 1-bit string 0represents a,a n dt h e
4-bit string 1100 represents f. This code requires
.45/SOH1C13/SOH3C12/SOH3C16/SOH3C9/SOH4C5/SOH4//SOH1,000D224,000 bits
to represent the ﬁle, a savings of approximately 25%. In fact, this is an optimal
character code for this ﬁle, as we shall see.
Preﬁx codes
We consider here only codes in which no codeword is also a preﬁx of some other
codeword. Such codes are called preﬁx codes .
3Although we won’t prove it here, a
preﬁx code can always achieve the optimal data compression among any charactercode, and so we suffer no loss of generality by restricting our attention to preﬁxcodes.
Encoding is always simple for any binary character code; we just concatenate the
codewords representing each character of the ﬁle. For example, with the variable-length preﬁx code of Figure 16.3, we code the 3-character ﬁle abc as0/SOH101/SOH100D
0101100 ,w h e r e“/SOH” denotes concatenation.
Preﬁx codes are desirable because they simplify decoding. Since no codeword
is a preﬁx of any other, the codeword that begins an encoded ﬁle is unambiguous.We can simply identify the initial codeword, translate it back to the original char-
3Perhaps “preﬁx-free codes” would be a better name, but the term “preﬁx codes” is standard in the
literature.430 Chapter 16 Greedy Algorithms
a:45 b:13 c:12 d:16 e:9 f:558 28 1486 14100
01 01 0101 001
e:9 f:514
01c:12 b:1325
01
d:1630
0155
01a:45100
01
(a) (b)
Figure 16.4 Trees corresponding to the coding schemes in Figure 16.3. Each leaf is labeled with
a character and its frequency of occurrence. Each internal node is labeled with the sum of the fre-
quencies of the leaves in its subtree. (a)The tree corresponding to the ﬁxed-length code a= 000, ...,
f= 101. (b)The tree corresponding to the optimal preﬁx code a=0 ,b= 101, ...,f= 1100.
acter, and repeat the decoding process on the remainder of the encoded ﬁle. In our
example, the string 001011101 parses uniquely as 0/SOH0/SOH101/SOH1101 , which decodes
toaabe .
The decoding process needs a convenient representation for the preﬁx code so
that we can easily pick off the initial codeword. A binary tree whose leaves arethe given characters provides one such representation. We interpret the binary
codeword for a character as the simple path from the root to that character, where 0
means “go to the left child” and 1means “go to the right child.” Figure 16.4 shows
the trees for the two codes of our example. Note that these are not binary search
trees, since the leaves need not appear in sorted order and internal nodes do not
contain character keys.
An optimal code for a ﬁle is always represented by a fullbinary tree, in which
every nonleaf node has two children (see Exercise 16.3-2). The ﬁxed-length codein our example is not optimal since its tree, shown in Figure 16.4(a), is not a full bi-n a r yt r e e : i tc o n t a i n sc o d e w o r d sb e g i n n i n g1 0 ...,b u t none beginning 11 .... S i n c e
we can now restrict our attention to full binary trees, we can say that if Cis the
alphabet from which the characters are drawn and all character frequencies are pos-itive, then the tree for an optimal preﬁx code has exactly jCjleaves, one for each
letter of the alphabet, and exactly jCj/NUL1internal nodes (see Exercise B.5-3).
Given a tree Tcorresponding to a preﬁx code, we can easily compute the number
of bits required to encode a ﬁle. For each character cin the alphabet C,l e tt h e
attribute c:freqdenote the frequency of cin the ﬁle and let d
T.c/denote the depth16.3 Huffman codes 431
ofc’s leaf in the tree. Note that dT.c/is also the length of the codeword for
character c. The number of bits required to encode a ﬁle is thus
B.T/DX
c2Cc:freq/SOHdT.c/ ; (16.4)
which we deﬁne as the costof the tree T.
Constructing a Huffman code
Huffman invented a greedy algorithm that constructs an optimal preﬁx code called
aHuffman code . In line with our observations in Section 16.2, its proof of cor-
rectness relies on the greedy-choice property and optimal substructure. Ratherthan demonstrating that these properties hold and then developing pseudocode, wepresent the pseudocode ﬁrst. Doing so will help clarify how the algorithm makes
greedy choices.
In the pseudocode that follows, we assume that Cis a set of ncharacters and
that each character c2Cis an object with an attribute c:freqgiving its frequency.
The algorithm builds the tree Tcorresponding to the optimal code in a bottom-up
manner. It begins with a set of jCjleaves and performs a sequence of jCj/NUL1
“merging” operations to create the ﬁnal tree. The algorithm uses a min-priorityqueue Q,k e y e do nt h e freqattribute, to identify the two least-frequent objects to
merge together. When we merge two objects, the result is a new object whosefrequency is the sum of the frequencies of the two objects that were merged.
H
UFFMAN .C /
1nDjCj
2QDC
3foriD1ton/NUL1
4 allocate a new node ´
5 ´:leftDxDEXTRACT -MIN.Q/
6 ´:rightDyDEXTRACT -MIN.Q/
7 ´:freqDx:freqCy:freq
8I NSERT .Q; ´/
9return EXTRACT -MIN.Q/ //return the root of the tree
For our example, Huffman’s algorithm proceeds as shown in Figure 16.5. Since
the alphabet contains 6letters, the initial queue size is nD6,a n d 5merge steps
build the tree. The ﬁnal tree represents the optimal preﬁx code. The codeword fora letter is the sequence of edge labels on the simple path from the root to the letter.
Line 2 initializes the min-priority queue Qwith the characters in C.T h e for
loop in lines 3–8 repeatedly extracts the two nodes xandyof lowest frequency432 Chapter 16 Greedy Algorithms
e:9 f:514
01c:12 b:1325
01
d:1630
0155
01a:45100
01
e:9 f:514
01c:12 b:1325
01
d:1630
0155
01a:45e:9 f:514
01c:12 b:1325
01
d:1630
01a:45
e:9 f:514
01
c:12 b:1325
01d:16 a:45e:9 f:514
01c:12 b:13 d:16 a:45 e:9 f:5 c:12 b:13 d:16 a:45 (a)
(c)
(e)(b)
(d)
(f)
Figure 16.5 The steps of Huffman’s algorithm for the frequencies given in Figure 16.3. Each part
shows the contents of the queue sorted into increasing order by frequency. At each step, the two
trees with lowest frequencies are merged. Leaves are shown as rectangles containing a character
and its frequency. Internal nodes are shown as circles containing the sum of the frequencies of their
children. An edge connecting an internal node with its children is labeled 0if it is an edge to a left
child and 1if it is an edge to a right child. The codeword for a letter is the sequence of labels on the
edges connecting the root to the leaf for that letter. (a)The initial set of nD6nodes, one for each
letter. (b)–(e) Intermediate stages. (f)The ﬁnal tree.
from the queue, replacing them in the queue with a new node ´representing their
merger. The frequency of ´is computed as the sum of the frequencies of xandy
in line 7. The node ´hasxas its left child and yas its right child. (This order is
arbitrary; switching the left and right child of any node yields a different code ofthe same cost.) After n/NUL1mergers, line 9 returns the one node left in the queue,
which is the root of the code tree.
Although the algorithm would produce the same result if we were to excise the
variables xandy—assigning directly to ´:leftand´:right in lines 5 and 6, and
changing line 7 to ´:freqD´:left:freqC´:right:freq—we shall use the node16.3 Huffman codes 433
names xandyin the proof of correctness. Therefore, we ﬁnd it convenient to
leave them in.
To analyze the running time of Huffman’s algorithm, we assume that Qis im-
plemented as a binary min-heap (see Chapter 6). For a set Cofncharacters, we
can initialize Qin line 2 in O.n/ time using the B UILD -MIN-HEAP procedure dis-
cussed in Section 6.3. The forloop in lines 3–8 executes exactly n/NUL1times, and
since each heap operation requires time O.lgn/, the loop contributes O.n lgn/to
the running time. Thus, the total running time of H UFFMAN on a set of ncharac-
ters is O.n lgn/. We can reduce the running time to O.n lg lgn/by replacing the
binary min-heap with a van Emde Boas tree (see Chapter 20).
Correctness of Huffman’s algorithm
To prove that the greedy algorithm H UFFMAN is correct, we show that the prob-
lem of determining an optimal preﬁx code exhibits the greedy-choice and optimal-substructure properties. The next lemma shows that the greedy-choice propertyholds.
Lemma 16.2
LetCbe an alphabet in which each character c2Chas frequency c:freq.L e t
xandybe two characters in Chaving the lowest frequencies. Then there exists
an optimal preﬁx code for Cin which the codewords for xandyhave the same
length and differ only in the last bit.
Proof The idea of the proof is to take the tree Trepresenting an arbitrary optimal
preﬁx code and modify it to make a tree representing another optimal preﬁx codesuch that the characters xandyappear as sibling leaves of maximum depth in the
new tree. If we can construct such a tree, then the codewords for xandywill have
the same length and differ only in the last bit.
Letaandbbe two characters that are sibling leaves of maximum depth in T.
Without loss of generality, we assume that a:freq/DC4b:freqandx:freq/DC4y:freq.
Since x:freq andy:freq are the two lowest leaf frequencies, in order, and a:freq
andb:freq are two arbitrary frequencies, in order, we have x:freq/DC4a:freq and
y:freq/DC4b:freq.
In the remainder of the proof, it is possible that we could have x:freqDa:freq
ory:freqDb:freq. However, if we had x:
freqDb:freq, then we would also have
a:freqDb:freqDx:freqDy:freq(see Exercise 16.3-1), and the lemma would
be trivially true. Thus, we will assume that x:freq¤b:freq, which means that
x¤b.
As Figure 16.6 shows, we exchange the positions in Tofaandxto produce a
treeT0, and then we exchange the positions in T0ofbandyto produce a tree T00434 Chapter 16 Greedy Algorithms
x
y
ab xya
b xya
bT′′ T T′
Figure 16.6 An illustration of the key step in the proof of Lemma 16.2. In the optimal tree T,
leaves aandbare two siblings of maximum depth. Leaves xandyare the two characters with the
lowest frequencies; they appear in arbitrary positions in T. Assuming that x¤b, swapping leaves a
andxproduces tree T0, and then swapping leaves bandyproduces tree T00. Since each swap does
not increase the cost, the resulting tree T00is also an optimal tree.
in which xandyare sibling leaves of maximum depth. (Note that if xDbbut
y¤a, then tree T00does not have xandyas sibling leaves of maximum depth.
Because we assume that x¤b, this situation cannot occur.) By equation (16.4),
the difference in cost between TandT0is
B.T //NULB.T0/
DX
c2Cc:freq/SOHdT.c//NULX
c2Cc:freq/SOHdT0.c/
Dx:freq/SOHdT.x/Ca:freq/SOHdT.a//NULx:freq/SOHdT0.x//NULa:freq/SOHdT0.a/
Dx:freq/SOHdT.x/Ca:freq/SOHdT.a//NULx:freq/SOHdT.a//NULa:freq/SOHdT.x/
D.a:freq/NULx:freq/.dT.a//NULdT.x//
/NAK0;
because both a:freq/NULx:freqanddT.a//NULdT.x/are nonnegative. More speciﬁ-
cally, a:freq/NULx:freqis nonnegative because xis a minimum-frequency leaf, and
dT.a//NULdT.x/is nonnegative because ais a leaf of maximum depth in T. Similarly,
exchanging yandbdoes not increase the cost, and so B.T0//NULB.T00/is nonnega-
tive. Therefore, B.T00//DC4B.T / , and since Tis optimal, we have B.T //DC4B.T00/,
which implies B.T00/DB.T/ . Thus, T00is an optimal tree in which xandy
appear as sibling leaves of maximum depth, from which the lemma follows.
Lemma 16.2 implies that the process of building up an optimal tree by mergers
can, without loss of generality, begin with the greedy choice of merging togetherthose two characters of lowest frequency. Why is this a greedy choice? We canview the cost of a single merger as being the sum of the frequencies of the two items
being merged. Exercise 16.3-4 shows that the total cost of the tree constructed
equals the sum of the costs of its mergers. Of all possible mergers at each step,
H
UFFMAN chooses the one that incurs the least cost.16.3 Huffman codes 435
The next lemma shows that the problem of constructing optimal preﬁx codes has
the optimal-substructure property.
Lemma 16.3
LetCbe a given alphabet with frequency c:freqdeﬁned for each character c2C.
Letxandybe two characters in Cwith minimum frequency. Let C0be the
alphabet Cwith the characters xandyremoved and a new character ´added,
so that C0DC/NULfx;yg[f´g.D e ﬁ n e fforC0as for C, except that
´:freqDx:freqCy:freq.L e t T0be any tree representing an optimal preﬁx code
for the alphabet C0. Then the tree T, obtained from T0by replacing the leaf node
for´with an internal node having xandyas children, represents an optimal preﬁx
code for the alphabet C.
Proof We ﬁrst show how to express the cost B.T/ of tree Tin terms of the
costB.T0/of tree T0, by considering the component costs in equation (16.4).
For each character c2C/NULfx;yg,w eh a v et h a t dT.c/DdT0.c/, and hence
c:freq/SOHdT.c/Dc:freq/SOHdT0.c/.S i n c e dT.x/DdT.y/DdT0.´/C1,w eh a v e
x:freq/SOHdT.x/Cy:freq/SOHdT.y/D.x:freqCy:freq/.dT0.´/C1/
D´:freq/SOHdT0.´/C.x:freqCy:freq/;
from which we conclude that
B.T/DB.T0/Cx:freqCy:freq
or, equivalently,B.T
0/DB.T //NULx:freq/NULy:freq:
We now prove the lemma by contradiction. Suppose that Tdoes not repre-
sent an optimal preﬁx code for C. Then there exists an optimal tree T00such that
B.T00/<B . T/ . Without loss of generality (by Lemma 16.2), T00hasxandyas
siblings. Let T000be the tree T00with the common parent of xandyreplaced by a
leaf´with frequency ´:freqDx:freqCy:freq.T h e n
B.T000/DB.T00//NULx:freq/NULy:freq
<B . T //NULx:freq/NULy:freq
DB.T0/;
yielding a contradiction to the assumption that T0represents an optimal preﬁx code
forC0. Thus, Tmust represent an optimal preﬁx code for the alphabet C.
Theorem 16.4
Procedure H UFFMAN produces an optimal preﬁx code.
Proof Immediate from Lemmas 16.2 and 16.3.
436 Chapter 16 Greedy Algorithms
Exercises
16.3-1
Explain why, in the proof of Lemma 16.2, if x:freqDb:freq, then we must have
a:freqDb:freqDx:freqDy:freq.
16.3-2
Prove that a binary tree that is not full cannot correspond to an optimal preﬁx code.
16.3-3
What is an optimal Huffman code for the following set of frequencies, based onthe ﬁrst 8 Fibonacci numbers?
a:1b:1c:2d:3e:5f:8g:13h:21
Can you generalize your answer to ﬁnd the optimal code when the frequencies are
the ﬁrst nFibonacci numbers?
16.3-4
Prove that we can also express the total cost of a tree for a code as the sum, overall internal nodes, of the combined frequencies of the two children of the node.
16.3-5
Prove that if we order the characters in an alphabet so that their frequencies
are monotonically decreasing, then there exists an optimal code whose codeword
lengths are monotonically increasing.
16.3-6
Suppose we have an optimal preﬁx code on a set CDf0; 1; : : : ; n/NUL1gof charac-
ters and we wish to transmit this code using as few bits as possible. Show how torepresent any optimal preﬁx code on Cusing only 2n/NUL1Cndlgnebits. ( Hint:
Use2n/NUL1bits to specify the structure of the tree, as discovered by a walk of the
tree.)
16.3-7
Generalize Huffman’s algorithm to ternary codewords (i.e., codewords using thesymbols 0,1,a n d 2), and prove that it yields optimal ternary codes.
16.3-8
Suppose that a data ﬁle contains a sequence of 8-bit characters such that all 256characters are about equally common: the maximum character frequency is lessthan twice the minimum character frequency. Prove that Huffman coding in this
case is no more efﬁcient than using an ordinary 8-bit ﬁxed-length code.16.4 Matroids and greedy methods 437
16.3-9
Show that no compression scheme can expect to compress a ﬁle of randomly cho-sen 8-bit characters by even a single bit. ( Hint: Compare the number of possible
ﬁles with the number of possible encoded ﬁles.)
?16.4 Matroids and greedy methods
In this section, we sketch a beautiful theory about greedy algorithms. This theorydescribes many situations in which the greedy method yields optimal solutions. Itinvolves combinatorial structures known as “matroids.” Although this theory doesnot cover all cases for which a greedy method applies (for example, it does notcover the activity-selection problem of Section 16.1 or the Huffman-coding prob-lem of Section 16.3), it does cover many cases of practical interest. Furthermore,this theory has been extended to cover many applications; see the notes at the endof this chapter for references.
Matroids
Amatroid is an ordered pair MD.S;/TAB/satisfying the following conditions.
1.Sis a ﬁnite set.
2./TABis a nonempty family of subsets of S, called the independent subsets of S,
such that if B2/TABandA/DC2B,t h e n A2/TAB. We say that /TABishereditary if it
satisﬁes this property. Note that the empty set ;is necessarily a member of /TAB.
3. If A2/TAB,B2/TAB,a n djAj<jBj, then there exists some element x2B/NULA
such that A[fxg2/TAB. We say that Msatisﬁes the exchange property .
The word “matroid” is due to Hassler Whitney. He was studying matric ma-
troids , in which the elements of Sare the rows of a given matrix and a set of rows is
independent if they are linearly independent in the usual sense. As Exercise 16.4-2asks you to show, this structure deﬁnes a matroid.
As another example of matroids, consider the graphic matroid M
GD.SG;/TABG/
deﬁned in terms of a given undirected graph GD.V; E/ as follows:
/SIThe set SGis deﬁned to be E, the set of edges of G.
/SIIfAis a subset of E,t h e n A2/TABGif and only if Ais acyclic. That is, a set of
edges Ais independent if and only if the subgraph GAD.V; A/ forms a forest.
The graphic matroid MGis closely related to the minimum-spanning-tree problem,
which Chapter 23 covers in detail.438 Chapter 16 Greedy Algorithms
Theorem 16.5
IfGD.V; E/ is an undirected graph, then MGD.SG;/TABG/is a matroid.
Proof Clearly, SGDEis a ﬁnite set. Furthermore, /TABGis hereditary, since a
subset of a forest is a forest. Putting it another way, removing edges from an
acyclic set of edges cannot create cycles.
Thus, it remains to show that MGsatisﬁes the exchange property. Suppose that
GAD.V; A/ andGBD.V; B/ are forests of Gand thatjBj>jAj.T h a t i s , A
andBare acyclic sets of edges, and Bcontains more edges than Adoes.
We claim that a forest FD.VF;EF/contains exactlyjVFj/NULjEFjtrees. To
see why, suppose that Fconsists of ttrees, where the ith tree contains /ETBivertices
andeiedges. Then, we have
jEFjDtX
iD1ei
DtX
iD1./ETBi/NUL1/(by Theorem B.2)
DtX
iD1/ETBi/NULt
DjVFj/NULt;
which implies that tDjVFj/NULjEFj. Thus, forest GAcontainsjVj/NULjAjtrees, and
forest GBcontainsjVj/NULjBjtrees.
Since forest GBhas fewer trees than forest GAdoes, forest GBmust contain
some tree Twhose vertices are in two different trees in forest GA. Moreover,
since Tis connected, it must contain an edge .u; /ETB/ such that vertices uand/ETB
are in different trees in forest GA. Since the edge .u; /ETB/ connects vertices in two
different trees in forest GA, we can add the edge .u; /ETB/ to forest GAwithout creating
a cycle. Therefore, MGsatisﬁes the exchange property, completing the proof that
MGis a matroid.
Given a matroid MD.S;/TAB/, we call an element x…Aanextension ofA2/TAB
if we can add xtoAwhile preserving independence; that is, xis an extension
ofAifA[fxg2/TAB. As an example, consider a graphic matroid MG.I fAis an
independent set of edges, then edge eis an extension of Aif and only if eis not
inAand the addition of etoAdoes not create a cycle.
IfAis an independent subset in a matroid M, we say that Aismaximal if it has
no extensions. That is, Ais maximal if it is not contained in any larger independent
subset of M. The following property is often useful.16.4 Matroids and greedy methods 439
Theorem 16.6
All maximal independent subsets in a matroid have the same size.
Proof Suppose to the contrary that Ais a maximal independent subset of M
and there exists another larger maximal independent subset BofM. Then, the
exchange property implies that for some x2B/NULA, we can extend Ato a larger
independent set A[fxg, contradicting the assumption that Ais maximal.
As an illustration of this theorem, consider a graphic matroid MGfor a con-
nected, undirected graph G. Every maximal independent subset of MGmust be a
free tree with exactly jVj/NUL1edges that connects all the vertices of G. Such a tree
is called a spanning tree ofG.
We say that a matroid MD.S;/TAB/isweighted if it is associated with a weight
function wthat assigns a strictly positive weight w.x/ to each element x2S.T h e
weight function wextends to subsets of Sby summation:
w.A/DX
x2Aw.x/
for any A/DC2S. For example, if we let w.e/ denote the weight of an edge ein a
graphic matroid MG,t h e n w.A/ is the total weight of the edges in edge set A.
Greedy algorithms on a weighted matroid
Many problems for which a greedy approach provides optimal solutions can be for-
mulated in terms of ﬁnding a maximum-weight independent subset in a weightedmatroid. That is, we are given a weighted matroid MD.S;/TAB/, and we wish to
ﬁnd an independent set A2/TABsuch that w.A/ is maximized. We call such a sub-
set that is independent and has maximum possible weight an optimal subset of the
matroid. Because the weight w.x/ of any element x2Sis positive, an optimal
subset is always a maximal independent subset—it always helps to make Aas large
as possible.
For example, in the minimum-spanning-tree problem , we are given a connected
undirected graph GD.V; E/ and a length function wsuch that w.e/ is the (posi-
tive) length of edge e. (We use the term “length” here to refer to the original edge
weights for the graph, reserving the term “weight” to refer to the weights in theassociated matroid.) We wish to ﬁnd a subset of the edges that connects all ofthe vertices together and has minimum total length. To view this as a problem ofﬁnding an optimal subset of a matroid, consider the weighted matroid M
Gwith
weight function w0,w h e r e w0.e/Dw0/NULw.e/ andw0is larger than the maximum
length of any edge. In this weighted matroid, all weights are positive and an opti-mal subset is a spanning tree of minimum total length in the original graph. Morespeciﬁcally, each maximal independent subset Acorresponds to a spanning tree440 Chapter 16 Greedy Algorithms
withjVj/NUL1edges, and since
w0.A/DX
e2Aw0.e/
DX
e2A.w0/NULw.e//
D.jVj/NUL1/w 0/NULX
e2Aw.e/
D.jVj/NUL1/w 0/NULw.A/
for any maximal independent subset A, an independent subset that maximizes the
quantity w0.A/must minimize w.A/ . Thus, any algorithm that can ﬁnd an optimal
subset Ain an arbitrary matroid can solve the minimum-spanning-tree problem.
Chapter 23 gives algorithms for the minimum-spanning-tree problem, but here
we give a greedy algorithm that works for any weighted matroid. The algorithmtakes as input a weighted matroid MD.S;/TAB/with an associated positive weight
function w, and it returns an optimal subset A. In our pseudocode, we denote the
components of MbyM:SandM:/TABand the weight function by w. The algorithm
is greedy because it considers in turn each element x2S, in order of monotoni-
cally decreasing weight, and immediately adds it to the set Abeing accumulated if
A[fxgis independent.
G
REEDY .M; w/
1AD;
2s o r t M:Sinto monotonically decreasing order by weight w
3foreachx2M:S, taken in monotonically decreasing order by weight w.x/
4 ifA[fxg2M:/TAB
5 ADA[fxg
6return A
Line 4 checks whether adding each element xtoAwould maintain Aas an inde-
pendent set. If Awould remain independent, then line 5 adds xtoA. Otherwise, x
is discarded. Since the empty set is independent, and since each iteration of the for
loop maintains A’s independence, the subset Ais always independent, by induc-
tion. Therefore, G REEDY always returns an independent subset A. We shall see in
a moment that Ais a subset of maximum possible weight, so that Ais an optimal
subset.
The running time of G REEDY is easy to analyze. Let ndenotejSj. The sorting
phase of G REEDY takes time O.n lgn/. Line 4 executes exactly ntimes, once for
each element of S. Each execution of line 4 requires a check on whether or not
the set A[fxgis independent. If each such check takes time O.f .n// , the entire
algorithm runs in time O.n lgnCnf .n// .16.4 Matroids and greedy methods 441
We now prove that G REEDY returns an optimal subset.
Lemma 16.7 (Matroids exhibit the greedy-choice property)
Suppose that MD.S;/TAB/is a weighted matroid with weight function wand that S
is sorted into monotonically decreasing order by weight. Let xbe the ﬁrst element
ofSsuch thatfxgis independent, if any such xexists. If xexists, then there exists
an optimal subset AofSthat contains x.
Proof If no such xexists, then the only independent subset is the empty set and
the lemma is vacuously true. Otherwise, let Bbe any nonempty optimal subset.
Assume that x…B; otherwise, letting ADBgives an optimal subset of Sthat
contains x.
No element of Bhas weight greater than w.x/ . To see why, observe that y2B
implies thatfygis independent, since B2/TABand/TABis hereditary. Our choice of x
therefore ensures that w.x//NAKw.y/ for any y2B.
Construct the set Aas follows. Begin with ADfxg. By the choice of x, setAis
independent. Using the exchange property, repeatedly ﬁnd a new element of Bthat
we can add to AuntiljAjDjBj, while preserving the independence of A.A tt h a t
point, AandBare the same except that AhasxandBhas some other element y.
That is, ADB/NULfyg[fxgfor some y2B,a n ds o
w.A/Dw.B//NULw.y/Cw.x/
/NAKw.B/ :
Because set Bis optimal, set A, which contains x, must also be optimal.
We next show that if an element is not an option initially, then it cannot be an
option later.
Lemma 16.8
LetMD.S;/TAB/be any matroid. If xis an element of Sthat is an extension of
some independent subset AofS,t h e n xis also an extension of ;.
Proof Since xis an extension of A,w eh a v et h a t A[fxgis independent. Since /TAB
is hereditary,fxgmust be independent. Thus, xis an extension of;.
Corollary 16.9
LetMD.S;/TAB/be any matroid. If xis an element of Ssuch that xis not an
extension of;,t h e n xis not an extension of any independent subset AofS.
Proof This corollary is simply the contrapositive of Lemma 16.8.
442 Chapter 16 Greedy Algorithms
Corollary 16.9 says that any element that cannot be used immediately can never
be used. Therefore, G REEDY cannot make an error by passing over any initial
elements in Sthat are not an extension of ;, since they can never be used.
Lemma 16.10 (Matroids exhibit the optimal-substructure property)
Letxbe the ﬁrst element of Schosen by G REEDY for the weighted matroid
MD.S;/TAB/. The remaining problem of ﬁnding a maximum-weight indepen-
dent subset containing xreduces to ﬁnding a maximum-weight independent subset
of the weighted matroid M0D.S0;/TAB0/,w h e r e
S0Dfy2SWfx;yg2/TABg;
/TAB0DfB/DC2S/NULfxgWB[fxg2/TABg;
and the weight function for M0is the weight function for M, restricted to S0.( W e
callM0thecontraction ofMby the element x.)
Proof IfAis any maximum-weight independent subset of Mcontaining x,t h e n
A0DA/NULfxgis an independent subset of M0. Conversely, any independent sub-
setA0ofM0yields an independent subset ADA0[fxgofM. Since we have in
both cases that w.A/Dw.A0/Cw.x/ , a maximum-weight solution in Mcontain-
ingxyields a maximum-weight solution in M0, and vice versa.
Theorem 16.11 (Correctness of the greedy algorithm on matroids)
IfMD.S;/TAB/is a weighted matroid with weight function w,t h e nG REEDY .M; w/
returns an optimal subset.
Proof By Corollary 16.9, any elements that G REEDY passes over initially be-
cause they are not extensions of ;can be forgotten about, since they can never
be useful. Once G REEDY selects the ﬁrst element x, Lemma 16.7 implies that
the algorithm does not err by adding xtoA, since there exists an optimal subset
containing x. Finally, Lemma 16.10 implies that the remaining problem is one of
ﬁnding an optimal subset in the matroid M0that is the contraction of Mbyx.
After the procedure G REEDY setsAtofxg, we can interpret all of its remaining
steps as acting in the matroid M0D.S0;/TAB0/, because Bis independent in M0if
and only if B[fxgis independent in M, for all sets B2/TAB0. Thus, the subsequent
operation of G REEDY will ﬁnd a maximum-weight independent subset for M0,a n d
the overall operation of G REEDY will ﬁnd a maximum-weight independent subset
forM.
16.5 A task-scheduling problem as a matroid 443
Exercises
16.4-1
Show that .S;/TABk/is a matroid, where Sis any ﬁnite set and /TABkis the set of all
subsets of Sof size at most k,w h e r e k/DC4jSj.
16.4-2 ?
Given an m/STXnmatrix Tover some ﬁeld (such as the reals), show that .S;/TAB/is a
matroid, where Sis the set of columns of TandA2/TABif and only if the columns
inAare linearly independent.
16.4-3 ?
Show that if .S;/TAB/is a matroid, then .S;/TAB0/is a matroid, where
/TAB0DfA0WS/NULA0contains some maximal A2/TABg:
That is, the maximal independent sets of .S;/TAB0/are just the complements of the
maximal independent sets of .S;/TAB/.
16.4-4 ?
LetSbe a ﬁnite set and let S1;S2;:::;S kbe a partition of Sinto nonempty disjoint
subsets. Deﬁne the structure .S;/TAB/by the condition that /TABDfAWjA\Sij/DC41
foriD1 ;2;:::;kg. Show that .S;/TAB/is a matroid. That is, the set of all sets A
that contain at most one member of each subset in the partition determines the
independent sets of a matroid.
16.4-5
Show how to transform the weight function of a weighted matroid problem, where
the desired optimal solution is a minimum-weight maximal independent subset, to
make it a standard weighted-matroid problem. Argue carefully that your transfor-
mation is correct.
?16.5 A task-scheduling problem as a matroid
An interesting problem that we can solve using matroids is the problem of op-timally scheduling unit-time tasks on a single processor, where each task has adeadline, along with a penalty paid if the task misses its deadline. The problemlooks complicated, but we can solve it in a surprisingly simple manner by castingit as a matroid and using a greedy algorithm.
Aunit-time task is a job, such as a program to be run on a computer, that requires
exactly one unit of time to complete. Given a ﬁnite set Sof unit-time tasks, a444 Chapter 16 Greedy Algorithms
schedule forSis a permutation of Sspecifying the order in which to perform
these tasks. The ﬁrst task in the schedule begins at time 0and ﬁnishes at time 1,
the second task begins at time 1and ﬁnishes at time 2, and so on.
The problem of scheduling unit-time tasks with deadlines and penalties for a
single processor has the following inputs:
/SIa setSDfa1;a2;:::;a ngofnunit-time tasks;
/SIa set of ninteger deadlines d1;d2;:::;d n, such that each disatisﬁes 1/DC4di/DC4n
and task aiis supposed to ﬁnish by time di;a n d
/SIa set of nnonnegative weights or penalties w1;w2;:::;w n, such that we incur
a penalty of wiif task aiis not ﬁnished by time di, and we incur no penalty if
a task ﬁnishes by its deadline.
We wish to ﬁnd a schedule for Sthat minimizes the total penalty incurred for
missed deadlines.
Consider a given schedule. We say that a task is latein this schedule if it ﬁnishes
after its deadline. Otherwise, the task is early in the schedule. We can always trans-
form an arbitrary schedule into early-ﬁrst form , in which the early tasks precede
the late tasks. To see why, note that if some early task aifollows some late task aj,
then we can switch the positions of aiandaj,a n d aiwill still be early and ajwill
still be late.
Furthermore, we claim that we can always transform an arbitrary schedule into
canonical form , in which the early tasks precede the late tasks and we schedule
the early tasks in order of monotonically increasing deadlines. To do so, we putthe schedule into early-ﬁrst form. Then, as long as there exist two early tasks a
i
andajﬁnishing at respective times kandkC1in the schedule such that dj<d i,
we swap the positions of aiandaj.S i n c e ajis early before the swap, kC1/DC4dj.
Therefore, kC1<d i,a n ds o aiis still early after the swap. Because task ajis
moved earlier in the schedule, it remains early after the swap.
The search for an optimal schedule thus reduces to ﬁnding a set Aof tasks that
we assign to be early in the optimal schedule. Having determined A, we can create
the actual schedule by listing the elements of Ain order of monotonically increas-
ing deadlines, then listing the late tasks (i.e., S/NULA) in any order, producing a
canonical ordering of the optimal schedule.
We say that a set Aof tasks is independent if there exists a schedule for these
tasks such that no tasks are late. Clearly, the set of early tasks for a schedule forms
an independent set of tasks. Let /TABdenote the set of all independent sets of tasks.
Consider the problem of determining whether a given set Aof tasks is indepen-
dent. For tD0; 1; 2; : : : ; n ,l e tNt.A/denote the number of tasks in Awhose
deadline is tor earlier. Note that N0.A/D0for any set A.16.5 A task-scheduling problem as a matroid 445
Lemma 16.12
For any set of tasks A, the following statements are equivalent.
1. The set Ais independent.
2. For tD0; 1; 2; : : : ; n ,w eh a v e Nt.A//DC4t.
3. If the tasks in Aare scheduled in order of monotonically increasing deadlines,
then no task is late.
Proof To show that (1) implies (2), we prove the contrapositive: if Nt.A/ > t for
some t, then there is no way to make a schedule with no late tasks for set A, because
more than ttasks must ﬁnish before time t. Therefore, (1) implies (2). If (2) holds,
then (3) must follow: there is no way to “get stuck” when scheduling the tasks inorder of monotonically increasing deadlines, since (2) implies that the ith largest
deadline is at least i. Finally, (3) trivially implies (1).
Using property 2 of Lemma 16.12, we can easily compute whether or not a given
set of tasks is independent (see Exercise 16.5-2).
The problem of minimizing the sum of the penalties of the late tasks is the same
as the problem of maximizing the sum of the penalties of the early tasks. Thefollowing theorem thus ensures that we can use the greedy algorithm to ﬁnd anindependent set Aof tasks with the maximum total penalty.
Theorem 16.13
IfSis a set of unit-time tasks with deadlines, and /TABis the set of all independent
sets of tasks, then the corresponding system .S;/TAB/is a matroid.
Proof Every subset of an independent set of tasks is certainly independent. To
prove the exchange property, suppose that BandAare independent sets of tasks
and thatjBj>jAj.L e t kbe the largest tsuch that N
t.B//DC4Nt.A/. (Such a value
oftexists, since N0.A/DN0.B/D0.) Since Nn.B/DjBjandNn.A/DjAj,
butjBj>jAj, we must have that k<n and that Nj.B/ > N j.A/for all jin
the range kC1/DC4j/DC4n. Therefore, Bcontains more tasks with deadline kC1
thanAdoes. Let aibe a task in B/NULAwith deadline kC1.L e t A0DA[faig.
We now show that A0must be independent by using property 2 of Lemma 16.12.
For0/DC4t/DC4k,w eh a v e Nt.A0/DNt.A//DC4t,s i n c e Ais independent. For
k<t/DC4n,w eh a v e Nt.A0//DC4Nt.B//DC4t,s i n c e Bis independent. Therefore, A0
is independent, completing our proof that .S;/TAB/is a matroid.
By Theorem 16.11, we can use a greedy algorithm to ﬁnd a maximum-weight
independent set of tasks A. We can then create an optimal schedule having the
tasks in Aas its early tasks. This method is an efﬁcient algorithm for scheduling446 Chapter 16 Greedy Algorithms
Task
ai
 1234567
di
 4243146
wi
 70 60 50 40 30 20 10
Figure 16.7 An instance of the problem of scheduling unit-time tasks with deadlines and penalties
for a single processor.
unit-time tasks with deadlines and penalties for a single processor. The running
time is O.n2/using G REEDY , since each of the O.n/ independence checks made
by that algorithm takes time O.n/ (see Exercise 16.5-2). Problem 16-4 gives a
faster implementation.
Figure 16.7 demonstrates an example of the problem of scheduling unit-time
tasks with deadlines and penalties for a single processor. In this example, thegreedy algorithm selects, in order, tasks a
1,a2,a3,a n d a4, then rejects a5(because
N4.fa1;a2;a3;a4;a5g/D5)a n d a6(because N4.fa1;a2;a3;a4;a6g/D5), and
ﬁnally accepts a7. The ﬁnal optimal schedule is
ha2;a4;a1;a3;a7;a5;a6i;
which has a total penalty incurred of w5Cw6D50.
Exercises
16.5-1
Solve the instance of the scheduling problem given in Figure 16.7, but with each
penalty wireplaced by 80/NULwi.
16.5-2
Show how to use property 2 of Lemma 16.12 to determine in time O.jAj/whether
or not a given set Aof tasks is independent.
Problems
16-1 Coin changing
Consider the problem of making change for ncents using the fewest number of
coins. Assume that each coin’s value is an integer.
a.Describe a greedy algorithm to make change consisting of quarters, dimes,
nickels, and pennies. Prove that your algorithm yields an optimal solution.Problems for Chapter 16 447
b.Suppose that the available coins are in the denominations that are powers of c,
i.e., the denominations are c0;c1;:::;ckfor some integers c>1 andk/NAK1.
Show that the greedy algorithm always yields an optimal solution.
c.Give a set of coin denominations for which the greedy algorithm does not yield
an optimal solution. Your set should include a penny so that there is a solutionfor every value of n.
d.Give an O.nk/ -time algorithm that makes change for any set of kdifferent coin
denominations, assuming that one of the coins is a penny.
16-2 Scheduling to minimize average completion time
Suppose you are given a set SDfa
1;a2;:::;a ngof tasks, where task aire-
quires piunits of processing time to complete, once it has started. You have one
computer on which to run these tasks, and the computer can run only one task at atime. Let c
ibe the completion time of task ai, that is, the time at which task aicom-
pletes processing. Your goal is to minimize the average completion time, that is,to minimize .1=n/P
n
iD1ci. For example, suppose there are two tasks, a1anda2,
withp1D3andp2D5, and consider the schedule in which a2runs ﬁrst, followed
bya1.T h e n c2D5,c1D8, and the average completion time is .5C8/=2D6:5.
If task a1runs ﬁrst, however, then c1D3,c2D8, and the average completion
time is .3C8/=2D5:5.
a.Give an algorithm that schedules the tasks so as to minimize the average com-
pletion time. Each task must run non-preemptively, that is, once task aistarts, it
must run continuously for piunits of time. Prove that your algorithm minimizes
the average completion time, and state the running time of your algorithm.
b.Suppose now that the tasks are not all available at once. That is, each task
cannot start until its release time ri. Suppose also that we allow preemption ,s o
that a task can be suspended and restarted at a later time. For example, a task ai
with processing time piD6and release time riD1might start running at
time 1 and be preempted at time 4. It might then resume at time 10 but bepreempted at time 11, and it might ﬁnally resume at time 13 and complete attime 15. Task a
ihas run for a total of 6 time units, but its running time has been
divided into three pieces. In this scenario, ai’s completion time is 15.G i v e
an algorithm that schedules the tasks so as to minimize the average completiontime in this new scenario. Prove that your algorithm minimizes the average
completion time, and state the running time of your algorithm.448 Chapter 16 Greedy Algorithms
16-3 Acyclic subgraphs
a.Theincidence matrix for an undirected graph GD.V; E/ is ajVj/STXjEjma-
trixMsuch that M/ETBeD1if edge eis incident on vertex /ETB,a n d M/ETBeD0other-
wise. Argue that a set of columns of Mis linearly independent over the ﬁeld
of integers modulo 2 if and only if the corresponding set of edges is acyclic.Then, use the result of Exercise 16.4-2 to provide an alternate proof that .E;/TAB/
of part (a) is a matroid.
b.Suppose that we associate a nonnegative weight w.e/ with each edge in an
undirected graph GD.V; E/ . Give an efﬁcient algorithm to ﬁnd an acyclic
subset of Eof maximum total weight.
c.LetG.V; E/ be an arbitrary directed graph, and let .E;/TAB/be deﬁned so that
A2/TABif and only if Adoes not contain any directed cycles. Give an example
of a directed graph Gsuch that the associated system .E;/TAB/is not a matroid.
Specify which deﬁning condition for a matroid fails to hold.
d.Theincidence matrix for a directed graph GD.V; E/ with no self-loops is a
jVj/STXjEjmatrix Msuch that M
/ETBeD/NUL1if edge eleaves vertex /ETB,M/ETBeD1if
edge eenters vertex /ETB,a n d M/ETBeD0otherwise. Argue that if a set of columns
ofMis linearly independent, then the corresponding set of edges does not
contain a directed cycle.
e.Exercise 16.4-2 tells us that the set of linearly independent sets of columns of
any matrix Mforms a matroid. Explain carefully why the results of parts (d)
and (e) are not contradictory. How can there fail to be a perfect correspon-dence between the notion of a set of edges being acyclic and the notion of theassociated set of columns of the incidence matrix being linearly independent?
16-4 Scheduling variations
Consider the following algorithm for the problem from Section 16.5 of schedulingunit-time tasks with deadlines and penalties. Let all ntime slots be initially empty,
where time slot iis the unit-length slot of time that ﬁnishes at time i. We consider
the tasks in order of monotonically decreasing penalty. When considering task a
j,
if there exists a time slot at or before aj’s deadline djthat is still empty, assign aj
to the latest such slot, ﬁlling it. If there is no such slot, assign task ajto the latest
of the as yet unﬁlled slots.
a.Argue that this algorithm always gives an optimal answer.
b.Use the fast disjoint-set forest presented in Section 21.3 to implement the algo-
rithm efﬁciently. Assume that the set of input tasks has already been sorted intoProblems for Chapter 16 449
monotonically decreasing order by penalty. Analyze the running time of your
implementation.
16-5 Off-line caching
Modern computers use a cache to store a small amount of data in a fast memory.Even though a program may access large amounts of data, by storing a small subsetof the main memory in the cache —a small but faster memory—overall access time
can greatly decrease. When a computer program executes, it makes a sequence
hr
1;r2;:::;r niofnmemory requests, where each request is for a particular data
element. For example, a program that accesses 4distinct elementsfa;b;c;dg
might make the sequence of requests hd; b; d; b; d; a; c;d; b; a;c; b i.L e t kbe the
size of the cache. When the cache contains kelements and the program requests the
.kC1/st element, the system must decide, for this and each subsequent request,
which kelements to keep in the cache. More precisely, for each request ri,t h e
cache-management algorithm checks whether element riis already in the cache. If
it is, then we have a cache hit ; otherwise, we have a cache miss . Upon a cache
miss, the system retrieves rifrom the main memory, and the cache-management
algorithm must decide whether to keep riin the cache. If it decides to keep riand
the cache already holds kelements, then it must evict one element to make room
forri. The cache-management algorithm evicts data with the goal of minimizing
the number of cache misses over the entire sequence of requests.
Typically, caching is an on-line problem. That is, we have to make decisions
about which data to keep in the cache without knowing the future requests. Here,
however, we consider the off-line version of this problem, in which we are givenin advance the entire sequence of nrequests and the cache size k, and we wish to
minimize the total number of cache misses.
We can solve this off-line problem by a greedy strategy called furthest-in-future ,
which chooses to evict the item in the cache whose next access in the requestsequence comes furthest in the future.
a.Write pseudocode for a cache manager that uses the furthest-in-future strategy.
The input should be a sequence hr
1;r2;:::;r niof requests and a cache size k,
and the output should be a sequence of decisions about which data element (ifany) to evict upon each request. What is the running time of your algorithm?
b.Show that the off-line caching problem exhibits optimal substructure.
c.Prove that furthest-in-future produces the minimum possible number of cache
misses.450 Chapter 16 Greedy Algorithms
Chapter notes
Much more material on greedy algorithms and matroids can be found in Lawler
[224] and Papadimitriou and Steiglitz [271].
The greedy algorithm ﬁrst appeared in the combinatorial optimization literature
in a 1971 article by Edmonds [101], though the theory of matroids dates back to
a 1935 article by Whitney [355].
Our proof of the correctness of the greedy algorithm for the activity-selection
problem is based on that of Gavril [131]. The task-scheduling problem is studiedin Lawler [224]; Horowitz, Sahni, and Rajasekaran [181]; and Brassard and Bratley[54].
Huffman codes were invented in 1952 [185]; Lelewer and Hirschberg [231] sur-
veys data-compression techniques known as of 1987.
An extension of matroid theory to greedoid theory was pioneered by Korte and
Lov´asz [216, 217, 218, 219], who greatly generalize the theory presented here.17 Amortized Analysis
In anamortized analysis , we average the time required to perform a sequence of
data-structure operations over all the operations performed. With amortized analy-sis, we can show that the average cost of an operation is small, if we average over asequence of operations, even though a single operation within the sequence mightbe expensive. Amortized analysis differs from average-case analysis in that prob-ability is not involved; an amortized analysis guarantees the average performance
of each operation in the worst case .
The ﬁrst three sections of this chapter cover the three most common techniques
used in amortized analysis. Section 17.1 starts with aggregate analysis, in which
we determine an upper bound T .n/ on the total cost of a sequence of noperations.
The average cost per operation is then T .n/=n . We take the average cost as the
amortized cost of each operation, so that all operations have the same amortizedcost.
Section 17.2 covers the accounting method, in which we determine an amortized
cost of each operation. When there is more than one type of operation, each type of
operation may have a different amortized cost. The accounting method overchargessome operations early in the sequence, storing the overcharge as “prepaid credit”on speciﬁc objects in the data structure. Later in the sequence, the credit pays foroperations that are charged less than they actually cost.
Section 17.3 discusses the potential method, which is like the accounting method
in that we determine the amortized cost of each operation and may overcharge op-erations early on to compensate for undercharges later. The potential method main-tains the credit as the “potential energy” of the data structure as a whole instead ofassociating the credit with individual objects within the data structure.
We shall use two examples to examine these three methods. One is a stack
with the additional operation M
ULTIPOP , which pops several objects at once. The
other is a binary counter that counts up from 0by means of the single operation
INCREMENT .452 Chapter 17 Amortized Analysis
While reading this chapter, bear in mind that the charges assigned during an
amortized analysis are for analysis purposes only. They need not—and shouldnot—appear in the code. If, for example, we assign a credit to an object xwhen
using the accounting method, we have no need to assign an appropriate amount tosome attribute, such as x:credit , in the code.
When we perform an amortized analysis, we often gain insight into a particular
data structure, and this insight can help us optimize the design. In Section 17.4,for example, we shall use the potential method to analyze a dynamically expanding
and contracting table.
17.1 Aggregate analysis
Inaggregate analysis , we show that for all n, a sequence of noperations takes
worst-case time T .n/ in total. In the worst case, the average cost, or amortized
cost, per operation is therefore T .n/=n . Note that this amortized cost applies to
each operation, even when there are several types of operations in the sequence.The other two methods we shall study in this chapter, the accounting method andthe potential method, may assign different amortized costs to different types of
operations.
Stack operations
In our ﬁrst example of aggregate analysis, we analyze stacks that have been aug-
mented with a new operation. Section 10.1 presented the two fundamental stack
operations, each of which takes O.1/ time:
P
USH.S; x/ pushes object xonto stack S.
POP.S/pops the top of stack Sand returns the popped object. Calling P OPon an
empty stack generates an error.
Since each of these operations runs in O.1/ time, let us consider the cost of each
to be 1. The total cost of a sequence of nPUSH and P OPoperations is therefore n,
and the actual running time for noperations is therefore ‚.n/ .
Now we add the stack operation M ULTIPOP .S; k/ , which removes the ktop ob-
jects of stack S, popping the entire stack if the stack contains fewer than kobjects.
Of course, we assume that kis positive; otherwise the M ULTIPOP operation leaves
the stack unchanged. In the following pseudocode, the operation S TACK -EMPTY
returns TRUE if there are no objects currently on the stack, and FALSE otherwise.17.1 Aggregate analysis 453
23
17
6
3910
47
(a)top
10
47
(b)top
(c)
Figure 17.1 The action of M ULTIPOP on a stack S, shown initially in (a).T h e t o p 4objects are
popped by M ULTIPOP .S; 4/ , whose result is shown in (b). The next operation is M ULTIPOP .S; 7/ ,
which empties the stack—shown in (c)—since there were fewer than 7objects remaining.
MULTIPOP .S; k/
1while not S TACK -EMPTY .S/andk>0
2P OP.S/
3 kDk/NUL1
Figure 17.1 shows an example of M ULTIPOP .
What is the running time of M ULTIPOP .S; k/ on a stack of sobjects? The
actual running time is linear in the number of P OPoperations actually executed,
and thus we can analyze M ULTIPOP in terms of the abstract costs of 1each for
PUSH and P OP. The number of iterations of the while loop is the number min .s; k/
of objects popped off the stack. Each iteration of the loop makes one call to P OPin
line 2. Thus, the total cost of M ULTIPOP is min .s; k/ , and the actual running time
is a linear function of this cost.
Let us analyze a sequence of nPUSH,POP,a n dM ULTIPOP operations on an ini-
tially empty stack. The worst-case cost of a M ULTIPOP operation in the sequence
isO.n/ , since the stack size is at most n. The worst-case time of any stack opera-
tion is therefore O.n/ , and hence a sequence of noperations costs O.n2/,s i n c ew e
may have O.n/ MULTIPOP operations costing O.n/ each. Although this analysis
is correct, the O.n2/result, which we obtained by considering the worst-case cost
of each operation individually, is not tight.
Using aggregate analysis, we can obtain a better upper bound that considers the
entire sequence of noperations. In fact, although a single M ULTIPOP operation
can be expensive, any sequence of nPUSH,POP,a n dM ULTIPOP operations on an
initially empty stack can cost at most O.n/ . Why? We can pop each object from the
stack at most once for each time we have pushed it onto the stack. Therefore, thenumber of times that P
OPcan be called on a nonempty stack, including calls within
MULTIPOP , is at most the number of P USH operations, which is at most n.F o ra n y
value of n, any sequence of nPUSH,POP,a n dM ULTIPOP operations takes a total
ofO.n/ time. The average cost of an operation is O.n/=nDO.1/ . In aggregate454 Chapter 17 Amortized Analysis
analysis, we assign the amortized cost of each operation to be the average cost. In
this example, therefore, all three stack operations have an amortized cost of O.1/ .
We emphasize again that although we have just shown that the average cost, and
hence the running time, of a stack operation is O.1/ , we did not use probabilistic
reasoning. We actually showed a worst-case bound of O.n/ on a sequence of n
operations. Dividing this total cost by nyielded the average cost per operation, or
the amortized cost.
Incrementing a binary counter
As another example of aggregate analysis, consider the problem of implementing
ak-bit binary counter that counts upward from 0. We use an array AŒ0 : : k/NUL1/c141of
bits, where A:lengthDk, as the counter. A binary number xthat is stored in the
counter has its lowest-order bit in AŒ0/c141 and its highest-order bit in AŒk/NUL1/c141,s ot h a t
xDPk/NUL1
iD0AŒi/c141/SOH2i. Initially, xD0, and thus AŒi/c141D0foriD0; 1; : : : ; k/NUL1.T o
add1(modulo 2k) to the value in the counter, we use the following procedure.
INCREMENT .A/
1iD0
2while i<A : length andAŒi/c141 ==1
3 AŒi/c141D0
4 iDiC1
5ifi<A : length
6 AŒi/c141D1
Figure 17.2 shows what happens to a binary counter as we increment it 16times,
starting with the initial value 0and ending with the value 16. At the start of
each iteration of the while loop in lines 2–4, we wish to add a 1into position i.
IfAŒi/c141D1, then adding 1ﬂips the bit to 0in position iand yields a carry of 1,
to be added into position iC1on the next iteration of the loop. Otherwise, the
loop ends, and then, if i<k , we know that AŒi/c141D0, so that line 6 adds a 1into
position i, ﬂipping the 0to a1. The cost of each I NCREMENT operation is linear
in the number of bits ﬂipped.
As with the stack example, a cursory analysis yields a bound that is correct but
not tight. A single execution of I NCREMENT takes time ‚.k/ in the worst case, in
which array Acontains all 1s. Thus, a sequence of nINCREMENT operations on
an initially zero counter takes time O.nk/ in the worst case.
We can tighten our analysis to yield a worst-case cost of O.n/ for a sequence of n
INCREMENT operations by observing that not all bits ﬂip each time I NCREMENT
is called. As Figure 17.2 shows, AŒ0/c141 does ﬂip each time I NCREMENT is called.
The next bit up, AŒ1/c141, ﬂips only every other time: a sequence of nINCREMENT17.1 Aggregate analysis 455
00000000 0
00000001 1
00000010 2
00000011 3
00000100 4
00000101 5
00000110 6
00000111 7
00001000 8
00001001 9
00001010 10
00001011 11
00001100 12
00001101 13
00001110 14
00001111 15
00010000 16A[0]A[1]A[2]A[3]A[4]A[5]A[6]A[7]Counter
valueTotal
cost
1
3
478
10
111516
18
192223
25
26310
Figure 17.2 An8-bit binary counter as its value goes from 0to16by a sequence of 16INCREMENT
operations. Bits that ﬂip to achieve the next value are shaded. The running cost for ﬂipping bits is
shown at the right. Notice that the total cost is always less than twice the total number of I NCREMENT
operations.
operations on an initially zero counter causes AŒ1/c141 to ﬂipbn=2ctimes. Similarly,
bitAŒ2/c141 ﬂips only every fourth time, or bn=4ctimes in a sequence of nINCREMENT
operations. In general, for iD0; 1; : : : ; k/NUL1,b i tAŒi/c141 ﬂipsbn=2ictimes in a
sequence of nINCREMENT operations on an initially zero counter. For i/NAKk,
bitAŒi/c141 does not exist, and so it cannot ﬂip. The total number of ﬂips in the
sequence is thus
k/NUL1X
iD0jn
2ik
<n1X
iD01
2i
D2n ;
by equation (A.6). The worst-case time for a sequence of nINCREMENT operations
on an initially zero counter is therefore O.n/ . The average cost of each operation,
and therefore the amortized cost per operation, is O.n/=nDO.1/ .456 Chapter 17 Amortized Analysis
Exercises
17.1-1
If the set of stack operations included a M ULTIPUSH operation, which pushes k
items onto the stack, would the O.1/ bound on the amortized cost of stack opera-
tions continue to hold?
17.1-2
Show that if a D ECREMENT operation were included in the k-bit counter example,
noperations could cost as much as ‚.nk/ time.
17.1-3
Suppose we perform a sequence of noperations on a data structure in which the ith
operation costs iifiis an exact power of 2,a n d 1otherwise. Use aggregate analysis
to determine the amortized cost per operation.
17.2 The accounting method
In the accounting method of amortized analysis, we assign differing charges to
different operations, with some operations charged more or less than they actu-ally cost. We call the amount we charge an operation its amortized cost .W h e n
an operation’s amortized cost exceeds its actual cost, we assign the difference tospeciﬁc objects in the data structure as credit . Credit can help pay for later oper-
ations whose amortized cost is less than their actual cost. Thus, we can view theamortized cost of an operation as being split between its actual cost and credit thatis either deposited or used up. Different operations may have different amortizedcosts. This method differs from aggregate analysis, in which all operations havethe same amortized cost.
We must choose the amortized costs of operations carefully. If we want to show
that in the worst case the average cost per operation is small by analyzing with
amortized costs, we must ensure that the total amortized cost of a sequence of oper-ations provides an upper bound on the total actual cost of the sequence. Moreover,as in aggregate analysis, this relationship must hold for all sequences of opera-tions. If we denote the actual cost of the ith operation by c
iand the amortized cost
of the ith operation byyci, we require
nX
iD1yci/NAKnX
iD1ci (17.1)
for all sequences of noperations. The total credit stored in the data structure
is the difference between the total amortized cost and the total actual cost, or17.2 The accounting method 457
Pn
iD1yci/NULPn
iD1ci. By inequality (17.1), the total credit associated with the data
structure must be nonnegative at all times. If we ever were to allow the total creditto become negative (the result of undercharging early operations with the promiseof repaying the account later on), then the total amortized costs incurred at thattime would be below the total actual costs incurred; for the sequence of operationsup to that time, the total amortized cost would not be an upper bound on the totalactual cost. Thus, we must take care that the total credit in the data structure neverbecomes negative.
Stack operations
To illustrate the accounting method of amortized analysis, let us return to the stack
example. Recall that the actual costs of the operations were
P
USH 1,
POP 1,
MULTIPOP min.k; s/ ,
where kis the argument supplied to M ULTIPOP andsis the stack size when it is
called. Let us assign the following amortized costs:
PUSH 2,
POP 0,
MULTIPOP 0.
Note that the amortized cost of M ULTIPOP is a constant ( 0), whereas the actual cost
is variable. Here, all three amortized costs are constant. In general, the amortizedcosts of the operations under consideration may differ from each other, and theymay even differ asymptotically.
We shall now show that we can pay for any sequence of stack operations by
charging the amortized costs. Suppose we use a dollar bill to represent each unitof cost. We start with an empty stack. Recall the analogy of Section 10.1 betweenthe stack data structure and a stack of plates in a cafeteria. When we push a plate
on the stack, we use 1dollar to pay the actual cost of the push and are left with a
credit of 1dollar (out of the 2dollars charged), which we leave on top of the plate.
At any point in time, every plate on the stack has a dollar of credit on it.
The dollar stored on the plate serves as prepayment for the cost of popping it
from the stack. When we execute a P
OPoperation, we charge the operation nothing
and pay its actual cost using the credit stored in the stack. To pop a plate, we takethe dollar of credit off the plate and use it to pay the actual cost of the operation.Thus, by charging the P
USH operation a little bit more, we can charge the P OP
operation nothing.458 Chapter 17 Amortized Analysis
Moreover, we can also charge M ULTIPOP operations nothing. To pop the ﬁrst
plate, we take the dollar of credit off the plate and use it to pay the actual cost of a
POPoperation. To pop a second plate, we again have a dollar of credit on the plate
to pay for the P OPoperation, and so on. Thus, we have always charged enough
up front to pay for M ULTIPOP operations. In other words, since each plate on the
stack has 1dollar of credit on it, and the stack always has a nonnegative number of
plates, we have ensured that the amount of credit is always nonnegative. Thus, foranysequence of nP
USH,POP,a n dM ULTIPOP operations, the total amortized cost
is an upper bound on the total actual cost. Since the total amortized cost is O.n/ ,
so is the total actual cost.
Incrementing a binary counter
As another illustration of the accounting method, we analyze the I NCREMENT op-
eration on a binary counter that starts at zero. As we observed earlier, the runningtime of this operation is proportional to the number of bits ﬂipped, which we shalluse as our cost for this example. Let us once again use a dollar bill to representeach unit of cost (the ﬂipping of a bit in this example).
For the amortized analysis, let us charge an amortized cost of 2dollars to set a
bit to 1. When a bit is set, we use 1dollar (out of the 2dollars charged) to pay
for the actual setting of the bit, and we place the other dollar on the bit as credit to
be used later when we ﬂip the bit back to 0. At any point in time, every 1in the
counter has a dollar of credit on it, and thus we can charge nothing to reset a bitto0; we just pay for the reset with the dollar bill on the bit.
Now we can determine the amortized cost of I
NCREMENT . The cost of resetting
the bits within the while loop is paid for by the dollars on the bits that are reset. The
INCREMENT procedure sets at most one bit, in line 6, and therefore the amortized
cost of an I NCREMENT operation is at most 2dollars. The number of 1si nt h e
counter never becomes negative, and thus the amount of credit stays nonnegativeat all times. Thus, for nI
NCREMENT operations, the total amortized cost is O.n/ ,
which bounds the total actual cost.
Exercises
17.2-1
Suppose we perform a sequence of stack operations on a stack whose size neverexceeds k. After every koperations, we make a copy of the entire stack for backup
purposes. Show that the cost of nstack operations, including copying the stack,
isO.n/ by assigning suitable amortized costs to the various stack operations.17.3 The potential method 459
17.2-2
Redo Exercise 17.1-3 using an accounting method of analysis.
17.2-3
Suppose we wish not only to increment a counter but also to reset it to zero (i.e.,make all bits in it 0). Counting the time to examine or modify a bit as ‚.1/ ,
show how to implement a counter as an array of bits so that any sequence of n
I
NCREMENT and R ESET operations takes time O.n/ on an initially zero counter.
(Hint: Keep a pointer to the high-order 1.)
17.3 The potential method
Instead of representing prepaid work as credit stored with speciﬁc objects in the
data structure, the potential method of amortized analysis represents the prepaid
work as “potential energy,” or just “potential,” which can be released to pay forfuture operations. We associate the potential with the data structure as a wholerather than with speciﬁc objects within the data structure.
The potential method works as follows. We will perform noperations, starting
with an initial data structure D
0. For each iD1 ;2;:::;n ,w el e t cibe the actual
cost of the ith operation and Dibe the data structure that results after applying
theith operation to data structure Di/NUL1.Apotential function ˆmaps each data
structure Dito a real number ˆ.D i/, which is the potential associated with data
structure Di.T h e amortized costyciof the ith operation with respect to potential
function ˆis deﬁned by
yciDciCˆ.D i//NULˆ.D i/NUL1/: (17.2)
The amortized cost of each operation is therefore its actual cost plus the change in
potential due to the operation. By equation (17.2), the total amortized cost of the n
operations is
nX
iD1yciDnX
iD1.ciCˆ.D i//NULˆ.D i/NUL1//
DnX
iD1ciCˆ.D n//NULˆ.D 0/: (17.3)
The second equality follows from equation (A.9) because the ˆ.D i/terms tele-
scope.
If we can deﬁne a potential function ˆso that ˆ.D n//NAKˆ.D 0/, then the total
amortized costPn
iD1ycigives an upper bound on the total actual costPn
iD1ci.460 Chapter 17 Amortized Analysis
In practice, we do not always know how many operations might be performed.
Therefore, if we require that ˆ.D i//NAKˆ.D 0/for all i, then we guarantee, as in
the accounting method, that we pay in advance. We usually just deﬁne ˆ.D 0/to
be0and then show that ˆ.D i//NAK0for all i. (See Exercise 17.3-1 for an easy way
to handle cases in which ˆ.D 0/¤0.)
Intuitively, if the potential difference ˆ.D i//NULˆ.D i/NUL1/of the ith operation is
positive, then the amortized cost ycirepresents an overcharge to the ith operation,
and the potential of the data structure increases. If the potential difference is neg-
ative, then the amortized cost represents an undercharge to the ith operation, and
the decrease in the potential pays for the actual cost of the operation.
The amortized costs deﬁned by equations (17.2) and (17.3) depend on the choice
of the potential function ˆ. Different potential functions may yield different amor-
tized costs yet still be upper bounds on the actual costs. We often ﬁnd trade-offsthat we can make in choosing a potential function; the best potential function touse depends on the desired time bounds.
Stack operations
To illustrate the potential method, we return once again to the example of the stack
operations P
USH,POP,a n dM ULTIPOP . We deﬁne the potential function ˆon a
stack to be the number of objects in the stack. For the empty stack D0with which
we start, we have ˆ.D 0/D0. Since the number of objects in the stack is never
negative, the stack Dithat results after the ith operation has nonnegative potential,
and thus
ˆ.D i//NAK0
Dˆ.D 0/:
The total amortized cost of noperations with respect to ˆtherefore represents an
upper bound on the actual cost.
Let us now compute the amortized costs of the various stack operations. If the ith
operation on a stack containing sobjects is a P USH operation, then the potential
difference is
ˆ.D i//NULˆ.D i/NUL1/D.sC1//NULs
D1:
By equation (17.2), the amortized cost of this P USH operation is
yciDciCˆ.D i//NULˆ.D i/NUL1/
D1C1
D2:17.3 The potential method 461
Suppose that the ith operation on the stack is M ULTIPOP .S; k/ , which causes
k0Dmin.k; s/ objects to be popped off the stack. The actual cost of the opera-
tion is k0, and the potential difference is
ˆ.D i//NULˆ.D i/NUL1/D/NULk0:
Thus, the amortized cost of the M ULTIPOP operation is
yciDciCˆ.D i//NULˆ.D i/NUL1/
Dk0/NULk0
D0:
Similarly, the amortized cost of an ordinary P OPoperation is 0.
The amortized cost of each of the three operations is O.1/ , and thus the total
amortized cost of a sequence of noperations is O.n/ . Since we have already argued
thatˆ.D i//NAKˆ.D 0/, the total amortized cost of noperations is an upper bound
on the total actual cost. The worst-case cost of noperations is therefore O.n/ .
Incrementing a binary counter
As another example of the potential method, we again look at incrementing a binary
counter. This time, we deﬁne the potential of the counter after the ith INCREMENT
operation to be bi, the number of 1s in the counter after the ith operation.
Let us compute the amortized cost of an I NCREMENT operation. Suppose that
theith I NCREMENT operation resets tibits. The actual cost of the operation is
therefore at most tiC1, since in addition to resetting tibits, it sets at most one
bit to 1.I fbiD0, then the ith operation resets all kbits, and so bi/NUL1DtiDk.
Ifbi>0,t h e n biDbi/NUL1/NULtiC1. In either case, bi/DC4bi/NUL1/NULtiC1,a n dt h e
potential difference is
ˆ.D i//NULˆ.D i/NUL1//DC4.bi/NUL1/NULtiC1//NULbi/NUL1
D1/NULti:
The amortized cost is therefore
yciDciCˆ.D i//NULˆ.D i/NUL1/
/DC4.tiC1/C.1/NULti/
D2:
If the counter starts at zero, then ˆ.D 0/D0.S i n c e ˆ.D i//NAK0for all i, the total
amortized cost of a sequence of nINCREMENT operations is an upper bound on the
total actual cost, and so the worst-case cost of nINCREMENT operations is O.n/ .
The potential method gives us an easy way to analyze the counter even when
it does not start at zero. The counter starts with b01s, and after nINCREMENT462 Chapter 17 Amortized Analysis
operations it has bn1s, where 0/DC4b0;bn/DC4k. (Recall that kis the number of bits
in the counter.) We can rewrite equation (17.3) as
nX
iD1ciDnX
iD1yci/NULˆ.D n/Cˆ.D 0/: (17.4)
We haveyci/DC42for all 1/DC4i/DC4n.S i n c e ˆ.D 0/Db0andˆ.D n/Dbn, the total
actual cost of nINCREMENT operations is
nX
iD1ci/DC4nX
iD12/NULbnCb0
D2n/NULbnCb0:
Note in particular that since b0/DC4k, as long as kDO.n/ , the total actual cost
isO.n/ . In other words, if we execute at least nD/DEL.k/ INCREMENT operations,
the total actual cost is O.n/ , no matter what initial value the counter contains.
Exercises
17.3-1
Suppose we have a potential function ˆsuch that ˆ.D i//NAKˆ.D 0/for all i,b u t
ˆ.D 0/¤0. Show that there exists a potential function ˆ0such that ˆ0.D0/D0,
ˆ0.Di//NAK0for all i/NAK1, and the amortized costs using ˆ0are the same as the
amortized costs using ˆ.
17.3-2
Redo Exercise 17.1-3 using a potential method of analysis.
17.3-3
Consider an ordinary binary min-heap data structure with nelements supporting
the instructions I NSERT and E XTRACT -MINinO.lgn/worst-case time. Give a
potential function ˆsuch that the amortized cost of I NSERT isO.lgn/and the
amortized cost of E XTRACT -MINisO.1/ , and show that it works.
17.3-4
What is the total cost of executing nof the stack operations P USH,POP,a n d
MULTIPOP , assuming that the stack begins with s0objects and ﬁnishes with sn
objects?
17.3-5
Suppose that a counter begins at a number with b1s in its binary representa-
tion, rather than at 0. Show that the cost of performing nINCREMENT operations
isO.n/ifnD/DEL.b/.(Do not assume that bis constant. )17.4 Dynamic tables 463
17.3-6
Show how to implement a queue with two ordinary stacks (Exercise 10.1-6) so thatthe amortized cost of each E
NQUEUE and each D EQUEUE operation is O.1/ .
17.3-7
Design a data structure to support the following two operations for a dynamicmultiset Sof integers, which allows duplicate values:
I
NSERT .S; x/ inserts xintoS.
DELETE -LARGER -HALF.S/deletes the largestdjSj=2eelements from S.
Explain how to implement this data structure so that any sequence of mINSERT
and D ELETE -LARGER -HALF operations runs in O.m/ time. Your implementation
should also include a way to output the elements of SinO.jSj/time.
17.4 Dynamic tables
We do not always know in advance how many objects some applications will store
in a table. We might allocate space for a table, only to ﬁnd out later that it is notenough. We must then reallocate the table with a larger size and copy all objectsstored in the original table over into the new, larger table. Similarly, if many objectshave been deleted from the table, it may be worthwhile to reallocate the table witha smaller size. In this section, we study this problem of dynamically expanding andcontracting a table. Using amortized analysis, we shall show that the amortized costof insertion and deletion is only O.1/ , even though the actual cost of an operation
is large when it triggers an expansion or a contraction. Moreover, we shall see how
to guarantee that the unused space in a dynamic table never exceeds a constant
fraction of the total space.
We assume that the dynamic table supports the operations T
ABLE -INSERT and
TABLE -DELETE .TABLE -INSERT inserts into the table an item that occupies a sin-
gleslot, that is, a space for one item. Likewise, T ABLE -DELETE removes an item
from the table, thereby freeing a slot. The details of the data-structuring method
used to organize the table are unimportant; we might use a stack (Section 10.1),a heap (Chapter 6), or a hash table (Chapter 11). We might also use an array orcollection of arrays to implement object storage, as we did in Section 10.3.
We shall ﬁnd it convenient to use a concept introduced in our analysis of hashing
(Chapter 11). We deﬁne the load factor ˛.T/ of a nonempty table Tto be the
number of items stored in the table divided by the size (number of slots) of thetable. We assign an empty table (one with no items) size 0, and we deﬁne its load
factor to be 1. If the load factor of a dynamic table is bounded below by a constant,464 Chapter 17 Amortized Analysis
the unused space in the table is never more than a constant fraction of the total
amount of space.
We start by analyzing a dynamic table in which we only insert items. We then
consider the more general case in which we both insert and delete items.
17.4.1 Table expansion
Let us assume that storage for a table is allocated as an array of slots. A table ﬁlls
up when all slots have been used or, equivalently, when its load factor is 1.1In some
software environments, upon attempting to insert an item into a full table, the onlyalternative is to abort with an error. We shall assume, however, that our softwareenvironment, like many modern ones, provides a memory-management system thatcan allocate and free blocks of storage on request. Thus, upon inserting an item
into a full table, we can expand the table by allocating a new table with more slots
than the old table had. Because we always need the table to reside in contiguousmemory, we must allocate a new array for the larger table and then copy items fromthe old table into the new table.
A common heuristic allocates a new table with twice as many slots as the old
one. If the only table operations are insertions, then the load factor of the table is
always at least 1=2, and thus the amount of wasted space never exceeds half the
total space in the table.
In the following pseudocode, we assume that Tis an object representing the
table. The attribute T:table contains a pointer to the block of storage representing
the table, T:num contains the number of items in the table, and T:sizegives the total
number of slots in the table. Initially, the table is empty: T:numDT:sizeD0.
T
ABLE -INSERT .T; x/
1ifT:size ==0
2 allocate T:table with1slot
3 T:sizeD1
4ifT:num ==T:size
5 allocate new-table with2/SOHT:sizeslots
6 insert all items in T:table intonew-table
7 free T:table
8 T:tableDnew-table
9 T:sizeD2/SOHT:size
10 insert xintoT:table
11T:numDT:numC1
1In some situations, such as an open-address hash table, we may wish to consider a table to be full if
its load factor equals some constant strictly less than 1.( S e eE x e r c i s e1 7 . 4 - 1 . )17.4 Dynamic tables 465
Notice that we have two “insertion” procedures here: the T ABLE -INSERT proce-
dure itself and the elementary insertion into a table in lines 6 and 10. We can
analyze the running time of T ABLE -INSERT in terms of the number of elementary
insertions by assigning a cost of 1to each elementary insertion. We assume that
the actual running time of T ABLE -INSERT is linear in the time to insert individual
items, so that the overhead for allocating an initial table in line 2 is constant andthe overhead for allocating and freeing storage in lines 5 and 7 is dominated bythe cost of transferring items in line 6. We call the event in which lines 5–9 are
executed an expansion .
Let us analyze a sequence of nT
ABLE -INSERT operations on an initially empty
table. What is the cost ciof the ith operation? If the current table has room for the
new item (or if this is the ﬁrst operation), then ciD1, since we need only perform
the one elementary insertion in line 10. If the current table is full, however, and anexpansion occurs, then c
iDi: the cost is 1for the elementary insertion in line 10
plusi/NUL1for the items that we must copy from the old table to the new table in
line 6. If we perform noperations, the worst-case cost of an operation is O.n/ ,
which leads to an upper bound of O.n2/on the total running time for noperations.
This bound is not tight, because we rarely expand the table in the course of n
TABLE -INSERT operations. Speciﬁcally, the ith operation causes an expansion
only when i/NUL1is an exact power of 2. The amortized cost of an operation is in
factO.1/ , as we can show using aggregate analysis. The cost of the ith operation
is
ciD(
iifi/NUL1is an exact power of 2;
1otherwise :
The total cost of nTABLE -INSERT operations is therefore
nX
iD1ci/DC4nCblgncX
jD02j
<nC2n
D3n ;
because at most noperations cost 1and the costs of the remaining operations form
a geometric series. Since the total cost of nTABLE -INSERT operations is bounded
by3n, the amortized cost of a single operation is at most 3.
By using the accounting method, we can gain some feeling for why the amor-
tized cost of a T ABLE -INSERT operation should be 3. Intuitively, each item pays
for3elementary insertions: inserting itself into the current table, moving itself
when the table expands, and moving another item that has already been movedonce when the table expands. For example, suppose that the size of the table is m
immediately after an expansion. Then the table holds m=2 items, and it contains466 Chapter 17 Amortized Analysis
no credit. We charge 3dollars for each insertion. The elementary insertion that
occurs immediately costs 1dollar. We place another dollar as credit on the item
inserted. We place the third dollar as credit on one of the m=2 items already in the
table. The table will not ﬁll again until we have inserted another m=2/NUL1items,
and thus, by the time the table contains mitems and is full, we will have placed a
dollar on each item to pay to reinsert it during the expansion.
We can use the potential method to analyze a sequence of nTABLE -INSERT
operations, and we shall use it in Section 17.4.2 to design a T ABLE -DELETE op-
eration that has an O.1/ amortized cost as well. We start by deﬁning a potential
function ˆthat is 0immediately after an expansion but builds to the table size by
the time the table is full, so that we can pay for the next expansion by the potential.The function
ˆ.T /D2/SOHT:num/NULT:size (17.5)
is one possibility. Immediately after an expansion, we have T:numDT:size=2,
and thus ˆ.T /D0, as desired. Immediately before an expansion, we have
T:numDT:size, and thus ˆ.T /DT:num, as desired. The initial value of the
potential is 0, and since the table is always at least half full, T:num/NAKT:size=2,
which implies that ˆ.T / is always nonnegative. Thus, the sum of the amortized
costs of nT
ABLE -INSERT operations gives an upper bound on the sum of the actual
costs.
To analyze the amortized cost of the ith T ABLE -INSERT operation, we let num i
denote the number of items stored in the table after the ith operation, size idenote
the total size of the table after the ith operation, and ˆidenote the potential after
theith operation. Initially, we have num 0D0,size 0D0,a n d ˆ0D0.
If the ith T ABLE -INSERT operation does not trigger an expansion, then we have
size iDsize i/NUL1and the amortized cost of the operation is
yciDciCˆi/NULˆi/NUL1
D1C.2/SOHnum i/NULsize i//NUL.2/SOHnum i/NUL1/NULsize i/NUL1/
D1C.2/SOHnum i/NULsize i//NUL.2.num i/NUL1//NULsize i/
D3:
If the ith operation does trigger an expansion, then we have size iD2/SOHsize i/NUL1and
size i/NUL1Dnum i/NUL1Dnum i/NUL1, which implies that size iD2/SOH.num i/NUL1/. Thus,
the amortized cost of the operation is
yciDciCˆi/NULˆi/NUL1
Dnum iC.2/SOHnum i/NULsize i//NUL.2/SOHnum i/NUL1/NULsize i/NUL1/
Dnum iC.2/SOHnum i/NUL2/SOH.num i/NUL1///NUL.2.num i/NUL1//NUL.num i/NUL1//
Dnum iC2/NUL.num i/NUL1/
D3:17.4 Dynamic tables 467
Φinumi sizei
0 8 16 24 3208162432
iFigure 17.3 The effect of a sequence of nTABLE -INSERT operations on the number num iof items
in the table, the number sizeiof slots in the table, and the potential ˆiD2/SOHnum i/NULsizei, each
being measured after the ith operation. The thin line shows num i, the dashed line shows sizei,a n d
the thick line shows ˆi. Notice that immediately before an expansion, the potential has built up to
the number of items in the table, and therefore it can pay for moving all the items to the new table.
Afterwards, the potential drops to 0, but it is immediately increased by 2upon inserting the item that
caused the expansion.
Figure 17.3 plots the values of num i,size i,a n d ˆiagainst i. Notice how the
potential builds to pay for expanding the table.
17.4.2 Table expansion and contraction
To implement a T ABLE -DELETE operation, it is simple enough to remove the spec-
iﬁed item from the table. In order to limit the amount of wasted space, however,
we might wish to contract the table when the load factor becomes too small. Table
contraction is analogous to table expansion: when the number of items in the tabledrops too low, we allocate a new, smaller table and then copy the items from theold table into the new one. We can then free the storage for the old table by return-ing it to the memory-management system. Ideally, we would like to preserve twoproperties:
/SIthe load factor of the dynamic table is bounded below by a positive constant,and
/SIthe amortized cost of a table operation is bounded above by a constant.468 Chapter 17 Amortized Analysis
We assume that we measure the cost in terms of elementary insertions and dele-
tions.
You might think that we should double the table size upon inserting an item into
a full table and halve the size when a deleting an item would cause the table tobecome less than half full. This strategy would guarantee that the load factor ofthe table never drops below 1=2, but unfortunately, it can cause the amortized cost
of an operation to be quite large. Consider the following scenario. We perform n
operations on a table T,w h e r e nis an exact power of 2.T h eﬁ r s t n=2operations are
insertions, which by our previous analysis cost a total of ‚.n/ . At the end of this
sequence of insertions, T:numDT:sizeDn=2. For the second n=2 operations,
we perform the following sequence:
i n s e r t ,d e l e t e ,d e l e t e ,i n s e r t ,i n s e r t ,d e l e t e ,d e l e t e ,i n s e r t ,i n s e r t ,....
The ﬁrst insertion causes the table to expand to size n. The two following deletions
cause the table to contract back to size n=2. Two further insertions cause another
expansion, and so forth. The cost of each expansion and contraction is ‚.n/ ,a n d
there are ‚.n/ of them. Thus, the total cost of the noperations is ‚.n
2/,m a k i n g
the amortized cost of an operation ‚.n/ .
The downside of this strategy is obvious: after expanding the table, we do not
delete enough items to pay for a contraction. Likewise, after contracting the table,we do not insert enough items to pay for an expansion.
We can improve upon this strategy by allowing the load factor of the table to
drop below 1=2. Speciﬁcally, we continue to double the table size upon inserting
an item into a full table, but we halve the table size when deleting an item causesthe table to become less than 1=4full, rather than 1=2full as before. The load
factor of the table is therefore bounded below by the constant 1=4.
Intuitively, we would consider a load factor of 1=2to be ideal, and the table’s
potential would then be 0. As the load factor deviates from 1=2, the potential
increases so that by the time we expand or contract the table, the table has garneredsufﬁcient potential to pay for copying all the items into the newly allocated table.Thus, we will need a potential function that has grown to T:num by the time that
the load factor has either increased to 1or decreased to 1=4. After either expanding
or contracting the table, the load factor goes back to 1=2and the table’s potential
reduces back to 0.
We omit the code for T
ABLE -DELETE , since it is analogous to T ABLE -INSERT .
For our analysis, we shall assume that whenever the number of items in the tabledrops to 0, we free the storage for the table. That is, if T:numD0,t h e n T:sizeD0.
We can now use the potential method to analyze the cost of a sequence of n
T
ABLE -INSERT and T ABLE -DELETE operations. We start by deﬁning a poten-
tial function ˆthat is 0immediately after an expansion or contraction and builds
as the load factor increases to 1or decreases to 1=4. Let us denote the load fac-17.4 Dynamic tables 469
numi
Φisizei
0 8 16 24 32 40 4808162432
iFigure 17.4 The effect of a sequence of nTABLE -INSERT and T ABLE -DELETE operations on the
number num iof items in the table, the number sizeiof slots in the table, and the potential
ˆiD/SUB
2/SOHnum i/NULsizeiif˛i/NAK1=2 ;
sizei=2/NULnum iif˛i<1 = 2;
each measured after the ith operation. The thin line shows num i, the dashed line shows sizei,a n d
the thick line shows ˆi. Notice that immediately before an expansion, the potential has built up to
the number of items in the table, and therefore it can pay for moving all the items to the new table.
Likewise, immediately before a contraction, the potential has built up to the number of items in the
table.
tor of a nonempty table Tby˛.T/DT:num=T:size. Since for an empty table,
T:numDT:sizeD0and˛.T/D1,w ea l w a y sh a v e T:numD˛.T//SOHT:size,
whether the table is empty or not. We shall use as our potential function
ˆ.T /D(
2/SOHT:num/NULT:size if˛.T//NAK1=2 ;
T:size=2/NULT:num if˛.T/ < 1=2 :(17.6)
Observe that the potential of an empty table is 0and that the potential is never
negative. Thus, the total amortized cost of a sequence of operations with respecttoˆprovides an upper bound on the actual cost of the sequence.
Before proceeding with a precise analysis, we pause to observe some properties
of the potential function, as illustrated in Figure 17.4. Notice that when the loadfactor is 1=2, the potential is 0. When the load factor is 1,w eh a v e T:sizeDT:num,
which implies ˆ.T /DT:num, and thus the potential can pay for an expansion if
an item is inserted. When the load factor is 1=4,w eh a v e T:sizeD4/SOHT:num,w h i c h470 Chapter 17 Amortized Analysis
implies ˆ.T /DT:num, and thus the potential can pay for a contraction if an item
is deleted.
To analyze a sequence of nTABLE -INSERT and T ABLE -DELETE operations,
we let cidenote the actual cost of the ith operation,ycidenote its amortized cost
with respect to ˆ,num idenote the number of items stored in the table after the ith
operation, size idenote the total size of the table after the ith operation, ˛idenote
the load factor of the table after the ith operation, and ˆidenote the potential after
theith operation. Initially, num 0D0,size 0D0,˛0D1,a n d ˆ0D0.
We start with the case in which the ith operation is T ABLE -INSERT . The analy-
sis is identical to that for table expansion in Section 17.4.1 if ˛i/NUL1/NAK1=2.W h e t h e r
the table expands or not, the amortized cost yciof the operation is at most 3.
If˛i/NUL1<1 = 2 , the table cannot expand as a result of the operation, since the ta-
ble expands only when ˛i/NUL1D1.I f˛i<1 = 2 as well, then the amortized cost of
theith operation is
yciDciCˆi/NULˆi/NUL1
D1C.size i=2/NULnum i//NUL.size i/NUL1=2/NULnum i/NUL1/
D1C.size i=2/NULnum i//NUL.size i=2/NUL.num i/NUL1//
D0:
If˛i/NUL1<1 = 2 but˛i/NAK1=2,t h e n
yciDciCˆi/NULˆi/NUL1
D1C.2/SOHnum i/NULsize i//NUL.size i/NUL1=2/NULnum i/NUL1/
D1C.2.num i/NUL1C1//NULsize i/NUL1//NUL.size i/NUL1=2/NULnum i/NUL1/
D3/SOHnum i/NUL1/NUL3
2size i/NUL1C3
D3˛i/NUL1size i/NUL1/NUL3
2size i/NUL1C3
<3
2size i/NUL1/NUL3
2size i/NUL1C3
D3:
Thus, the amortized cost of a T ABLE -INSERT operation is at most 3.
We now turn to the case in which the ith operation is T ABLE -DELETE .I n t h i s
case, num iDnum i/NUL1/NUL1.I f˛i/NUL1<1 = 2 , then we must consider whether the
operation causes the table to contract. If it does not, then size iDsize i/NUL1and the
amortized cost of the operation is
yciDciCˆi/NULˆi/NUL1
D1C.size i=2/NULnum i//NUL.size i/NUL1=2/NULnum i/NUL1/
D1C.size i=2/NULnum i//NUL.size i=2/NUL.num iC1//
D2:17.4 Dynamic tables 471
If˛i/NUL1<1 = 2 and the ith operation does trigger a contraction, then the actual cost
of the operation is ciDnum iC1, since we delete one item and move num iitems.
We have size i=2Dsize i/NUL1=4Dnum i/NUL1Dnum iC1, and the amortized cost of
the operation is
yciDciCˆi/NULˆi/NUL1
D.num iC1/C.size i=2/NULnum i//NUL.size i/NUL1=2/NULnum i/NUL1/
D.num iC1/C..num iC1//NULnum i//NUL..2/SOHnum iC2//NUL.num iC1//
D1:
When the ith operation is a T ABLE -DELETE and˛i/NUL1/NAK1=2, the amortized cost
is also bounded above by a constant. We leave the analysis as Exercise 17.4-2.
In summary, since the amortized cost of each operation is bounded above by
a constant, the actual time for any sequence of noperations on a dynamic table
isO.n/ .
Exercises
17.4-1
Suppose that we wish to implement a dynamic, open-address hash table. Why
might we consider the table to be full when its load factor reaches some value ˛
that is strictly less than 1? Describe brieﬂy how to make insertion into a dynamic,
open-address hash table run in such a way that the expected value of the amortizedcost per insertion is O.1/ . Why is the expected value of the actual cost per insertion
not necessarily O.1/ for all insertions?
17.4-2
Show that if ˛
i/NUL1/NAK1=2 and the ith operation on a dynamic table is T ABLE -
DELETE , then the amortized cost of the operation with respect to the potential
function (17.6) is bounded above by a constant.
17.4-3
Suppose that instead of contracting a table by halving its size when its load factordrops below 1=4, we contract it by multiplying its size by 2=3when its load factor
drops below 1=3. Using the potential function
ˆ.T /Dj2/SOHT:num/NULT:sizej;
show that the amortized cost of a T
ABLE -DELETE that uses this strategy is bounded
above by a constant.472 Chapter 17 Amortized Analysis
Problems
17-1 Bit-reversed binary counter
Chapter 30 examines an important algorithm called the fast Fourier transform,or FFT. The ﬁrst step of the FFT algorithm performs a bit-reversal permutation on
an input array AŒ0 : : n/NUL1/c141whose length is nD2
kfor some nonnegative integer k.
This permutation swaps elements whose indices have binary representations that
are the reverse of each other.
We can express each index aas ak-bit sequencehak/NUL1;ak/NUL2;:::;a 0i,w h e r e
aDPk/NUL1
iD0ai2i.W ed e ﬁ n e
revk.hak/NUL1;ak/NUL2;:::;a 0i/Dha0;a1;:::;a k/NUL1iI
thus,
revk.a/Dk/NUL1X
iD0ak/NULi/NUL12i:
For example, if nD16(or, equivalently, kD4), then rev k.3/D12,s i n c e
the4-bit representation of 3is0011 , which when reversed gives 1100 ,t h e 4-bit
representation of 12.
a.Given a function rev kthat runs in ‚.k/ time, write an algorithm to perform the
bit-reversal permutation on an array of length nD2kinO.nk/ time.
We can use an algorithm based on an amortized analysis to improve the running
time of the bit-reversal permutation. We maintain a “bit-reversed counter” and aprocedure B
IT-REVERSED -INCREMENT that, when given a bit-reversed-counter
value a, produces rev k.revk.a/C1/.I fkD4, for example, and the bit-reversed
counter starts at 0, then successive calls to B IT-REVERSED -INCREMENT produce
the sequence
0000; 1000; 0100; 1100; 0010; 1010; : : : D0; 8; 4; 12; 2; 10; : : : :
b.Assume that the words in your computer store k-bit values and that in unit time,
your computer can manipulate the binary values with operations such as shiftingleft or right by arbitrary amounts, bitwise-AND, bitwise-OR, etc. Describean implementation of the B
IT-REVERSED -INCREMENT procedure that allows
the bit-reversal permutation on an n-element array to be performed in a total
ofO.n/ time.
c.Suppose that you can shift a word left or right by only one bit in unit time. Is it
still possible to implement an O.n/ -time bit-reversal permutation?Problems for Chapter 17 473
17-2 Making binary search dynamic
Binary search of a sorted array takes logarithmic search time, but the time to inserta new element is linear in the size of the array. We can improve the time forinsertion by keeping several sorted arrays.
Speciﬁcally, suppose that we wish to support S
EARCH and I NSERT on a set
ofnelements. Let kDdlg.nC1/e, and let the binary representation of n
behnk/NUL1;nk/NUL2;:::;n 0i.W e h a v e ksorted arrays A0;A1;:::;A k/NUL1, where for
iD0; 1; : : : ; k/NUL1, the length of array Aiis2i. Each array is either full or empty,
depending on whether niD1orniD0, respectively. The total number of ele-
ments held in all karrays is thereforePk/NUL1
iD0ni2iDn. Although each individual
array is sorted, elements in different arrays bear no particular relationship to eachother.
a.Describe how to perform the S
EARCH operation for this data structure. Analyze
its worst-case running time.
b.Describe how to perform the I NSERT operation. Analyze its worst-case and
amortized running times.
c.Discuss how to implement D ELETE .
17-3 Amortized weight-balanced trees
Consider an ordinary binary search tree augmented by adding to each node xthe
attribute x:sizegiving the number of keys stored in the subtree rooted at x.L e t ˛
be a constant in the range 1=2/DC4˛<1 . We say that a given node xis˛-balanced
ifx:left:size/DC4˛/SOHx:sizeandx:right:size/DC4˛/SOHx:size. The tree as a whole
is˛-balanced if every node in the tree is ˛-balanced. The following amortized
approach to maintaining weight-balanced trees was suggested by G. Varghese.
a.A1=2-balanced tree is, in a sense, as balanced as it can be. Given a node x
in an arbitrary binary search tree, show how to rebuild the subtree rooted at x
so that it becomes 1=2-balanced. Your algorithm should run in time ‚.x: size/,
and it can use O.x: size/auxiliary storage.
b.Show that performing a search in an n-node ˛-balanced binary search tree
takes O.lgn/worst-case time.
For the remainder of this problem, assume that the constant ˛is strictly greater
than1=2. Suppose that we implement I NSERT and D ELETE as usual for an n-node
binary search tree, except that after every such operation, if any node in the treeis no longer ˛-balanced, then we “rebuild” the subtree rooted at the highest such
node in the tree so that it becomes 1=2-balanced.474 Chapter 17 Amortized Analysis
We shall analyze this rebuilding scheme using the potential method. For a node x
in a binary search tree T,w ed e ﬁ n e
/c129.x/Djx:left:size/NULx:right:sizej;
and we deﬁne the potential of Tas
ˆ.T /DcX
x2TW/c129.x/ /NAK2/c129.x/ ;
where cis a sufﬁciently large constant that depends on ˛.
c.Argue that any binary search tree has nonnegative potential and that a 1=2-
balanced tree has potential 0.
d.Suppose that munits of potential can pay for rebuilding an m-node subtree.
How large must cbe in terms of ˛in order for it to take O.1/ amortized time
to rebuild a subtree that is not ˛-balanced?
e.Show that inserting a node into or deleting a node from an n-node ˛-balanced
tree costs O.lgn/amortized time.
17-4 The cost of restructuring red-black trees
There are four basic operations on red-black trees that perform structural modi-
ﬁcations : node insertions, node deletions, rotations, and color changes. We have
seen that RB-I NSERT and RB-D ELETE use only O.1/ rotations, node insertions,
and node deletions to maintain the red-black properties, but they may make manymore color changes.
a.Describe a legal red-black tree with nnodes such that calling RB-I
NSERT to
add the .nC1/st node causes /DEL.lgn/color changes. Then describe a legal
red-black tree with nnodes for which calling RB-D ELETE on a particular node
causes /DEL.lgn/color changes.
Although the worst-case number of color changes per operation can be logarithmic,
we shall prove that any sequence of mRB-I NSERT and RB-D ELETE operations on
an initially empty red-black tree causes O.m/ structural modiﬁcations in the worst
case. Note that we count each color change as a structural modiﬁcation.
b.Some of the cases handled by the main loop of the code of both RB-I NSERT -
FIXUP and RB-D ELETE -FIXUP areterminating : once encountered, they cause
the loop to terminate after a constant number of additional operations. For each
of the cases of RB-I NSERT -FIXUP and RB-D ELETE -FIXUP , specify which are
terminating and which are not. ( Hint: Look at Figures 13.5, 13.6, and 13.7.)Problems for Chapter 17 475
We shall ﬁrst analyze the structural modiﬁcations when only insertions are per-
formed. Let Tbe a red-black tree, and deﬁne ˆ.T / to be the number of red nodes
inT. Assume that 1unit of potential can pay for the structural modiﬁcations per-
formed by any of the three cases of RB-I NSERT -FIXUP .
c.LetT0be the result of applying Case 1 of RB-I NSERT -FIXUP toT. Argue that
ˆ.T0/Dˆ.T //NUL1.
d.When we insert a node into a red-black tree using RB-I NSERT , we can break
the operation into three parts. List the structural modiﬁcations and potentialchanges resulting from lines 1–16 of RB-I
NSERT , from nonterminating cases
of RB-I NSERT -FIXUP , and from terminating cases of RB-I NSERT -FIXUP .
e.Using part (d), argue that the amortized number of structural modiﬁcations per-
formed by any call of RB-I NSERT isO.1/ .
We now wish to prove that there are O.m/ structural modiﬁcations when there are
both insertions and deletions. Let us deﬁne, for each node x,
w.x/D„
0ifxis red ;
1ifxis black and has no red children ;
0ifxis black and has one red child ;
2ifxis black and has two red children :
Now we redeﬁne the potential of a red-black tree Tas
ˆ.T /DX
x2Tw.x/ ;
and let T0be the tree that results from applying any nonterminating case of RB-
INSERT -FIXUP or RB-D ELETE -FIXUP toT.
f.Show that ˆ.T0//DC4ˆ.T //NUL1for all nonterminating cases of RB-I NSERT -
FIXUP . Argue that the amortized number of structural modiﬁcations performed
by any call of RB-I NSERT -FIXUP isO.1/ .
g.Show that ˆ.T0//DC4ˆ.T //NUL1for all nonterminating cases of RB-D ELETE -
FIXUP . Argue that the amortized number of structural modiﬁcations performed
by any call of RB-D ELETE -FIXUP isO.1/ .
h.Complete the proof that in the worst case, any sequence of mRB-I NSERT and
RB-D ELETE operations performs O.m/ structural modiﬁcations.476 Chapter 17 Amortized Analysis
17-5 Competitive analysis of self-organizing lists with move-to-front
Aself-organizing list is a linked list of nelements, in which each element has a
unique key. When we search for an element in the list, we are given a key, and wewant to ﬁnd an element with that key.
A self-organizing list has two important properties:
1. To ﬁnd an element in the list, given its key, we must traverse the list from the
beginning until we encounter the element with the given key. If that element isthekth element from the start of the list, then the cost to ﬁnd the element is k.
2. We may reorder the list elements after any operation, according to a given rule
with a given cost. We may choose any heuristic we like to decide how to reorderthe list.
Assume that we start with a given list of nelements, and we are given an access
sequence /ESCDh/ESC
1;/ESC2;:::;/ESC miof keys to ﬁnd, in order. The cost of the sequence
is the sum of the costs of the individual accesses in the sequence.
Out of the various possible ways to reorder the list after an operation, this prob-
lem focuses on transposing adjacent list elements—switching their positions in thelist—with a unit cost for each transpose operation. You will show, by means of apotential function, that a particular heuristic for reordering the list, move-to-front,entails a total cost no worse than 4times that of any other heuristic for maintaining
the list order—even if the other heuristic knows the access sequence in advance!
We call this type of analysis a competitive analysis .
For a heuristic H and a given initial ordering of the list, denote the access cost of
sequence /ESCbyC
H./ESC/.L e t mbe the number of accesses in /ESC.
a.Argue that if heuristic H does not know the access sequence in advance, then
the worst-case cost for H on an access sequence /ESCisCH./ESC/D/DEL.mn/ .
With the move-to-front heuristic, immediately after searching for an element x,
we move xto the ﬁrst position on the list (i.e., the front of the list).
Let rank L.x/denote the rank of element xin list L, that is, the position of xin
listL. For example, if xis the fourth element in L,t h e nr a n k L.x/D4.L e t ci
denote the cost of access /ESCiusing the move-to-front heuristic, which includes the
cost of ﬁnding the element in the list and the cost of moving it to the front of thelist by a series of transpositions of adjacent list elements.
b.Show that if /ESC
iaccesses element xin list Lusing the move-to-front heuristic,
thenciD2/SOHrank L.x//NUL1.
Now we compare move-to-front with any other heuristic H that processes an
access sequence according to the two properties above. Heuristic H may transposeProblems for Chapter 17 477
elements in the list in any way it wants, and it might even know the entire access
sequence in advance.
LetLibe the list after access /ESCiusing move-to-front, and let L/ETX
ibe the list after
access /ESCiusing heuristic H. We denote the cost of access /ESCibycifor move-to-
front and by c/ETX
ifor heuristic H. Suppose that heuristic H performs t/ETX
itranspositions
during access /ESCi.
c.In part (b), you showed that ciD2/SOHrank Li/NUL1.x//NUL1. Now show that c/ETX
iD
rank L/ETX
i/NUL1.x/Ct/ETX
i.
We deﬁne an inversion in list Lias a pair of elements yand´such that y
precedes ´inLiand´precedes yin list L/ETX
i. Suppose that list Lihasqiinversions
after processing the access sequence h/ESC1;/ESC2;:::;/ESC ii. Then, we deﬁne a potential
function ˆthat maps Lito a real number by ˆ.L i/D2qi. For example, if Lihas
the elementshe; c; a; d; biandL/ETX
ihas the elementshc; a; b; d; ei,t h e n Lihas 5
inversions ( .e; c/; .e; a/; .e; d/; .e; b/; .d; b/ ), and so ˆ.L i/D10. Observe that
ˆ.L i//NAK0for all iand that, if move-to-front and heuristic H start with the same
listL0,t h e n ˆ.L 0/D0.
d.Argue that a transposition either increases the potential by 2or decreases the
potential by 2.
Suppose that access /ESCiﬁnds the element x. To understand how the potential
changes due to /ESCi, let us partition the elements other than xinto four sets, depend-
ing on where they are in the lists just before the ith access:
/SISetAconsists of elements that precede xin both Li/NUL1andL/ETX
i/NUL1.
/SISetBconsists of elements that precede xinLi/NUL1and follow xinL/ETX
i/NUL1.
/SISetCconsists of elements that follow xinLi/NUL1and precede xinL/ETX
i/NUL1.
/SISetDconsists of elements that follow xin both Li/NUL1andL/ETX
i/NUL1.
e.Argue that rank Li/NUL1.x/DjAjCjBjC1and rank L/ETX
i/NUL1.x/DjAjCjCjC1.
f.Show that access /ESCicauses a change in potential of
ˆ.L i//NULˆ.L i/NUL1//DC42.jAj/NULjBjCt/ETX
i/;
where, as before, heuristic H performs t/ETX
itranspositions during access /ESCi.
Deﬁne the amortized cost yciof access /ESCibyyciDciCˆ.L i//NULˆ.L i/NUL1/.
g.Show that the amortized cost yciof access /ESCiis bounded from above by 4c/ETX
i.
h.Conclude that the cost CMTF./ESC/of access sequence /ESCwith move-to-front is at
most 4times the cost CH./ESC/of/ESCwith any other heuristic H, assuming that
both heuristics start with the same list.478 Chapter 17 Amortized Analysis
Chapter notes
Aho, Hopcroft, and Ullman [5] used aggregate analysis to determine the running
time of operations on a disjoint-set forest; we shall analyze this data structure us-ing the potential method in Chapter 21. Tarjan [331] surveys the accounting andpotential methods of amortized analysis and presents several applications. He at-
tributes the accounting method to several authors, including M. R. Brown, R. E.
Tarjan, S. Huddleston, and K. Mehlhorn. He attributes the potential method toD. D. Sleator. The term “amortized” is due to D. D. Sleator and R. E. Tarjan.
Potential functions are also useful for proving lower bounds for certain types of
problems. For each conﬁguration of the problem, we deﬁne a potential functionthat maps the conﬁguration to a real number. Then we determine the potential ˆ
init
of the initial conﬁguration, the potential ˆﬁnalof the ﬁnal conﬁguration, and the
maximum change in potential /c129ˆ maxdue to any step. The number of steps must
therefore be at least jˆﬁnal/NULˆinitj=j/c129ˆ maxj. Examples of potential functions to
prove lower bounds in I/O complexity appear in works by Cormen, Sundquist, andWisniewski [79]; Floyd [107]; and Aggarwal and Vitter [3]. Krumme, Cybenko,and Venkataraman [221] applied potential functions to prove lower bounds on gos-
siping : communicating a unique item from each vertex in a graph to every other
vertex.
The move-to-front heuristic from Problem 17-5 works quite well in practice.
Moreover, if we recognize that when we ﬁnd an element, we can splice it out of itsposition in the list and relocate it to the front of the list in constant time, we canshow that the cost of move-to-front is at most twice the cost of any other heuristicincluding, again, one that knows the entire access sequence in advance.V Advanced Data StructuresIntroduction
This part returns to studying data structures that support operations on dynamic
sets, but at a more advanced level than Part III. Two of the chapters, for example,make extensive use of the amortized analysis techniques we saw in Chapter 17.
Chapter 18 presents B-trees, which are balanced search trees speciﬁcally de-
signed to be stored on disks. Because disks operate much more slowly thanrandom-access memory, we measure the performance of B-trees not only by howmuch computing time the dynamic-set operations consume but also by how manydisk accesses they perform. For each B-tree operation, the number of disk accessesincreases with the height of the B-tree, but B-tree operations keep the height low.
Chapter 19 gives an implementation of a mergeable heap, which supports the
operations I
NSERT ,M INIMUM ,EXTRACT -MIN,a n dU NION .1The U NION oper-
ation unites, or merges, two heaps. Fibonacci heaps—the data structure in Chap-
ter 19—also support the operations D ELETE and D ECREASE -KEY. We use amor-
tized time bounds to measure the performance of Fibonacci heaps. The opera-
tions I NSERT ,M INIMUM ,a n dU NION take only O.1/ actual and amortized time
on Fibonacci heaps, and the operations E XTRACT -MINand D ELETE takeO.lgn/
amortized time. The most signiﬁcant advantage of Fibonacci heaps, however, is
that D ECREASE -KEYtakes only O.1/ amortized time. Because the D ECREASE -
1As in Problem 10-2, we have deﬁned a mergeable heap to support M INIMUM and E XTRACT -MIN,
and so we can also refer to it as a mergeable min-heap . Alternatively, if it supported M AXIMUM
and E XTRACT -MAX, it would be a mergeable max-heap . Unless we specify otherwise, mergeable
heaps will be by default mergeable min-heaps.482 Part V Advanced Data Structures
KEYoperation takes constant amortized time, Fibonacci heaps are key components
of some of the asymptotically fastest algorithms to date for graph problems.
Noting that we can beat the /DEL.n lgn/lower bound for sorting when the keys
are integers in a restricted range, Chapter 20 asks whether we can design a datastructure that supports the dynamic-set operations S
EARCH ,INSERT ,D ELETE ,
MINIMUM ,M AXIMUM ,SUCCESSOR ,a n dP REDECESSOR ino.lgn/time when
the keys are integers in a restricted range. The answer turns out to be that we can,by using a recursive data structure known as a van Emde Boas tree. If the keys are
unique integers drawn from the set f0; 1; 2; : : : ; u/NUL1g,w h e r e uis an exact power
of2, then van Emde Boas trees support each of the above operations in O.lg lgu/
time.
Finally, Chapter 21 presents data structures for disjoint sets. We have a universe
ofnelements that are partitioned into dynamic sets. Initially, each element belongs
to its own singleton set. The operation U
NION unites two sets, and the query F IND-
SETidentiﬁes the unique set that contains a given element at the moment. By
representing each set as a simple rooted tree, we obtain surprisingly fast operations:
a sequence of moperations runs in O.m ˛.n// time, where ˛.n/ is an incredibly
slowly growing function— ˛.n/ is at most 4in any conceivable application. The
amortized analysis that proves this time bound is as complex as the data structureis simple.
The topics covered in this part are by no means the only examples of “advanced”
data structures. Other advanced data structures include the following:
/SIDynamic trees , introduced by Sleator and Tarjan [319] and discussed by Tarjan
[330], maintain a forest of disjoint rooted trees. Each edge in each tree hasa real-valued cost. Dynamic trees support queries to ﬁnd parents, roots, edgecosts, and the minimum edge cost on a simple path from a node up to a root.Trees may be manipulated by cutting edges, updating all edge costs on a simplepath from a node up to a root, linking a root into another tree, and making a
node the root of the tree it appears in. One implementation of dynamic trees
gives an O.lgn/amortized time bound for each operation; a more complicated
implementation yields O.lgn/worst-case time bounds. Dynamic trees are used
in some of the asymptotically fastest network-ﬂow algorithms.
/SISplay trees , developed by Sleator and Tarjan [320] and, again, discussed by
Tarjan [330], are a form of binary search tree on which the standard search-tree operations run in O.lgn/amortized time. One application of splay trees
simpliﬁes dynamic trees.
/SIPersistent data structures allow queries, and sometimes updates as well, on past
versions of a data structure. Driscoll, Sarnak, Sleator, and Tarjan [97] presenttechniques for making linked data structures persistent with only a small timePart V Advanced Data Structures 483
and space cost. Problem 13-1 gives a simple example of a persistent dynamic
set.
/SIAs in Chapter 20, several data structures allow a faster implementation of dic-tionary operations (I
NSERT ,DELETE ,a n dS EARCH ) for a restricted universe
of keys. By taking advantage of these restrictions, they are able to achieve bet-ter worst-case asymptotic running times than comparison-based data structures.Fredman and Willard introduced fusion trees [115], which were the ﬁrst data
structure to allow faster dictionary operations when the universe is restricted tointegers. They showed how to implement these operations in O.lgn=lg lgn/
time. Several subsequent data structures, including exponential search trees
[16], have also given improved bounds on some or all of the dictionary opera-tions and are mentioned in the chapter notes throughout this book.
/SIDynamic graph data structures support various queries while allowing the
structure of a graph to change through operations that insert or delete vertices
or edges. Examples of the queries that they support include vertex connectivity
[166], edge connectivity, minimum spanning trees [165], biconnectivity, and
transitive closure [164].
Chapter notes throughout this book mention additional data structures.18 B-Trees
B-trees are balanced search trees designed to work well on disks or other direct-
access secondary storage devices. B-trees are similar to red-black trees (Chap-ter 13), but they are better at minimizing disk I/O operations. Many database sys-tems use B-trees, or variants of B-trees, to store information.
B-trees differ from red-black trees in that B-tree nodes may have many children,
from a few to thousands. That is, the “branching factor” of a B-tree can be quitelarge, although it usually depends on characteristics of the disk unit used. B-treesare similar to red-black trees in that every n-node B-tree has height O.lgn/.T h e
exact height of a B-tree can be considerably less than that of a red-black tree,
however, because its branching factor, and hence the base of the logarithm that
expresses its height, can be much larger. Therefore, we can also use B-trees to
implement many dynamic-set operations in time O.lgn/.
B-trees generalize binary search trees in a natural manner. Figure 18.1 shows a
simple B-tree. If an internal B-tree node xcontains x:nkeys, then xhasx:nC1
children. The keys in node xserve as dividing points separating the range of keys
handled by xintox:nC1subranges, each handled by one child of x.W h e n
searching for a key in a B-tree, we make an .x:nC1/-way decision based on
comparisons with the x:nkeys stored at node x. The structure of leaf nodes differs
from that of internal nodes; we will examine these differences in Section 18.1.
Section 18.1 gives a precise deﬁnition of B-trees and proves that the height of
a B-tree grows only logarithmically with the number of nodes it contains. Sec-tion 18.2 describes how to search for a key and insert a key into a B-tree, andSection 18.3 discusses deletion. Before proceeding, however, we need to ask whywe evaluate data structures designed to work on a disk differently from data struc-tures designed to work in main random-access memory.
Data structures on secondary storage
Computer systems take advantage of various technologies that provide memory
capacity. The primary memory (ormain memory ) of a computer system normallyChapter 18 B-Trees 485
BC FG JKLDH
NP RS VW YZQTXMT:root
Figure 18.1 A B-tree whose keys are the consonants of English. An internal node xcontaining
x:nkeys has x:nC1children. All leaves are at the same depth in the tree. The lightly shaded nodes
are examined in a search for the letter R.
platter track
armsread/write
headspindle
Figure 18.2 A typical disk drive. It comprises one or more platters (two platters are shown here)
that rotate around a spindle. Each platter is read and written with a head at the end of an arm. Arms
rotate around a common pivot axis. A track is th e surface that passes beneath the read/write head
when the head is stationary.
consists of silicon memory chips. This technology is typically more than an order
of magnitude more expensive per bit stored than magnetic storage technology, suchas tapes or disks. Most computer systems also have secondary storage based on
magnetic disks; the amount of such secondary storage often exceeds the amount ofprimary memory by at least two orders of magnitude.
Figure 18.2 shows a typical disk drive. The drive consists of one or more plat-
ters, which rotate at a constant speed around a common spindle . A magnetizable
material covers the surface of each platter. The drive reads and writes each platter
by ahead at the end of an arm. The arms can move their heads toward or away486 Chapter 18 B-Trees
from the spindle. When a given head is stationary, the surface that passes under-
neath it is called a track . Multiple platters increase only the disk drive’s capacity
and not its performance.
Although disks are cheaper and have higher capacity than main memory, they are
much, much slower because they have moving mechanical parts.1The mechanical
motion has two components: platter rotation and arm movement. As of this writing,commodity disks rotate at speeds of 5400–15,000 revolutions per minute (RPM).We typically see 15,000 RPM speeds in server-grade drives, 7200 RPM speeds
in drives for desktops, and 5400 RPM speeds in drives for laptops. Although
7200 RPM may seem fast, one rotation takes 8.33 milliseconds, which is over 5orders of magnitude longer than the 50 nanosecond access times (more or less)commonly found for silicon memory. In other words, if we have to wait a full rota-tion for a particular item to come under the read/write head, we could access mainmemory more than 100,000 times during that span. On average we have to waitfor only half a rotation, but still, the difference in access times for silicon memorycompared with disks is enormous. Moving the arms also takes some time. As ofthis writing, average access times for commodity disks are in the range of 8 to 11milliseconds.
In order to amortize the time spent waiting for mechanical movements, disks
access not just one item but several at a time. Information is divided into a numberof equal-sized pages of bits that appear consecutively within tracks, and each disk
read or write is of one or more entire pages. For a typical disk, a page might be 2
11
to214bytes in length. Once the read/write head is positioned correctly and the disk
has rotated to the beginning of the desired page, reading or writing a magnetic diskis entirely electronic (aside from the rotation of the disk), and the disk can quicklyread or write large amounts of data.
Often, accessing a page of information and reading it from a disk takes longer
than examining all the information read. For this reason, in this chapter we shalllook separately at the two principal components of the running time:
/SIthe number of disk accesses, and
/SIthe CPU (computing) time.
We measure the number of disk accesses in terms of the number of pages of infor-
mation that need to be read from or written to the disk. We note that disk-accesstime is not constant—it depends on the distance between the current track andthe desired track and also on the initial rotational position of the disk. We shall
1As of this writing, solid-state drives have recently come onto the consumer market. Although they
are faster than mechanical disk drives, they cost more per gigabyte and have lower capacities than
mechanical disk drives.Chapter 18 B-Trees 487
nonetheless use the number of pages read or written as a ﬁrst-order approximation
of the total time spent accessing the disk.
In a typical B-tree application, the amount of data handled is so large that all
the data do not ﬁt into main memory at once. The B-tree algorithms copy selectedpages from disk into main memory as needed and write back onto disk the pagesthat have changed. B-tree algorithms keep only a constant number of pages inmain memory at any time; thus, the size of main memory does not limit the size ofB-trees that can be handled.
We model disk operations in our pseudocode as follows. Let xbe a pointer to an
object. If the object is currently in the computer’s main memory, then we can referto the attributes of the object as usual: x:key, for example. If the object referred to
byxresides on disk, however, then we must perform the operation D
ISK-READ.x/
to read object xinto main memory before we can refer to its attributes. (We as-
sume that if xis already in main memory, then D ISK-READ.x/requires no disk
accesses; it is a “no-op.”) Similarly, the operation D ISK-WRITE.x/is used to save
any changes that have been made to the attributes of object x. That is, the typical
pattern for working with an object is as follows:
xDa pointer to some object
DISK-READ.x/
operations that access and/or modify the attributes of x
DISK-WRITE.x/ //omitted if no attributes of xwere changed
other operations that access but do not modify attributes of x
The system can keep only a limited number of pages in main memory at any one
time. We shall assume that the system ﬂushes from main memory pages no longerin use; our B-tree algorithms will ignore this issue.
Since in most systems the running time of a B-tree algorithm depends primar-
ily on the number of D
ISK-READ and D ISK-WRITE operations it performs, we
typically want each of these operations to read or write as much information aspossible. Thus, a B-tree node is usually as large as a whole disk page, and this sizelimits the number of children a B-tree node can have.
For a large B-tree stored on a disk, we often see branching factors between 50
and2000 , depending on the size of a key relative to the size of a page. A large
branching factor dramatically reduces both the height of the tree and the number of
disk accesses required to ﬁnd any key. Figure 18.3 shows a B-tree with a branchingfactor of 1001 and height 2that can store over one billion keys; nevertheless, since
we can keep the root node permanently in main memory, we can ﬁnd any key inthis tree by making at most only two disk accesses.488 Chapter 18 B-Trees
1000
1001
1000
10011000
10011000
1001
1000 1000 1000…1 node,
   1000 keys
1001 nodes,
   1,001,000 keys
1,002,001 nodes,
   1,002,001,000 keys …T:root
Figure 18.3 A B-tree of height 2 containing over one billion keys. Shown inside each node x
isx:n, the number of keys in x. Each internal node and leaf contains 1000 keys. This B-tree has
1001 nodes at depth 1 and over one million leaves at depth 2.
18.1 Deﬁnition of B-trees
To keep things simple, we assume, as we have for binary search trees and red-black
trees, that any “satellite information” associated with a key resides in the samenode as the key. In practice, one might actually store with each key just a pointer toanother disk page containing the satellite information for that key. The pseudocodein this chapter implicitly assumes that the satellite information associated with akey, or the pointer to such satellite information, travels with the key whenever the
key is moved from node to node. A common variant on a B-tree, known as a
B
C-tree , stores all the satellite information in the leaves and stores only keys and
child pointers in the internal nodes, thus maximizing the branching factor of the
internal nodes.
AB-tree Tis a rooted tree (whose root is T:root) having the following proper-
ties:
1. Every node xhas the following attributes:
a.x:n, the number of keys currently stored in node x,
b. the x:nkeys themselves, x:key1;x:key2;:::;x: keyx:n, stored in nondecreas-
ing order, so that x:key1/DC4x:key2/DC4/SOH/SOH/SOH/DC4 x:keyx:n,
c.x:leaf, a boolean value that is TRUE ifxis a leaf and FALSE ifxis an internal
node.
2. Each internal node xalso contains x:nC1pointers x:c 1;x:c 2;:::;x:c x:nC1to
its children. Leaf nodes have no children, and so their ciattributes are unde-
ﬁned.18.1 Deﬁnition of B-trees 489
3. The keys x:keyiseparate the ranges of keys stored in each subtree: if kiis any
key stored in the subtree with root x:c i,t h e n
k1/DC4x:key1/DC4k2/DC4x:key2/DC4/SOH/SOH/SOH/DC4 x:keyx:n/DC4kx:nC1:
4. All leaves have the same depth, which is the tree’s height h.
5. Nodes have lower and upper bounds on the number of keys they can contain.
We express these bounds in terms of a ﬁxed integer t/NAK2called the minimum
degree of the B-tree:
a. Every node other than the root must have at least t/NUL1keys. Every internal
node other than the root thus has at least tchildren. If the tree is nonempty,
the root must have at least one key.
b. Every node may contain at most 2t/NUL1keys. Therefore, an internal node
may have at most 2tchildren. We say that a node is fullif it contains exactly
2t/NUL1keys.2
The simplest B-tree occurs when tD2. Every internal node then has either 2,
3,o r4children, and we have a 2-3-4 tree . In practice, however, much larger values
oftyield B-trees with smaller height.
The height of a B-tree
The number of disk accesses required for most operations on a B-tree is propor-
tional to the height of the B-tree. We now analyze the worst-case height of a B-tree.
Theorem 18.1
Ifn/NAK1, then for any n-key B-tree Tof height hand minimum degree t/NAK2,
h/DC4logtnC1
2:
Proof The root of a B-tree Tcontains at least one key, and all other nodes contain
at least t/NUL1keys. Thus, T, whose height is h, has at least 2nodes at depth 1,a t
least 2tnodes at depth 2, at least 2t2nodes at depth 3, and so on, until at depth h
it has at least 2th/NUL1nodes. Figure 18.4 illustrates such a tree for hD3. Thus, the
2Another common variant on a B-tree, known as a B/ETX-tree , requires each internal node to be at
least2=3full, rather than at least half full, as a B-tree requires.490 Chapter 18 B-Trees
t – 1
t – 1 t – 1…tt – 1
t
…1
t – 1
t – 1 t – 1…tt – 1
t – 1 t – 1…tt – 1
t
… t – 1
t – 1 t – 1…tdepthnumber
of nodes
32 t21
201
2
2tT:root
Figure 18.4 A B-tree of height 3 containing a minimum possible number of keys. Shown inside
each node xisx:n.
number nof keys satisﬁes the inequality
n/NAK1C.t/NUL1/hX
iD12ti/NUL1
D1C2.t/NUL1//DC2th/NUL1
t/NUL1/DC3
D2th/NUL1:
By simple algebra, we get th/DC4.nC1/=2 . Taking base- tlogarithms of both sides
proves the theorem.
Here we see the power of B-trees, as compared with red-black trees. Although
the height of the tree grows as O.lgn/in both cases (recall that tis a constant), for
B-trees the base of the logarithm can be many times larger. Thus, B-trees save afactor of about lg tover red-black trees in the number of nodes examined for most
tree operations. Because we usually have to access the disk to examine an arbitrary
node in a tree, B-trees avoid a substantial number of disk accesses.
Exercises
18.1-1
Why don’t we allow a minimum degree of tD1?
18.1-2
F o rw h a tv a l u e so f tis the tree of Figure 18.1 a legal B-tree?18.2 Basic operations on B-trees 491
18.1-3
Show all legal B-trees of minimum degree 2that representf1; 2; 3; 4; 5g.
18.1-4
As a function of the minimum degree t, what is the maximum number of keys that
can be stored in a B-tree of height h?
18.1-5
Describe the data structure that would result if each black node in a red-black treewere to absorb its red children, incorporating their children with its own.
18.2 Basic operations on B-trees
In this section, we present the details of the operations B-T REE-SEARCH ,B -
TREE-CREATE ,a n dB - T REE-INSERT . In these procedures, we adopt two con-
ventions:
/SIThe root of the B-tree is always in main memory, so that we never need toperform a D
ISK-READ on the root; we do have to perform a D ISK-WRITE of
the root, however, whenever the root node is changed.
/SIAny nodes that are passed as parameters must already have had a D ISK-READ
operation performed on them.
The procedures we present are all “one-pass” algorithms that proceed downward
from the root of the tree, without having to back up.
Searching a B-tree
Searching a B-tree is much like searching a binary search tree, except that instead
of making a binary, or “two-way,” branching decision at each node, we make amultiway branching decision according to the number of the node’s children. More
precisely, at each internal node x,w em a k ea n .x:nC1/-way branching decision.
B-T
REE-SEARCH is a straightforward generalization of the T REE-SEARCH pro-
cedure deﬁned for binary search trees. B-T REE-SEARCH takes as input a pointer
to the root node xof a subtree and a key kto be searched for in that subtree. The
top-level call is thus of the form B-T REE-SEARCH .T:root;k/.I fkis in the B-tree,
B-T REE-SEARCH returns the ordered pair .y; i/ consisting of a node yand an
index isuch that y:keyiDk. Otherwise, the procedure returns NIL.492 Chapter 18 B-Trees
B-T REE-SEARCH .x; k/
1iD1
2while i/DC4x:nandk>x : keyi
3 iDiC1
4ifi/DC4x:nandk==x:keyi
5 return .x; i/
6elseif x:leaf
7 return NIL
8elseDISK-READ.x:c i/
9 return B-T REE-SEARCH .x:c i;k/
Using a linear-search procedure, lines 1–3 ﬁnd the smallest index isuch that
k/DC4x:keyi, or else they set itox:nC1. Lines 4–5 check to see whether we
have now discovered the key, returning if we have. Otherwise, lines 6–9 either ter-minate the search unsuccessfully (if xis a leaf) or recurse to search the appropriate
subtree of x, after performing the necessary D
ISK-READ on that child.
Figure 18.1 illustrates the operation of B-T REE-SEARCH . The procedure exam-
ines the lightly shaded nodes during a search for the key R.
As in the T REE-SEARCH procedure for binary search trees, the nodes encoun-
tered during the recursion form a simple path downward from the root of the
tree. The B-T REE-SEARCH procedure therefore accesses O.h/DO.logtn/disk
pages, where his the height of the B-tree and nis the number of keys in the B-tree.
Since x:n<2 t ,t h ewhile loop of lines 2–3 takes O.t/ time within each node, and
the total CPU time is O.th/DO.tlogtn/.
Creating an empty B-tree
To build a B-tree T,w eﬁ r s tu s eB - T REE-CREATE to create an empty root node
and then call B-T REE-INSERT to add new keys. Both of these procedures use an
auxiliary procedure A LLOCATE -NODE, which allocates one disk page to be used
as a new node in O.1/ time. We can assume that a node created by A LLOCATE -
NODE requires no D ISK-READ, since there is as yet no useful information stored
on the disk for that node.
B-T REE-CREATE .T /
1xDALLOCATE -NODE./
2x:leafDTRUE
3x:nD0
4D ISK-WRITE.x/
5T:rootDx
B-T REE-CREATE requires O.1/ disk operations and O.1/ CPU time.18.2 Basic operations on B-trees 493
Inserting a key into a B-tree
Inserting a key into a B-tree is signiﬁcantly more complicated than inserting a key
into a binary search tree. As with binary search trees, we search for the leaf positionat which to insert the new key. With a B-tree, however, we cannot simply createa new leaf node and insert it, as the resulting tree would fail to be a valid B-tree.Instead, we insert the new key into an existing leaf node. Since we cannot insert akey into a leaf node that is full, we introduce an operation that splits a full node y
(having 2t/NUL1keys) around its median key y:key
tinto two nodes having only t/NUL1
keys each. The median key moves up into y’s parent to identify the dividing point
between the two new trees. But if y’s parent is also full, we must split it before we
can insert the new key, and thus we could end up splitting full nodes all the way upthe tree.
As with a binary search tree, we can insert a key into a B-tree in a single pass
down the tree from the root to a leaf. To do so, we do not wait to ﬁnd out whether
we will actually need to split a full node in order to do the insertion. Instead, as we
travel down the tree searching for the position where the new key belongs, we spliteach full node we come to along the way (including the leaf itself). Thus wheneverwe want to split a full node y, we are assured that its parent is not full.
Splitting a node in a B-tree
The procedure B-T
REE-SPLIT-CHILD takes as input a nonfull internal node x(as-
s u m e dt ob ei nm a i nm e m o r y )a n da ni n d e x isuch that x:c i(also assumed to be in
main memory) is a fullchild of x. The procedure then splits this child in two and
adjusts xso that it has an additional child. To split a full root, we will ﬁrst make the
root a child of a new empty root node, so that we can use B-T REE-SPLIT-CHILD .
The tree thus grows in height by one; splitting is the only means by which the tree
grows.
Figure 18.5 illustrates this process. We split the full node yDx:c iabout its
median key S, which moves up into y’s parent node x. Those keys in ythat are
greater than the median key move into a new node ´, which becomes a new child
ofx.494 Chapter 18 B-Trees
RSTQPU VNW……
RQP TUVNWS ……x x
T1 T1 T2 T2 T3 T3 T4 T4 T5 T5 T6 T6 T7 T7 T8 T8yDx:c i yDx:c i ´Dx:c iC1x:keyi/NUL1
x:keyi/NUL1
x:keyi
x:keyi
x:keyiC1
Figure 18.5 Splitting a node with tD4. Node yDx:c isplits into two nodes, yand ´,a n dt h e
median key Sofymoves up into y’s parent.
B-T REE-SPLIT-CHILD .x; i /
1 ´DALLOCATE -NODE ./
2 yDx:c i
3 ´:leafDy:leaf
4 ´:nDt/NUL1
5for jD1tot/NUL1
6 ´:keyjDy:keyjCt
7ifnot y:leaf
8 for jD1tot
9 ´: c jDy:c jCt
10 y:nDt/NUL1
11 for jDx:nC1downto iC1
12 x:c jC1Dx:c j
13 x:c iC1D´
14 for jDx:ndownto i
15 x:keyjC1Dx:keyj
16 x:keyiDy:keyt
17 x:nDx:nC1
18 D ISK-WRITE .y/
19 D ISK-WRITE .´/
20 D ISK-WRITE .x/
B-T REE-SPLIT-CHILD works by straightforward “cutting and pasting.” Here, x
is the node being split, and yisx’sith child (set in line 2). Node yoriginally has 2t
children ( 2t/NUL1keys) but is reduced to tchildren ( t/NUL1keys) by this operation.
Node ´takes the tlargest children ( t/NUL1keys) from y,a n d ´becomes a new child18.2 Basic operations on B-trees 495
ofx, positioned just after yinx’s table of children. The median key of ymoves
up to become the key in xthat separates yand´.
Lines 1–9 create node ´and give it the largest t/NUL1keys and corresponding t
children of y. Line 10 adjusts the key count for y. Finally, lines 11–17 insert ´as
a child of x, move the median key from yup to xin order to separate yfrom ´,
and adjust x’s key count. Lines 18–20 write out all modiﬁed disk pages. The
CPU time used by B-T REE-SPLIT-CHILD is‚.t/ , due to the loops on lines 5–6
and 8–9. (The other loops run for O.t/ iterations.) The procedure performs O.1/
disk operations.
Inserting a key into a B-tree in a single pass down the tree
We insert a key kinto a B-tree Tof height hin a single pass down the tree, re-
quiring O.h/ disk accesses. The CPU time required is O.th/DO.tlogtn/.T h e
B-T REE-INSERT procedure uses B-T REE-SPLIT-CHILD to guarantee that the re-
cursion never descends to a full node.
B-T REE-INSERT .T; k/
1rDT:root
2ifr:n==2t/NUL1
3 sDALLOCATE -NODE./
4 T:rootDs
5 s:leafDFALSE
6 s:nD0
7 s:c1Dr
8B - T REE-SPLIT-CHILD .s; 1/
9B - T REE-INSERT -NONFULL .s; k/
10elseB-T REE-INSERT -NONFULL .r; k/
Lines 3–9 handle the case in which the root node ris full: the root splits and a
new node s(having two children) becomes the root. Splitting the root is the only
way to increase the height of a B-tree. Figure 18.6 illustrates this case. Unlike abinary search tree, a B-tree increases in height at the top instead of at the bottom.The procedure ﬁnishes by calling B-T
REE-INSERT -NONFULL to insert key kinto
the tree rooted at the nonfull root node. B-T REE-INSERT -NONFULL recurses as
necessary down the tree, at all times guaranteeing that the node to which it recursesis not full by calling B-T
REE-SPLIT-CHILD as necessary.
The auxiliary recursive procedure B-T REE-INSERT -NONFULL inserts key kinto
node x, which is assumed to be nonfull when the procedure is called. The operation
of B-T REE-INSERT and the recursive operation of B-T REE-INSERT -NONFULL
guarantee that this assumption is true.496 Chapter 18 B-Trees
T8 T7 T6 T5 T4 T3 T2 T1 T8 T7 T6 T5 T4 T3 T2 T1FHLDAN P FDA LNPs
H
rrT:rootT:root
Figure 18.6 Splitting the root with tD4. Root node rsplits in two, and a new root node sis
created. The new root contains the median key of rand has the two halves of ras children. The
B-tree grows in height by one when the root is split.
B-T REE-INSERT -NONFULL .x; k/
1iDx:n
2ifx:leaf
3 while i/NAK1andk<x : keyi
4 x:keyiC1Dx:keyi
5 iDi/NUL1
6 x:keyiC1Dk
7 x:nDx:nC1
8D ISK-WRITE.x/
9else while i/NAK1andk<x : keyi
10 iDi/NUL1
11 iDiC1
12 D ISK-READ.x:c i/
13 ifx:c i:n==2t/NUL1
14 B-T REE-SPLIT-CHILD .x; i/
15 ifk>x : keyi
16 iDiC1
17 B-T REE-INSERT -NONFULL .x:c i;k/
The B-T REE-INSERT -NONFULL procedure works as follows. Lines 3–8 handle
the case in which xis a leaf node by inserting key kintox.I fxis not a leaf
node, then we must insert kinto the appropriate leaf node in the subtree rooted
at internal node x. In this case, lines 9–11 determine the child of xto which the
recursion descends. Line 13 detects whether the recursion would descend to a fullchild, in which case line 14 uses B-T
REE-SPLIT-CHILD to split that child into two
nonfull children, and lines 15–16 determine which of the two children is now the18.2 Basic operations on B-trees 497
correct one to descend to. (Note that there is no need for a D ISK-READ.x:c i/after
line 16 increments i, since the recursion will descend in this case to a child that
was just created by B-T REE-SPLIT-CHILD .) The net effect of lines 13–16 is thus
to guarantee that the procedure never recurses to a full node. Line 17 then recursesto insert kinto the appropriate subtree. Figure 18.7 illustrates the various cases of
inserting into a B-tree.
For a B-tree of height h,B - T
REE-INSERT performs O.h/ disk accesses, since
only O.1/ DISK-READ and D ISK-WRITE operations occur between calls to
B-T REE-INSERT -NONFULL . The total CPU time used is O.th/DO.tlogtn/.
Since B-T REE-INSERT -NONFULL is tail-recursive, we can alternatively imple-
ment it as a while loop, thereby demonstrating that the number of pages that need
to be in main memory at any time is O.1/ .
Exercises
18.2-1
Show the results of inserting the keys
F;S;Q;K;C;L;H;T;V;W;M;R;N;P;A;B;X;Y;D;Z;Ein order into an empty B-tree with minimum degree 2. Draw only the conﬁgura-
tions of the tree just before some node must split, and also draw the ﬁnal conﬁgu-ration.
18.2-2
Explain under what circumstances, if any, redundant D
ISK-READ or D ISK-WRITE
operations occur during the course of executing a call to B-T REE-INSERT .( A
redundant D ISK-READ is a D ISK-READ for a page that is already in memory.
A redundant D ISK-WRITE writes to disk a page of information that is identical to
what is already stored there.)
18.2-3
Explain how to ﬁnd the minimum key stored in a B-tree and how to ﬁnd the prede-cessor of a given key stored in a B-tree.
18.2-4 ?
Suppose that we insert the keys f1 ;2;:::;nginto an empty B-tree with minimum
degree 2. How many nodes does the ﬁnal B-tree have?
18.2-5
Since leaf nodes require no pointers to children, they could conceivably use a dif-ferent (larger) tvalue than internal nodes for the same disk page size. Show how
to modify the procedures for creating and inserting into a B-tree to handle this
variation.498 Chapter 18 B-Trees
JK NO RST DECAU V YZPXMG (a)
JK NO RST DE BAU V YZPXMG (b)
C
JK NO DE BAU V YZPXMG (c)
C RSQT
JK NO DE BAU V YZMG(d)
C RSQ LP
XT
JK NO DE BAU V YZMG(e)
C
RSQ LP
XT
FQ inserted
L inserted
F insertedinitial tree
B inserted
Figure 18.7 Inserting keys into a B-tree. The minimum degree tfor this B-tree is 3, so a node can
hold at most 5keys. Nodes that are modiﬁed by the insertion process are lightly shaded. (a)The
initial tree for this example. (b)The result of inserting Binto the initial tree; this is a simple insertion
into a leaf node. (c)The result of inserting Qinto the previous tree. The node RST U V splits into
two nodes containing RSandUV,t h ek e y Tmoves up to the root, and Qis inserted in the leftmost
of the two halves (the RSnode). (d)The result of inserting Linto the previous tree. The root
splits right away, since it is full, and the B-tree grows in height by one. Then Lis inserted into the
leaf containing JK.(e)The result of inserting Finto the previous tree. The node ABCDE splits
before Fis inserted into the rightmost of the two halves (the DEnode).18.3 Deleting a key from a B-tree 499
18.2-6
Suppose that we were to implement B-T REE-SEARCH to use binary search rather
than linear search within each node. Show that this change makes the CPU timerequired O.lgn/, independently of how tmight be chosen as a function of n.
18.2-7
Suppose that disk hardware allows us to choose the size of a disk page arbitrarily,but that the time it takes to read the disk page is aCbt,w h e r e aandbare speciﬁed
constants and tis the minimum degree for a B-tree using pages of the selected size.
Describe how to choose tso as to minimize (approximately) the B-tree search time.
Suggest an optimal value of tfor the case in which aD5milliseconds and bD10
microseconds.
18.3 Deleting a key from a B-tree
Deletion from a B-tree is analogous to insertion but a little more complicated, be-cause we can delete a key from any node—not just a leaf—and when we delete a
key from an internal node, we will have to rearrange the node’s children. As in
insertion, we must guard against deletion producing a tree whose structure violatesthe B-tree properties. Just as we had to ensure that a node didn’t get too big due toinsertion, we must ensure that a node doesn’t get too small during deletion (exceptthat the root is allowed to have fewer than the minimum number t/NUL1of keys).
Just as a simple insertion algorithm might have to back up if a node on the pathto where the key was to be inserted was full, a simple approach to deletion mighthave to back up if a node (other than the root) along the path to where the key is tobe deleted has the minimum number of keys.
The procedure B-T
REE-DELETE deletes the key kfrom the subtree rooted at x.
We design this procedure to guarantee that whenever it calls itself recursively on anode x, the number of keys in xis at least the minimum degree t. Note that this
condition requires one more key than the minimum required by the usual B-tree
conditions, so that sometimes a key may have to be moved into a child node before
recursion descends to that child. This strengthened condition allows us to delete a
key from the tree in one downward pass without having to “back up” (with one ex-
ception, which we’ll explain). You should interpret the following speciﬁcation for
deletion from a B-tree with the understanding that if the root node xever becomes
an internal node having no keys (this situation can occur in cases 2c and 3b on
pages 501–502), then we delete x,a n d x’s only child x:c
1becomes the new root
of the tree, decreasing the height of the tree by one and preserving the property thatthe root of the tree contains at least one key (unless the tree is empty).500 Chapter 18 B-Trees
JK NO DE BAU V YZMG(a)
C
RSQ LP
XT
Finitial tree
JK NO DE BAU V YZMG(b)
C
RSQ LP
XTF deleted: case 1
JK NO DE BAU V YZG(c)
C
RSQLP
XTM deleted: case 2a
JK NO DE BAU V YZ(d)
C
RSQLP
XTG deleted: case 2c
Figure 18.8 Deleting keys from a B-tree. The minimum degree for this B-tree is tD3, so a node
(other than the root) cannot have fewer than 2 keys. Nodes that are modiﬁed are lightly shaded.
(a)The B-tree of Figure 18.7(e). (b)Deletion of F. This is case 1: simple deletion from a leaf.
(c)Deletion of M. This is case 2a: the predecessor LofMmoves up to take M’s position. (d)Dele-
tion of G. This is case 2c: we push Gdown to make node DEGJK and then delete Gfrom this leaf
(case 1).
We sketch how deletion works instead of presenting the pseudocode. Figure 18.8
illustrates the various cases of deleting keys from a B-tree.
1. If the key kis in node xandxis a leaf, delete the key kfrom x.
2. If the key kis in node xandxis an internal node, do the following:18.3 Deleting a key from a B-tree 501
JK NO E BAU V YZ(e)
C
RSQLPXTD deleted: case 3b
JK NO E BAU V YZC
RSQLP X T
JK NO AU V YZ C RSQLP X T (f) B deleted: case 3a E(e′) tree shrinks
in height
Figure 18.8, continued (e) Deletion of D. This is case 3b: the recursion cannot descend to
node CLbecause it has only 2 keys, so we push Pdown and merge it with CLandTXto form
CLPTX ; then we delete Dfrom a leaf (case 1). (e0)After (e), we delete the root and the tree shrinks
in height by one. (f)Deletion of B. This is case 3a: Cmoves to ﬁll B’s position and Emoves to
ﬁllC’s position.
a. If the child ythat precedes kin node xhas at least tkeys, then ﬁnd the
predecessor k0ofkin the subtree rooted at y. Recursively delete k0,a n d
replace kbyk0inx. (We can ﬁnd k0and delete it in a single downward
pass.)
b. If yhas fewer than tkeys, then, symmetrically, examine the child ´that
follows kin node x.I f´has at least tkeys, then ﬁnd the successor k0ofkin
the subtree rooted at ´. Recursively delete k0, and replace kbyk0inx.( W e
can ﬁnd k0and delete it in a single downward pass.)
c. Otherwise, if both yand´have only t/NUL1keys, merge kand all of ´intoy,
so that xloses both kand the pointer to ´,a n d ynow contains 2t/NUL1keys.
Then free ´and recursively delete kfrom y.
3. If the key kis not present in internal node x, determine the root x:c iof the
appropriate subtree that must contain k,i fkis in the tree at all. If x:c ihas
onlyt/NUL1keys, execute step 3a or 3b as necessary to guarantee that we descend
to a node containing at least tkeys. Then ﬁnish by recursing on the appropriate
child of x.502 Chapter 18 B-Trees
a. Ifx:c ihas only t/NUL1keys but has an immediate sibling with at least tkeys,
givex:c ian extra key by moving a key from xdown into x:c i,m o v i n ga
key from x:c i’s immediate left or right sibling up into x, and moving the
appropriate child pointer from the sibling into x:c i.
b. Ifx:c iand both of x:c i’s immediate siblings have t/NUL1keys, merge x:c i
with one sibling, which involves moving a key from xdown into the new
merged node to become the median key for that node.
Since most of the keys in a B-tree are in the leaves, we may expect that in
practice, deletion operations are most often used to delete keys from leaves. The
B-T REE-DELETE procedure then acts in one downward pass through the tree,
without having to back up. When deleting a key in an internal node, however,the procedure makes a downward pass through the tree but may have to return tothe node from which the key was deleted to replace the key with its predecessor orsuccessor (cases 2a and 2b).
Although this procedure seems complicated, it involves only O.h/ disk oper-
ations for a B-tree of height h, since only O.1/ calls to D
ISK-READ and D ISK-
WRITE are made between recursive invocations of the procedure. The CPU time
required is O.th/DO.tlogtn/.
Exercises
18.3-1
Show the results of deleting C,P,a n d V, in order, from the tree of Figure 18.8(f).
18.3-2
Write pseudocode for B-T REE-DELETE .
Problems
18-1 Stacks on secondary storage
Consider implementing a stack in a computer that has a relatively small amountof fast primary memory and a relatively large amount of slower disk storage. Theoperations P
USH and P OPwork on single-word values. The stack we wish to
support can grow to be much larger than can ﬁt in memory, and thus most of it
must be stored on disk.
A simple, but inefﬁcient, stack implementation keeps the entire stack on disk.
We maintain in memory a stack pointer, which is the disk address of the top elementon the stack. If the pointer has value p, the top element is the .pmodm/th word
on pagebp=mcof the disk, where mis the number of words per page.Problems for Chapter 18 503
To implement the P USH operation, we increment the stack pointer, read the ap-
propriate page into memory from disk, copy the element to be pushed to the ap-propriate word on the page, and write the page back to disk. A P
OPoperation is
similar. We decrement the stack pointer, read in the appropriate page from disk,and return the top of the stack. We need not write back the page, since it was notmodiﬁed.
Because disk operations are relatively expensive, we count two costs for any
implementation: the total number of disk accesses and the total CPU time. Any
disk access to a page of mwords incurs charges of one disk access and ‚.m/ CPU
time.
a.Asymptotically, what is the worst-case number of disk accesses for nstack
operations using this simple implementation? What is the CPU time for nstack
operations? (Express your answer in terms of mandnfor this and subsequent
parts.)
Now consider a stack implementation in which we keep one page of the stack in
memory. (We also maintain a small amount of memory to keep track of which pageis currently in memory.) We can perform a stack operation only if the relevant diskpage resides in memory. If necessary, we can write the page currently in memoryto the disk and read in the new page from the disk to memory. If the relevant diskpage is already in memory, then no disk accesses are required.
b.What is the worst-case number of disk accesses required for nP
USH opera-
tions? What is the CPU time?
c.What is the worst-case number of disk accesses required for nstack operations?
What is the CPU time?
Suppose that we now implement the stack by keeping two pages in memory (in
addition to a small number of words for bookkeeping).
d.Describe how to manage the stack pages so that the amortized number of disk
accesses for any stack operation is O.1=m/ and the amortized CPU time for
any stack operation is O.1/ .
18-2 Joining and splitting 2-3-4 trees
Thejoin operation takes two dynamic sets S0andS00a n da ne l e m e n t xsuch that
for any x02S0andx002S00,w eh a v e x0:key<x : key<x00:key. It returns a set
SDS0[fxg[S00.T h esplit operation is like an “inverse” join: given a dynamic
setSand an element x2S, it creates a set S0that consists of all elements in
S/NULfxgwhose keys are less than x:keyand a set S00that consists of all elements
inS/NULfxgwhose keys are greater than x:key. In this problem, we investigate504 Chapter 18 B-Trees
how to implement these operations on 2-3-4 trees. We assume for convenience that
elements consist only of keys and that all key values are distinct.
a.Show how to maintain, for every node xof a 2-3-4 tree, the height of the subtree
rooted at xas an attribute x:height . Make sure that your implementation does
not affect the asymptotic running times of searching, insertion, and deletion.
b.Show how to implement the join operation. Given two 2-3-4 trees T0andT00
and a key k, the join operation should run in O.1Cjh0/NULh00j/time, where h0
andh00are the heights of T0andT00, respectively.
c.Consider the simple path pfrom the root of a 2-3-4 tree Tto a given key k,
the set S0of keys in Tthat are less than k, and the set S00of keys in Tthat are
greater than k. Show that pbreaks S0into a set of treesfT0
0;T0
1;:::;T0
mgand a
set of keysfk0
1;k0
2;:::;k0
mg, where, for iD1 ;2;:::;m ,w eh a v e y<k0
i<´
for any keys y2T0
i/NUL1and´2T0
i. What is the relationship between the heights
ofT0
i/NUL1andT0
i? Describe how pbreaks S00into sets of trees and keys.
d.Show how to implement the split operation on T. Use the join operation to
assemble the keys in S0into a single 2-3-4 tree T0and the keys in S00into a
single 2-3-4 tree T00. The running time of the split operation should be O.lgn/,
where nis the number of keys in T.(Hint: The costs for joining should tele-
scope.)
Chapter notes
Knuth [211], Aho, Hopcroft, and Ullman [5], and Sedgewick [306] give furtherdiscussions of balanced-tree schemes and B-trees. Comer [74] provides a compre-hensive survey of B-trees. Guibas and Sedgewick [155] discuss the relationshipsamong various kinds of balanced-tree schemes, including red-black trees and 2-3-4trees.
In 1970, J. E. Hopcroft invented 2-3 trees, a precursor to B-trees and 2-3-4
trees, in which every internal node has either two or three children. Bayer andMcCreight [35] introduced B-trees in 1972; they did not explain their choice ofname.
Bender, Demaine, and Farach-Colton [40] studied how to make B-trees perform
well in the presence of memory-hierarchy effects. Their cache-oblivious algo-
rithms work efﬁciently without explicitly knowing the data transfer sizes within
the memory hierarchy.19 Fibonacci Heaps
The Fibonacci heap data structure serves a dual purpose. First, it supports a set of
operations that constitutes what is known as a “mergeable heap.” Second, severalFibonacci-heap operations run in constant amortized time, which makes this datastructure well suited for applications that invoke these operations frequently.
Mergeable heaps
Amergeable heap is any data structure that supports the following ﬁve operations,
in which each element has a key:
M
AKE-HEAP./creates and returns a new heap containing no elements.
INSERT .H; x/ inserts element x, whose keyhas already been ﬁlled in, into heap H.
MINIMUM .H/ returns a pointer to the element in heap Hwhose key is minimum.
EXTRACT -MIN.H/ deletes the element from heap Hwhose key is minimum, re-
turning a pointer to the element.
UNION .H1;H2/creates and returns a new heap that contains all the elements of
heaps H1andH2. Heaps H1andH2are “destroyed” by this operation.
In addition to the mergeable-heap operations above, Fibonacci heaps also support
the following two operations:
DECREASE -KEY. H;x;k/ assigns to element xwithin heap Hthe new key
value k, which we assume to be no greater than its current key value.1
DELETE .H; x/ deletes element xfrom heap H.
1As mentioned in the introduction to Part V, our default mergeable heaps are mergeable min-
heaps, and so the operations M INIMUM ,EXTRACT -MIN,a n dD ECREASE -KEYapply. Alterna-
tively, we could deﬁne a mergeable max-heap with the operations M AXIMUM ,EXTRACT -MAX,
and I NCREASE -KEY.506 Chapter 19 Fibonacci Heaps
Binary heap Fibonacci heap
Procedure (worst-case) (amortized)
MAKE-HEAP ‚.1/ ‚.1/
INSERT ‚.lgn/ ‚.1/
MINIMUM ‚.1/ ‚.1/
EXTRACT -MIN ‚.lgn/ O. lgn/
UNION ‚.n/ ‚.1/
DECREASE -KEY ‚.lgn/ ‚.1/
DELETE ‚.lgn/ O. lgn/
Figure 19.1 Running times for operations on two implementations of mergeable heaps. The num-
ber of items in the heap(s) at the time of an operation is denoted by n.
As the table in Figure 19.1 shows, if we don’t need the U NION operation, ordi-
nary binary heaps, as used in heapsort (Chapter 6), work fairly well. Operationsother than U
NION run in worst-case time O.lgn/on a binary heap. If we need
to support the U NION operation, however, binary heaps perform poorly. By con-
catenating the two arrays that hold the binary heaps to be merged and then running
BUILD -MIN-HEAP (see Section 6.3), the U NION operation takes ‚.n/ time in the
worst case.
Fibonacci heaps, on the other hand, have better asymptotic time bounds than
binary heaps for the I NSERT ,UNION ,a n dD ECREASE -KEYoperations, and they
have the same asymptotic running times for the remaining operations. Note, how-ever, that the running times for Fibonacci heaps in Figure 19.1 are amortized timebounds, not worst-case per-operation time bounds. The U
NION operation takes
only constant amortized time in a Fibonacci heap, which is signiﬁcantly betterthan the linear worst-case time required in a binary heap (assuming, of course, thatan amortized time bound sufﬁces).
Fibonacci heaps in theory and practice
From a theoretical standpoint, Fibonacci heaps are especially desirable when the
number of E
XTRACT -MINand D ELETE operations is small relative to the number
of other operations performed. This situation arises in many applications. For
example, some algorithms for graph problems may call D ECREASE -KEYonce per
edge. For dense graphs, which have many edges, the ‚.1/ amortized time of each
call of D ECREASE -KEYadds up to a big improvement over the ‚.lgn/worst-case
time of binary heaps. Fast algorithms for problems such as computing minimum
spanning trees (Chapter 23) and ﬁnding single-source shortest paths (Chapter 24)
make essential use of Fibonacci heaps.19.1 Structure of Fibonacci heaps 507
From a practical point of view, however, the constant factors and program-
ming complexity of Fibonacci heaps make them less desirable than ordinary binary(ork-ary) heaps for most applications, except for certain applications that manage
large amounts of data. Thus, Fibonacci heaps are predominantly of theoretical in-terest. If a much simpler data structure with the same amortized time bounds asFibonacci heaps were developed, it would be of practical use as well.
Both binary heaps and Fibonacci heaps are inefﬁcient in how they support the
operation S
EARCH ; it can take a while to ﬁnd an element with a given key. For this
reason, operations such as D ECREASE -KEYand D ELETE that refer to a given ele-
ment require a pointer to that element as part of their input. As in our discussion ofpriority queues in Section 6.5, when we use a mergeable heap in an application, weoften store a handle to the corresponding application object in each mergeable-heapelement, as well as a handle to the corresponding mergeable-heap element in eachapplication object. The exact nature of these handles depends on the applicationand its implementation.
Like several other data structures that we have seen, Fibonacci heaps are based
on rooted trees. We represent each element by a node within a tree, and each
node has a keyattribute. For the remainder of this chapter, we shall use the term
“node” instead of “element.” We shall also ignore issues of allocating nodes priorto insertion and freeing nodes following deletion, assuming instead that the codecalling the heap procedures deals with these details.
Section 19.1 deﬁnes Fibonacci heaps, discusses how we represent them, and
presents the potential function used for their amortized analysis. Section 19.2shows how to implement the mergeable-heap operations and achieve the amortizedtime bounds shown in Figure 19.1. The remaining two operations, D
ECREASE -
KEYand D ELETE , form the focus of Section 19.3. Finally, Section 19.4 ﬁnishes a
key part of the analysis and also explains the curious name of the data structure.
19.1 Structure of Fibonacci heaps
AFibonacci heap is a collection of rooted trees that are min-heap ordered .T h a t
is, each tree obeys the min-heap property : the key of a node is greater than or equal
to the key of its parent. Figure 19.2(a) shows an example of a Fibonacci heap.
As Figure 19.2(b) shows, each node xcontains a pointer x:pto its parent and
a pointer x:child to any one of its children. The children of xare linked together
in a circular, doubly linked list, which we call the child list ofx. Each child yin
a child list has pointers y:leftandy:right that point to y’s left and right siblings,
respectively. If node yis an only child, then y:leftDy:rightDy. Siblings may
appear in a child list in any order.508 Chapter 19 Fibonacci Heaps
17
30 26 46
3524
18 52 383
39 4123 7
17
30 26 46
3524
18 52 383
39 4123 7(a)
(b)H:minH:min
Figure 19.2 (a) A Fibonacci heap consisting of ﬁve min-heap-ordered trees and 14 nodes. The
dashed line indicates the root list. The minimum node of the heap is the node containing the key 3.
Black nodes are marked. The potential of this particular Fibonacci heap is 5C2/SOH3D11.(b)Am o r e
complete representation showing pointers p(up arrows), child (down arrows), and leftandright
(sideways arrows). The remaining ﬁgures in this chapter omit these details, since all the information
shown here can be determined from what appears in part (a).
Circular, doubly linked lists (see Section 10.2) have two advantages for use in
Fibonacci heaps. First, we can insert a node into any location or remove a node
from anywhere in a circular, doubly linked list in O.1/ time. Second, given two
such lists, we can concatenate them (or “splice” them together) into one circular,doubly linked list in O.1/ time. In the descriptions of Fibonacci heap operations,
we shall refer to these operations informally, letting you ﬁll in the details of theirimplementations if you wish.
Each node has two other attributes. We store the number of children in the child
list of node xinx:degree . The boolean-valued attribute x:mark indicates whether
node xhas lost a child since the last time xwas made the child of another node.
Newly created nodes are unmarked, and a node xbecomes unmarked whenever it
is made the child of another node. Until we look at the D
ECREASE -KEYoperation
in Section 19.3, we will just set all mark attributes to FALSE .
We access a given Fibonacci heap Hby a pointer H:minto the root of a tree
containing the minimum key; we call this node the minimum node of the Fibonacci19.1 Structure of Fibonacci heaps 509
heap. If more than one root has a key with the minimum value, then any such root
may serve as the minimum node. When a Fibonacci heap His empty, H:min
isNIL.
The roots of all the trees in a Fibonacci heap are linked together using their
leftandright pointers into a circular, doubly linked list called the root list of the
Fibonacci heap. The pointer H:minthus points to the node in the root list whose
key is minimum. Trees may appear in any order within a root list.
We rely on one other attribute for a Fibonacci heap H:H:n, the number of
nodes currently in H.
Potential function
As mentioned, we shall use the potential method of Section 17.3 to analyze the
performance of Fibonacci heap operations. For a given Fibonacci heap H,w e
indicate by t.H/ the number of trees in the root list of Hand by m.H/ the number
of marked nodes in H. We then deﬁne the potential ˆ.H/ of Fibonacci heap H
by
ˆ.H/Dt.H/C2m . H/: (19.1)
(We will gain some intuition for this potential function in Section 19.3.) For exam-
ple, the potential of the Fibonacci heap shown in Figure 19.2 is 5C2/SOH3D11.T h e
potential of a set of Fibonacci heaps is the sum of the potentials of its constituentFibonacci heaps. We shall assume that a unit of potential can pay for a constantamount of work, where the constant is sufﬁciently large to cover the cost of any ofthe speciﬁc constant-time pieces of work that we might encounter.
We assume that a Fibonacci heap application begins with no heaps. The initial
potential, therefore, is 0, and by equation (19.1), the potential is nonnegative at
all subsequent times. From equation (17.3), an upper bound on the total amortizedcost provides an upper bound on the total actual cost for the sequence of operations.
Maximum degree
The amortized analyses we shall perform in the remaining sections of this chapter
assume that we know an upper bound D.n/ on the maximum degree of any node
in an n-node Fibonacci heap. We won’t prove it, but when only the mergeable-
heap operations are supported, D.n//DC4blgnc. (Problem 19-2(d) asks you to prove
this property.) In Sections 19.3 and 19.4, we shall show that when we support
D
ECREASE -KEYand D ELETE as well, D.n/DO.lgn/.510 Chapter 19 Fibonacci Heaps
19.2 Mergeable-heap operations
The mergeable-heap operations on Fibonacci heaps delay work as long as possible.
The various operations have performance trade-offs. For example, we insert a nodeby adding it to the root list, which takes just constant time. If we were to startwith an empty Fibonacci heap and then insert knodes, the Fibonacci heap would
consist of just a root list of knodes. The trade-off is that if we then perform
an E
XTRACT -MINoperation on Fibonacci heap H, after removing the node that
H:minpoints to, we would have to look through each of the remaining k/NUL1nodes
in the root list to ﬁnd the new minimum node. As long as we have to go throughthe entire root list during the E
XTRACT -MINoperation, we also consolidate nodes
into min-heap-ordered trees to reduce the size of the root list. We shall see that, nomatter what the root list looks like before a E
XTRACT -MINoperation, afterward
each node in the root list has a degree that is unique within the root list, which leadsto a root list of size at most D.n/C1.
Creating a new Fibonacci heap
To make an empty Fibonacci heap, the M
AKE-FIB-HEAP procedure allocates and
returns the Fibonacci heap object H,w h e r e H:nD0andH:minDNIL;t h e r e
are no trees in H. Because t.H/D0andm.H/D0, the potential of the empty
Fibonacci heap is ˆ.H/D0. The amortized cost of M AKE-FIB-HEAP is thus
equal to its O.1/ actual cost.
Inserting a node
The following procedure inserts node xinto Fibonacci heap H, assuming that the
node has already been allocated and that x:keyhas already been ﬁlled in.
FIB-HEAP-INSERT .H; x/
1x:degreeD0
2x:pDNIL
3x:childDNIL
4x:markDFALSE
5ifH:min ==NIL
6 create a root list for Hcontaining just x
7 H:minDx
8elseinsert xintoH’s root list
9 ifx:key<H : min:key
10 H:minDx
11 nD nC1 H: H:19.2 Mergeable-heap operations 511
(a) (b)17
3024 23
26
3546721
18 52 38
39 413 17
3024 23
26
35467
18 52 38
39 413H:min H:min
Figure 19.3 Inserting a node into a Fibonacci heap. (a)A Fibonacci heap H.(b)Fibonacci heap H
after inserting the node with key 21. The node becomes its own min-heap-ordered tree and is then
added to the root list, becoming the left sibling of the root.
Lines 1–4 initialize some of the structural attributes of node x. Line 5 tests to see
whether Fibonacci heap His empty. If it is, then lines 6–7 make xbe the only
node in H’s root list and set H:minto point to x. Otherwise, lines 8–10 insert x
intoH’s root list and update H:minif necessary. Finally, line 11 increments H:n
to reﬂect the addition of the new node. Figure 19.3 shows a node with key 21
inserted into the Fibonacci heap of Figure 19.2.
To determine the amortized cost of F IB-HEAP-INSERT ,l e tHbe the input Fi-
bonacci heap and H0be the resulting Fibonacci heap. Then, t.H0/Dt.H/C1
andm.H0/Dm.H/ , and the increase in potential is
..t.H/C1/C2 m.H///NUL.t.H/C2 m.H//D1:
Since the actual cost is O.1/ , the amortized cost is O.1/C1DO.1/ .
Finding the minimum node
The minimum node of a Fibonacci heap His given by the pointer H:min,s ow e
can ﬁnd the minimum node in O.1/ actual time. Because the potential of Hdoes
not change, the amortized cost of this operation is equal to its O.1/ actual cost.
Uniting two Fibonacci heaps
The following procedure unites Fibonacci heaps H1andH2, destroying H1andH2
in the process. It simply concatenates the root lists of H1andH2and then deter-
mines the new minimum node. Afterward, the objects representing H1andH2will
never be used again.512 Chapter 19 Fibonacci Heaps
FIB-HEAP-UNION .H1;H2/
1HDMAKE-FIB-HEAP./
2H:minDH1:min
3 concatenate the root list of H2with the root list of H
4if.H1:min ==NIL/or.H2:min¤NILandH2:min:key<H 1:min:key/
5 H:minDH2:min
6H:nDH1:nCH2:n
7return H
Lines 1–3 concatenate the root lists of H1andH2into a new root list H.L i n e s
2, 4, and 5 set the minimum node of H, and line 6 sets H:nto the total number
of nodes. Line 7 returns the resulting Fibonacci heap H.A s i n t h e F IB-HEAP-
INSERT procedure, all roots remain roots.
The change in potential is
ˆ.H//NUL.ˆ.H 1/Cˆ.H 2//
D.t.H/C2 m.H///NUL..t.H 1/C2m . H 1//C.t.H 2/C2m . H 2///
D0;
because t.H/Dt.H 1/Ct.H 2/andm.H/Dm.H 1/Cm.H 2/. The amortized
cost of F IB-HEAP-UNION is therefore equal to its O.1/ actual cost.
Extracting the minimum node
The process of extracting the minimum node is the most complicated of the oper-
ations presented in this section. It is also where the delayed work of consolidatingtrees in the root list ﬁnally occurs. The following pseudocode extracts the mini-mum node. The code assumes for convenience that when a node is removed froma linked list, pointers remaining in the list are updated, but pointers in the extracted
node are left unchanged. It also calls the auxiliary procedure C
ONSOLIDATE ,
which we shall see shortly.19.2 Mergeable-heap operations 513
FIB-HEAP-EXTRACT -MIN.H/
1´DH:min
2if´¤NIL
3 foreach child xof´
4a d d xto the root list of H
5 x:pDNIL
6 remove ´from the root list of H
7 if´==´:right
8 H:minDNIL
9 elseH:minD´:right
10 C ONSOLIDATE .H/
11 H:nDH:n/NUL1
12return ´
As Figure 19.4 illustrates, F IB-HEAP-EXTRACT -MINworks by ﬁrst making a root
out of each of the minimum node’s children and removing the minimum node fromthe root list. It then consolidates the root list by linking roots of equal degree untilat most one root remains of each degree.
We start in line 1 by saving a pointer ´to the minimum node; the procedure
returns this pointer at the end. If ´is
NIL, then Fibonacci heap His already empty
and we are done. Otherwise, we delete node ´from Hby making all of ´’s chil-
dren roots of Hin lines 3–5 (putting them into the root list) and removing ´from
the root list in line 6. If ´is its own right sibling after line 6, then ´was the
only node on the root list and it had no children, so all that remains is to make
the Fibonacci heap empty in line 8 before returning ´. Otherwise, we set the
pointer H:mininto the root list to point to a root other than ´(in this case, ´’s
right sibling), which is not necessarily going to be the new minimum node when
FIB-HEAP-EXTRACT -MINis done. Figure 19.4(b) shows the Fibonacci heap of
Figure 19.4(a) after executing line 9.
The next step, in which we reduce the number of trees in the Fibonacci heap, is
consolidating the root list of H, which the call C ONSOLIDATE .H/ accomplishes.
Consolidating the root list consists of repeatedly executing the following steps untilevery root in the root list has a distinct degree value:
1. Find two roots xandyin the root list with the same degree. Without loss of
generality, let x:key/DC4y:key.
2.Link ytox: remove yfrom the root list, and make ya child of xby calling the
F
IB-HEAP-LINKprocedure. This procedure increments the attribute x:degree
and clears the mark on y.514 Chapter 19 Fibonacci Heaps
A0123
A0123
A0123
A0123A0123A0123
(c) (d)
(e)
17
3024 23
26
3546717
3024 23
26
3546721
18 52 38
39 41(a) 3 (b)
(f)
(g) 2118 52 38
39 41(h)17
3024 23
26
354672118 52 38
39 41
17
3024 23
26
354672118 52 38
39 4117
3024 23
26
354672118 52 38
39 41
17
3024 23
26
354672118 52 38
39 4117
3024
23 26
354672118 52 38
39 41
17
3024
23 26
35467 2118 52 38
39 41w,x w,x
w,x w,x
w,x w,xH:min H:min
Figure 19.4 The action of F IB-HEAP-EXTRACT -MIN.(a)A Fibonacci heap H.(b)The situa-
tion after removing the minimum node ´from the root list and adding its children to the root list.
(c)–(e) The array Aand the trees after each of the ﬁrst three iterations of the forloop of lines 4–14 of
the procedure C ONSOLIDATE . The procedure processes the root list by starting at the node pointed
to by H:minand following right pointers. Each part shows the values of wandxat the end of an
iteration. (f)–(h) The next iteration of the forloop, with the values of wandxs h o w na tt h ee n do f
each iteration of the while loop of lines 7–13. Part (f) shows the situation after the ﬁrst time through
thewhile loop. The node with key 23has been linked to the node with key 7,w h i c h xnow points to.
In part (g), the node with key 17has been linked to the node with key 7,w h i c h xstill points to. In
part (h), the node with key 24has been linked to the node with key 7. Since no node was previously
pointed to by AŒ3/c141, at the end of the forloop iteration, AŒ3/c141 is set to point to the root of the resulting
tree.19.2 Mergeable-heap operations 515
A0123
A0123A0123
A0123
17
3024 23
26
35467 21 18 52 38
39 41(i)
17
3024 23
26
35467 21 18 52 38
39 41(j)
17
3024 23
26
35467 38
41(k)
2118
5239 17
3024 23
26
35467 38
41(l)
2118
5239
17
3024 23
26
35467 38
41(m)
2118
5239w,x w,x
x w,x
w
H:min
Figure 19.4, continued (i)–(l) The situation after each of the next four iterations of the forloop.
(m)Fibonacci heap Hafter reconstructing the root list from the array Aand determining the new
H:minpointer.
The procedure C ONSOLIDATE uses an auxiliary array AŒ0 : : D.H: n//c141to keep
track of roots according to their degrees. If AŒi/c141Dy,t h e n yis currently a root
withy:degreeDi. Of course, in order to allocate the array we have to know how
to calculate the upper bound D.H: n/on the maximum degree, but we will see how
to do so in Section 19.4.516 Chapter 19 Fibonacci Heaps
CONSOLIDATE .H/
1l e t AŒ0 : : D.H: n//c141b ean e wa r r a y
2foriD0toD.H: n/
3 AŒi/c141DNIL
4foreach node win the root list of H
5 xDw
6 dDx:degree
7 while AŒd/c141¤NIL
8 yDAŒd/c141 //another node with the same degree as x
9 ifx:key>y : key
10 exchange xwithy
11 F IB-HEAP-LINK. H;y;x/
12 AŒd/c141DNIL
13 dDdC1
14 AŒd/c141Dx
15H:minDNIL
16foriD0toD.H: n/
17 ifAŒi/c141¤NIL
18 ifH:min ==NIL
19 create a root list for Hcontaining just AŒi/c141
20 H:minDAŒi/c141
21 elseinsert AŒi/c141 intoH’s root list
22 ifAŒi/c141: key<H : min:key
23 H:minDAŒi/c141
FIB-HEAP-LINK. H;y;x/
1 remove yfrom the root list of H
2m a k e ya child of x, incrementing x:degree
3y:markDFALSE
In detail, the C ONSOLIDATE procedure works as follows. Lines 1–3 allocate
and initialize the array Aby making each entry NIL.T h e forloop of lines 4–14
processes each root win the root list. As we link roots together, wmay be linked
to some other node and no longer be a root. Nevertheless, wis always in a tree
rooted at some node x, which may or may not be witself. Because we want at
most one root with each degree, we look in the array Ato see whether it contains
a root ywith the same degree as x. If it does, then we link the roots xandybut
guaranteeing that xremains a root after linking. That is, we link ytoxafter ﬁrst
exchanging the pointers to the two roots if y’s key is smaller than x’s key. After
we link ytox, the degree of xhas increased by 1, and so we continue this process,
linking xand another root whose degree equals x’s new degree, until no other root19.2 Mergeable-heap operations 517
that we have processed has the same degree as x. We then set the appropriate entry
ofAto point to x, so that as we process roots later on, we have recorded that xis
the unique root of its degree that we have already processed. When this forloop
terminates, at most one root of each degree will remain, and the array Awill point
to each remaining root.
Thewhile loop of lines 7–13 repeatedly links the root xof the tree containing
node wto another tree whose root has the same degree as x, until no other root has
the same degree. This while loop maintains the following invariant:
At the start of each iteration of the while loop, dDx:degree .
We use this loop invariant as follows:
Initialization: Line 6 ensures that the loop invariant holds the ﬁrst time we enter
the loop.
Maintenance: In each iteration of the while loop, AŒd/c141 points to some root y.
Because dDx:degreeDy:degree , we want to link xandy. Whichever of
xandyhas the smaller key becomes the parent of the other as a result of the
link operation, and so lines 9–10 exchange the pointers to xandyif necessary.
Next, we link ytoxby the call F IB-HEAP-LINK. H;y;x/ in line 11. This
call increments x:degree but leaves y:degree asd. Node yis no longer a root,
and so line 12 removes the pointer to it in array A. Because the call of F IB-
HEAP-LINK increments the value of x:degree , line 13 restores the invariant
thatdDx:degree .
Termination: We repeat the while loop until AŒd/c141DNIL, in which case there is
no other root with the same degree as x.
After the while loop terminates, we set AŒd/c141 toxin line 14 and perform the next
iteration of the forloop.
Figures 19.4(c)–(e) show the array Aand the resulting trees after the ﬁrst three
iterations of the forloop of lines 4–14. In the next iteration of the forloop, three
links occur; their results are shown in Figures 19.4(f)–(h). Figures 19.4(i)–(l) showthe result of the next four iterations of the forloop.
All that remains is to clean up. Once the forloop of lines 4–14 completes,
line 15 empties the root list, and lines 16–23 reconstruct it from the array A.T h e
resulting Fibonacci heap appears in Figure 19.4(m). After consolidating the rootlist, F
IB-HEAP-EXTRACT -MINﬁnishes up by decrementing H:nin line 11 and
returning a pointer to the deleted node ´in line 12.
We are now ready to show that the amortized cost of extracting the minimum
node of an n-node Fibonacci heap is O.D.n// .L e t Hdenote the Fibonacci heap
just prior to the F IB-HEAP-EXTRACT -MINoperation.
We start by accounting for the actual cost of extracting the minimum node.
AnO.D.n// contribution comes from F IB-HEAP-EXTRACT -MINprocessing at518 Chapter 19 Fibonacci Heaps
most D.n/ children of the minimum node and from the work in lines 2–3 and
16–23 of C ONSOLIDATE . It remains to analyze the contribution from the forloop
of lines 4–14 in C ONSOLIDATE , for which we use an aggregate analysis. The size
of the root list upon calling C ONSOLIDATE is at most D.n/Ct.H//NUL1, since it
consists of the original t.H/ root-list nodes, minus the extracted root node, plus
the children of the extracted node, which number at most D.n/ . Within a given
iteration of the forloop of lines 4–14, the number of iterations of the while loop of
lines 7–13 depends on the root list. But we know that every time through the while
loop, one of the roots is linked to another, and thus the total number of iterations
of the while loop over all iterations of the forloop is at most the number of roots
in the root list. Hence, the total amount of work performed in the forloop is at
most proportional to D.n/Ct.H/ . Thus, the total actual work in extracting the
minimum node is O.D.n/Ct.H// .
The potential before extracting the minimum node is t.H/C2m . H/ ,a n dt h e
potential afterward is at most .D.n/C1/C2m . H/ , since at most D.n/C1roots
remain and no nodes become marked during the operation. The amortized cost is
thus at most
O.D.n/Ct.H//C..D.n/C1/C2 m.H///NUL.t.H/C2 m.H//
DO.D.n//CO.t.H///NULt.H/
DO.D.n// ;
since we can scale up the units of potential to dominate the constant hidden
inO.t.H// . Intuitively, the cost of performing each link is paid for by the re-
duction in potential due to the link’s reducing the number of roots by one. We shallsee in Section 19.4 that D.n/DO.lgn/, so that the amortized cost of extracting
the minimum node is O.lgn/.
Exercises
19.2-1
Show the Fibonacci heap that results from calling F
IB-HEAP-EXTRACT -MINon
the Fibonacci heap shown in Figure 19.4(m).
19.3 Decreasing a key and deleting a node
In this section, we show how to decrease the key of a node in a Fibonacci heapinO.1/ amortized time and how to delete any node from an n-node Fibonacci
heap in O.D.n// amortized time. In Section 19.4, we will show that the maxi-19.3 Decreasing a key and deleting a node 519
mum degree D.n/ isO.lgn/, which will imply that F IB-HEAP-EXTRACT -MIN
and F IB-HEAP-DELETE run in O.lgn/amortized time.
Decreasing a key
In the following pseudocode for the operation F IB-HEAP-DECREASE -KEY,w e
assume as before that removing a node from a linked list does not change any ofthe structural attributes in the removed node.
F
IB-HEAP-DECREASE -KEY. H;x;k/
1ifk>x : key
2 error “new key is greater than current key”
3x:keyDk
4yDx:p
5ify¤NILandx:key<y : key
6C UT. H;x;y/
7C ASCADING -CUT.H; y/
8ifx:key<H : min:key
9 H:minDx
CUT. H;x;y/
1 remove xfrom the child list of y, decrementing y:degree
2a d d xto the root list of H
3x:pDNIL
4x:markDFALSE
CASCADING -CUT.H; y/
1´Dy:p
2if´¤NIL
3 ify:mark ==FALSE
4 y:markDTRUE
5 elseCUT. H;y;´ /
6C ASCADING -CUT.H; ´/
The F IB-HEAP-DECREASE -KEYprocedure works as follows. Lines 1–3 ensure
that the new key is no greater than the current key of xand then assign the new key
tox.I fxis a root or if x:key/NAKy:key,w h e r e yisx’s parent, then no structural
changes need occur, since min-heap order has not been violated. Lines 4–5 test forthis condition.
If min-heap order has been violated, many changes may occur. We start by
cutting xin line 6. The C
UTprocedure “cuts” the link between xand its parent y,
making xa root.520 Chapter 19 Fibonacci Heaps
We use the mark attributes to obtain the desired time bounds. They record a little
piece of the history of each node. Suppose that the following events have happenedto node x:
1. at some time, xw a sar o o t ,
2. then xwas linked to (made the child of) another node,
3. then two children of xwere removed by cuts.
As soon as the second child has been lost, we cut xfrom its parent, making it a new
root. The attribute x:mark is
TRUE if steps 1 and 2 have occurred and one child
ofxhas been cut. The C UTprocedure, therefore, clears x:mark in line 4, since it
performs step 1. (We can now see why line 3 of F IB-HEAP-LINK clears y:mark :
node yis being linked to another node, and so step 2 is being performed. The next
time a child of yis cut, y:mark will be set to TRUE .)
We are not yet done, because xmight be the second child cut from its parent y
since the time that ywas linked to another node. Therefore, line 7 of F IB-HEAP-
DECREASE -KEYattempts to perform a cascading-cut operation on y.I fyis a
root, then the test in line 2 of C ASCADING -CUTcauses the procedure to just return.
Ifyis unmarked, the procedure marks it in line 4, since its ﬁrst child has just been
cut, and returns. If yis marked, however, it has just lost its second child; yis cut
in line 5, and C ASCADING -CUTcalls itself recursively in line 6 on y’s parent ´.
The C ASCADING -CUTprocedure recurses its way up the tree until it ﬁnds either a
root or an unmarked node.
Once all the cascading cuts have occurred, lines 8–9 of F IB-HEAP-DECREASE -
KEYﬁnish up by updating H:minif necessary. The only node whose key changed
was the node xwhose key decreased. Thus, the new minimum node is either the
original minimum node or node x.
Figure 19.5 shows the execution of two calls of F IB-HEAP-DECREASE -KEY,
starting with the Fibonacci heap shown in Figure 19.5(a). The ﬁrst call, shownin Figure 19.5(b), involves no cascading cuts. The second call, shown in Fig-ures 19.5(c)–(e), invokes two cascading cuts.
We shall now show that the amortized cost of F
IB-HEAP-DECREASE -KEYis
onlyO.1/ . We start by determining its actual cost. The F IB-HEAP-DECREASE -
KEYprocedure takes O.1/ time, plus the time to perform the cascading cuts. Sup-
pose that a given invocation of F IB-HEAP-DECREASE -KEYresults in ccalls of
CASCADING -CUT(the call made from line 7 of F IB-HEAP-DECREASE -KEYfol-
lowed by c/NUL1recursive calls of C ASCADING -CUT). Each call of C ASCADING -
CUTtakes O.1/ time exclusive of recursive calls. Thus, the actual cost of F IB-
HEAP-DECREASE -KEY, including all recursive calls, is O.c/ .
We next compute the change in potential. Let Hdenote the Fibonacci heap just
prior to the F IB-HEAP-DECREASE -KEYoperation. The call to C UTin line 6 of19.3 Decreasing a key and deleting a node 521
17
3024 23
26
3515 7
2118
5238
39 41(b)
17
3024 23
265 15 7
2118
5238
39 41(c)
17
3024 23265 15 7
2118
5238
39 41(d)
17
3024
23265 15 7
2118
5238
39 41(e)17
3024 23
26
35467
2118
5238
39 41(a)H:min
H:minH:min H:minH:min
Figure 19.5 Two calls of F IB-HEAP-DECREASE -KEY.(a)The initial Fibonacci heap. (b)The
node with key 46has its key decreased to 15. The node becomes a root, and its parent (with key 24),
which had previously been unmarked, becomes marked. (c)–(e) The node with key 35has its key
decreased to 5. In part (c), the node, now with key 5, becomes a root. Its parent, with key 26,
is marked, so a cascading cut occurs. The node with key 26is cut from its parent and made an
unmarked root in (d). Another cascading cut occurs, since the node with key 24is marked as well.
This node is cut from its parent and made an unmarked root in part (e). The cascading cuts stop
at this point, since the node with key 7is a root. (Even if this node were not a root, the cascading
cuts would stop, since it is unmarked.) Part (e) shows the result of the F IB-HEAP-DECREASE -KEY
operation, with H:minpointing to the new minimum node.
FIB-HEAP-DECREASE -KEYcreates a new tree rooted at node xand clears x’s
mark bit (which may have already been FALSE ). Each call of C ASCADING -CUT,
except for the last one, cuts a marked node and clears the mark bit. Afterward, theFibonacci heap contains t.H/Cctrees (the original t.H/ trees, c/NUL1trees produced
by cascading cuts, and the tree rooted at x) and at most m.H//NULcC2marked nodes
(c/NUL1were unmarked by cascading cuts and the last call of C
ASCADING -CUTmay
have marked a node). The change in potential is therefore at most
..t.H/Cc/C2.m.H//NULcC2///NUL.t.H/C2 m.H//D4/NULc:522 Chapter 19 Fibonacci Heaps
Thus, the amortized cost of F IB-HEAP-DECREASE -KEYis at most
O.c/C4/NULcDO.1/ ;
since we can scale up the units of potential to dominate the constant hidden in O.c/ .
You can now see why we deﬁned the potential function to include a term that is
twice the number of marked nodes. When a marked node yis cut by a cascading
cut, its mark bit is cleared, which reduces the potential by 2. One unit of potential
pays for the cut and the clearing of the mark bit, and the other unit compensates
for the unit increase in potential due to node ybecoming a root.
Deleting a node
The following pseudocode deletes a node from an n-node Fibonacci heap in
O.D.n// amortized time. We assume that there is no key value of /NUL1 currently
in the Fibonacci heap.
FIB-HEAP-DELETE .H; x/
1F IB-HEAP-DECREASE -KEY.H; x;/NUL1/
2F IB-HEAP-EXTRACT -MIN.H/
FIB-HEAP-DELETE makes xbecome the minimum node in the Fibonacci heap by
giving it a uniquely small key of /NUL1.T h eF IB-HEAP-EXTRACT -MINprocedure
then removes node xfrom the Fibonacci heap. The amortized time of F IB-HEAP-
DELETE is the sum of the O.1/ amortized time of F IB-HEAP-DECREASE -KEY
and the O.D.n// amortized time of F IB-HEAP-EXTRACT -MIN. Since we shall see
in Section 19.4 that D.n/DO.lgn/, the amortized time of F IB-HEAP-DELETE
isO.lgn/.
Exercises
19.3-1
Suppose that a root xin a Fibonacci heap is marked. Explain how xcame to be
a marked root. Argue that it doesn’t matter to the analysis that xis marked, even
though it is not a root that was ﬁrst linked to another node and then lost one child.
19.3-2
Justify the O.1/ amortized time of F IB-HEAP-DECREASE -KEYas an average cost
per operation by using aggregate analysis.19.4 Bounding the maximum degree 523
19.4 Bounding the maximum degree
To prove that the amortized time of F IB-HEAP-EXTRACT -MINand F IB-HEAP-
DELETE isO.lgn/, we must show that the upper bound D.n/ on the degree of
any node of an n-node Fibonacci heap is O.lgn/. In particular, we shall show that
D.n//DC4/EOT
log/RSn˘
,w h e r e /RSis the golden ratio, deﬁned in equation (3.24) as
/RSD.1Cp
5/=2D1:61803 : : : :
The key to the analysis is as follows. For each node xwithin a Fibonacci heap,
deﬁne size .x/to be the number of nodes, including xitself, in the subtree rooted
atx. (Note that xneed not be in the root list—it can be any node at all.) We shall
show that size .x/is exponential in x:degree . Bear in mind that x:degree is always
maintained as an accurate count of the degree of x.
Lemma 19.1
Letxbe any node in a Fibonacci heap, and suppose that x:degreeDk.L e t
y1;y2;:::;y kdenote the children of xin the order in which they were linked to x,
from the earliest to the latest. Then, y1:degree/NAK0andyi:degree/NAKi/NUL2for
iD2;3 ;:::;k .
Proof Obviously, y1:degree/NAK0.
Fori/NAK2, we note that when yiwas linked to x,a l lo f y1;y2;:::;y i/NUL1were
children of x, and so we must have had x:degree/NAKi/NUL1. Because node yiis
linked to x(by C ONSOLIDATE ) only if x:degreeDyi:degree ,w em u s th a v ea l s o
hadyi:degree/NAKi/NUL1at that time. Since then, node yihas lost at most one
child, since it would have been cut from x(by C ASCADING -CUT) if it had lost
two children. We conclude that yi:degree/NAKi/NUL2.
We ﬁnally come to the part of the analysis that explains the name “Fibonacci
heaps.” Recall from Section 3.2 that for kD0; 1; 2; : : : ,t h ekth Fibonacci number
is deﬁned by the recurrence
FkD/c128
0 ifkD0;
1 ifkD1;
Fk/NUL1CFk/NUL2ifk/NAK2:
The following lemma gives another way to express Fk.524 Chapter 19 Fibonacci Heaps
Lemma 19.2
For all integers k/NAK0,
FkC2D1CkX
iD0Fi:
Proof The proof is by induction on k.W h e n kD0,
1C0X
iD0FiD1CF0
D1C0
DF2:
We now assume the inductive hypothesis that FkC1D1CPk/NUL1
iD0Fi,a n dw e
have
FkC2DFkCFkC1
DFkC 
1Ck/NUL1X
iD0Fi!
D1CkX
iD0Fi:
Lemma 19.3
For all integers k/NAK0,t h e.kC2/nd Fibonacci number satisﬁes FkC2/NAK/RSk.
Proof The proof is by induction on k. The base cases are for kD0andkD1.
When kD0we have F2D1D/RS0,a n dw h e n kD1we have F3D2>
1:619 > /RS1. The inductive step is for k/NAK2, and we assume that FiC2>/RSifor
iD0; 1; : : : ; k/NUL1. Recall that /RSis the positive root of equation (3.23), x2DxC1.
Thus, we have
FkC2DFkC1CFk
/NAK/RSk/NUL1C/RSk/NUL2(by the inductive hypothesis)
D/RSk/NUL2./RSC1/
D/RSk/NUL2/SOH/RS2(by equation (3.23))
D/RSk:
The following lemma and its corollary complete the analysis.19.4 Bounding the maximum degree 525
Lemma 19.4
Letxbe any node in a Fibonacci heap, and let kDx:degree .T h e n s i z e .x//NAK
FkC2/NAK/RSk,w h e r e /RSD.1Cp
5/=2 .
Proof Letskdenote the minimum possible size of any node of degree kin any
Fibonacci heap. Trivially, s0D1ands1D2. The number skis at most size .x/
and, because adding children to a node cannot decrease the node’s size, the valueofs
kincreases monotonically with k. Consider some node ´, in any Fibonacci
heap, such that ´:degreeDkand size .´/Dsk. Because sk/DC4size.x/,w e
compute a lower bound on size .x/by computing a lower bound on sk.A s i n
Lemma 19.1, let y1;y2;:::;y kdenote the children of ´in the order in which they
were linked to ´. To bound sk, we count one for ´itself and one for the ﬁrst child y1
(for which size .y1//NAK1), giving
size.x//NAKsk
/NAK2CkX
iD2syi:degree
/NAK2CkX
iD2si/NUL2;
where the last line follows from Lemma 19.1 (so that yi:degree/NAKi/NUL2)a n dt h e
monotonicity of sk(so that syi:degree/NAKsi/NUL2).
We now show by induction on kthatsk/NAKFkC2for all nonnegative integers k.
The bases, for kD0andkD1, are trivial. For the inductive step, we assume that
k/NAK2and that si/NAKFiC2foriD0; 1; : : : ; k/NUL1.W eh a v e
sk/NAK2CkX
iD2si/NUL2
/NAK2CkX
iD2Fi
D1CkX
iD0Fi
DFkC2 (by Lemma 19.2)
/NAK/RSk(by Lemma 19.3) .
Thus, we have shown that size .x//NAKsk/NAKFkC2/NAK/RSk.
526 Chapter 19 Fibonacci Heaps
Corollary 19.5
The maximum degree D.n/ of any node in an n-node Fibonacci heap is O.lgn/.
Proof Letxbe any node in an n-node Fibonacci heap, and let kDx:degree .
By Lemma 19.4, we have n/NAKsize.x//NAK/RSk. Taking base- /RSlogarithms gives
usk/DC4log/RSn. (In fact, because kis an integer, k/DC4/EOT
log/RSn˘
.) The maximum
degree D.n/ of any node is thus O.lgn/.
Exercises
19.4-1
Professor Pinocchio claims that the height of an n-node Fibonacci heap is O.lgn/.
Show that the professor is mistaken by exhibiting, for any positive integer n,a
sequence of Fibonacci-heap operations that creates a Fibonacci heap consisting ofjust one tree that is a linear chain of nnodes.
19.4-2
Suppose we generalize the cascading-cut rule to cut a node xfrom its parent as
soon as it loses its kth child, for some integer constant k. (The rule in Section 19.3
useskD2.) For what values of kisD.n/DO.lgn/?
Problems
19-1 Alternative implementation of deletion
Professor Pisano has proposed the following variant of the F IB-HEAP-DELETE
procedure, claiming that it runs faster when the node being deleted is not the nodepointed to by H:min.
P
ISANO -DELETE .H; x/
1ifx==H:min
2F IB-HEAP-EXTRACT -MIN.H/
3elseyDx:p
4 ify¤NIL
5C UT. H;x;y/
6C ASCADING -CUT.H; y/
7a d d x’s child list to the root list of H
8 remove xfrom the root list of HProblems for Chapter 19 527
a.The professor’s claim that this procedure runs faster is based partly on the as-
sumption that line 7 can be performed in O.1/ actual time. What is wrong with
this assumption?
b.Give a good upper bound on the actual time of P ISANO -DELETE when xis
notH:min. Your bound should be in terms of x:degree and the number cof
calls to the C ASCADING -CUTprocedure.
c.Suppose that we call P ISANO -DELETE .H; x/ ,a n dl e t H0be the Fibonacci heap
that results. Assuming that node xis not a root, bound the potential of H0in
terms of x:degree ,c,t.H/ ,a n d m.H/ .
d.Conclude that the amortized time for P ISANO -DELETE is asymptotically no
better than for F IB-HEAP-DELETE ,e v e nw h e n x¤H:min.
19-2 Binomial trees and binomial heaps
Thebinomial tree Bkis an ordered tree (see Section B.5.2) deﬁned recursively.
As shown in Figure 19.6(a), the binomial tree B0consists of a single node. The
binomial tree Bkconsists of two binomial trees Bk/NUL1that are linked together so
that the root of one is the leftmost child of the root of the other. Figure 19.6(b)shows the binomial trees B
0through B4.
a.Show that for the binomial tree Bk,
1. there are 2knodes,
2. the height of the tree is k,
3. there are exactly/NULk
i/SOH
nodes at depth iforiD0; 1; : : : ; k ,a n d
4. the root has degree k, which is greater than that of any other node; moreover,
as Figure 19.6(c) shows, if we number the children of the root from left toright by k/NUL1; k/NUL2;:::;0 , then child iis the root of a subtree B
i.
Abinomial heap His a set of binomial trees that satisﬁes the following proper-
ties:
1. Each node has a key(like a Fibonacci heap).
2. Each binomial tree in Hobeys the min-heap property.
3. For any nonnegative integer k, there is at most one binomial tree in Hwhose
root has degree k.
b.Suppose that a binomial heap Hhas a total of nnodes. Discuss the relationship
between the binomial trees that Hcontains and the binary representation of n.
Conclude that Hconsists of at mostblgncC1binomial trees.528 Chapter 19 Fibonacci Heaps
B4
Bk–1Bk–2
BkB2B1B0B3 B2 B1 B0BkBk–1Bk–1
B0(a)
depth
0
1
2
3
4(b)
(c)
Figure 19.6 (a) The recursive deﬁnition of the binomial tree Bk. Triangles represent rooted sub-
trees. (b)The binomial trees B0through B4. Node depths in B4are shown. (c)Another way of
looking at the binomial tree Bk.
Suppose that we represent a binomial heap as follows. The left-child, right-
sibling scheme of Section 10.4 represents each binomial tree within a binomialheap. Each node contains its key; pointers to its parent, to its leftmost child, andto the sibling immediately to its right (these pointers are
NILwhen appropriate);
and its degree (as in Fibonacci heaps, how many children it has). The roots form asingly linked root list, ordered by the degrees of the roots (from low to high), and
we access the binomial heap by a pointer to the ﬁrst node on the root list.
c.Complete the description of how to represent a binomial heap (i.e., name the
attributes, describe when attributes have the value
NIL, and deﬁne how the root
list is organized), and show how to implement the same seven operations onbinomial heaps as this chapter implemented on Fibonacci heaps. Each opera-tion should run in O.lgn/worst-case time, where nis the number of nodes inProblems for Chapter 19 529
the binomial heap (or in the case of the U NION operation, in the two binomial
heaps that are being united). The M AKE-HEAP operation should take constant
time.
d.Suppose that we were to implement only the mergeable-heap operations on a
Fibonacci heap (i.e., we do not implement the D ECREASE -KEYor D ELETE op-
erations). How would the trees in a Fibonacci heap resemble those in a binomialheap? How would they differ? Show that the maximum degree in an n-node
Fibonacci heap would be at most blgnc.
e.Professor McGee has devised a new data structure based on Fibonacci heaps.
A McGee heap has the same structure as a Fibonacci heap and supports justthe mergeable-heap operations. The implementations of the operations are thesame as for Fibonacci heaps, except that insertion and union consolidate theroot list as their last step. What are the worst-case running times of operationson McGee heaps?
19-3 More Fibonacci-heap operations
We wish to augment a Fibonacci heap Hto support two new operations without
changing the amortized running time of any other Fibonacci-heap operations.
a.The operation F
IB-HEAP-CHANGE -KEY. H;x;k/ changes the key of node x
to the value k. Give an efﬁcient implementation of F IB-HEAP-CHANGE -KEY,
and analyze the amortized running time of your implementation for the cases
in which kis greater than, less than, or equal to x:key.
b.Give an efﬁcient implementation of F IB-HEAP-PRUNE .H; r/ , which deletes
qDmin.r; H: n/nodes from H. You may choose any qnodes to delete. Ana-
lyze the amortized running time of your implementation. ( Hint: You may need
to modify the data structure and potential function.)
19-4 2-3-4 heaps
Chapter 18 introduced the 2-3-4 tree, in which every internal node (other than pos-sibly the root) has two, three, or four children and all leaves have the same depth. Inthis problem, we shall implement 2-3-4 heaps , which support the mergeable-heap
operations.
The 2-3-4 heaps differ from 2-3-4 trees in the following ways. In 2-3-4 heaps,
only leaves store keys, and each leaf xstores exactly one key in the attribute x:key.
The keys in the leaves may appear in any order. Each internal node xcontains
av a l u e x:small that is equal to the smallest key stored in any leaf in the subtree
rooted at x. The root rcontains an attribute r:height that gives the height of the530 Chapter 19 Fibonacci Heaps
tree. Finally, 2-3-4 heaps are designed to be kept in main memory, so that disk
reads and writes are not needed.
Implement the following 2-3-4 heap operations. In parts (a)–(e), each operation
should run in O.lgn/time on a 2-3-4 heap with nelements. The U NION operation
in part (f) should run in O.lgn/time, where nis the number of elements in the two
input heaps.
a.MINIMUM , which returns a pointer to the leaf with the smallest key.
b.DECREASE -KEY, which decreases the key of a given leaf xto a given value
k/DC4x:key.
c.INSERT , which inserts leaf xwith key k.
d.DELETE , which deletes a given leaf x.
e.EXTRACT -MIN, which extracts the leaf with the smallest key.
f.UNION , which unites two 2-3-4 heaps, returning a single 2-3-4 heap and de-
stroying the input heaps.
Chapter notes
Fredman and Tarjan [114] introduced Fibonacci heaps. Their paper also describesthe application of Fibonacci heaps to the problems of single-source shortest paths,all-pairs shortest paths, weighted bipartite matching, and the minimum-spanning-tree problem.
Subsequently, Driscoll, Gabow, Shrairman, and Tarjan [96] developed “relaxed
heaps” as an alternative to Fibonacci heaps. They devised two varieties of re-laxed heaps. One gives the same amortized time bounds as Fibonacci heaps. Theother allows D
ECREASE -KEYto run in O.1/ worst-case (not amortized) time and
EXTRACT -MINand D ELETE to run in O.lgn/worst-case time. Relaxed heaps
also have some advantages over Fibonacci heaps in parallel algorithms.
See also the chapter notes for Chapter 6 for other data structures that support fast
DECREASE -KEYoperations when the sequence of values returned by E XTRACT -
MINcalls are monotonically increasing over time and the data are integers in a
speciﬁc range.20 van Emde Boas Trees
In previous chapters, we saw data structures that support the operations of a priority
queue—binary heaps in Chapter 6, red-black trees in Chapter 13,1and Fibonacci
heaps in Chapter 19. In each of these data structures, at least one important op-eration took O.lgn/time, either worst case or amortized. In fact, because each
of these data structures bases its decisions on comparing keys, the /DEL.n lgn/lower
bound for sorting in Section 8.1 tells us that at least one operation will have totake/DEL.lgn/time. Why? If we could perform both the I
NSERT and E XTRACT -MIN
operations in o.lgn/time, then we could sort nkeys in o.nlgn/time by ﬁrst per-
forming nINSERT operations, followed by nEXTRACT -MINoperations.
We saw in Chapter 8, however, that sometimes we can exploit additional infor-
mation about the keys to sort in o.nlgn/time. In particular, with counting sort
we can sort nkeys, each an integer in the range 0tok, in time ‚.nCk/,w h i c h
is‚.n/ when kDO.n/ .
Since we can circumvent the /DEL.n lgn/lower bound for sorting when the keys are
integers in a bounded range, you might wonder whether we can perform each of the
priority-queue operations in o.lgn/time in a similar scenario. In this chapter, we
shall see that we can: van Emde Boas trees support the priority-queue operations,and a few others, each in O.lg lgn/worst-case time. The hitch is that the keys
must be integers in the range 0ton/NUL1, with no duplicates allowed.
Speciﬁcally, van Emde Boas trees support each of the dynamic set operations
listed on page 230—S
EARCH ,INSERT ,DELETE ,M INIMUM ,M AXIMUM ,SUC-
CESSOR ,a n dP REDECESSOR —in O.lg lgn/time. In this chapter, we will omit
discussion of satellite data and focus only on storing keys. Because we concentrateon keys and disallow duplicate keys to be stored, instead of describing the S
EARCH
1Chapter 13 does not explicitly discuss how to implement E XTRACT -MINand D ECREASE -KEY,b u t
we can easily build these operations for any data structure that supports M INIMUM ,DELETE ,a n d
INSERT .532 Chapter 20 van Emde Boas Trees
operation, we will implement the simpler operation M EMBER .S; x/ , which returns
a boolean indicating whether the value xis currently in dynamic set S.
So far, we have used the parameter nfor two distinct purposes: the number of
elements in the dynamic set, and the range of the possible values. To avoid anyfurther confusion, from here on we will use nto denote the number of elements
currently in the set and uas the range of possible values, so that each van Emde
Boas tree operation runs in O.lg lgu/time. We call the set f0; 1; 2; : : : ; u/NUL1g
theuniverse of values that can be stored and utheuniverse size . We assume
throughout this chapter that uis an exact power of 2, i.e., uD2
kfor some integer
k/NAK1.
Section 20.1 starts us out by examining some simple approaches that will get
us going in the right direction. We enhance these approaches in Section 20.2,introducing proto van Emde Boas structures, which are recursive but do not achieveour goal of O.lg lgu/-time operations. Section 20.3 modiﬁes proto van Emde Boas
structures to develop van Emde Boas trees, and it shows how to implement eachoperation in O.lg lgu/time.
20.1 Preliminary approaches
In this section, we shall examine various approaches for storing a dynamic set.
Although none will achieve the O.lg lgu/time bounds that we desire, we will gain
insights that will help us understand van Emde Boas trees when we see them laterin this chapter.
Direct addressing
Direct addressing, as we saw in Section 11.1, provides the simplest approach to
storing a dynamic set. Since in this chapter we are concerned only with storingkeys, we can simplify the direct-addressing approach to store the dynamic set as abit vector, as discussed in Exercise 11.1-2. To store a dynamic set of values fromthe universef0; 1; 2; : : : ; u/NUL1g, we maintain an array AŒ0 : : u/NUL1/c141ofubits. The
entry AŒx/c141 holds a 1if the value xis in the dynamic set, and it holds a 0otherwise.
Although we can perform each of the I
NSERT ,DELETE ,a n dM EMBER operations
inO.1/ time with a bit vector, the remaining operations—M INIMUM ,M AXIMUM ,
SUCCESSOR ,a n dP REDECESSOR —each take ‚.u/ time in the worst case because20.1 Preliminary approaches 533
0
00
11
21
31
41
50
61
70
80
90
100
110
120
131
141
150 1 1 1 0 0 0 11 1 0 11 11
A
Figure 20.1 A binary tree of bits superimposed on top of a bit vector representing the set
f2; 3; 4; 5; 7; 14; 15gwhen uD16. Each internal node contains a 1if and only if some leaf in
its subtree contains a 1. The arrows show the path followed to determine the predecessor of 14in the
set.
we might have to scan through ‚.u/ elements.2For example, if a set contains only
the values 0andu/NUL1, then to ﬁnd the successor of 0, we would have to scan
entries 1through u/NUL2before ﬁnding a 1inAŒu/NUL1/c141.
Superimposing a binary tree structure
We can short-cut long scans in the bit vector by superimposing a binary tree of bits
on top of it. Figure 20.1 shows an example. The entries of the bit vector form theleaves of the binary tree, and each internal node contains a 1if and only if any leaf
in its subtree contains a 1. In other words, the bit stored in an internal node is the
logical-or of its two children.
The operations that took ‚.u/ worst-case time with an unadorned bit vector now
use the tree structure:
/SITo ﬁnd the minimum value in the set, start at the root and head down toward
the leaves, always taking the leftmost node containing a 1.
/SITo ﬁnd the maximum value in the set, start at the root and head down toward
the leaves, always taking the rightmost node containing a 1.
2We assume throughout this chapter that M INIMUM and M AXIMUM return NILif the dynamic set
is empty and that S UCCESSOR and P REDECESSOR return NILif the element they are given has no
successor or predecessor, respectively.534 Chapter 20 van Emde Boas Trees
/SITo ﬁnd the successor of x, start at the leaf indexed by x, and head up toward the
root until we enter a node from the left and this node has a 1in its right child ´.
Then head down through node ´, always taking the leftmost node containing
a1(i.e., ﬁnd the minimum value in the subtree rooted at the right child ´).
/SITo ﬁnd the predecessor of x, start at the leaf indexed by x, and head up toward
the root until we enter a node from the right and this node has a 1in its left
child ´. Then head down through node ´, always taking the rightmost node
containing a 1(i.e., ﬁnd the maximum value in the subtree rooted at the left
child ´).
Figure 20.1 shows the path taken to ﬁnd the predecessor, 7,o ft h ev a l u e 14.
We also augment the I NSERT and D ELETE operations appropriately. When in-
serting a value, we store a 1in each node on the simple path from the appropriate
leaf up to the root. When deleting a value, we go from the appropriate leaf up tothe root, recomputing the bit in each internal node on the path as the logical-or ofits two children.
Since the height of the tree is lg uand each of the above operations makes at
most one pass up the tree and at most one pass down, each operation takes O.lgu/
time in the worst case.
This approach is only marginally better than just using a red-black tree. We can
still perform the M
EMBER operation in O.1/ time, whereas searching a red-black
tree takes O.lgn/time. Then again, if the number nof elements stored is much
smaller than the size uof the universe, a red-black tree would be faster for all the
other operations.
Superimposing a tree of constant height
What happens if we superimpose a tree with greater degree? Let us assume that
the size of the universe is uD22kfor some integer k,s ot h a tp
uis an integer.
Instead of superimposing a binary tree on top of the bit vector, we superimpose atree of degreep
u. Figure 20.2(a) shows such a tree for the same bit vector as in
Figure 20.1. The height of the resulting tree is always 2.
As before, each internal node stores the logical-or of the bits within its sub-
tree, so that thep
uinternal nodes at depth 1summarize each group ofp
uval-
ues. As Figure 20.2(b) demonstrates, we can think of these nodes as an arraysummary Œ0 : :p
u/NUL1/c141,w h e r e summary Œi/c141contains a 1if and only if the subar-
rayAŒip
u::. iC1/p
u/NUL1/c141contains a 1. We call thisp
u-bit subarray of A
theithcluster .F o r a g i v e n v a l u e o f x, the bit AŒx/c141 appears in cluster num-
berbx=p
uc.N o w I NSERT becomes an O.1/ -time operation: to insert x, set
bothAŒx/c141 andsummary Œbx=p
uc/c141to1. We can use the summary array to perform20.1 Preliminary approaches 535
0
00
11
21
31
41
50
61
70
80
90
100
110
120
131
141
1511
1 0 1
(a)0
00
11
21
31
41
50
61
70
80
90
100
110
120
131
141
15
(b)10
11
02
13
A Asummary
p
ubitsp
ubits
Figure 20.2 (a) A tree of degreep
usuperimposed on top of the same bit vector as in Figure 20.1.
Each internal node stores the logical-or of the bits in its subtree. (b)A view of the same structure,
but with the internal nodes at depth 1treated as an array summary Œ0 : :p
u/NUL1/c141,w h e r e summary Œi/c141is
the logical-or of the subarray AŒip
u::. iC1/p
u/NUL1/c141.
each of the operations M INIMUM ,MAXIMUM ,SUCCESSOR ,PREDECESSOR ,a n d
DELETE inO.p
u/time:
/SITo ﬁnd the minimum (maximum) value, ﬁnd the leftmost (rightmost) entry in
summary that contains a 1, say summary Œi/c141, and then do a linear search within
theith cluster for the leftmost (rightmost) 1.
/SITo ﬁnd the successor (predecessor) of x, ﬁrst search to the right (left) within its
cluster. If we ﬁnd a 1, that position gives the result. Otherwise, let iDbx=p
uc
and search to the right (left) within the summary array from index i.T h e ﬁ r s t
position that holds a 1gives the index of a cluster. Search within that cluster
for the leftmost (rightmost) 1. That position holds the successor (predecessor).
/SITo delete the value x,l e tiDbx=p
uc/c141.S e t AŒx/c141 to0and then set summary Œi/c141
to the logical-or of the bits in the ith cluster.
In each of the above operations, we search through at most two clusters ofp
ubits
plus the summary array, and so each operation takes O.p
u/time.
At ﬁrst glance, it seems as though we have made negative progress. Superimpos-
ing a binary tree gave us O.lgu/-time operations, which are asymptotically faster
thanO.p
u/time. Using a tree of degreep
uwill turn out to be a key idea of van
Emde Boas trees, however. We continue down this path in the next section.
Exercises
20.1-1
Modify the data structures in this section to support duplicate keys.536 Chapter 20 van Emde Boas Trees
20.1-2
Modify the data structures in this section to support keys that have associated satel-lite data.
20.1-3
Observe that, using the structures in this section, the way we ﬁnd the successor andpredecessor of a value xdoes not depend on whether xis in the set at the time.
Show how to ﬁnd the successor of xin a binary search tree when xis not stored in
the tree.
20.1-4
Suppose that instead of superimposing a tree of degreep
u,w ew e r et os u p e r i m -
pose a tree of degree u1=k,w h e r e k>1 is a constant. What would be the height of
such a tree, and how long would each of the operations take?
20.2 A recursive structure
In this section, we modify the idea of superimposing a tree of degreep
uon top of
a bit vector. In the previous section, we used a summary structure of sizep
u, with
each entry pointing to another stucture of sizep
u. Now, we make the structure
recursive, shrinking the universe size by the square root at each level of recursion.Starting with a universe of size u, we make structures holdingp
uDu1=2items,
which themselves hold structures of u1=4items, which hold structures of u1=8items,
and so on, down to a base size of 2.
For simplicity, in this section, we assume that uD22kfor some integer k,s o
thatu; u1=2;u1=4;:::are integers. This restriction would be quite severe in practice,
allowing only values of uin the sequence 2; 4; 16; 256; 65536; : : : . We shall see in
the next section how to relax this assumption and assume only that uD2kfor
some integer k. Since the structure we examine in this section is only a precursor
to the true van Emde Boas tree structure, we tolerate this restriction in favor of
aiding our understanding.
Recalling that our goal is to achieve running times of O.lg lgu/for the oper-
ations, let’s think about how we might obtain such running times. At the end of
Section 4.3, we saw that by changing variables, we could show that the recurrence
T .n/D2T/NUL/EOTp
n˘/SOH
Clgn (20.1)
has the solution T .n/DO.lgnlg lgn/. Let’s consider a similar, but simpler,
recurrence:
T. u /DT.p
u/CO.1/ : (20.2)20.2 A recursive structure 537
If we use the same technique, changing variables, we can show that recur-
rence (20.2) has the solution T. u /DO.lg lgu/.L e t mDlgu,s ot h a t uD2m
and we have
T. 2m/DT. 2m=2/CO.1/ :
Now we rename S.m/DT. 2m/, giving the new recurrence
S.m/DS.m=2/CO.1/ :
By case 2 of the master method, this recurrence has the solution S.m/DO.lgm/.
We change back from S.m/ toT. u / ,g i v i n g T. u /DT. 2m/DS.m/DO.lgm/D
O.lg lgu/.
Recurrence (20.2) will guide our search for a data structure. We will design a
recursive data structure that shrinks by a factor ofp
uin each level of its recursion.
When an operation traverses this data structure, it will spend a constant amount oftime at each level before recursing to the level below. Recurrence (20.2) will thencharacterize the running time of the operation.
Here is another way to think of how the term lg lg uends up in the solution to
recurrence (20.2). As we look at the universe size in each level of the recursive datastructure, we see the sequence u; u
1=2;u1=4;u1=8;:::. If we consider how many bits
we need to store the universe size at each level, we need lg uat the top level, and
each level needs half the bits of the previous level. In general, if we start with b
bits and halve the number of bits at each level, then after lg blevels, we get down
to just one bit. Since bDlgu, we see that after lg lg ulevels, we have a universe
size of 2.
Looking back at the data structure in Figure 20.2, a given value xresides in
cluster numberbx=p
uc.I f w e v i e w xas a lg u-bit binary integer, that cluster
number,bx=p
uc, is given by the most signiﬁcant .lgu/=2 bits of x. Within its
cluster, xappears in position xmodp
u, which is given by the least signiﬁcant
.lgu/=2 bits of x. We will need to index in this way, and so let us deﬁne some
functions that will help us do so:
high.x/D/EOT
x=p
u˘
;
low.x/Dxmodp
u;
index .x; y/Dxp
uCy:
The function high .x/gives the most signiﬁcant .lgu/=2 bits of x, producing the
number of x’s cluster. The function low .x/gives the least signiﬁcant .lgu/=2 bits
ofxand provides x’s position within its cluster. The function index .x; y/ builds an
element number from xandy, treating xas the most signiﬁcant .lgu/=2 bits of the
element number and yas the least signiﬁcant .lgu/=2 bits. We have the identity
xDindex .high.x/;low.x//.T h e v a l u e o f uused by each of these functions will538 Chapter 20 van Emde Boas Trees
… 0123p
u/NUL1 proto -/ETBEB.u/
u summary cluster
proto -/ETBEB.p
u/structurep
uproto -/ETBEB.p
u/structures
Figure 20.3 The information in a proto -/ETBEB.u/structure when u/NAK4. The structure contains the
universe size u, a pointer summary to aproto -/ETBEB.p
u/structure, and an array cluster Œ0 : :p
u/NUL1/c141
ofp
upointers to proto -/ETBEB.p
u/structures.
always be the universe size of the data structure in which we call the function,
which changes as we descend into the recursive structure.
20.2.1 Proto van Emde Boas structures
Taking our cue from recurrence (20.2), let us design a recursive data structure to
support the operations. Although this data structure will fail to achieve our goal ofO.lg lgu/time for some operations, it serves as a basis for the van Emde Boas tree
structure that we will see in Section 20.3.
For the universef0; 1; 2; : : : ; u/NUL1g,w ed e ﬁ n ea proto van Emde Boas struc-
ture,o rproto-vEB structure , which we denote as proto -/ETBEB.u/, recursively as
follows. Each proto -/ETBEB.u/structure contains an attribute ugiving its universe
size. In addition, it contains the following:
/SIIfuD2, then it is the base size, and it contains an array AŒ0 : : 1/c141 of two bits.
/SIOtherwise, uD22kfor some integer k/NAK1,s ot h a t u/NAK4. In addition
to the universe size u, the data structure proto -/ETBEB.u/contains the following
attributes, illustrated in Figure 20.3:
/SIa pointer named summary to aproto -/ETBEB.p
u/structure and
/SIan array cluster Œ0 : :p
u/NUL1/c141ofp
upointers, each to a proto -/ETBEB.p
u/struc-
ture.
The element x,w h e r e 0/DC4x<u , is recursively stored in the cluster numbered
high.x/as element low .x/within that cluster.
In the two-level structure of the previous section, each node stores a summary
array of sizep
u, in which each entry contains a bit. From the index of each
entry, we can compute the starting index of the subarray of sizep
uthat the bit
summarizes. In the proto-vEB structure, we use explicit pointers rather than index20.2 A recursive structure 539
0123
cluster u16 summaryproto-vEB (16)
01cluster
u
4summaryproto-vEB (4)
0
1Aproto-vEB (2)
1
101cluster
u
4summaryproto-vEB (4)
01cluster
u
4summaryproto-vEB (4)
01cluster
u
4summaryproto-vEB (4)
01cluster
u
4summaryproto-vEB (4)elements 0,1 elements 2,3 clusters 0,1 clusters 2,3 elements 4,5 elements 6,7
elements 8,9 elements 10,11 elements 12,13 elements 14,15u
2
0
1Aproto-vEB (2)
1
1u
2
0
1Aproto-vEB (2)
0
0u
2
0
1Aproto-vEB (2)
0
1u
2
0
1Aproto-vEB (2)
0
0u
2
0
1Aproto-vEB (2)
0
0u
2
0
1Aproto-vEB (2)
0
0u
20
1Aproto-vEB (2)
0
1u
2
0
1Aproto-vEB (2)
1
1u
2
0
1Aproto-vEB (2)
1
1u
2
0
1Aproto-vEB (2)
1
1u
2
0
1Aproto-vEB (2)
0
0u
2
0
1Aproto-vEB (2)
0
1u
2
0
1Aproto-vEB (2)
0
1u
2
0
1Aproto-vEB (2)
1
1u
2
Figure 20.4 Aproto -/ETBEB.16/ structure representing the set f2; 3; 4; 5; 7; 14; 15g. It points to four
proto -/ETBEB.4/structures in cluster Œ0 : : 3/c141 , and to a summary structure, which is also a proto -/ETBEB.4/.
Each proto -/ETBEB.4/structure points to two proto -/ETBEB.2/structures in cluster Œ0 : : 1/c141 ,a n dt oa
proto -/ETBEB.2/summary. Each proto -/ETBEB.2/structure contains just an array AŒ0 : : 1/c141 of two bits.
Theproto -/ETBEB.2/structures above “elements i,j” store bits iandjof the actual dynamic set, and
theproto -/ETBEB.2/structures above “clusters i,j” store the summary bits for clusters iandjin the
top-level proto -/ETBEB.16/ structure. For clarity, heavy shading indicates the top level of a proto-vEB
structure that stores summary information for its parent structure; such a proto-vEB structure is
otherwise identical to any other proto-vEB structure with the same universe size.540 Chapter 20 van Emde Boas Trees
calculations. The array summary contains the summary bits stored recursively in a
proto-vEB structure, and the array cluster containsp
upointers.
Figure 20.4 shows a fully expanded proto -/ETBEB.16/ structure representing the
setf2; 3; 4; 5; 7; 14; 15g.I ft h ev a l u e iis in the proto-vEB structure pointed to by
summary , then the ith cluster contains some value in the set being represented.
As in the tree of constant height, cluster Œi/c141represents the values ip
uthrough
.iC1/p
u/NUL1, which form the ith cluster.
At the base level, the elements of the actual dynamic sets are stored in some
of the proto -/ETBEB.2/structures, and the remaining proto -/ETBEB.2/structures store
summary bits. Beneath each of the non-summary base structures, the ﬁgure in-dicates which bits it stores. For example, the proto -/ETBEB.2/structure labeled
“elements 6,7” stores bit 6(0, since element 6is not in the set) in its AŒ0/c141 and
bit7(1, since element 7is in the set) in its AŒ1/c141.
Like the clusters, each summary is just a dynamic set with universe sizep
u,
and so we represent each summary as a proto -/ETBEB.p
u/structure. The four sum-
mary bits for the main proto -/ETBEB.16/ structure are in the leftmost proto -/ETBEB.4/
structure, and they ultimately appear in two proto -/ETBEB.2/structures. For exam-
ple, the proto -/ETBEB.2/structure labeled “clusters 2,3” has AŒ0/c141D0, indicating that
cluster 2of the proto -/ETBEB.16/ structure (containing elements 8; 9; 10; 11 )i sa l l 0,
andAŒ1/c141D1, telling us that cluster 3(containing elements 12; 13; 14; 15 ) has at
least one 1. Each proto -/ETBEB.4/structure points to its own summary, which is itself
stored as a proto -/ETBEB.2/structure. For example, look at the proto -/ETBEB.2/struc-
ture just to the left of the one labeled “elements 0,1.” Because its AŒ0/c141 is0, it tells
us that the “elements 0,1” structure is all 0, and because its AŒ1/c141 is1, we know that
the “elements 2,3” structure contains at least one 1.
20.2.2 Operations on a proto van Emde Boas structure
We shall now describe how to perform operations on a proto-vEB structure.
We ﬁrst examine the query operations—M EMBER ,M INIMUM ,M AXIMUM ,a n d
SUCCESSOR —which do not change the proto-vEB structure. We then discuss
INSERT and D ELETE .W e l e a v e M AXIMUM and P REDECESSOR , which are sym-
metric to M INIMUM and S UCCESSOR , respectively, as Exercise 20.2-1.
Each of the M EMBER ,SUCCESSOR ,PREDECESSOR ,INSERT ,a n dD ELETE op-
erations takes a parameter x, along with a proto-vEB structure V. Each of these
operations assumes that 0/DC4x<V : u.
Determining whether a value is in the set
To perform M EMBER .x/, we need to ﬁnd the bit corresponding to xwithin the
appropriate proto -/ETBEB.2/structure. We can do so in O.lg lgu/time, bypassing20.2 A recursive structure 541
thesummary structures altogether. The following procedure takes a proto -/ETBEB
structure Vand a value x, and it returns a bit indicating whether xis in the dynamic
set held by V.
PROTO -VEB-M EMBER .V; x/
1ifV:u==2
2 return V:AŒx/c141
3else return PROTO -VEB-M EMBER .V:cluster Œhigh.x//c141; low.x//
The P ROTO -VEB-M EMBER procedure works as follows. Line 1 tests whether
we are in a base case, where Vis aproto -/ETBEB.2/structure. Line 2 handles the
base case, simply returning the appropriate bit of array A. Line 3 deals with the
recursive case, “drilling down” into the appropriate smaller proto-vEB structure.The value high .x/says which proto -/ETBEB.p
u/structure we visit, and low .x/de-
termines which element within that proto -/ETBEB.p
u/structure we are querying.
Let’s see what happens when we call P ROTO -VEB-M EMBER .V; 6/ on the
proto -/ETBEB.16/ structure in Figure 20.4. Since high .6/D1when uD16,w e
recurse into the proto -/ETBEB.4/structure in the upper right, and we ask about ele-
ment low .6/D2of that structure. In this recursive call, uD4, and so we recurse
again. With uD4,w eh a v eh i g h .2/D1and low .2/D0, and so we ask about
element 0of the proto -/ETBEB.2/structure in the upper right. This recursive call turns
out to be a base case, and so it returns AŒ0/c141D0back up through the chain of re-
cursive calls. Thus, we get the result that P ROTO -VEB-M EMBER .V; 6/ returns 0,
indicating that 6is not in the set.
To determine the running time of P ROTO -VEB-M EMBER ,l e t T. u / denote
its running time on a proto -/ETBEB.u/structure. Each recursive call takes con-
stant time, not including the time taken by the recursive calls that it makes.When P
ROTO -VEB-M EMBER makes a recursive call, it makes a call on a
proto -/ETBEB.p
u/structure. Thus, we can characterize the running time by the recur-
rence T. u /DT.p
u/CO.1/ , which we have already seen as recurrence (20.2).
Its solution is T. u /DO.lg lgu/, and so we conclude that P ROTO -VEB-M EMBER
runs in time O.lg lgu/.
Finding the minimum element
Now we examine how to perform the M INIMUM operation. The procedure
PROTO -VEB-M INIMUM .V /returns the minimum element in the proto-vEB struc-
tureV,o r NILifVrepresents an empty set.542 Chapter 20 van Emde Boas Trees
PROTO -VEB-M INIMUM .V /
1ifV:u==2
2 ifV:AŒ0/c141==1
3 return 0
4 elseif V:AŒ1/c141==1
5 return 1
6 else return NIL
7elsemin-clusterDPROTO -VEB-M INIMUM .V:summary /
8 ifmin-cluster ==NIL
9 return NIL
10 elseoffsetDPROTO -VEB-M INIMUM .V:cluster Œmin-cluster /c141/
11 return index .min-cluster ;offset /
This procedure works as follows. Line 1 tests for the base case, which lines 2–6
handle by brute force. Lines 7–11 handle the recursive case. First, line 7 ﬁnds thenumber of the ﬁrst cluster that contains an element of the set. It does so by recur-sively calling P
ROTO -VEB-M INIMUM onV:summary ,w h i c hi sa proto -/ETBEB.p
u/
structure. Line 7 assigns this cluster number to the variable min-cluster . If the set
is empty, then the recursive call returned NIL, and line 9 returns NIL. Otherwise,
the minimum element of the set is somewhere in cluster number min-cluster .T h e
recursive call in line 10 ﬁnds the offset within the cluster of the minimum element
in this cluster. Finally, line 11 constructs the value of the minimum element from
the cluster number and offset, and it returns this value.
Although querying the summary information allows us to quickly ﬁnd the clus-
ter containing the minimum element, because this procedure makes two recursive
calls on proto -/ETBEB.p
u/structures, it does not run in O.lg lgu/time in the worst
case. Letting T. u / denote the worst-case time for P ROTO -VEB-M INIMUM on a
proto -/ETBEB.u/structure, we have the recurrence
T. u /D2T .p
u/CO.1/ : (20.3)
Again, we use a change of variables to solve this recurrence, letting mDlgu,
which gives
T. 2m/D2T .2m=2/CO.1/ :
Renaming S.m/DT. 2m/gives
S.m/D2S.m=2/CO.1/ ;
which, by case 1 of the master method, has the solution S.m/D‚.m/ . By chang-
ing back from S.m/ toT. u / ,w eh a v et h a t T. u /DT. 2m/DS.m/D‚.m/D
‚.lgu/. Thus, we see that because of the second recursive call, P ROTO -VEB-
MINIMUM runs in ‚.lgu/time rather than the desired O.lg lgu/time.20.2 A recursive structure 543
Finding the successor
The S UCCESSOR operation is even worse. In the worst case, it makes two recursive
calls, along with a call to P ROTO -VEB-M INIMUM . The procedure P ROTO -VEB-
SUCCESSOR .V; x/ returns the smallest element in the proto-vEB structure Vthat
is greater than x,o r NILif no element in Vis greater than x. It does not require x
to be a member of the set, but it does assume that 0/DC4x<V : u.
PROTO -VEB-S UCCESSOR .V; x/
1ifV:u==2
2 ifx==0andV:AŒ1/c141==1
3 return 1
4 else return NIL
5elseoffsetDPROTO -VEB-S UCCESSOR .V:cluster Œhigh.x//c141; low.x//
6 ifoffset¤NIL
7 return index .high.x/;offset /
8 elsesucc-clusterDPROTO -VEB-S UCCESSOR .V:summary ;high.x//
9 ifsucc-cluster ==NIL
10 return NIL
11 elseoffsetDPROTO -VEB-M INIMUM .V:cluster Œsucc-cluster /c141/
12 return index .succ-cluster ;offset /
The P ROTO -VEB-S UCCESSOR procedure works as follows. As usual, line 1
tests for the base case, which lines 2–4 handle by brute force: the only way that x
can have a successor within a proto -/ETBEB.2/structure is when xD0andAŒ1/c141
is1. Lines 5–12 handle the recursive case. Line 5 searches for a successor to x
within x’s cluster, assigning the result to offset . Line 6 determines whether xhas
a successor within its cluster; if it does, then line 7 computes and returns the value
of this successor. Otherwise, we have to search in other clusters. Line 8 assigns tosucc-cluster the number of the next nonempty cluster, using the summary informa-
tion to ﬁnd it. Line 9 tests whether succ-cluster is
NIL, with line 10 returning NIL
if all succeeding clusters are empty. If succ-cluster is non- NIL, line 11 assigns
the ﬁrst element within that cluster to offset , and line 12 computes and returns the
minimum element in that cluster.
In the worst case, P ROTO -VEB-S UCCESSOR calls itself recursively twice on
proto -/ETBEB.p
u/structures, and it makes one call to P ROTO -VEB-M INIMUM on
aproto -/ETBEB.p
u/structure. Thus, the recurrence for the worst-case running
timeT. u / of P ROTO -VEB-S UCCESSOR is
T. u /D2T .p
u/C‚.lgp
u/
D2T .p
u/C‚.lgu/ :544 Chapter 20 van Emde Boas Trees
We can employ the same technique that we used for recurrence (20.1) to show
that this recurrence has the solution T. u /D‚.lgulg lgu/. Thus, P ROTO -VEB-
SUCCESSOR is asymptotically slower than P ROTO -VEB-M INIMUM .
Inserting an element
To insert an element, we need to insert it into the appropriate cluster and also set
the summary bit for that cluster to 1. The procedure P ROTO -VEB-I NSERT .V; x/
inserts the value xinto the proto-vEB structure V.
PROTO -VEB-I NSERT .V; x/
1ifV:u==2
2 V:AŒx/c141D1
3elsePROTO -VEB-I NSERT .V:cluster Œhigh.x//c141; low.x//
4P ROTO -VEB-I NSERT .V:summary ;high.x//
In the base case, line 2 sets the appropriate bit in the array Ato1. In the recursive
case, the recursive call in line 3 inserts xinto the appropriate cluster, and line 4
sets the summary bit for that cluster to 1.
Because P ROTO -VEB-I NSERT makes two recursive calls in the worst case, re-
currence (20.3) characterizes its running time. Hence, P ROTO -VEB-I NSERT runs
in‚.lgu/time.
Deleting an element
The D ELETE operation is more complicated than insertion. Whereas we can always
set a summary bit to 1when inserting, we cannot always reset the same summary
bit to 0when deleting. We need to determine whether any bit in the appropriate
cluster is 1. As we have deﬁned proto-vEB structures, we would have to examine
allp
ubits within a cluster to determine whether any of them are 1. Alternatively,
we could add an attribute nto the proto-vEB structure, counting how many el-
ements it has. We leave implementation of P ROTO -VEB-D ELETE as Exercises
20.2-2 and 20.2-3.
Clearly, we need to modify the proto-vEB structure to get each operation down
to making at most one recursive call. We will see in the next section how to do so.
Exercises
20.2-1
Write pseudocode for the procedures P ROTO -VEB-M AXIMUM and P ROTO -VEB-
PREDECESSOR .20.3 The van Emde Boas tree 545
20.2-2
Write pseudocode for P ROTO -VEB-D ELETE . It should update the appropriate
summary bit by scanning the related bits within the cluster. What is the worst-case running time of your procedure?
20.2-3
Add the attribute nto each proto-vEB structure, giving the number of elements
currently in the set it represents, and write pseudocode for P
ROTO -VEB-D ELETE
that uses the attribute nto decide when to reset summary bits to 0. What is the
worst-case running time of your procedure? What other procedures need to change
because of the new attribute? Do these changes affect their running times?
20.2-4
Modify the proto-vEB structure to support duplicate keys.
20.2-5
Modify the proto-vEB structure to support keys that have associated satellite data.
20.2-6
Write pseudocode for a procedure that creates a proto -/ETBEB.u/structure.
20.2-7
Argue that if line 9 of P ROTO -VEB-M INIMUM is executed, then the proto-vEB
structure is empty.
20.2-8
Suppose that we designed a proto-vEB structure in which each cluster array had
onlyu1=4elements. What would the running times of each operation be?
20.3 The van Emde Boas tree
The proto-vEB structure of the previous section is close to what we need to achieve
O.lg lgu/running times. It falls short because we have to recurse too many times
in most of the operations. In this section, we shall design a data structure thatis similar to the proto-vEB structure but stores a little more information, therebyremoving the need for some of the recursion.
In Section 20.2, we observed that the assumption that we made about the uni-
verse size—that uD2
2kfor some integer k—is unduly restrictive, conﬁning the
possible values of uan overly sparse set. From this point on, therefore, we will
allow the universe size uto be any exact power of 2,a n dw h e np
uis not an inte-546 Chapter 20 van Emde Boas Trees
… 0123"p
u/NUL1/ETBEB.u/ u min max
summary cluster
/ETBEB."p
u/
"p
u/ETBEB.#p
u/trees
Figure 20.5 The information in a /ETBEB.u/tree when u>2 . The structure contains the uni-
verse size u,e l e m e n t s min and max, a pointer summary to a /ETBEB."p
u/tree, and an array
cluster Œ0 : :"p
u/NUL1/c141of"p
upointers to /ETBEB.#p
u/trees.
ger—that is, if uis an odd power of 2(uD22kC1for some integer k/NAK0)—then
we will divide the lg ubits of a number into the most signiﬁcant d.lgu/=2ebits and
the least signiﬁcant b.lgu/=2cbits. For convenience, we denote 2d.lgu/=2 e(the “up-
per square root” of u)b y"p
uand2b.lgu/=2 c(the “lower square root” of u)b y#p
u,
so that uD"p
u/SOH#p
uand, when uis an even power of 2(uD22kfor some
integer k),"p
uD#p
uDp
u. Because we now allow uto be an odd power of 2,
we must redeﬁne our helpful functions from Section 20.2:
high.x/D/EOT
x=#p
u˘
;
low.x/Dxmod#p
u;
index .x; y/Dx#p
uCy:
20.3.1 van Emde Boas trees
Thevan Emde Boas tree ,o rvEB tree , modiﬁes the proto-vEB structure. We
denote a vEB tree with a universe size of uas/ETBEB.u/and, unless uequals the
base size of 2, the attribute summary points to a /ETBEB."p
u/tree and the array
cluster Œ0 : :"p
u/NUL1/c141points to"p
u/ETBEB.#p
u/trees. As Figure 20.5 illustrates, a
vEB tree contains two attributes not found in a proto-vEB structure:
/SIminstores the minimum element in the vEB tree, and
/SImax stores the maximum element in the vEB tree.
Furthermore, the element stored in min does not appear in any of the recur-
sive/ETBEB.#p
u/trees that the cluster array points to. The elements stored in a
/ETBEB.u/treeV, therefore, are V:min plus all the elements recursively stored in
the/ETBEB.#p
u/trees pointed to by V:cluster Œ0 : :"p
u/NUL1/c141. Note that when a vEB
tree contains two or more elements, we treat minandmax differently: the element20.3 The van Emde Boas tree 547
stored in mindoes not appear in any of the clusters, but the element stored in max
does.
Since the base size is 2,a/ETBEB.2/tree does not need the array Athat the cor-
responding proto -/ETBEB.2/structure has. Instead, we can determine its elements
from its minandmax attributes. In a vEB tree with no elements, regardless of its
universe size u, both minandmax areNIL.
Figure 20.6 shows a /ETBEB.16/ treeVholding the setf2; 3; 4; 5; 7; 14; 15g.B e -
cause the smallest element is 2,V:minequals 2, and even though high .2/D0,t h e
element 2does not appear in the /ETBEB.4/tree pointed to by V:cluster Œ0/c141: notice
thatV:cluster Œ0/c141:minequals 3,a n ds o 2is not in this vEB tree. Similarly, since
V:cluster Œ0/c141:minequals 3,a n d 2and3are the only elements in V:cluster Œ0/c141,t h e
/ETBEB.2/clusters within V:cluster Œ0/c141are empty.
The minandmax attributes will turn out to be key to reducing the number of
recursive calls within the operations on vEB trees. These attributes will help us infour ways:
1. The M
INIMUM and M AXIMUM operations do not even need to recurse, for they
can just return the values of minormax.
2. The S UCCESSOR operation can avoid making a recursive call to determine
whether the successor of a value xlies within high .x/. That is because x’s
successor lies within its cluster if and only if xis strictly less than the max
attribute of its cluster. A symmetric argument holds for P REDECESSOR and
min.
3. We can tell whether a vEB tree has no elements, exactly one element, or at least
two elements in constant time from its minandmax values. This ability will
help in the I NSERT and D ELETE operations. If minandmax are both NIL,t h e n
the vEB tree has no elements. If minandmax are non- NILbut are equal to each
other, then the vEB tree has exactly one element. Otherwise, both minandmax
are non- NILbut are unequal, and the vEB tree has two or more elements.
4. If we know that a vEB tree is empty, we can insert an element into it by updating
only its minandmax attributes. Hence, we can insert into an empty vEB tree in
constant time. Similarly, if we know that a vEB tree has only one element, wecan delete that element in constant time by updating only minandmax. These
properties will allow us to cut short the chain of recursive calls.
Even if the universe size uis an odd power of 2, the difference in the sizes
of the summary vEB tree and the clusters will not turn out to affect the asymptoticrunning times of the vEB-tree operations. The recursive procedures that implementthe vEB-tree operations will all have running times characterized by the recurrence
T. u //DC4T.
"p
u/CO.1/ : (20.4)548 Chapter 20 van Emde Boas Trees
0123
clusteru16
summaryvEB(16) min 2max 15
01
clusteru4
summaryvEB(4) min 0max 3
u2
min 0
max 1vEB(2)
u2
min 1
max 1vEB(2)
u2
min 1
max 1vEB(2)01
cluster summary
u2
min
maxvEB(2)
u2
min
maxvEB(2)
u2
min
maxvEB(2)01
cluster summary
u2
min 0
max 1vEB(2)
u2
min 1
max 1vEB(2)
u2
min 1
max 1vEB(2)
01
cluster summary
u2
min
maxvEB(2)
u2
min
maxvEB(2)
u2
min
maxvEB(2)01
cluster summary
u2
min 1
max 1vEB(2)
u2
min
maxvEB(2)
u2
min 1
max 1vEB(2)u4 vEB(4) min 3max 3 u4 vEB(4) min 0max 3
u4 vEB(4) min max u4 vEB(4) min 2max 3
Figure 20.6 A/ETBEB.16/ tree corresponding to the proto-vEB tree in Figure 20.4. It stores the set
f2; 3; 4; 5; 7; 14; 15g. Slashes indicate NILvalues. The value stored in the minattribute of a vEB tree
does not appear in any of its clusters. Heavy shading serves the same purpose here as in Figure 20.4.20.3 The van Emde Boas tree 549
This recurrence looks similar to recurrence (20.2), and we will solve it in a similar
fashion. Letting mDlgu, we rewrite it as
T. 2m//DC4T. 2dm=2e/CO.1/ :
Noting thatdm=2e/DC42m=3 for all m/NAK2,w eh a v e
T. 2m//DC4T. 22m=3/CO.1/ :
Letting S.m/DT. 2m/, we rewrite this last recurrence as
S.m//DC4S.2m=3/CO.1/ ;
which, by case 2 of the master method, has the solution S.m/DO.lgm/. (In
terms of the asymptotic solution, the fraction 2=3 does not make any difference
compared with the fraction 1=2, because when we apply the master method, we
ﬁnd that log3=21Dlog21D0:) Thus, we have T. u /DT. 2m/DS.m/D
O.lgm/DO.lg lgu/.
Before using a van Emde Boas tree, we must know the universe size u,s ot h a t
we can create a van Emde Boas tree of the appropriate size that initially represents
an empty set. As Problem 20-1 asks you to show, the total space requirement ofa van Emde Boas tree is O.u/ , and it is straightforward to create an empty tree
inO.u/ time. In contrast, we can create an empty red-black tree in constant time.
Therefore, we might not want to use a van Emde Boas tree when we perform onlya small number of operations, since the time to create the data structure wouldexceed the time saved in the individual operations. This drawback is usually notsigniﬁcant, since we typically use a simple data structure, such as an array or linkedlist, to represent a set with only a few elements.
20.3.2 Operations on a van Emde Boas tree
We are now ready to see how to perform operations on a van Emde Boas tree. As
we did for the proto van Emde Boas structure, we will consider the querying oper-ations ﬁrst, and then I
NSERT and D ELETE . Due to the slight asymmetry between
the minimum and maximum elements in a vEB tree—when a vEB tree containsat least two elements, the minumum element does not appear within a cluster butthe maximum element does—we will provide pseudocode for all ﬁve querying op-erations. As in the operations on proto van Emde Boas structures, the operationshere that take parameters Vandx,w h e r e Vis a van Emde Boas tree and xis an
element, assume that 0/DC4x<V : u.
Finding the minimum and maximum elements
Because we store the minimum and maximum in the attributes minandmax,t w o
of the operations are one-liners, taking constant time:550 Chapter 20 van Emde Boas Trees
VEB-T REE-MINIMUM .V /
1return V:min
VEB-T REE-MAXIMUM .V /
1return V:max
Determining whether a value is in the set
The procedure VEB-T REE-MEMBER .V; x/ has a recursive case like that of
PROTO -VEB-M EMBER , but the base case is a little different. We also check di-
rectly whether xequals the minimum or maximum element. Since a vEB tree
doesn’t store bits as a proto-vEB structure does, we design VEB-T REE-MEMBER
to return TRUE orFALSE rather than 1or0.
VEB-T REE-MEMBER .V; x/
1ifx==V:minorx==V:max
2 return TRUE
3elseif V:u==2
4 return FALSE
5else return VEB-T REE-MEMBER .V:cluster Œhigh.x//c141; low.x//
Line 1 checks to see whether xequals either the minimum or maximum element.
If it does, line 2 returns TRUE . Otherwise, line 3 tests for the base case. Since
a/ETBEB.2/tree has no elements other than those in minandmax, if it is the base
case, line 4 returns FALSE . The other possibility—it is not a base case and xequals
neither minnormax—is handled by the recursive call in line 5.
Recurrence (20.4) characterizes the running time of the VEB-T REE-MEMBER
procedure, and so this procedure takes O.lg lgu/time.
Finding the successor and predecessor
Next we see how to implement the S UCCESSOR operation. Recall that the pro-
cedure P ROTO -VEB-S UCCESSOR .V; x/ could make two recursive calls: one to
determine whether x’s successor resides in the same cluster as xand, if it does
not, one to ﬁnd the cluster containing x’s successor. Because we can access the
maximum value in a vEB tree quickly, we can avoid making two recursive calls,and instead make one recursive call on either a cluster or on the summary, but noton both.20.3 The van Emde Boas tree 551
VEB-T REE-SUCCESSOR .V; x/
1ifV:u==2
2 ifx==0andV:max ==1
3 return 1
4 else return NIL
5elseif V:min¤NILandx<V : min
6 return V:min
7elsemax-lowDVEB-T REE-MAXIMUM .V:cluster Œhigh.x//c141/
8 ifmax-low¤NILand low .x/ < max-low
9 offsetDVEB-T REE-SUCCESSOR .V:cluster Œhigh.x//c141; low.x//
10 return index .high.x/;offset /
11 elsesucc-clusterDVEB-T REE-SUCCESSOR .V:summary ;high.x//
12 ifsucc-cluster ==NIL
13 return NIL
14 elseoffsetDVEB-T REE-MINIMUM .V:cluster Œsucc-cluster /c141/
15 return index .succ-cluster ;offset /
This procedure has six return statements and several cases. We start with the
base case in lines 2–4, which returns 1in line 3 if we are trying to ﬁnd the successor
of0and1is in the 2-element set; otherwise, the base case returns NILin line 4.
If we are not in the base case, we next check in line 5 whether xis strictly less
than the minimum element. If so, then we simply return the minimum element inline 6.
If we get to line 7, then we know that we are not in a base case and that xis
greater than or equal to the minimum value in the vEB tree V. Line 7 assigns to
max-lowthe maximum element in x’s cluster. If x’s cluster contains some element
that is greater than x, then we know that x’s successor lies somewhere within x’s
cluster. Line 8 tests for this condition. If x’s successor is within x’s cluster, then
line 9 determines where in the cluster it is, and line 10 returns the successor in the
same way as line 7 of P
ROTO -VEB-S UCCESSOR .
We get to line 11 if xis greater than or equal to the greatest element in its
cluster. In this case, lines 11–15 ﬁnd x’s successor in the same way as lines 8–12
of P ROTO -VEB-S UCCESSOR .
It is easy to see how recurrence (20.4) characterizes the running time of VEB-
TREE-SUCCESSOR . Depending on the result of the test in line 7, the procedure
calls itself recursively in either line 9 (on a vEB tree with universe size#p
u)o r
line 11 (on a vEB tree with universe size"p
u). In either case, the one recursive
call is on a vEB tree with universe size at most"p
u. The remainder of the proce-
dure, including the calls to VEB-T REE-MINIMUM and VEB-T REE-MAXIMUM ,
takes O.1/ time. Hence, VEB-T REE-SUCCESSOR runs in O.lg lgu/worst-case
time.552 Chapter 20 van Emde Boas Trees
The VEB-T REE-PREDECESSOR procedure is symmetric to the VEB-T REE-
SUCCESSOR procedure, but with one additional case:
VEB-T REE-PREDECESSOR .V; x/
1ifV:u==2
2 ifx==1andV:min ==0
3 return 0
4 else return NIL
5elseif V:max¤NILandx>V : max
6 return V:max
7elsemin-lowDVEB-T REE-MINIMUM .V:cluster Œhigh.x//c141/
8 ifmin-low¤NILand low .x/ > min-low
9 offsetDVEB-T REE-PREDECESSOR .V:cluster Œhigh.x//c141; low.x//
10 return index .high.x/;offset /
11 elsepred-clusterDVEB-T REE-PREDECESSOR .V:summary ;high.x//
12 ifpred-cluster ==NIL
13 ifV:min¤NILandx>V : min
14 return V:min
15 else return NIL
16 elseoffsetDVEB-T REE-MAXIMUM .V:cluster Œpred-cluster /c141/
17 return index .pred-cluster ;offset /
Lines 13–14 form the additional case. This case occurs when x’s predecessor,
if it exists, does not reside in x’s cluster. In VEB-T REE-SUCCESSOR ,w ew e r e
assured that if x’s successor resides outside of x’s cluster, then it must reside in
a higher-numbered cluster. But if x’s predecessor is the minimum value in vEB
treeV, then the successor resides in no cluster at all. Line 13 checks for this
condition, and line 14 returns the minimum value as appropriate.
This extra case does not affect the asymptotic running time of VEB-T REE-
PREDECESSOR when compared with VEB-T REE-SUCCESSOR ,a n d s o VEB-
TREE-PREDECESSOR runs in O.lg lgu/worst-case time.
Inserting an element
Now we examine how to insert an element into a vEB tree. Recall that P ROTO -
VEB-I NSERT made two recursive calls: one to insert the element and one to insert
the element’s cluster number into the summary. The VEB-T REE-INSERT proce-
dure will make only one recursive call. How can we get away with just one? When
we insert an element, either the cluster that it goes into already has another elementor it does not. If the cluster already has another element, then the cluster number
is already in the summary, and so we do not need to make that recursive call. If20.3 The van Emde Boas tree 553
the cluster does not already have another element, then the element being inserted
becomes the only element in the cluster, and we do not need to recurse to insert anelement into an empty vEB tree:
VEB-E MPTY -TREE-INSERT .V; x/
1V:minDx
2V:maxDx
With this procedure in hand, here is the pseudocode for VEB-T REE-INSERT .V; x/ ,
which assumes that xis not already an element in the set represented by vEB
treeV:
VEB-T REE-INSERT .V; x/
1ifV:min ==NIL
2 VEB-E MPTY -TREE-INSERT .V; x/
3else if x<V : min
4 exchange xwithV:min
5 ifV:u>2
6 ifVEB-T REE-MINIMUM .V:cluster Œhigh.x//c141/ ==NIL
7 VEB-T REE-INSERT .V:summary ;high.x//
8 VEB-E MPTY -TREE-INSERT .V:cluster Œhigh.x//c141; low.x//
9 else VEB-T REE-INSERT .V:cluster Œhigh.x//c141; low.x//
10 ifx>V : max
11 V:maxDx
This procedure works as follows. Line 1 tests whether Vis an empty vEB tree
and, if it is, then line 2 handles this easy case. Lines 3–11 assume that Vis not
empty, and therefore some element will be inserted into one of V’s clusters. But
that element might not necessarily be the element xpassed to VEB-T REE-INSERT .
Ifx< min, as tested in line 3, then xneeds to become the new min. We don’t
want to lose the original min, however, and so we need to insert it into one of V’s
clusters. In this case, line 4 exchanges xwith min, so that we insert the original
mininto one of V’s clusters.
We execute lines 6–9 only if Vis not a base-case vEB tree. Line 6 determines
whether the cluster that xwill go into is currently empty. If so, then line 7 in-
serts x’s cluster number into the summary and line 8 handles the easy case of
inserting xinto an empty cluster. If x’s cluster is not currently empty, then line 9
inserts xinto its cluster. In this case, we do not need to update the summary,
since x’s cluster number is already a member of the summary.
Finally, lines 10–11 take care of updating max ifx> max. Note that if Vis a
base-case vEB tree that is not empty, then lines 3–4 and 10–11 update minandmax
properly.554 Chapter 20 van Emde Boas Trees
Once again, we can easily see how recurrence (20.4) characterizes the running
time. Depending on the result of the test in line 6, either the recursive call in line 7(run on a vEB tree with universe size
"p
u) or the recursive call in line 9 (run on
a vEB with universe size#p
u) executes. In either case, the one recursive call is
on a vEB tree with universe size at most"p
u. Because the remainder of VEB-
TREE-INSERT takes O.1/ time, recurrence (20.4) applies, and so the running time
isO.lg lgu/.
Deleting an element
Finally, we look at how to delete an element from a vEB tree. The procedure
VEB-T REE-DELETE .V; x/ assumes that xis currently an element in the set repre-
sented by the vEB tree V.
VEB-T REE-DELETE .V; x/
1ifV:min ==V:max
2 V:minDNIL
3 V:maxDNIL
4elseif V:u==2
5 ifx==0
6 V:minD1
7 elseV:minD0
8 V:maxDV:min
9else if x==V:min
10 ﬁrst-clusterDVEB-T REE-MINIMUM .V:summary /
11 xDindex .ﬁrst-cluster ;
VEB-T REE-MINIMUM .V:cluster Œﬁrst-cluster /c141//
12 V:minDx
13 VEB-T REE-DELETE .V:cluster Œhigh.x//c141; low.x//
14 ifVEB-T REE-MINIMUM .V:cluster Œhigh.x//c141/ ==NIL
15 VEB-T REE-DELETE .V:summary ;high.x//
16 ifx==V:max
17 summary -maxDVEB-T REE-MAXIMUM .V:summary /
18 ifsummary -max ==NIL
19 V:maxDV:min
20 elseV:maxDindex .summary -max;
VEB-T REE-MAXIMUM .V:cluster Œsummary -max/c141//
21 elseif x==V:max
22 V:maxDindex .high.x/;
VEB-T REE-MAXIMUM .V:cluster Œhigh.x//c141//20.3 The van Emde Boas tree 555
The VEB-T REE-DELETE procedure works as follows. If the vEB tree Vcon-
tains only one element, then it’s just as easy to delete it as it was to insert an elementinto an empty vEB tree: just set minandmax to
NIL. Lines 1–3 handle this case.
Otherwise, Vhas at least two elements. Line 4 tests whether Vis a base-case vEB
tree and, if so, lines 5–8 set minandmax to the one remaining element.
Lines 9–22 assume that Vhas two or more elements and that u/NAK4.I n t h i s
case, we will have to delete an element from a cluster. The element we delete froma cluster might not be x, however, because if xequals min, then once we have
deleted x, some other element within one of V’s clusters becomes the new min,
and we have to delete that other element from its cluster. If the test in line 9 revealsthat we are in this case, then line 10 sets ﬁrst-cluster to the number of the cluster
that contains the lowest element other than min, and line 11 sets xto the value of
the lowest element in that cluster. This element becomes the new minin line 12
and, because we set xto its value, it is the element that will be deleted from its
cluster.
When we reach line 13, we know that we need to delete element xfrom its
cluster, whether xwas the value originally passed to
VEB-T REE-DELETE orx
is the element becoming the new minimum. Line 13 deletes xfrom its cluster.
That cluster might now become empty, which line 14 tests, and if it does, thenwe need to remove x’s cluster number from the summary, which line 15 handles.
After updating the summary, we might need to update max. Line 16 checks to see
whether we are deleting the maximum element in Vand, if we are, then line 17 sets
summary -max to the number of the highest-numbered nonempty cluster. (The call
VEB-T REE-MAXIMUM .V:summary /works because we have already recursively
called VEB-T REE-DELETE onV:summary , and therefore V:summary :max has al-
ready been updated as necessary.) If all of V’s clusters are empty, then the only
remaining element in Vismin; line 18 checks for this case, and line 19 updates
max appropriately. Otherwise, line 20 sets max to the maximum element in the
highest-numbered cluster. (If this cluster is where the element has been deleted,we again rely on the recursive call in line 13 having already corrected that cluster’smax attribute.)
Finally, we have to handle the case in which x’s cluster did not become empty
due to xbeing deleted. Although we do not have to update the summary in this
case, we might have to update max. Line 21 tests for this case, and if we have to
update max, line 22 does so (again relying on the recursive call to have corrected
max in the cluster).
Now we show that
VEB-T REE-DELETE runs in O.lg lgu/time in the worst
case. At ﬁrst glance, you might think that recurrence (20.4) does not always apply,because a single call of
VEB-T REE-DELETE can make two recursive calls: one
on line 13 and one on line 15. Although the procedure can make both recursivecalls, let’s think about what happens when it does. In order for the recursive call on556 Chapter 20 van Emde Boas Trees
line 15 to occur, the test on line 14 must show that x’s cluster is empty. The only
way that x’s cluster can be empty is if xwas the only element in its cluster when
we made the recursive call on line 13. But if xwas the only element in its cluster,
then that recursive call took O.1/ time, because it executed only lines 1–3. Thus,
we have two mutually exclusive possibilities:
/SIThe recursive call on line 13 took constant time.
/SIThe recursive call on line 15 did not occur.
In either case, recurrence (20.4) characterizes the running time of VEB-T REE-
DELETE , and hence its worst-case running time is O.lg lgu/.
Exercises
20.3-1
Modify vEB trees to support duplicate keys.
20.3-2
Modify vEB trees to support keys that have associated satellite data.
20.3-3
Write pseudocode for a procedure that creates an empty van Emde Boas tree.
20.3-4
What happens if you call VEB-T REE-INSERT with an element that is already in
the vEB tree? What happens if you call VEB-T REE-DELETE with an element that
is not in the vEB tree? Explain why the procedures exhibit the behavior that theydo. Show how to modify vEB trees and their operations so that we can check in
constant time whether an element is present.
20.3-5
Suppose that instead of
"p
uclusters, each with universe size#p
u, we constructed
vEB trees to have u1=kclusters, each with universe size u1/NUL1=k,w h e r e k>1 is a
constant. If we were to modify the operations appropriately, what would be theirrunning times? For the purpose of analysis, assume that u
1=kandu1/NUL1=kare always
integers.
20.3-6
Creating a vEB tree with universe size urequires O.u/ time. Suppose we wish to
explicitly account for that time. What is the smallest number of operations nfor
which the amortized time of each operation in a vEB tree is O.lg lgu/?Problems for Chapter 20 557
Problems
20-1 Space requirements for van Emde Boas trees
This problem explores the space requirements for van Emde Boas trees and sug-gests a way to modify the data structure to make its space requirement depend onthe number nof elements actually stored in the tree, rather than on the universe
sizeu. For simplicity, assume thatp
uis always an integer.
a.Explain why the following recurrence characterizes the space requirement P.u/
of a van Emde Boas tree with universe size u:
P.u/D.p
uC1/P.p
u/C‚.p
u/ : (20.5)
b.Prove that recurrence (20.5) has the solution P.u/DO.u/ .
In order to reduce the space requirements, let us deﬁne a reduced-space van Emde
Boas tree ,o rRS-vEB tree , as a vEB tree Vbut with the following changes:
/SIThe attribute V:cluster , rather than being stored as a simple array of pointers to
vEB trees with universe sizep
u, is a hash table (see Chapter 11) stored as a dy-
namic table (see Section 17.4). Corresponding to the array version of V:cluster ,
the hash table stores pointers to RS-vEB trees with universe sizep
u.T o ﬁ n d
theith cluster, we look up the key iin the hash table, so that we can ﬁnd the
ith cluster by a single search in the hash table.
/SIThe hash table stores only pointers to nonempty clusters. A search in the hash
table for an empty cluster returns NIL, indicating that the cluster is empty.
/SIThe attribute V:summary isNILif all clusters are empty. Otherwise, V:summary
points to an RS-vEB tree with universe sizep
u.
Because the hash table is implemented with a dynamic table, the space it requires
is proportional to the number of nonempty clusters.
When we need to insert an element into an empty RS-vEB tree, we create the RS-
vEB tree by calling the following procedure, where the parameter uis the universe
size of the RS-vEB tree:
CREATE -NEW-RS- VEB-T REE.u/
1 allocate a new vEB tree V
2V:uDu
3V:minDNIL
4V:maxDNIL
5V:summaryDNIL
6 create V:cluster as an empty dynamic hash table
7return V558 Chapter 20 van Emde Boas Trees
c.Modify the VEB-T REE-INSERT procedure to produce pseudocode for the pro-
cedure RS- VEB-T REE-INSERT .V; x/ , which inserts xinto the RS-vEB tree V,
calling C REATE -NEW-RS- VEB-T REE as appropriate.
d.Modify the VEB-T REE-SUCCESSOR procedure to produce pseudocode for
the procedure RS- VEB-T REE-SUCCESSOR .V; x/ , which returns the successor
ofxin RS-vEB tree V,o r NILifxhas no successor in V.
e.Prove that, under the assumption of simple uniform hashing, your RS- VEB-
TREE-INSERT and RS- VEB-T REE-SUCCESSOR procedures run in O.lg lgu/
expected time.
f.Assuming that elements are never deleted from a vEB tree, prove that the space
requirement for the RS-vEB tree structure is O.n/ ,w h e r e nis the number of
elements actually stored in the RS-vEB tree.
g.RS-vEB trees have another advantage over vEB trees: they require less time to
create. How long does it take to create an empty RS-vEB tree?
20-2 y-fast tries
This problem investigates D. Willard’s “ y-fast tries” which, like van Emde Boas
trees, perform each of the operations M EMBER ,M INIMUM ,M AXIMUM ,PRE-
DECESSOR ,a n dS UCCESSOR on elements drawn from a universe with size uin
O.lg lgu/worst-case time. The I NSERT and D ELETE operations take O.lg lgu/
amortized time. Like reduced-space van Emde Boas trees (see Problem 20-1), y-
fast tries use only O.n/ space to store nelements. The design of y-fast tries relies
on perfect hashing (see Section 11.5).
As a preliminary structure, suppose that we create a perfect hash table containing
not only every element in the dynamic set, but every preﬁx of the binary represen-tation of every element in the set. For example, if uD16,s ot h a tl g uD4,a n d
xD13is in the set, then because the binary representation of 13is1101 ,t h e
perfect hash table would contain the strings 1,11,110,a n d 1101 . In addition to
the hash table, we create a doubly linked list of the elements currently in the set, inincreasing order.
a.How much space does this structure require?
b.Show how to perform the M
INIMUM and M AXIMUM operations in O.1/ time;
the M EMBER ,PREDECESSOR ,a n dS UCCESSOR operations in O.lg lgu/time;
and the I NSERT and D ELETE operations in O.lgu/time.
To reduce the space requirement to O.n/ , we make the following changes to the
data structure:Notes for Chapter 20 559
/SIWe cluster the nelements into n=lgugroups of size lg u. (Assume for now
that lg udivides n.) The ﬁrst group consists of the lg usmallest elements in the
set, the second group consists of the next lg usmallest elements, and so on.
/SIWe designate a “representative” value for each group. The representative of
theith group is at least as large as the largest element in the ith group, and it is
smaller than every element of the .iC1/st group. (The representative of the last
group can be the maximum possible element u/NUL1.) Note that a representative
might be a value not currently in the set.
/SIWe store the lg uelements of each group in a balanced binary search tree, such
as a red-black tree. Each representative points to the balanced binary searchtree for its group, and each balanced binary search tree points to its group’srepresentative.
/SIThe perfect hash table stores only the representatives, which are also stored ina doubly linked list in increasing order.
We call this structure a y-fast trie .
c.Show that a y-fast trie requires only O.n/ space to store nelements.
d.Show how to perform the M
INIMUM and M AXIMUM operations in O.lg lgu/
time with a y-fast trie.
e.Show how to perform the M EMBER operation in O.lg lgu/time.
f.Show how to perform the P REDECESSOR and S UCCESSOR operations in
O.lg lgu/time.
g.Explain why the I NSERT and D ELETE operations take /DEL.lg lgu/time.
h.Show how to relax the requirement that each group in a y-fast trie has exactly
lguelements to allow I NSERT and D ELETE to run in O.lg lgu/amortized time
without affecting the asymptotic running times of the other operations.
Chapter notes
The data structure in this chapter is named after P. van Emde Boas, who describedan early form of the idea in 1975 [339]. Later papers by van Emde Boas [340]and van Emde Boas, Kaas, and Zijlstra [341] reﬁned the idea and the exposition.Mehlhorn and N¨ aher [252] subsequently extended the ideas to apply to universe560 Chapter 20 van Emde Boas Trees
sizes that are prime. Mehlhorn’s book [249] contains a slightly different treatment
of van Emde Boas trees than the one in this chapter.
Using the ideas behind van Emde Boas trees, Dementiev et al. [83] developed
a nonrecursive, three-level search tree that ran faster than van Emde Boas trees intheir own experiments.
Wang and Lin [347] designed a hardware-pipelined version of van Emde Boas
trees, which achieves constant amortized time per operation and uses O.lg lgu/
stages in the pipeline.
A lower bound by Pˇ atras¸cu and Thorup [273, 274] for ﬁnding the predecessor
shows that van Emde Boas trees are optimal for this operation, even if randomiza-tion is allowed.21 Data Structures for Disjoint Sets
Some applications involve grouping ndistinct elements into a collection of disjoint
sets. These applications often need to perform two operations in particular: ﬁndingthe unique set that contains a given element and uniting two sets. This chapterexplores methods for maintaining a data structure that supports these operations.
Section 21.1 describes the operations supported by a disjoint-set data structure
and presents a simple application. In Section 21.2, we look at a simple linked-listimplementation for disjoint sets. Section 21.3 presents a more efﬁcient represen-tation using rooted trees. The running time using the tree representation is theo-retically superlinear, but for all practical purposes it is linear. Section 21.4 deﬁnesand discusses a very quickly growing function and its very slowly growing inverse,which appears in the running time of operations on the tree-based implementation,and then, by a complex amortized analysis, proves an upper bound on the runningtime that is just barely superlinear.
21.1 Disjoint-set operations
Adisjoint-set data structure maintains a collection SDfS1;S2;:::;S kgof dis-
joint dynamic sets. We identify each set by a representative , which is some mem-
ber of the set. In some applications, it doesn’t matter which member is used as therepresentative; we care only that if we ask for the representative of a dynamic settwice without modifying the set between the requests, we get the same answer bothtimes. Other applications may require a prespeciﬁed rule for choosing the repre-sentative, such as choosing the smallest member in the set (assuming, of course,that the elements can be ordered).
As in the other dynamic-set implementations we have studied, we represent each
element of a set by an object. Letting xdenote an object, we wish to support the
following operations:562 Chapter 21 Data Structures for Disjoint Sets
MAKE-SET.x/creates a new set whose only member (and thus representative)
isx. Since the sets are disjoint, we require that xnot already be in some other
set.
UNION .x; y/ unites the dynamic sets that contain xandy, say SxandSy,i n t oa
new set that is the union of these two sets. We assume that the two sets are dis-joint prior to the operation. The representative of the resulting set is any memberofS
x[Sy, although many implementations of U NION speciﬁcally choose the
representative of either SxorSyas the new representative. Since we require
the sets in the collection to be disjoint, conceptually we destroy sets SxandSy,
removing them from the collection S. In practice, we often absorb the elements
of one of the sets into the other set.
FIND-SET.x/returns a pointer to the representative of the (unique) set contain-
ingx.
Throughout this chapter, we shall analyze the running times of disjoint-set data
structures in terms of two parameters: n, the number of M AKE-SEToperations,
andm, the total number of M AKE-SET,UNION ,a n dF IND-SEToperations. Since
the sets are disjoint, each U NION operation reduces the number of sets by one.
After n/NUL1UNION operations, therefore, only one set remains. The number of
UNION operations is thus at most n/NUL1. Note also that since the M AKE-SET
operations are included in the total number of operations m,w eh a v e m/NAKn.W e
assume that the nMAKE-SEToperations are the ﬁrst noperations performed.
An application of disjoint-set data structures
One of the many applications of disjoint-set data structures arises in determin-
ing the connected components of an undirected graph (see Section B.4). Fig-ure 21.1(a), for example, shows a graph with four connected components.
The procedure C
ONNECTED -COMPONENTS that follows uses the disjoint-set
operations to compute the connected components of a graph. Once C ONNECTED -
COMPONENTS has preprocessed the graph, the procedure S AME -COMPONENT
answers queries about whether two vertices are in the same connected component.1
(In pseudocode, we denote the set of vertices of a graph GbyG:Vand the set of
edges by G:E.)
1When the edges of the graph are static—not changing over time—we can compute the connected
components faster by using depth-ﬁrst search (Exercise 22.3-12). Sometimes, however, the edges
are added dynamically and we need to maintain the connected components as each edge is added. In
this case, the implementation given here can be more efﬁcient than running a new depth-ﬁrst search
for each new edge.21.1 Disjoint-set operations 563
ab
cdef
gh
ij
Edge processed
initial sets
(b,d)
(e,g)
(a,c)
(h,i)
(a,b)
(e,f)
(b,c){a,b,c,d}{a,b,c,d}{a,c}{a,c}{a}{a}{a}
{a,b,c,d}{b,d}{b,d}{b,d}{b,d}{b}
{c}{c}{c}{d}
{e, f,g}{e, f,g}{e,g}{e,g}{e,g}{e,g}{e}{e}{ f}
{f}
{f}
{f}
{f}
{f}{g}
{g}
{h,i}{h,i}{h,i}{h,i}{h}{h}{h}{h}{ i}
{i}
{i}
{i}{j}
{j}
{j}
{j}
{j}
{j}
{j}
{j}Collection of disjoint sets(a)
(b)
Figure 21.1 (a) A graph with four connected components: fa; b; c; dg,fe;f;gg,fh; ig,a n dfjg.
(b)The collection of disjoint sets after processing each edge.
CONNECTED -COMPONENTS .G/
1foreach vertex /ETB2G:V
2M AKE-SET./ETB/
3foreach edge .u; /ETB/2G:E
4 ifFIND-SET.u/¤FIND-SET./ETB/
5U NION .u; /ETB/
SAME -COMPONENT .u; /ETB/
1ifFIND-SET.u/==FIND-SET./ETB/
2 return TRUE
3else return FALSE
The procedure C ONNECTED -COMPONENTS initially places each vertex /ETBin its
own set. Then, for each edge .u; /ETB/ , it unites the sets containing uand/ETB.B y
Exercise 21.1-2, after processing all the edges, two vertices are in the same con-nected component if and only if the corresponding objects are in the same set.Thus, C
ONNECTED -COMPONENTS computes sets in such a way that the proce-
dure S AME -COMPONENT can determine whether two vertices are in the same con-564 Chapter 21 Data Structures for Disjoint Sets
nected component. Figure 21.1(b) illustrates how C ONNECTED -COMPONENTS
computes the disjoint sets.
In an actual implementation of this connected-components algorithm, the repre-
sentations of the graph and the disjoint-set data structure would need to referenceeach other. That is, an object representing a vertex would contain a pointer tothe corresponding disjoint-set object, and vice versa. These programming detailsdepend on the implementation language, and we do not address them further here.
Exercises
21.1-1
Suppose that C
ONNECTED -COMPONENTS is run on the undirected graph GD
.V; E/ ,w h e r e VDfa;b;c;d;e;f;g;h;i;j;k gand the edges of Eare pro-
cessed in the order .d; i/; .f; k/; .g; i/; .b; g/; .a; h/; .i; j /; .d; k/; .b; j /; .d; f /;
.g; j /; .a; e/ . List the vertices in each connected component after each iteration of
lines 3–5.
21.1-2
Show that after all edges are processed by C ONNECTED -COMPONENTS ,t w ov e r -
tices are in the same connected component if and only if they are in the same set.
21.1-3
During the execution of C ONNECTED -COMPONENTS on an undirected graph GD
.V; E/ withkconnected components, how many times is F IND-SETcalled? How
many times is U NION called? Express your answers in terms of jVj,jEj,a n d k.
21.2 Linked-list representation of disjoint sets
Figure 21.2(a) shows a simple way to implement a disjoint-set data structure: each
set is represented by its own linked list. The object for each set has attributes head ,
pointing to the ﬁrst object in the list, and tail, pointing to the last object. Each
object in the list contains a set member, a pointer to the next object in the list, anda pointer back to the set object. Within each linked list, the objects may appear inany order. The representative is the set member in the ﬁrst object in the list.
With this linked-list representation, both M
AKE-SETand F IND-SETare easy,
requiring O.1/ time. To carry out M AKE-SET.x/, we create a new linked list
whose only object is x.F o rF IND-SET.x/, we just follow the pointer from xback
to its set object and then return the member in the object that head points to. For
example, in Figure 21.2(a), the call F IND-SET.g/would return f.21.2 Linked-list representation of disjoint sets 565
fgd cheb(a)
(b)
head
tailS1che
head
tailS2b fgd
head
tailS1
Figure 21.2 (a) Linked-list representations of two sets. Set S1contains members d,f,a n d g, with
representative f,a n ds e t S2contains members b,c,e,a n d h, with representative c. Each object in
the list contains a set member, a pointer to the next object in the list, and a pointer back to the set
object. Each set object has pointers head andtailto the ﬁrst and last objects, respectively. (b)The
result of U NION .g; e/ , which appends the linked list containing eto the linked list containing g.T h e
representative of the resulting set is f. The set object for e’s list, S2, is destroyed.
A simple implementation of union
The simplest implementation of the U NION operation using the linked-list set rep-
resentation takes signiﬁcantly more time than M AKE-SETor F IND-SET.A s F i g -
ure 21.2(b) shows, we perform U NION .x; y/ by appending y’s list onto the end
ofx’s list. The representative of x’s list becomes the representative of the resulting
set. We use the tailpointer for x’s list to quickly ﬁnd where to append y’s list. Be-
cause all members of y’s list join x’s list, we can destroy the set object for y’s list.
Unfortunately, we must update the pointer to the set object for each object origi-nally on y’s list, which takes time linear in the length of y’s list. In Figure 21.2, for
example, the operation U
NION .g; e/ causes pointers to be updated in the objects
forb,c,e,a n d h.
In fact, we can easily construct a sequence of moperations on nobjects that
requires ‚.n2/time. Suppose that we have objects x1;x2;:::;x n. We execute
the sequence of nMAKE-SEToperations followed by n/NUL1UNION operations
shown in Figure 21.3, so that mD2n/NUL1. We spend ‚.n/ time performing the n
MAKE-SEToperations. Because the ith U NION operation updates iobjects, the
total number of objects updated by all n/NUL1UNION operations is566 Chapter 21 Data Structures for Disjoint Sets
Operation Number of objects updated
MAKE-SET.x1/ 1
MAKE-SET.x2/ 1
::::::
MAKE-SET.xn/ 1
UNION .x2;x1/ 1
UNION .x3;x2/ 2
UNION .x4;x3/ 3
::::::
UNION .xn;xn/NUL1/n/NUL1
Figure 21.3 A sequence of 2n/NUL1operations on nobjects that takes ‚.n2/time, or ‚.n/ time
per operation on average, using the linked-list set r epresentation and the simple implementation of
UNION .
n/NUL1X
iD1iD‚.n2/:
The total number of operations is 2n/NUL1, and so each operation on average requires
‚.n/ time. That is, the amortized time of an operation is ‚.n/ .
A weighted-union heuristic
In the worst case, the above implementation of the U NION procedure requires an
average of ‚.n/ time per call because we may be appending a longer list onto
a shorter list; we must update the pointer to the set object for each member ofthe longer list. Suppose instead that each list also includes the length of the list(which we can easily maintain) and that we always append the shorter list onto thelonger, breaking ties arbitrarily. With this simple weighted-union heuristic ,as i n -
gle U
NION operation can still take /DEL.n/ t i m ei fb o t hs e t sh a v e /DEL.n/ members. As
the following theorem shows, however, a sequence of mMAKE-SET,UNION ,a n d
FIND-SEToperations, nof which are M AKE-SEToperations, takes O.mCnlgn/
time.
Theorem 21.1
Using the linked-list representation of disjoint sets and the weighted-union heuris-tic, a sequence of mM
AKE-SET,UNION ,a n dF IND-SEToperations, nof which
are M AKE-SEToperations, takes O.mCnlgn/time.21.2 Linked-list representation of disjoint sets 567
Proof Because each U NION operation unites two disjoint sets, we perform at
most n/NUL1UNION operations over all. We now bound the total time taken by these
UNION operations. We start by determining, for each object, an upper bound on the
number of times the object’s pointer back to its set object is updated. Consider aparticular object x. We know that each time x’s pointer was updated, xmust have
started in the smaller set. The ﬁrst time x’s pointer was updated, therefore, the
resulting set must have had at least 2members. Similarly, the next time x’s pointer
was updated, the resulting set must have had at least 4members. Continuing on,
we observe that for any k/DC4n, after x’s pointer has been updated dlgketimes,
the resulting set must have at least kmembers. Since the largest set has at most n
members, each object’s pointer is updated at most dlgnetimes over all the U
NION
operations. Thus the total time spent updating object pointers over all U NION
operations is O.n lgn/. We must also account for updating the tailpointers and
the list lengths, which take only ‚.1/ time per U NION operation. The total time
spent in all U NION operations is thus O.n lgn/.
The time for the entire sequence of moperations follows easily. Each M AKE-
SETand F IND-SEToperation takes O.1/ time, and there are O.m/ of them. The
total time for the entire sequence is thus O.mCnlgn/.
Exercises
21.2-1
Write pseudocode for M AKE-SET,FIND-SET,a n dU NION using the linked-list
representation and the weighted-union heuristic. Make sure to specify the attributesthat you assume for set objects and list objects.
21.2-2
Show the data structure that results and the answers returned by the F
IND-SET
operations in the following program. Use the linked-list representation with theweighted-union heuristic.
1foriD1to16
2M
AKE-SET.xi/
3foriD1to15by2
4U NION .xi;xiC1/
5foriD1to13by4
6U NION .xi;xiC2/
7U NION .x1;x5/
8U NION .x11;x13/
9U NION .x1;x10/
10 F IND-SET.x2/
11 F IND-SET.x9/568 Chapter 21 Data Structures for Disjoint Sets
Assume that if the sets containing xiandxjhave the same size, then the operation
UNION .xi;xj/appends xj’s list onto xi’s list.
21.2-3
Adapt the aggregate proof of Theorem 21.1 to obtain amortized time boundsofO.1/ for M
AKE-SETand F IND-SETandO.lgn/for U NION using the linked-
list representation and the weighted-union heuristic.
21.2-4
Give a tight asymptotic bound on the running time of the sequence of operations in
Figure 21.3 assuming the linked-list representation and the weighted-union heuris-
tic.
21.2-5
Professor Gompers suspects that it might be possible to keep just one pointer ineach set object, rather than two ( head andtail), while keeping the number of point-
ers in each list element at two. Show that the professor’s suspicion is well foundedby describing how to represent each set by a linked list such that each operationhas the same running time as the operations described in this section. Describealso how the operations work. Your scheme should allow for the weighted-unionheuristic, with the same effect as described in this section. ( Hint: Use the tail of a
linked list as its set’s representative.)
21.2-6
Suggest a simple change to the U
NION procedure for the linked-list representation
that removes the need to keep the tailpointer to the last object in each list. Whether
or not the weighted-union heuristic is used, your change should not change the
asymptotic running time of the U NION procedure. ( Hint: Rather than appending
one list to another, splice them together.)
21.3 Disjoint-set forests
In a faster implementation of disjoint sets, we represent sets by rooted trees, witheach node containing one member and each tree representing one set. In a disjoint-
set forest , illustrated in Figure 21.4(a), each member points only to its parent. The
root of each tree contains the representative and is its own parent. As we shall
see, although the straightforward algorithms that use this representation are nofaster than ones that use the linked-list representation, by introducing two heuris-tics—“union by rank” and “path compression”—we can achieve an asymptoticallyoptimal disjoint-set data structure.21.3 Disjoint-set forests 569
c
hebf
dg
(a)f
c
hebdg
(b)
Figure 21.4 A disjoint-set forest. (a)Two trees representing the two sets of Figure 21.2. The
tree on the left represents the set fb;c; e;hg,w i t h cas the representative, and the tree on the right
represents the setfd;f;gg, with fas the representative. (b)The result of U NION .e; g/ .
We perform the three disjoint-set operations as follows. A M AKE-SEToperation
simply creates a tree with just one node. We perform a F IND-SEToperation by
following parent pointers until we ﬁnd the root of the tree. The nodes visited onthis simple path toward the root constitute the ﬁnd path .A U
NION operation,
shown in Figure 21.4(b), causes the root of one tree to point to the root of the other.
Heuristics to improve the running time
So far, we have not improved on the linked-list implementation. A sequence of
n/NUL1UNION operations may create a tree that is just a linear chain of nnodes. By
using two heuristics, however, we can achieve a running time that is almost linearin the total number of operations m.
The ﬁrst heuristic, union by rank , is similar to the weighted-union heuristic we
used with the linked-list representation. The obvious approach would be to make
the root of the tree with fewer nodes point to the root of the tree with more nodes.
Rather than explicitly keeping track of the size of the subtree rooted at each node,we shall use an approach that eases the analysis. For each node, we maintain a
rank , which is an upper bound on the height of the node. In union by rank, we
make the root with smaller rank point to the root with larger rank during a U
NION
operation.
The second heuristic, path compression , is also quite simple and highly effec-
tive. As shown in Figure 21.5, we use it during F IND-SEToperations to make each
node on the ﬁnd path point directly to the root. Path compression does not changeany ranks.570 Chapter 21 Data Structures for Disjoint Sets
abcde
f
abcdef
(a) (b)
Figure 21.5 Path compression during the operation F IND-SET. Arrows and self-loops at roots are
omitted. (a)A tree representing a set prior to executing F IND-SET.a/. Triangles represent subtrees
whose roots are the nodes shown. Each node has a pointer to its parent. (b)The same set after
executing F IND-SET.a/. Each node on the ﬁnd path now points directly to the root.
Pseudocode for disjoint-set forests
To implement a disjoint-set forest with the union-by-rank heuristic, we must keep
track of ranks. With each node x, we maintain the integer value x:rank,w h i c hi s
an upper bound on the height of x(the number of edges in the longest simple path
between xand a descendant leaf). When M AKE-SETcreates a singleton set, the
single node in the corresponding tree has an initial rank of 0. Each F IND-SEToper-
ation leaves all ranks unchanged. The U NION operation has two cases, depending
on whether the roots of the trees have equal rank. If the roots have unequal rank,
we make the root with higher rank the parent of the root with lower rank, but theranks themselves remain unchanged. If, instead, the roots have equal ranks, wearbitrarily choose one of the roots as the parent and increment its rank.
Let us put this method into pseudocode. We designate the parent of node x
byx:p.T h eL
INKprocedure, a subroutine called by U NION , takes pointers to two
roots as inputs.21.3 Disjoint-set forests 571
MAKE-SET.x/
1x:pDx
2x:rankD0
UNION .x; y/
1L INK.FIND-SET.x/;FIND-SET.y//
LINK.x; y/
1ifx:rank >y : rank
2 y:pDx
3elsex:pDy
4 ifx:rank ==y:rank
5 y:rankDy:rankC1
The F IND-SETprocedure with path compression is quite simple:
FIND-SET.x/
1ifx¤x:p
2 x:pDFIND-SET.x:p/
3return x:p
The F IND-SETprocedure is a two-pass method : as it recurses, it makes one pass
up the ﬁnd path to ﬁnd the root, and as the recursion unwinds, it makes a secondpass back down the ﬁnd path to update each node to point directly to the root. Eachcall of F
IND-SET.x/returns x:pin line 3. If xis the root, then F IND-SETskips
line 2 and instead returns x:p,w h i c hi s x; this is the case in which the recursion
bottoms out. Otherwise, line 2 executes, and the recursive call with parameter x:p
returns a pointer to the root. Line 2 updates node xto point directly to the root,
and line 3 returns this pointer.
Effect of the heuristics on the running time
Separately, either union by rank or path compression improves the running time of
the operations on disjoint-set forests, and the improvement is even greater whenwe use the two heuristics together. Alone, union by rank yields a running timeofO.m lgn/(see Exercise 21.4-4), and this bound is tight (see Exercise 21.3-3).
Although we shall not prove it here, for a sequence of nM
AKE-SETopera-
tions (and hence at most n/NUL1UNION operations) and fFIND-SETopera-
tions, the path-compression heuristic alone gives a worst-case running time of
‚.nCf/SOH.1Clog2Cf=nn//.572 Chapter 21 Data Structures for Disjoint Sets
When we use both union by rank and path compression, the worst-case running
time is O.m ˛.n// ,w h e r e ˛.n/ is avery slowly growing function, which we de-
ﬁne in Section 21.4. In any conceivable application of a disjoint-set data structure,˛.n//DC44; thus, we can view the running time as linear in min all practical situa-
tions. Strictly speaking, however, it is superlinear. In Section 21.4, we prove thisupper bound.
Exercises
21.3-1
Redo Exercise 21.2-2 using a disjoint-set forest with union by rank and path com-pression.
21.3-2
Write a nonrecursive version of F
IND-SETwith path compression.
21.3-3
Give a sequence of mMAKE-SET,UNION ,a n dF IND-SEToperations, nof which
are M AKE-SEToperations, that takes /DEL.m lgn/time when we use union by rank
only.
21.3-4
Suppose that we wish to add the operation P RINT -SET.x/, which is given a node x
and prints all the members of x’s set, in any order. Show how we can add just
a single attribute to each node in a disjoint-set forest so that P RINT -SET.x/takes
time linear in the number of members of x’s set and the asymptotic running times
of the other operations are unchanged. Assume that we can print each member of
the set in O.1/ time.
21.3-5 ?
Show that any sequence of mMAKE-SET,FIND-SET,a n dL INKoperations, where
all the L INKoperations appear before any of the F IND-SEToperations, takes only
O.m/ time if we use both path compression and union by rank. What happens in
the same situation if we use only the path-compression heuristic?21.4 Analysis of union by rank with path compression 573
?21.4 Analysis of union by rank with path compression
As noted in Section 21.3, the combined union-by-rank and path-compression heu-
ristic runs in time O.m ˛.n// formdisjoint-set operations on nelements. In this
section, we shall examine the function ˛to see just how slowly it grows. Then we
prove this running time using the potential method of amortized analysis.
A very quickly growing function and its very slowly growing inverse
For integers k/NAK0andj/NAK1, we deﬁne the function Ak.j /as
Ak.j /D(
jC1 ifkD0;
A.jC1/
k/NUL1.j / ifk/NAK1;
where the expression A.jC1/
k/NUL1.j /uses the functional-iteration notation given in Sec-
tion 3.2. Speciﬁcally, A.0/
k/NUL1.j /DjandA.i/
k/NUL1.j /DAk/NUL1.A.i/NUL1/
k/NUL1.j // fori/NAK1.
We will refer to the parameter kas the level of the function A.
The function Ak.j /strictly increases with both jandk. To see just how quickly
this function grows, we ﬁrst obtain closed-form expressions for A1.j /andA2.j /.
Lemma 21.2
For any integer j/NAK1,w eh a v e A1.j /D2jC1.
Proof We ﬁrst use induction on ito show that A.i/
0.j /DjCi. For the base case,
we have A.0/
0.j /DjDjC0. For the inductive step, assume that A.i/NUL1/
0.j /D
jC.i/NUL1/.T h e n A.i/
0.j /DA0.A.i/NUL1/
0.j //D.jC.i/NUL1//C1DjCi. Finally,
we note that A1.j /DA.jC1/
0.j /DjC.jC1/D2jC1.
Lemma 21.3
For any integer j/NAK1,w eh a v e A2.j /D2jC1.jC1//NUL1.
Proof We ﬁrst use induction on ito show that A.i/
1.j /D2i.jC1//NUL1.F o r
the base case, we have A.0/
1.j /DjD20.jC1//NUL1. For the inductive step,
assume that A.i/NUL1/
1.j /D2i/NUL1.jC1//NUL1.T h e n A.i/
1.j /DA1.A.i/NUL1/
1.j //D
A1.2i/NUL1.jC1//NUL1/D2/SOH.2i/NUL1.jC1//NUL1/C1D2i.jC1//NUL2C1D2i.jC1//NUL1.
Finally, we note that A2.j /DA.jC1/
1.j /D2jC1.jC1//NUL1.
Now we can see how quickly Ak.j /grows by simply examining Ak.1/for levels
kD0; 1; 2; 3; 4 . From the deﬁnition of A0.k/and the above lemmas, we have
A0.1/D1C1D2,A1.1/D2/SOH1C1D3,a n d A2.1/D21C1/SOH.1C1//NUL1D7.574 Chapter 21 Data Structures for Disjoint Sets
We also have
A3.1/DA.2/
2.1/
DA2.A2.1//
DA2.7/
D28/SOH8/NUL1
D211/NUL1
D2047
andA
4.1/DA.2/
3.1/
DA3.A3.1//
DA3.2047/
DA.2048/
2.2047/
/GSA2.2047/
D22048/SOH2048/NUL1
>22048
D.24/512
D16512
/GS1080;
which is the estimated number of atoms in the observable universe. (The symbol
“/GS” denotes the “much-greater-than” relation.)
We deﬁne the inverse of the function Ak.n/, for integer n/NAK0,b y
˛.n/DminfkWAk.1//NAKng:
In words, ˛.n/ is the lowest level kfor which Ak.1/is at least n. From the above
values of Ak.1/, we see that
˛.n/D˚
0for0/DC4n/DC42;
1fornD3;
2for4/DC4n/DC47;
3for8/DC4n/DC42047 ;
4for2048/DC4n/DC4A4.1/ :
It is only for values of nso large that the term “astronomical” understates them
(greater than A4.1/, a huge number) that ˛.n/ > 4 ,a n ds o ˛.n//DC44for all
practical purposes.21.4 Analysis of union by rank with path compression 575
Properties of ranks
In the remainder of this section, we prove an O.m˛.n// bound on the running time
of the disjoint-set operations with union by rank and path compression. In order toprove this bound, we ﬁrst prove some simple properties of ranks.
Lemma 21.4
For all nodes x,w eh a v e x:rank/DC4x:p:rank, with strict inequality if x¤x:p.
The value of x:rank is initially 0 and increases through time until x¤x:p; from
then on, x:rank does not change. The value of x:p:rank monotonically increases
over time.
Proof The proof is a straightforward induction on the number of operations, us-
ing the implementations of M
AKE-SET,U NION ,a n dF IND-SETthat appear in
Section 21.3. We leave it as Exercise 21.4-1.
Corollary 21.5As we follow the simple path from any node toward a root, the node ranks strictlyincrease.
Lemma 21.6Every node has rank at most n/NUL1.
Proof Each node’s rank starts at 0, and it increases only upon L
INKoperations.
Because there are at most n/NUL1UNION operations, there are also at most n/NUL1
LINK operations. Because each L INK operation either leaves all ranks alone or
increases some node’s rank by 1, all ranks are at most n/NUL1.
Lemma 21.6 provides a weak bound on ranks. In fact, every node has rank at
mostblgnc(see Exercise 21.4-2). The looser bound of Lemma 21.6 will sufﬁce
for our purposes, however.
Proving the time bound
We shall use the potential method of amortized analysis (see Section 17.3) to prove
theO.m ˛.n// time bound. In performing the amortized analysis, we will ﬁnd it
convenient to assume that we invoke the L INK operation rather than the U NION
operation. That is, since the parameters of the L INKprocedure are pointers to two
roots, we act as though we perform the appropriate F IND-SEToperations sepa-
rately. The following lemma shows that even if we count the extra F IND-SETop-
erations induced by U NION calls, the asymptotic running time remains unchanged.576 Chapter 21 Data Structures for Disjoint Sets
Lemma 21.7
Suppose we convert a sequence S0ofm0MAKE-SET,UNION ,a n dF IND-SETop-
erations into a sequence SofmMAKE-SET,LINK,a n dF IND-SEToperations by
turning each U NION into two F IND-SEToperations followed by a L INK. Then, if
sequence Sruns in O.m ˛.n// time, sequence S0runs in O.m0˛.n// time.
Proof Since each U NION operation in sequence S0is converted into three opera-
tions in S,w eh a v e m0/DC4m/DC43m0.S i n c e mDO.m0/,a nO.m ˛.n// time bound
for the converted sequence Simplies an O.m0˛.n// time bound for the original
sequence S0.
In the remainder of this section, we shall assume that the initial sequence of m0
MAKE-SET,UNION ,a n dF IND-SEToperations has been converted to a sequence
ofmMAKE-SET,LINK,a n dF IND-SEToperations. We now prove an O.m ˛.n//
time bound for the converted sequence and appeal to Lemma 21.7 to prove theO.m
0˛.n// running time of the original sequence of m0operations.
Potential function
The potential function we use assigns a potential /RSq.x/to each node xin the
disjoint-set forest after qoperations. We sum the node potentials for the poten-
tial of the entire forest: ˆqDP
x/RSq.x/,w h e r e ˆqdenotes the potential of the
forest after qoperations. The forest is empty prior to the ﬁrst operation, and we
arbitrarily set ˆ0D0. No potential ˆqwill ever be negative.
The value of /RSq.x/depends on whether xis a tree root after the qth operation.
If it is, or if x:rankD0,t h e n /RSq.x/D˛.n//SOHx:rank.
Now suppose that after the qth operation, xis not a root and that x:rank/NAK1.
We need to deﬁne two auxiliary functions on xbefore we can deﬁne /RSq.x/. First
we deﬁne
level.x/DmaxfkWx:p:rank/NAKAk.x:rank/g:
That is, level .x/is the greatest level kfor which Ak, applied to x’s rank, is no
greater than x’s parent’s rank.
We claim that
0/DC4level.x/ < ˛.n/ ; (21.1)
which we see as follows. We havex:p:rank/NAKx:rankC1(by Lemma 21.4)
DA
0.x:rank/(by deﬁnition of A0.j /),
which implies that level .x//NAK0, and we have21.4 Analysis of union by rank with path compression 577
A˛.n/.x:rank//NAKA˛.n/.1/ (because Ak.j /is strictly increasing)
/NAKn (by the deﬁnition of ˛.n/ )
>x : p:rank (by Lemma 21.6) ,
which implies that level .x/ < ˛.n/ . Note that because x:p:rank monotonically
increases over time, so does level .x/.
The second auxiliary function applies when x:rank/NAK1:
iter.x/Dmax˚
iWx:p:rank/NAKA.i/
level.x/.x:rank//TAB
:
That is, iter .x/is the largest number of times we can iteratively apply Alevel.x/,
applied initially to x’s rank, before we get a value greater than x’s parent’s rank.
We claim that when x:rank/NAK1,w eh a v e
1/DC4iter.x//DC4x:rank; (21.2)
which we see as follows. We havex:p:rank/NAKA
level.x/.x:rank/(by deﬁnition of level .x/)
DA.1/
level.x/.x:rank/(by deﬁnition of functional iteration) ,
which implies that iter .x//NAK1, and we have
A.x:rankC1/
level.x/.x:rank/DAlevel.x/C1.x:rank/(by deﬁnition of Ak.j /)
>x : p:rank (by deﬁnition of level .x/),
which implies that iter .x//DC4x:rank. Note that because x:p:rank monotonically
increases over time, in order for iter .x/to decrease, level .x/must increase. As long
as level .x/remains unchanged, iter .x/must either increase or remain unchanged.
With these auxiliary functions in place, we are ready to deﬁne the potential of
node xafterqoperations:
/RSq.x/D(
˛.n//SOHx:rank ifxis a root or x:rankD0;
.˛.n//NULlevel.x///SOHx:rank/NULiter.x/ifxis not a root and x:rank/NAK1:
We next investigate some useful properties of node potentials.
Lemma 21.8
For every node x, and for all operation counts q,w eh a v e
0/DC4/RSq.x//DC4˛.n//SOHx:rank:578 Chapter 21 Data Structures for Disjoint Sets
Proof Ifxis a root or x:rankD0,t h e n /RSq.x/D˛.n//SOHx:rank by deﬁnition. Now
suppose that xis not a root and that x:rank/NAK1. We obtain a lower bound on /RSq.x/
by maximizing level .x/and iter .x/. By the bound (21.1), level .x//DC4˛.n//NUL1,a n d
by the bound (21.2), iter .x//DC4x:rank. Thus,
/RSq.x/D.˛.n//NULlevel.x///SOHx:rank/NULiter.x/
/NAK.˛.n//NUL.˛.n//NUL1///SOHx:rank/NULx:rank
Dx:rank/NULx:rank
D0:
Similarly, we obtain an upper bound on /RSq.x/by minimizing level .x/and iter .x/.
By the bound (21.1), level .x//NAK0, and by the bound (21.2), iter .x//NAK1. Thus,
/RSq.x//DC4.˛.n//NUL0//SOHx:rank/NUL1
D˛.n//SOHx:rank/NUL1
< ˛.n//SOHx:rank:
Corollary 21.9
If node xis not a root and x:rank >0,t h e n /RSq.x/ < ˛.n//SOHx:rank.
Potential changes and amortized costs of operations
We are now ready to examine how the disjoint-set operations affect node potentials.
With an understanding of the change in potential due to each operation, we candetermine each operation’s amortized cost.
Lemma 21.10
Letxbe a node that is not a root, and suppose that the qth operation is either a
L
INKor F IND-SET. Then after the qth operation, /RSq.x//DC4/RSq/NUL1.x/. Moreover, if
x:rank/NAK1and either level .x/or iter .x/changes due to the qth operation, then
/RSq.x//DC4/RSq/NUL1.x//NUL1.T h a ti s , x’s potential cannot increase, and if it has positive
rank and either level .x/or iter .x/changes, then x’s potential drops by at least 1.
Proof Because xis not a root, the qth operation does not change x:rank,a n d
because ndoes not change after the initial nMAKE-SEToperations, ˛.n/ remains
unchanged as well. Hence, these components of the formula for x’s potential re-
main the same after the qth operation. If x:rankD0,t h e n /RSq.x/D/RSq/NUL1.x/D0.
Now assume that x:rank/NAK1.
Recall that level .x/monotonically increases over time. If the qth operation
leaves level .x/unchanged, then iter .x/either increases or remains unchanged.
If both level .x/and iter .x/are unchanged, then /RSq.x/D/RSq/NUL1.x/.I f l e v e l .x/21.4 Analysis of union by rank with path compression 579
is unchanged and iter .x/ increases, then it increases by at least 1,a n ds o
/RSq.x//DC4/RSq/NUL1.x//NUL1.
Finally, if the qth operation increases level .x/, it increases by at least 1,s ot h a t
the value of the term .˛.n//NULlevel.x///SOHx:rank drops by at least x:rank.B e -
cause level .x/increased, the value of iter .x/might drop, but according to the
bound (21.2), the drop is by at most x:rank/NUL1. Thus, the increase in poten-
tial due to the change in iter .x/is less than the decrease in potential due to the
change in level .x/, and we conclude that /RSq.x//DC4/RSq/NUL1.x//NUL1.
Our ﬁnal three lemmas show that the amortized cost of each M AKE-SET,LINK,
and F IND-SEToperation is O.˛.n// . Recall from equation (17.2) that the amor-
tized cost of each operation is its actual cost plus the increase in potential due tothe operation.
Lemma 21.11
The amortized cost of each M
AKE-SEToperation is O.1/ .
Proof Suppose that the qth operation is M AKE-SET.x/. This operation creates
node xwith rank 0,s ot h a t /RSq.x/D0. No other ranks or potentials change, and
soˆqDˆq/NUL1. Noting that the actual cost of the M AKE-SEToperation is O.1/
completes the proof.
Lemma 21.12The amortized cost of each L
INKoperation is O.˛.n// .
Proof Suppose that the qth operation is L INK.x; y/ . The actual cost of the L INK
operation is O.1/ . Without loss of generality, suppose that the L INKmakes ythe
parent of x.
To determine the change in potential due to the L INK, we note that the only
nodes whose potentials may change are x,y, and the children of yjust prior to the
operation. We shall show that the only node whose potential can increase due tothe L
INKisy, and that its increase is at most ˛.n/ :
/SIBy Lemma 21.10, any node that is y’s child just before the L INKcannot have
its potential increase due to the L INK.
/SIFrom the deﬁnition of /RSq.x/, we see that, since xwas a root just before the qth
operation, /RSq/NUL1.x/D˛.n//SOHx:rank.I fx:rankD0,t h e n /RSq.x/D/RSq/NUL1.x/D0.
Otherwise,
/RSq.x/ < ˛.n//SOHx:rank (by Corollary 21.9)
D/RSq/NUL1.x/ ;
and so x’s potential decreases.580 Chapter 21 Data Structures for Disjoint Sets
/SIBecause yis a root prior to the L INK,/RSq/NUL1.y/D˛.n//SOHy:rank.T h e L INK
operation leaves yas a root, and it either leaves y’s rank alone or it increases y’s
rank by 1. Therefore, either /RSq.y/D/RSq/NUL1.y/or/RSq.y/D/RSq/NUL1.y/C˛.n/ .
The increase in potential due to the L INKoperation, therefore, is at most ˛.n/ .
The amortized cost of the L INKoperation is O.1/C˛.n/DO.˛.n// .
Lemma 21.13
The amortized cost of each F IND-SEToperation is O.˛.n// .
Proof Suppose that the qth operation is a F IND-SETand that the ﬁnd path con-
tains snodes. The actual cost of the F IND-SEToperation is O.s/ .W e s h a l l
show that no node’s potential increases due to the F IND-SETand that at least
max.0; s/NUL.˛.n/C2//nodes on the ﬁnd path have their potential decrease by
at least 1.
To see that no node’s potential increases, we ﬁrst appeal to Lemma 21.10 for all
nodes other than the root. If xis the root, then its potential is ˛.n//SOHx:rank,w h i c h
does not change.
Now we show that at least max .0; s/NUL.˛.n/C2//nodes have their potential
decrease by at least 1.L e t xbe a node on the ﬁnd path such that x:rank >0
andxis followed somewhere on the ﬁnd path by another node ythat is not a root,
where level .y/Dlevel.x/just before the F IND-SEToperation. (Node yneed not
immediately follow xon the ﬁnd path.) All but at most ˛.n/C2nodes on the ﬁnd
path satisfy these constraints on x. Those that do not satisfy them are the ﬁrst node
on the ﬁnd path (if it has rank 0), the last node on the path (i.e., the root), and the
last node won the path for which level .w/Dk, for each kD0; 1; 2; : : : ; ˛.n//NUL1.
Let us ﬁx such a node x, and we shall show that x’s potential decreases by at
least1.L e t kDlevel.x/Dlevel.y/. Just prior to the path compression caused by
the F IND-SET,w eh a v e
x:p:rank/NAKA.iter.x//
k.x:rank/(by deﬁnition of iter .x/),
y:p:rank/NAKAk.y:rank/ (by deﬁnition of level .y/),
y:rank/NAKx:p:rank (by Corollary 21.5 and because
yfollows xon the ﬁnd path) .
Putting these inequalities together and letting ibe the value of iter .x/before path
compression, we have
y:p:rank/NAKAk.y:rank/
/NAKAk.x:p:rank/ (because Ak.j /is strictly increasing)
/NAKAk.A.iter.x//
k.x:rank//
DA.iC1/
k.x:rank/:21.4 Analysis of union by rank with path compression 581
Because path compression will make xandyhave the same parent, we know
that after path compression, x:p:rankDy:p:rank and that the path compression
does not decrease y:p:rank.S i n c e x:rank does not change, after path compression
we have that x:p:rank/NAKA.iC1/
k.x:rank/. Thus, path compression will cause ei-
ther iter .x/to increase (to at least iC1)o rl e v e l .x/to increase (which occurs if
iter.x/increases to at least x:rankC1). In either case, by Lemma 21.10, we have
/RSq.x//DC4/RSq/NUL1.x//NUL1. Hence, x’s potential decreases by at least 1.
The amortized cost of the F IND-SEToperation is the actual cost plus the change
in potential. The actual cost is O.s/ , and we have shown that the total potential
decreases by at least max .0; s/NUL.˛.n/C2//. The amortized cost, therefore, is at
most O.s//NUL.s/NUL.˛.n/C2//DO.s//NULsCO.˛.n//DO.˛.n// , since we can
scale up the units of potential to dominate the constant hidden in O.s/ .
Putting the preceding lemmas together yields the following theorem.
Theorem 21.14
A sequence of mMAKE-SET,UNION ,a n dF IND-SEToperations, nof which are
MAKE-SEToperations, can be performed on a disjoint-set forest with union by
rank and path compression in worst-case time O.m ˛.n// .
Proof Immediate from Lemmas 21.7, 21.11, 21.12, and 21.13.
Exercises
21.4-1
Prove Lemma 21.4.
21.4-2
Prove that every node has rank at most blgnc.
21.4-3
In light of Exercise 21.4-2, how many bits are necessary to store x:rank for each
node x?
21.4-4
Using Exercise 21.4-2, give a simple proof that operations on a disjoint-set forestwith union by rank but without path compression run in O.m lgn/time.
21.4-5
Professor Dante reasons that because node ranks increase strictly along a simplepath to the root, node levels must monotonically increase along the path. In other582 Chapter 21 Data Structures for Disjoint Sets
words, if x:rank >0 andx:pis not a root, then level .x//DC4level.x:p/.I s t h e
professor correct?
21.4-6 ?
Consider the function ˛0.n/DminfkWAk.1//NAKlg.nC1/g. Show that ˛0.n//DC43
for all practical values of nand, using Exercise 21.4-2, show how to modify the
potential-function argument to prove that we can perform a sequence of mMAKE-
SET,UNION ,a n dF IND-SEToperations, nof which are M AKE-SEToperations, on
a disjoint-set forest with union by rank and path compression in worst-case time
O.m ˛0.n//.
Problems
21-1 Off-line minimum
Theoff-line minimum problem asks us to maintain a dynamic set Tof elements
from the domainf1 ;2;:::;ngunder the operations I NSERT and E XTRACT -MIN.
We are given a sequence SofnINSERT andmEXTRACT -MINcalls, where each
key inf1 ;2;:::;ngis inserted exactly once. We wish to determine which key
is returned by each E XTRACT -MINcall. Speciﬁcally, we wish to ﬁll in an array
extracted Œ 1::m /c141 , where for iD1 ;2;:::;m ,extracted Œi/c141is the key returned by
theith E XTRACT -MINcall. The problem is “off-line” in the sense that we are
allowed to process the entire sequence Sbefore determining any of the returned
keys.
a.In the following instance of the off-line minimum problem, each operation
INSERT .i/is represented by the value of iand each E XTRACT -MINis rep-
resented by the letter E:
4;8; E;3 ;E;9 ;2;6 ; E;E;E;1 ;7 ; E;5:
Fill in the correct values in the extracted array.
To develop an algorithm for this problem, we break the sequence Sinto homoge-
neous subsequences. That is, we represent Sby
I1;E;I2;E;I3;:::; Im;E;ImC1;
where each E represents a single E XTRACT -MINcall and each I jrepresents a (pos-
sibly empty) sequence of I NSERT calls. For each subsequence I j, we initially place
the keys inserted by these operations into a set Kj, which is empty if I jis empty.
We then do the following:Problems for Chapter 21 583
OFF-LINE-MINIMUM .m; n/
1foriD1ton
2 determine jsuch that i2Kj
3 ifj¤mC1
4 extracted Œj /c141Di
5l e t lbe the smallest value greater than j
for which set Klexists
6 KlDKj[Kl, destroying Kj
7return extracted
b.Argue that the array extracted returned by O FF-LINE-MINIMUM is correct.
c.Describe how to implement O FF-LINE-MINIMUM efﬁciently with a disjoint-
set data structure. Give a tight bound on the worst-case running time of yourimplementation.
21-2 Depth determination
In the depth-determination problem , we maintain a forest FDfT
igof rooted
trees under three operations:
MAKE-TREE./ETB/creates a tree whose only node is /ETB.
FIND-DEPTH ./ETB/returns the depth of node /ETBwithin its tree.
GRAFT .r; /ETB/ makes node r, which is assumed to be the root of a tree, become the
child of node /ETB, which is assumed to be in a different tree than rbut may or may
not itself be a root.
a.Suppose that we use a tree representation similar to a disjoint-set forest: /ETB:p
is the parent of node /ETB, except that /ETB:pD/ETBif/ETBis a root. Suppose further
that we implement G RAFT .r; /ETB/ by setting r:pD/ETBand F IND-DEPTH ./ETB/by
following the ﬁnd path up to the root, returning a count of all nodes other than /ETB
encountered. Show that the worst-case running time of a sequence of mMAKE-
TREE,FIND-DEPTH ,a n dG RAFT operations is ‚.m2/.
By using the union-by-rank and path-compression heuristics, we can reduce the
worst-case running time. We use the disjoint-set forest SDfSig, where each
setSi(which is itself a tree) corresponds to a tree Tiin the forest F. The tree
structure within a set Si, however, does not necessarily correspond to that of Ti.I n
fact, the implementation of Sidoes not record the exact parent-child relationships
but nevertheless allows us to determine any node’s depth in Ti.
The key idea is to maintain in each node /ETBa “pseudodistance” /ETB:d,w h i c hi s
deﬁned so that the sum of the pseudodistances along the simple path from /ETBto the584 Chapter 21 Data Structures for Disjoint Sets
root of its set Siequals the depth of /ETBinTi. That is, if the simple path from /ETBto its
root in Siis/ETB0;/ETB1;:::;/ETB k,w h e r e /ETB0D/ETBand/ETBkisSi’s root, then the depth of /ETB
inTiisPk
jD0/ETBj:d.
b.Give an implementation of M AKE-TREE.
c.Show how to modify F IND-SETto implement F IND-DEPTH . Your implemen-
tation should perform path compression, and its running time should be linearin the length of the ﬁnd path. Make sure that your implementation updates
pseudodistances correctly.
d.Show how to implement G
RAFT .r; /ETB/ , which combines the sets containing r
and/ETB, by modifying the U NION and L INK procedures. Make sure that your
implementation updates pseudodistances correctly. Note that the root of a set Si
is not necessarily the root of the corresponding tree Ti.
e.Give a tight bound on the worst-case running time of a sequence of mMAKE-
TREE,FIND-DEPTH ,a n dG RAFT operations, nof which are M AKE-TREE op-
erations.
21-3 Tarjan’s off-line least-common-ancestors algorithm
Theleast common ancestor of two nodes uand/ETBin a rooted tree Tis the node w
that is an ancestor of both uand/ETBand that has the greatest depth in T.I n t h e
off-line least-common-ancestors problem , we are given a rooted tree Tand an
arbitrary set PDffu; /ETBggof unordered pairs of nodes in T, and we wish to deter-
mine the least common ancestor of each pair in P.
To solve the off-line least-common-ancestors problem, the following procedure
performs a tree walk of Twith the initial call LCA .T:root/. We assume that each
node is colored WHITE prior to the walk.
LCA .u/
1M AKE-SET.u/
2F IND-SET.u/:ancestorDu
3foreach child /ETBofuinT
4L C A ./ETB/
5U NION .u; /ETB/
6F IND-SET.u/:ancestorDu
7u:colorDBLACK
8foreach node /ETBsuch thatfu; /ETBg2P
9 if/ETB:color ==BLACK
10 print “The least common ancestor of”
u“and” /ETB“is” F IND-SET./ETB/:ancestorNotes for Chapter 21 585
a.Argue that line 10 executes exactly once for each pair fu; /ETBg2P.
b.Argue that at the time of the call LCA .u/, the number of sets in the disjoint-set
data structure equals the depth of uinT.
c.Prove that LCA correctly prints the least common ancestor of uand/ETBfor each
pairfu; /ETBg2P.
d.Analyze the running time of LCA, assuming that we use the implementation of
the disjoint-set data structure in Section 21.3.
Chapter notes
Many of the important results for disjoint-set data structures are due at least in partto R. E. Tarjan. Using aggregate analysis, Tarjan [328, 330] gave the ﬁrst tightupper bound in terms of the very slowly growing inverse y˛.m;n/ of Ackermann’s
function. (The function A
k.j /g i v e ni nS e c t i o n2 1 . 4i ss i m i l a rt oA c k e r m a n n ’ s
function, and the function ˛.n/ is similar to the inverse. Both ˛.n/ andy˛.m;n/
are at most 4for all conceivable values of mandn.) An O.m lg/ETXn/upper bound
was proven earlier by Hopcroft and Ullman [5, 179]. The treatment in Section 21.4is adapted from a later analysis by Tarjan [332], which is in turn based on an anal-
ysis by Kozen [220]. Harfst and Reingold [161] give a potential-based version of
Tarjan’s earlier bound.
Tarjan and van Leeuwen [333] discuss variants on the path-compression heuris-
tic, including “one-pass methods,” which sometimes offer better constant factorsin their performance than do two-pass methods. As with Tarjan’s earlier analysesof the basic path-compression heuristic, the analyses by Tarjan and van Leeuwenare aggregate. Harfst and Reingold [161] later showed how to make a small changeto the potential function to adapt their path-compression analysis to these one-passvariants. Gabow and Tarjan [121] show that in certain applications, the disjoint-setoperations can be made to run in O.m/ time.
Tarjan [329] showed that a lower bound of /DEL.my˛.m; n// time is required for
operations on any disjoint-set data structure satisfying certain technical conditions.This lower bound was later generalized by Fredman and Saks [113], who showedthat in the worst case, /DEL.my˛.m; n// . lgn/-bit words of memory must be accessed.VI Graph AlgorithmsIntroduction
Graph problems pervade computer science, and algorithms for working with them
are fundamental to the ﬁeld. Hundreds of interesting computational problems are
couched in terms of graphs. In this part, we touch on a few of the more signiﬁcantones.
Chapter 22 shows how we can represent a graph in a computer and then discusses
algorithms based on searching a graph using either breadth-ﬁrst search or depth-ﬁrst search. The chapter gives two applications of depth-ﬁrst search: topologicallysorting a directed acyclic graph and decomposing a directed graph into its stronglyconnected components.
Chapter 23 describes how to compute a minimum-weight spanning tree of a
graph: the least-weight way of connecting all of the vertices together when each
edge has an associated weight. The algorithms for computing minimum spanningtrees serve as good examples of greedy algorithms (see Chapter 16).
Chapters 24 and 25 consider how to compute shortest paths between vertices
when each edge has an associated length or “weight.” Chapter 24 shows how to
ﬁnd shortest paths from a given source vertex to all other vertices, and Chapter 25
examines methods to compute shortest paths between every pair of vertices.
Finally, Chapter 26 shows how to compute a maximum ﬂow of material in a ﬂow
network, which is a directed graph having a speciﬁed source vertex of material, aspeciﬁed sink vertex, and speciﬁed capacities for the amount of material that cantraverse each directed edge. This general problem arises in many forms, and agood algorithm for computing maximum ﬂows can help solve a variety of relatedproblems efﬁciently.588 Part VI Graph Algorithms
When we characterize the running time of a graph algorithm on a given graph
GD.V; E/ , we usually measure the size of the input in terms of the number of
verticesjVjand the number of edges jEjof the graph. That is, we describe the
size of the input with two parameters, not just one. We adopt a common notationalconvention for these parameters. Inside asymptotic notation (such as O-notation
or‚-notation), and only inside such notation, the symbol VdenotesjVjand
the symbol EdenotesjEj. For example, we might say, “the algorithm runs in
timeO.VE/ ,” meaning that the algorithm runs in time O.jVjjEj/. This conven-
tion makes the running-time formulas easier to read, without risk of ambiguity.
Another convention we adopt appears in pseudocode. We denote the vertex set
of a graph GbyG:Vand its edge set by G:E. That is, the pseudocode views vertex
and edge sets as attributes of a graph.22 Elementary Graph Algorithms
This chapter presents methods for representing a graph and for searching a graph.
Searching a graph means systematically following the edges of the graph so as tovisit the vertices of the graph. A graph-searching algorithm can discover muchabout the structure of a graph. Many algorithms begin by searching their inputgraph to obtain this structural information. Several other graph algorithms elabo-rate on basic graph searching. Techniques for searching a graph lie at the heart ofthe ﬁeld of graph algorithms.
Section 22.1 discusses the two most common computational representations of
graphs: as adjacency lists and as adjacency matrices. Section 22.2 presents a sim-
ple graph-searching algorithm called breadth-ﬁrst search and shows how to cre-
ate a breadth-ﬁrst tree. Section 22.3 presents depth-ﬁrst search and proves some
standard results about the order in which depth-ﬁrst search visits vertices. Sec-tion 22.4 provides our ﬁrst real application of depth-ﬁrst search: topologically sort-ing a directed acyclic graph. A second application of depth-ﬁrst search, ﬁnding the
strongly connected components of a directed graph, is the topic of Section 22.5.
22.1 Representations of graphs
We can choose between two standard ways to represent a graph GD.V; E/ :
as a collection of adjacency lists or as an adjacency matrix. Either way appliesto both directed and undirected graphs. Because the adjacency-list representationprovides a compact way to represent sparse graphs—those for which jEjis much
less thanjVj
2—it is usually the method of choice. Most of the graph algorithms
presented in this book assume that an input graph is represented in adjacency-
list form. We may prefer an adjacency-matrix representation, however, when the
graph is dense —jEjis close tojVj2—or when we need to be able to tell quickly
if there is an edge connecting two given vertices. For example, two of the all-pairs590 Chapter 22 Elementary Graph Algorithms
12
3
4 51
234525
1
2
2
4 1 25 344 5 31001
01111010110110100100112345
12345
(a) (b) (c)
Figure 22.1 Two representations of an undirected graph. (a)An undirected graph Gwith 5 vertices
a n d7e d g e s . (b)An adjacency-list representation of G.(c)The adjacency-matrix representation
ofG.
12
5 41
234524
5
6
2
4
65101000010001100000100000012345
12345
(a) (b) (c)
3
6 66
600000100100
Figure 22.2 Two representations of a directed graph. (a)A directed graph Gwith 6 vertices and 8
edges. (b)An adjacency-list representation of G.(c)The adjacency-matrix representation of G.
shortest-paths algorithms presented in Chapter 25 assume that their input graphs
are represented by adjacency matrices.
Theadjacency-list representation of a graph GD.V; E/ consists of an ar-
rayAdjofjVjlists, one for each vertex in V. For each u2V, the adjacency list
AdjŒu/c141contains all the vertices /ETBsuch that there is an edge .u; /ETB/2E.T h a t i s ,
AdjŒu/c141consists of all the vertices adjacent to uinG. (Alternatively, it may contain
pointers to these vertices.) Since the adjacency lists represent the edges of a graph,in pseudocode we treat the array Adjas an attribute of the graph, just as we treat
the edge set E. In pseudocode, therefore, we will see notation such as G:AdjŒu/c141.
Figure 22.1(b) is an adjacency-list representation of the undirected graph in Fig-ure 22.1(a). Similarly, Figure 22.2(b) is an adjacency-list representation of thedirected graph in Figure 22.2(a).
IfGis a directed graph, the sum of the lengths of all the adjacency lists is jEj,
since an edge of the form .u; /ETB/ is represented by having /ETBappear in AdjŒu/c141.I fGis22.1 Representations of graphs 591
an undirected graph, the sum of the lengths of all the adjacency lists is 2jEj,s i n c e
if.u; /ETB/ is an undirected edge, then uappears in /ETB’s adjacency list and vice versa.
For both directed and undirected graphs, the adjacency-list representation has thedesirable property that the amount of memory it requires is ‚.VCE/.
We can readily adapt adjacency lists to represent weighted graphs , that is, graphs
for which each edge has an associated weight , typically given by a weight function
wWE! R. For example, let GD.V; E/ be a weighted graph with weight
function w. We simply store the weight w.u;/ETB/ of the edge .u; /ETB/2Ewith
vertex /ETBinu’s adjacency list. The adjacency-list representation is quite robust in
that we can modify it to support many other graph variants.
A potential disadvantage of the adjacency-list representation is that it provides
no quicker way to determine whether a given edge .u; /ETB/ is present in the graph
than to search for /ETBin the adjacency list AdjŒu/c141. An adjacency-matrix representa-
tion of the graph remedies this disadvantage, but at the cost of using asymptoticallymore memory. (See Exercise 22.1-8 for suggestions of variations on adjacency liststhat permit faster edge lookup.)
For the adjacency-matrix representation of a graph GD.V; E/ , we assume
that the vertices are numbered 1 ;2;:::;jVjin some arbitrary manner. Then the
adjacency-matrix representation of a graph Gconsists of ajVj/STXjVjmatrix
AD.a
ij/such that
aijD(
1if.i; j /2E;
0otherwise :
Figures 22.1(c) and 22.2(c) are the adjacency matrices of the undirected and di-
rected graphs in Figures 22.1(a) and 22.2(a), respectively. The adjacency matrix ofa graph requires ‚.V
2/memory, independent of the number of edges in the graph.
Observe the symmetry along the main diagonal of the adjacency matrix in Fig-
ure 22.1(c). Since in an undirected graph, .u; /ETB/ and./ETB; u/ represent the same
edge, the adjacency matrix Aof an undirected graph is its own transpose: ADAT.
In some applications, it pays to store only the entries on and above the diagonal ofthe adjacency matrix, thereby cutting the memory needed to store the graph almostin half.
Like the adjacency-list representation of a graph, an adjacency matrix can repre-
sent a weighted graph. For example, if GD.V; E/ is a weighted graph with edge-
weight function w, we can simply store the weight w.u;/ETB/ of the edge .u; /ETB/2E
as the entry in row uand column /ETBof the adjacency matrix. If an edge does not
exist, we can store a
NILvalue as its corresponding matrix entry, though for many
problems it is convenient to use a value such as 0or1.
Although the adjacency-list representation is asymptotically at least as space-
efﬁcient as the adjacency-matrix representation, adjacency matrices are simpler,and so we may prefer them when graphs are reasonably small. Moreover, adja-592 Chapter 22 Elementary Graph Algorithms
cency matrices carry a further advantage for unweighted graphs: they require only
one bit per entry.
Representing attributes
Most algorithms that operate on graphs need to maintain attributes for vertices
and/or edges. We indicate these attributes using our usual notation, such as /ETB:d
for an attribute dof a vertex /ETB. When we indicate edges as pairs of vertices, we
use the same style of notation. For example, if edges have an attribute f,t h e nw e
denote this attribute for edge .u; /ETB/ by.u; /ETB/: f. For the purpose of presenting and
understanding algorithms, our attribute notation sufﬁces.
Implementing vertex and edge attributes in real programs can be another story
entirely. There is no one best way to store and access vertex and edge attributes.
For a given situation, your decision will likely depend on the programming lan-
guage you are using, the algorithm you are implementing, and how the rest of yourprogram uses the graph. If you represent a graph using adjacency lists, one designrepresents vertex attributes in additional arrays, such as an array dŒ1::jVj/c141that
parallels the Adjarray. If the vertices adjacent to uare in AdjŒu/c141, then what we call
the attribute u:dwould actually be stored in the array entry dŒu/c141. Many other ways
of implementing attributes are possible. For example, in an object-oriented pro-
gramming language, vertex attributes might be represented as instance variables
within a subclass of a Vertex class.
Exercises
22.1-1
Given an adjacency-list representation of a directed graph, how long does it taketo compute the out-degree of every vertex? How long does it take to compute thein-degrees?
22.1-2
Give an adjacency-list representation for a complete binary tree on 7vertices. Give
an equivalent adjacency-matrix representation. Assume that vertices are numberedfrom 1to7as in a binary heap.
22.1-3
Thetranspose of a directed graph GD.V; E/ is the graph G
TD.V; ET/,w h e r e
ETDf./ETB; u/2V/STXVW.u; /ETB/2Eg. Thus, GTisGwith all its edges reversed.
Describe efﬁcient algorithms for computing GTfrom G, for both the adjacency-
list and adjacency-matrix representations of G. Analyze the running times of your
algorithms.22.1 Representations of graphs 593
22.1-4
Given an adjacency-list representation of a multigraph GD.V; E/ , describe an
O.VCE/-time algorithm to compute the adjacency-list representation of the
“equivalent” undirected graph G0D.V; E0/,w h e r e E0consists of the edges in E
with all multiple edges between two vertices replaced by a single edge and with allself-loops removed.
22.1-5
Thesquare of a directed graph GD.V; E/ is the graph G
2D.V; E2/such that
.u; /ETB/2E2if and only Gcontains a path with at most two edges between uand/ETB.
Describe efﬁcient algorithms for computing G2from Gfor both the adjacency-
list and adjacency-matrix representations of G. Analyze the running times of your
algorithms.
22.1-6
Most graph algorithms that take an adjacency-matrix representation as input re-quire time /DEL.V
2/, but there are some exceptions. Show how to determine whether
a directed graph Gcontains a universal sink —a vertex with in-degree jVj/NUL1and
out-degree 0—in time O.V / , given an adjacency matrix for G.
22.1-7
Theincidence matrix of a directed graph GD.V; E/ with no self-loops is a
jVj/STXjEjmatrix BD.bij/such that
bijD/c128
/NUL1if edge jleaves vertex i;
1 if edge jenters vertex i;
0 otherwise :
Describe what the entries of the matrix product BBTrepresent, where BTis the
transpose of B.
22.1-8
Suppose that instead of a linked list, each array entry AdjŒu/c141is a hash table contain-
ing the vertices /ETBfor which .u; /ETB/2E. If all edge lookups are equally likely, what
is the expected time to determine whether an edge is in the graph? What disadvan-tages does this scheme have? Suggest an alternate data structure for each edge listthat solves these problems. Does your alternative have disadvantages compared tothe hash table?594 Chapter 22 Elementary Graph Algorithms
22.2 Breadth-ﬁrst search
Breadth-ﬁrst search is one of the simplest algorithms for searching a graph and
the archetype for many important graph algorithms. Prim’s minimum-spanning-tree algorithm (Section 23.2) and Dijkstra’s single-source shortest-paths algorithm(Section 24.3) use ideas similar to those in breadth-ﬁrst search.
Given a graph GD.V; E/ and a distinguished source vertex s, breadth-ﬁrst
search systematically explores the edges of Gto “discover” every vertex that is
reachable from s. It computes the distance (smallest number of edges) from s
to each reachable vertex. It also produces a “breadth-ﬁrst tree” with root sthat
contains all reachable vertices. For any vertex /ETBreachable from s, the simple path
in the breadth-ﬁrst tree from sto/ETBcorresponds to a “shortest path” from sto/ETB
inG, that is, a path containing the smallest number of edges. The algorithm works
on both directed and undirected graphs.
Breadth-ﬁrst search is so named because it expands the frontier between discov-
ered and undiscovered vertices uniformly across the breadth of the frontier. Thatis, the algorithm discovers all vertices at distance kfrom sbefore discovering any
vertices at distance kC1.
To keep track of progress, breadth-ﬁrst search colors each vertex white, gray, or
black. All vertices start out white and may later become gray and then black. A
vertex is discovered the ﬁrst time it is encountered during the search, at which time
it becomes nonwhite. Gray and black vertices, therefore, have been discovered, but
breadth-ﬁrst search distinguishes between them to ensure that the search proceedsin a breadth-ﬁrst manner.
1If.u; /ETB/2Eand vertex uis black, then vertex /ETB
is either gray or black; that is, all vertices adjacent to black vertices have beendiscovered. Gray vertices may have some adjacent white vertices; they representthe frontier between discovered and undiscovered vertices.
Breadth-ﬁrst search constructs a breadth-ﬁrst tree, initially containing only its
root, which is the source vertex s. Whenever the search discovers a white vertex /ETB
in the course of scanning the adjacency list of an already discovered vertex u,t h e
vertex /ETBand the edge .u; /ETB/ are added to the tree. We say that uis thepredecessor
orparent of/ETBin the breadth-ﬁrst tree. Since a vertex is discovered at most once, it
has at most one parent. Ancestor and descendant relationships in the breadth-ﬁrsttree are deﬁned relative to the root sas usual: if uis on the simple path in the tree
from the root sto vertex /ETB,t h e n uis an ancestor of /ETBand/ETBis a descendant of u.
1We distinguish between gray and black vertices to help us understand how breadth-ﬁrst search op-
erates. In fact, as Exercise 22.2-3 shows, we would get the same result even if we did not distinguish
between gray and black vertices.22.2 Breadth-ﬁrst search 595
The breadth-ﬁrst-search procedure BFS below assumes that the input graph
GD.V; E/ is represented using adjacency lists. It attaches several additional
attributes to each vertex in the graph. We store the color of each vertex u2V
in the attribute u:color and the predecessor of uin the attribute u:/EM.I fuhas no
predecessor (for example, if uDsoruhas not been discovered), then u:/EMDNIL.
The attribute u:dholds the distance from the source sto vertex ucomputed by the
algorithm. The algorithm also uses a ﬁrst-in, ﬁrst-out queue Q(see Section 10.1)
to manage the set of gray vertices.
BFS.G; s/
1foreach vertex u2G:V/NULfsg
2 u:colorDWHITE
3 u:dD1
4 u:/EMDNIL
5s:colorDGRAY
6s:dD0
7s:/EMDNIL
8QD;
9E NQUEUE .Q; s/
10while Q¤;
11 uDDEQUEUE .Q/
12 foreach/ETB2G:AdjŒu/c141
13 if/ETB:color ==WHITE
14 /ETB:colorDGRAY
15 /ETB:dDu:dC1
16 /ETB:/EMDu
17 E NQUEUE .Q; /ETB/
18 u:colorDBLACK
Figure 22.3 illustrates the progress of BFS on a sample graph.
The procedure BFS works as follows. With the exception of the source vertex s,
lines 1–4 paint every vertex white, set u:dto be inﬁnity for each vertex u, and set
the parent of every vertex to be NIL. Line 5 paints sgray, since we consider it to be
discovered as the procedure begins. Line 6 initializes s:dto0,a n dl i n e7s e t st h e
predecessor of the source to be NIL. Lines 8–9 initialize Qto the queue containing
just the vertex s.
Thewhile loop of lines 10–18 iterates as long as there remain gray vertices,
which are discovered vertices that have not yet had their adjacency lists fully ex-
amined. This while loop maintains the following invariant:
At the test in line 10, the queue Qconsists of the set of gray vertices.596 Chapter 22 Elementary Graph Algorithms
rstu
vwxy0 ∞∞ ∞
∞ ∞ ∞ ∞s
0Q (a)tu
vwxy0 1 ∞∞
∞ ∞ ∞ 1w
1Q (b) r
1
tu
vwxy0 12 ∞
∞ 2 ∞ 1Q (c) r
1tu
vwxy0 1 ∞
∞Q (d)
(e) (f)
(g) (h)
Q (i)rs
rs rs
t
2x
22
2 1 2t
2x
2v
2
tu
vwxy0 1
∞Qrs
2
2 1 2x
2v
2u
33tu
vwxy0 1
3Qrs
2
2 1 2v
2u
33
y
3
tu
vwxy0 1
3Qrs
2
2 1u
33
y
3 2tu
vwxy0 1
3Qrs
2
2 13
y
3 2
tu
vwxy0 1rs
2
2 13
2 3;
Figure 22.3 The operation of BFS on an undirected graph. Tree edges are shown shaded as they
are produced by BFS. The value of u:dappears within each vertex u. The queue Qis shown at the
beginning of each iteration of the while loop of lines 10–18. Vertex distances appear below vertices
in the queue.
Although we won’t use this loop invariant to prove correctness, it is easy to see
that it holds prior to the ﬁrst iteration and that each iteration of the loop maintainsthe invariant. Prior to the ﬁrst iteration, the only gray vertex, and the only vertexinQ, is the source vertex s. Line 11 determines the gray vertex uat the head of
the queue Qand removes it from Q.T h e forloop of lines 12–17 considers each
vertex /ETBin the adjacency list of u.I f/ETBis white, then it has not yet been discovered,
and the procedure discovers it by executing lines 14–17. The procedure paintsvertex /ETBgray, sets its distance /ETB:dtou:dC1, records uas its parent /ETB:/EM, and places
it at the tail of the queue Q. Once the procedure has examined all the vertices on u’s22.2 Breadth-ﬁrst search 597
adjacency list, it blackens uin line 18. The loop invariant is maintained because
whenever a vertex is painted gray (in line 14) it is also enqueued (in line 17), andwhenever a vertex is dequeued (in line 11) it is also painted black (in line 18).
The results of breadth-ﬁrst search may depend upon the order in which the neigh-
bors of a given vertex are visited in line 12: the breadth-ﬁrst tree may vary, but thedistances dcomputed by the algorithm will not. (See Exercise 22.2-5.)
Analysis
Before proving the various properties of breadth-ﬁrst search, we take on the some-
what easier job of analyzing its running time on an input graph GD.V; E/ .W e
use aggregate analysis, as we saw in Section 17.1. After initialization, breadth-ﬁrstsearch never whitens a vertex, and thus the test in line 13 ensures that each vertex
is enqueued at most once, and hence dequeued at most once. The operations of
enqueuing and dequeuing take O.1/ time, and so the total time devoted to queue
operations is O.V / . Because the procedure scans the adjacency list of each vertex
only when the vertex is dequeued, it scans each adjacency list at most once. Sincethe sum of the lengths of all the adjacency lists is ‚.E/ , the total time spent in
scanning adjacency lists is O.E/ . The overhead for initialization is O.V / ,a n d
thus the total running time of the BFS procedure is O.VCE/. Thus, breadth-ﬁrst
search runs in time linear in the size of the adjacency-list representation of G.
Shortest paths
At the beginning of this section, we claimed that breadth-ﬁrst search ﬁnds the dis-
tance to each reachable vertex in a graph GD.V; E/ from a given source vertex
s2V.D e ﬁ n et h e shortest-path distance ı.s; /ETB/ from sto/ETBas the minimum num-
ber of edges in any path from vertex sto vertex /ETB; if there is no path from sto/ETB,
thenı.s; /ETB/D1 . We call a path of length ı.s; /ETB/ from sto/ETBashortest path
2
from sto/ETB. Before showing that breadth-ﬁrst search correctly computes shortest-
path distances, we investigate an important property of shortest-path distances.
2In Chapters 24 and 25, we shall generalize our study of shortest paths to weighted graphs, in which
every edge has a real-valued weight and the weight of a path is the sum of the weights of its con-
stituent edges. The graphs considered in the present chapter are unweighted or, equivalently, all
edges have unit weight.598 Chapter 22 Elementary Graph Algorithms
Lemma 22.1
LetGD.V; E/ be a directed or undirected graph, and let s2Vbe an arbitrary
vertex. Then, for any edge .u; /ETB/2E,
ı.s; /ETB//DC4ı.s; u/C1:
Proof Ifuis reachable from s,t h e ns oi s /ETB. In this case, the shortest path from s
to/ETBcannot be longer than the shortest path from stoufollowed by the edge .u; /ETB/ ,
and thus the inequality holds. If uis not reachable from s,t h e n ı.s; u/D1 ,a n d
the inequality holds.
We want to show that BFS properly computes /ETB:dDı.s; /ETB/ for each ver-
tex/ETB2V. We ﬁrst show that /ETB:dbounds ı.s; /ETB/ from above.
Lemma 22.2
LetGD.V; E/ be a directed or undirected graph, and suppose that BFS is run
onGfrom a given source vertex s2V. Then upon termination, for each ver-
tex/ETB2V,t h ev a l u e /ETB:dcomputed by BFS satisﬁes /ETB:d/NAKı.s; /ETB/ .
Proof We use induction on the number of E NQUEUE operations. Our inductive
hypothesis is that /ETB:d/NAKı.s; /ETB/ for all /ETB2V.
The basis of the induction is the situation immediately after enqueuing sin line 9
of BFS. The inductive hypothesis holds here, because s:dD0Dı.s; s/ and
/ETB:dD1/NAK ı.s; /ETB/ for all /ETB2V/NULfsg.
For the inductive step, consider a white vertex /ETBthat is discovered during the
search from a vertex u. The inductive hypothesis implies that u:d/NAKı.s; u/ .F r o m
the assignment performed by line 15 and from Lemma 22.1, we obtain
/ETB:dDu:dC1
/NAKı.s; u/C1
/NAKı.s; /ETB/ :
Vertex /ETBis then enqueued, and it is never enqueued again because it is also grayed
and the then clause of lines 14–17 is executed only for white vertices. Thus, the
value of /ETB:dnever changes again, and the inductive hypothesis is maintained.
To prove that /ETB:dDı.s; /ETB/ , we must ﬁrst show more precisely how the queue Q
operates during the course of BFS. The next lemma shows that at all times, thequeue holds at most two distinct dvalues.22.2 Breadth-ﬁrst search 599
Lemma 22.3
Suppose that during the execution of BFS on a graph GD.V; E/ , the queue Q
contains the vertices h/ETB1;/ETB2;:::;/ETB ri,w h e r e /ETB1is the head of Qand/ETBris the tail.
Then, /ETBr:d/DC4/ETB1:dC1and/ETBi:d/DC4/ETBiC1:dforiD1 ;2;:::;r/NUL1.
Proof The proof is by induction on the number of queue operations. Initially,
when the queue contains only s, the lemma certainly holds.
For the inductive step, we must prove that the lemma holds after both dequeuing
and enqueuing a vertex. If the head /ETB1of the queue is dequeued, /ETB2becomes the
new head. (If the queue becomes empty, then the lemma holds vacuously.) By theinductive hypothesis, /ETB
1:d/DC4/ETB2:d. But then we have /ETBr:d/DC4/ETB1:dC1/DC4/ETB2:dC1,
and the remaining inequalities are unaffected. Thus, the lemma follows with /ETB2as
the head.
In order to understand what happens upon enqueuing a vertex, we need to ex-
amine the code more closely. When we enqueue a vertex /ETBin line 17 of BFS, it
becomes /ETBrC1. At that time, we have already removed vertex u, whose adjacency
list is currently being scanned, from the queue Q, and by the inductive hypothesis,
the new head /ETB1has/ETB1:d/NAKu:d. Thus, /ETBrC1:dD/ETB:dDu:dC1/DC4/ETB1:dC1.F r o m
the inductive hypothesis, we also have /ETBr:d/DC4u:dC1,a n ds o /ETBr:d/DC4u:dC1D
/ETB:dD/ETBrC1:d, and the remaining inequalities are unaffected. Thus, the lemma
follows when /ETBis enqueued.
The following corollary shows that the dvalues at the time that vertices are
enqueued are monotonically increasing over time.
Corollary 22.4
Suppose that vertices /ETBiand/ETBjare enqueued during the execution of BFS, and
that/ETBiis enqueued before /ETBj.T h e n /ETBi:d/DC4/ETBj:dat the time that /ETBjis enqueued.
Proof Immediate from Lemma 22.3 and the property that each vertex receives a
ﬁnite dvalue at most once during the course of BFS.
We can now prove that breadth-ﬁrst search correctly ﬁnds shortest-path dis-
tances.
Theorem 22.5 (Correctness of breadth-ﬁrst search)
LetGD.V; E/ be a directed or undirected graph, and suppose that BFS is run
onGfrom a given source vertex s2V. Then, during its execution, BFS discovers
every vertex /ETB2Vthat is reachable from the source s, and upon termination,
/ETB:dDı.s; /ETB/ for all /ETB2V. Moreover, for any vertex /ETB¤sthat is reachable600 Chapter 22 Elementary Graph Algorithms
from s, one of the shortest paths from sto/ETBis a shortest path from sto/ETB:/EM
followed by the edge ./ETB:/EM; /ETB/ .
Proof Assume, for the purpose of contradiction, that some vertex receives a d
value not equal to its shortest-path distance. Let /ETBbe the vertex with min-
imum ı.s; /ETB/ that receives such an incorrect dvalue; clearly /ETB¤s.B y
Lemma 22.2, /ETB:d/NAKı.s; /ETB/ , and thus we have that /ETB:d>ı . s ;/ETB / . Vertex /ETBmust be
reachable from s, for if it is not, then ı.s; /ETB/D1/NAK /ETB:d.L e t ube the vertex im-
mediately preceding /ETBon a shortest path from sto/ETB,s ot h a t ı.s; /ETB/Dı.s; u/C1.
Because ı.s; u/ < ı.s; /ETB/ , and because of how we chose /ETB,w eh a v e u:dDı.s; u/ .
Putting these properties together, we have
/ETB:d>ı . s ;/ETB /Dı.s; u/C1Du:dC1: (22.1)
Now consider the time when BFS chooses to dequeue vertex ufrom Qin
line 11. At this time, vertex /ETBis either white, gray, or black. We shall show
that in each of these cases, we derive a contradiction to inequality (22.1). If /ETBis
white, then line 15 sets /ETB:dDu:dC1, contradicting inequality (22.1). If /ETBis
black, then it was already removed from the queue and, by Corollary 22.4, we have/ETB:d/DC4u:d, again contradicting inequality (22.1). If /ETBis gray, then it was painted
gray upon dequeuing some vertex w, which was removed from Qearlier than u
and for which /ETB:dDw:dC1. By Corollary 22.4, however, w:d/DC4u:d,a n ds ow e
have /ETB:dDw:dC1/DC4u:dC1, once again contradicting inequality (22.1).
Thus we conclude that /ETB:dDı.s; /ETB/ for all /ETB2V. All vertices /ETBreachable
from smust be discovered, for otherwise they would have 1D /ETB:d>ı . s ;/ETB / .T o
conclude the proof of the theorem, observe that if /ETB:/EMDu,t h e n /ETB:dDu:dC1.
Thus, we can obtain a shortest path from sto/ETBby taking a shortest path from s
to/ETB:/EMand then traversing the edge ./ETB:/EM; /ETB/ .
Breadth-ﬁrst trees
The procedure BFS builds a breadth-ﬁrst tree as it searches the graph, as Fig-
ure 22.3 illustrates. The tree corresponds to the /EMattributes. More formally, for
ag r a p h GD.V; E/ with source s,w ed e ﬁ n et h e predecessor subgraph ofGas
G/EMD.V/EM;E/EM/,w h e r e
V/EMDf/ETB2VW/ETB:/EM¤NILg[fsg
and
E/EMDf./ETB:/EM; /ETB/W/ETB2V/EM/NULfsgg:
The predecessor subgraph G/EMis abreadth-ﬁrst tree ifV/EMconsists of the vertices
reachable from sand, for all /ETB2V/EM, the subgraph G/EMcontains a unique simple22.2 Breadth-ﬁrst search 601
path from sto/ETBthat is also a shortest path from sto/ETBinG. A breadth-ﬁrst tree
is in fact a tree, since it is connected and jE/EMjDjV/EMj/NUL1(see Theorem B.2). We
call the edges in E/EMtree edges .
The following lemma shows that the predecessor subgraph produced by the BFS
procedure is a breadth-ﬁrst tree.
Lemma 22.6
When applied to a directed or undirected graph GD.V; E/ , procedure BFS con-
structs /EMso that the predecessor subgraph G/EMD.V/EM;E/EM/is a breadth-ﬁrst tree.
Proof Line 16 of BFS sets /ETB:/EMDuif and only if .u; /ETB/2Eandı.s; /ETB/ <1—
that is, if /ETBis reachable from s—and thus V/EMconsists of the vertices in Vreachable
from s.S i n c e G/EMforms a tree, by Theorem B.2, it contains a unique simple path
from sto each vertex in V/EM. By applying Theorem 22.5 inductively, we conclude
that every such path is a shortest path in G.
The following procedure prints out the vertices on a shortest path from sto/ETB,
assuming that BFS has already computed a breadth-ﬁrst tree:
PRINT -PATH. G ;s ;/ETB/
1if/ETB==s
2 print s
3elseif /ETB:/EM ==NIL
4 print “no path from” s“to”/ETB“exists”
5elsePRINT -PATH. G ;s ;/ETB:/EM/
6 print /ETB
This procedure runs in time linear in the number of vertices in the path printed,
since each recursive call is for a path one vertex shorter.
Exercises
22.2-1
Show the dand/EMvalues that result from running breadth-ﬁrst search on the di-
rected graph of Figure 22.2(a), using vertex 3as the source.
22.2-2
Show the dand/EMvalues that result from running breadth-ﬁrst search on the undi-
rected graph of Figure 22.3, using vertex uas the source.602 Chapter 22 Elementary Graph Algorithms
22.2-3
Show that using a single bit to store each vertex color sufﬁces by arguing that the
BFS procedure would produce the same result if lines 5 and 14 were removed.
22.2-4
What is the running time of BFS if we represent its input graph by an adjacencymatrix and modify the algorithm to handle this form of input?
22.2-5
Argue that in a breadth-ﬁrst search, the value u:dassigned to a vertex uis inde-
pendent of the order in which the vertices appear in each adjacency list. Using
Figure 22.3 as an example, show that the breadth-ﬁrst tree computed by BFS candepend on the ordering within adjacency lists.
22.2-6
Give an example of a directed graph GD.V; E/ , a source vertex s2V,a n da
set of tree edges E
/EM/DC2Esuch that for each vertex /ETB2V, the unique simple path
in the graph .V; E /EM/from sto/ETBis a shortest path in G, yet the set of edges E/EM
cannot be produced by running BFS on G, no matter how the vertices are ordered
in each adjacency list.
22.2-7
There are two types of professional wrestlers: “babyfaces” (“good guys”) and“heels” (“bad guys”). Between any pair of professional wrestlers, there may ormay not be a rivalry. Suppose we have nprofessional wrestlers and we have a list
ofrpairs of wrestlers for which there are rivalries. Give an O.nCr/-time algo-
rithm that determines whether it is possible to designate some of the wrestlers as
babyfaces and the remainder as heels such that each rivalry is between a babyface
and a heel. If it is possible to perform such a designation, your algorithm shouldproduce it.
22.2-8 ?
Thediameter of a tree TD.V; E/ is deﬁned as max
u;/ETB2Vı.u;/ETB/ ,t h a ti s ,t h e
largest of all shortest-path distances in the tree. Give an efﬁcient algorithm tocompute the diameter of a tree, and analyze the running time of your algorithm.
22.2-9
LetGD.V; E/ be a connected, undirected graph. Give an O.VCE/-time algo-
rithm to compute a path in Gthat traverses each edge in Eexactly once in each
direction. Describe how you can ﬁnd your way out of a maze if you are given alarge supply of pennies.22.3 Depth-ﬁrst search 603
22.3 Depth-ﬁrst search
The strategy followed by depth-ﬁrst search is, as its name implies, to search
“deeper” in the graph whenever possible. Depth-ﬁrst search explores edges outof the most recently discovered vertex /ETBthat still has unexplored edges leaving it.
Once all of /ETB’s edges have been explored, the search “backtracks” to explore edges
leaving the vertex from which /ETBwas discovered. This process continues until we
have discovered all the vertices that are reachable from the original source vertex.If any undiscovered vertices remain, then depth-ﬁrst search selects one of them asa new source, and it repeats the search from that source. The algorithm repeats thisentire process until it has discovered every vertex.
3
As in breadth-ﬁrst search, whenever depth-ﬁrst search discovers a vertex /ETBdur-
ing a scan of the adjacency list of an already discovered vertex u, it records this
event by setting /ETB’s predecessor attribute /ETB:/EM tou. Unlike breadth-ﬁrst search,
whose predecessor subgraph forms a tree, the predecessor subgraph produced bya depth-ﬁrst search may be composed of several trees, because the search mayrepeat from multiple sources. Therefore, we deﬁne the predecessor subgraph of
a depth-ﬁrst search slightly differently from that of a breadth-ﬁrst search: we letG
/EMD.V; E /EM/,w h e r e
E/EMDf./ETB:/EM; /ETB/W/ETB2Vand/ETB:/EM¤NILg:
The predecessor subgraph of a depth-ﬁrst search forms a depth-ﬁrst forest com-
prising several depth-ﬁrst trees . The edges in E/EMaretree edges .
As in breadth-ﬁrst search, depth-ﬁrst search colors vertices during the search to
indicate their state. Each vertex is initially white, is grayed when it is discovered
in the search, and is blackened when it is ﬁnished , that is, when its adjacency list
has been examined completely. This technique guarantees that each vertex ends upin exactly one depth-ﬁrst tree, so that these trees are disjoint.
Besides creating a depth-ﬁrst forest, depth-ﬁrst search also timestamps each ver-
tex. Each vertex /ETBhas two timestamps: the ﬁrst timestamp /ETB:drecords when /ETB
is ﬁrst discovered (and grayed), and the second timestamp /ETB:frecords when the
search ﬁnishes examining /ETB’s adjacency list (and blackens /ETB). These timestamps
3It may seem arbitrary that breadth-ﬁrst search is limited to only one source whereas depth-ﬁrst
search may search from multiple sources. Although conceptually, breadth-ﬁrst search could proceed
from multiple sources and depth-ﬁrst search could be limited to one source, our approach reﬂects how
the results of these searches are typically used. Breadth-ﬁrst search usually serves to ﬁnd shortest-
path distances (and the associated predecessor subgr aph) from a given source. Depth-ﬁrst search is
often a subroutine in another algorithm, as we shall see later in this chapter.604 Chapter 22 Elementary Graph Algorithms
provide important information about the structure of the graph and are generally
helpful in reasoning about the behavior of depth-ﬁrst search.
The procedure DFS below records when it discovers vertex uin the attribute u:d
and when it ﬁnishes vertex uin the attribute u:f. These timestamps are integers
between 1and2jVj, since there is one discovery event and one ﬁnishing event for
each of thejVjvertices. For every vertex u,
u:d<u : f: (22.2)
Vertex uisWHITE before time u:d,GRAY between time u:dand time u:f,a n d
BLACK thereafter.
The following pseudocode is the basic depth-ﬁrst-search algorithm. The input
graph Gmay be undirected or directed. The variable time is a global variable that
we use for timestamping.
DFS.G/
1foreach vertex u2G:V
2 u:colorDWHITE
3 u:/EMDNIL
4timeD0
5foreach vertex u2G:V
6 ifu:color ==WHITE
7 DFS-V ISIT.G; u/
DFS-V ISIT.G; u/
1timeDtimeC1 //white vertex uhas just been discovered
2u:dDtime
3u:colorDGRAY
4foreach/ETB2G:AdjŒu/c141 //explore edge .u; /ETB/
5 if/ETB:color ==WHITE
6 /ETB:/EMDu
7 DFS-V ISIT.G; /ETB/
8u:colorDBLACK //blacken u;i ti sﬁ n i s h e d
9timeDtimeC1
10u:fDtime
Figure 22.4 illustrates the progress of DFS on the graph shown in Figure 22.2.
Procedure DFS works as follows. Lines 1–3 paint all vertices white and ini-
tialize their /EMattributes to NIL. Line 4 resets the global time counter. Lines 5–7
check each vertex in Vin turn and, when a white vertex is found, visit it using
DFS-V ISIT. Every time DFS-V ISIT.G; u/ is called in line 7, vertex ubecomes22.3 Depth-ﬁrst search 605
uvw
xyz1/ 1/ 2/ 1/ 2/
3/1/ 2/
3/ 4/
1/ 2/
3/ 4/B1/ 2/
3/B
4/51/ 2/
B
4/5 3/61/
B
4/5 3/62/7
1/
B
4/5 3/62/7
F B
4/5 3/62/7
F1/8
B
4/5 3/62/7
F1/8 9/
B
4/5 3/62/7
F1/8 9/
C
B
4/5 3/62/7
F1/8 9/
C B
4/5 3/62/7
F1/8 9/
C
BB
4/5 3/62/7
F1/8 9/
C
B
10/11B
4/5 3/62/7
F1/8
C
B
10/119/12uvw
xyzuvw
xyzuvw
xyz
uvw
xyzuvw
xyzuvw
xyzuvw
xyz
uvw
xyzuvw
xyzuvw
xyzuvw
xyz
uvw
xyzuvw
xyzuvw
xyzuvw
xyz
(m) (n) (o) (p)(i) (j) (k) (l)(e) (f) (g) (h)(a) (b) (c) (d)
10/ 10/
Figure 22.4 The progress of the depth-ﬁrst-search algorithm DFS on a directed graph. As edges
are explored by the algorithm, they are shown as either shaded (if they are tree edges) or dashed
(otherwise). Nontree edges are labeled B, C, or F according to whether they are back, cross, or
forward edges. Timestamps within vertices i ndicate discovery ti me/ﬁnishing times.
the root of a new tree in the depth-ﬁrst forest. When DFS returns, every vertex u
has been assigned a discovery time u:dand aﬁnishing time u:f.
In each call DFS-V ISIT.G; u/ , vertex uis initially white. Line 1 increments
the global variable time, line 2 records the new value of time as the discovery
timeu:d, and line 3 paints ugray. Lines 4–7 examine each vertex /ETBadjacent to u
and recursively visit /ETBif it is white. As each vertex /ETB2AdjŒu/c141is considered in
line 4, we say that edge .u; /ETB/ isexplored by the depth-ﬁrst search. Finally, after
every edge leaving uhas been explored, lines 8–10 paint ublack, increment time,
and record the ﬁnishing time in u:f.
Note that the results of depth-ﬁrst search may depend upon the order in which
line 5 of DFS examines the vertices and upon the order in which line 4 of DFS-
VISIT visits the neighbors of a vertex. These different visitation orders tend not606 Chapter 22 Elementary Graph Algorithms
to cause problems in practice, as we can usually use anydepth-ﬁrst search result
effectively, with essentially equivalent results.
What is the running time of DFS? The loops on lines 1–3 and lines 5–7 of DFS
take time ‚.V / , exclusive of the time to execute the calls to DFS-V ISIT. A sw ed i d
for breadth-ﬁrst search, we use aggregate analysis. The procedure DFS-V ISIT is
called exactly once for each vertex /ETB2V, since the vertex uon which DFS-V ISIT
is invoked must be white and the ﬁrst thing DFS-V ISIT does is paint vertex ugray.
During an execution of DFS-V ISIT.G; /ETB/ , the loop on lines 4–7 executes jAdjŒ/ETB/c141j
times. SinceX
/ETB2VjAdjŒ/ETB/c141jD‚.E/ ;
the total cost of executing lines 4–7 of DFS-V ISIT is‚.E/ . The running time of
DFS is therefore ‚.VCE/.
Properties of depth-ﬁrst search
Depth-ﬁrst search yields valuable information about the structure of a graph. Per-
haps the most basic property of depth-ﬁrst search is that the predecessor sub-graph G
/EMdoes indeed form a forest of trees, since the structure of the depth-
ﬁrst trees exactly mirrors the structure of recursive calls of DFS-V ISIT.T h a t i s ,
uD/ETB:/EM if and only if DFS-V ISIT.G; /ETB/ was called during a search of u’s ad-
jacency list. Additionally, vertex /ETBis a descendant of vertex uin the depth-ﬁrst
forest if and only if /ETBis discovered during the time in which uis gray.
Another important property of depth-ﬁrst search is that discovery and ﬁnishing
times have parenthesis structure . If we represent the discovery of vertex uwith
a left parenthesis “ .u” and represent its ﬁnishing by a right parenthesis “ u/”, then
the history of discoveries and ﬁnishings makes a well-formed expression in thesense that the parentheses are properly nested. For example, the depth-ﬁrst searchof Figure 22.5(a) corresponds to the parenthesization shown in Figure 22.5(b). Thefollowing theorem provides another way to characterize the parenthesis structure.
Theorem 22.7 (Parenthesis theorem)
In any depth-ﬁrst search of a (directed or undirected) graph GD.V; E/ ,f o ra n y
two vertices uand/ETB, exactly one of the following three conditions holds:
/SIthe intervals Œu:d;u :f/c141andŒ/ETB:d;/ETB:f/c141are entirely disjoint, and neither unor/ETB
is a descendant of the other in the depth-ﬁrst forest,
/SIthe interval Œu:d;u :f/c141is contained entirely within the interval Œ/ETB:d;/ETB:f/c141,a n d u
is a descendant of /ETBin a depth-ﬁrst tree, or
/SIthe interval Œ/ETB:d;/ETB:f/c141is contained entirely within the interval Œu:d;u :f/c141,a n d /ETB
is a descendant of uin a depth-ﬁrst tree.22.3 Depth-ﬁrst search 607
3/6 2/9 1/10 11/16
14/15 12/13 7/8 4/5yzst
u v w xB
CF
CC
CB
123456789 1 0 1 1 1 2 1 3 1 4 1 5 1 6s t
z
yw
xvu
s
z
yw
xt
vuC
F
B
CCB
C(a)
(b)
(c)(s(z(y(xx)y)(ww )z)s)(t(vv)(uu)t)
Figure 22.5 Properties of depth-ﬁrst search. (a)The result of a depth-ﬁrst search of a directed
graph. Vertices are timestamped and edge types are indicated as in Figure 22.4. (b)Intervals for
the discovery time and ﬁnishing time of each vertex correspond to the parenthesization shown. Each
rectangle spans the interval given by the discovery and ﬁnishing times of the corresponding vertex.
Only tree edges are shown. If two intervals overlap, then one is nested within the other, and the
vertex corresponding to the smaller interval is a descendant of the vertex corresponding to the larger.
(c)The graph of part (a) redrawn with all tree and forward edges going down within a depth-ﬁrst tree
and all back edges going up from a descendant to an ancestor.608 Chapter 22 Elementary Graph Algorithms
Proof We begin with the case in which u:d</ETB : d. We consider two subcases,
according to whether /ETB:d<u : for not. The ﬁrst subcase occurs when /ETB:d<u : f,
so/ETBwas discovered while uwas still gray, which implies that /ETBis a descendant
ofu. Moreover, since /ETBwas discovered more recently than u, all of its outgo-
ing edges are explored, and /ETBis ﬁnished, before the search returns to and ﬁn-
ishes u. In this case, therefore, the interval Œ/ETB:d;/ETB:f/c141is entirely contained within
the interval Œu:d;u :f/c141. In the other subcase, u:f</ETB : d, and by inequality (22.2),
u:d<u : f</ETB : d</ETB : f; thus the intervals Œu:d;u :f/c141andŒ/ETB:d;/ETB:f/c141are disjoint.
Because the intervals are disjoint, neither vertex was discovered while the other
was gray, and so neither vertex is a descendant of the other.
The case in which /ETB:d<u : dis similar, with the roles of uand/ETBreversed in the
above argument.
Corollary 22.8 (Nesting of descendants’ intervals)Vertex /ETBis a proper descendant of vertex uin the depth-ﬁrst forest for a (directed
or undirected) graph Gif and only if u:d</ETB : d</ETB : f<u : f.
Proof Immediate from Theorem 22.7.
The next theorem gives another important characterization of when one vertex
is a descendant of another in the depth-ﬁrst forest.
Theorem 22.9 (White-path theorem)
In a depth-ﬁrst forest of a (directed or undirected) graph GD.V; E/ , vertex /ETBis
a descendant of vertex uif and only if at the time u:dthat the search discovers u,
there is a path from uto/ETBconsisting entirely of white vertices.
Proof):I f/ETBDu, then the path from uto/ETBcontains just vertex u, which is still
white when we set the value of u:d. Now, suppose that /ETBis a proper descendant
ofuin the depth-ﬁrst forest. By Corollary 22.8, u:d</ETB : d,a n ds o /ETBis white at
time u:d.S i n c e /ETBcan be any descendant of u, all vertices on the unique simple
path from uto/ETBin the depth-ﬁrst forest are white at time u:d.
(: Suppose that there is a path of white vertices from uto/ETBat time u:d,b u t/ETB
does not become a descendant of uin the depth-ﬁrst tree. Without loss of general-
ity, assume that every vertex other than /ETBalong the path becomes a descendant of u.
(Otherwise, let /ETBbe the closest vertex to ualong the path that doesn’t become a de-
scendant of u.) Let wbe the predecessor of /ETBin the path, so that wis a descendant
ofu(wandumay in fact be the same vertex). By Corollary 22.8, w:f/DC4u:f.B e -
cause /ETBmust be discovered after uis discovered, but before wis ﬁnished, we have
u:d</ETB : d<w : f/DC4u:f. Theorem 22.7 then implies that the interval Œ/ETB:d;/ETB:f/c14122.3 Depth-ﬁrst search 609
is contained entirely within the interval Œu:d;u :f/c141. By Corollary 22.8, /ETBmust after
all be a descendant of u.
Classiﬁcation of edges
Another interesting property of depth-ﬁrst search is that the search can be used
to classify the edges of the input graph GD.V; E/ . The type of each edge can
provide important information about a graph. For example, in the next section, weshall see that a directed graph is acyclic if and only if a depth-ﬁrst search yields no“back” edges (Lemma 22.11).
We can deﬁne four edge types in terms of the depth-ﬁrst forest G
/EMproduced by
a depth-ﬁrst search on G:
1.Tree edges are edges in the depth-ﬁrst forest G/EM. Edge .u; /ETB/ is a tree edge if /ETB
was ﬁrst discovered by exploring edge .u; /ETB/ .
2.Back edges are those edges .u; /ETB/ connecting a vertex uto an ancestor /ETBin a
depth-ﬁrst tree. We consider self-loops, which may occur in directed graphs, to
be back edges.
3.Forward edges are those nontree edges .u; /ETB/ connecting a vertex uto a de-
scendant /ETBin a depth-ﬁrst tree.
4.Cross edges are all other edges. They can go between vertices in the same
depth-ﬁrst tree, as long as one vertex is not an ancestor of the other, or they cango between vertices in different depth-ﬁrst trees.
In Figures 22.4 and 22.5, edge labels indicate edge types. Figure 22.5(c) also shows
how to redraw the graph of Figure 22.5(a) so that all tree and forward edges headdownward in a depth-ﬁrst tree and all back edges go up. We can redraw any graphin this fashion.
The DFS algorithm has enough information to classify some edges as it encoun-
ters them. The key idea is that when we ﬁrst explore an edge .u; /ETB/ ,t h ec o l o ro f
vertex /ETBtells us something about the edge:
1.
WHITE indicates a tree edge,
2.GRAY indicates a back edge, and
3.BLACK indicates a forward or cross edge.
The ﬁrst case is immediate from the speciﬁcation of the algorithm. For the sec-
ond case, observe that the gray vertices always form a linear chain of descendantscorresponding to the stack of active DFS-V
ISIT invocations; the number of gray
vertices is one more than the depth in the depth-ﬁrst forest of the vertex most re-cently discovered. Exploration always proceeds from the deepest gray vertex, so610 Chapter 22 Elementary Graph Algorithms
an edge that reaches another gray vertex has reached an ancestor. The third case
handles the remaining possibility; Exercise 22.3-5 asks you to show that such anedge .u; /ETB/ is a forward edge if u:d</ETB : dand a cross edge if u:d>/ETB : d.
An undirected graph may entail some ambiguity in how we classify edges,
since .u; /ETB/ and./ETB; u/ are really the same edge. In such a case, we classify the
edge as the ﬁrst type in the classiﬁcation list that applies. Equivalently (see Ex-
ercise 22.3-6), we classify the edge according to whichever of .u; /ETB/ or./ETB; u/ the
search encounters ﬁrst.
We now show that forward and cross edges never occur in a depth-ﬁrst search of
an undirected graph.
Theorem 22.10
In a depth-ﬁrst search of an undirected graph G, every edge of Gis either a tree
edge or a back edge.
Proof Let.u; /ETB/ be an arbitrary edge of G, and suppose without loss of generality
thatu:d</ETB : d. Then the search must discover and ﬁnish /ETBbefore it ﬁnishes u
(while uis gray), since /ETBis on u’s adjacency list. If the ﬁrst time that the search
explores edge .u; /ETB/ , it is in the direction from uto/ETB,t h e n /ETBis undiscovered
(white) until that time, for otherwise the search would have explored this edge
already in the direction from /ETBtou. Thus, .u; /ETB/ becomes a tree edge. If the
search explores .u; /ETB/ ﬁrst in the direction from /ETBtou,t h e n .u; /ETB/ is a back edge,
since uis still gray at the time the edge is ﬁrst explored.
We shall see several applications of these theorems in the following sections.
Exercises
22.3-1
Make a 3-by-3chart with row and column labels WHITE ,GRAY ,a n d BLACK .I n
each cell .i; j / , indicate whether, at any point during a depth-ﬁrst search of a di-
rected graph, there can be an edge from a vertex of color ito a vertex of color j.
For each possible edge, indicate what edge types it can be. Make a second suchchart for depth-ﬁrst search of an undirected graph.
22.3-2
Show how depth-ﬁrst search works on the graph of Figure 22.6. Assume that theforloop of lines 5–7 of the DFS procedure considers the vertices in alphabetical
order, and assume that each adjacency list is ordered alphabetically. Show thediscovery and ﬁnishing times for each vertex, and show the classiﬁcation of eachedge.22.3 Depth-ﬁrst search 611
qr
stu
vw xy
z
Figure 22.6 A directed graph for use in Exercises 22.3-2 and 22.5-2.
22.3-3
Show the parenthesis structure of the depth-ﬁrst search of Figure 22.4.
22.3-4
Show that using a single bit to store each vertex color sufﬁces by arguing thatthe DFS procedure would produce the same result if line 3 of DFS-V
ISIT was
removed.
22.3-5
Show that edge .u; /ETB/ is
a.a tree edge or forward edge if and only if u:d</ETB : d</ETB : f<u : f,
b.a back edge if and only if /ETB:d/DC4u:d<u : f/DC4/ETB:f,a n d
c.a cross edge if and only if /ETB:d</ETB : f<u : d<u : f.
22.3-6
Show that in an undirected graph, classifying an edge .u; /ETB/ as a tree edge or a back
edge according to whether .u; /ETB/ or./ETB; u/ is encountered ﬁrst during the depth-ﬁrst
search is equivalent to classifying it according to the ordering of the four types in
the classiﬁcation scheme.
22.3-7
Rewrite the procedure DFS, using a stack to eliminate recursion.
22.3-8
Give a counterexample to the conjecture that if a directed graph Gcontains a path
from uto/ETB,a n di f u:d</ETB : din a depth-ﬁrst search of G,t h e n /ETBis a descendant
ofuin the depth-ﬁrst forest produced.612 Chapter 22 Elementary Graph Algorithms
22.3-9
Give a counterexample to the conjecture that if a directed graph Gcontains a path
from uto/ETB, then any depth-ﬁrst search must result in /ETB:d/DC4u:f.
22.3-10
Modify the pseudocode for depth-ﬁrst search so that it prints out every edge in thedirected graph G, together with its type. Show what modiﬁcations, if any, you need
to make if Gis undirected.
22.3-11
Explain how a vertex uof a directed graph can end up in a depth-ﬁrst tree contain-
ing only u, even though uhas both incoming and outgoing edges in G.
22.3-12
Show that we can use a depth-ﬁrst search of an undirected graph Gto identify the
connected components of G, and that the depth-ﬁrst forest contains as many trees
asGhas connected components. More precisely, show how to modify depth-ﬁrst
search so that it assigns to each vertex /ETBan integer label /ETB:ccbetween 1andk,
where kis the number of connected components of G, such that u:ccD/ETB:ccif
and only if uand/ETBare in the same connected component.
22.3-13 ?
A directed graph GD.V; E/ issingly connected ifu;/ETBimplies that Gcontains
at most one simple path from uto/ETBfor all vertices u; /ETB2V. Give an efﬁcient
algorithm to determine whether or not a directed graph is singly connected.
22.4 Topological sort
This section shows how we can use depth-ﬁrst search to perform a topological sortof a directed acyclic graph, or a “dag” as it is sometimes called. A topological sort
of a dag GD.V; E/ is a linear ordering of all its vertices such that if Gcontains an
edge .u; /ETB/ ,t h e n uappears before /ETBin the ordering. (If the graph contains a cycle,
then no linear ordering is possible.) We can view a topological sort of a graph asan ordering of its vertices along a horizontal line so that all directed edges go fromleft to right. Topological sorting is thus different from the usual kind of “sorting”
studied in Part II.
Many applications use directed acyclic graphs to indicate precedences among
events. Figure 22.7 gives an example that arises when Professor Bumstead gets
dressed in the morning. The professor must don certain garments before others
(e.g., socks before shoes). Other items may be put on in any order (e.g., socks and22.4 Topological sort 613
11/16
12/15
6/71/8
2/5
3/417/18
13/149/10
17/18 11/16 12/15 13/14 9/10 1/8 6/7 2/5 3/4(a)
(b)undershorts
pants
beltshirt
tie
jacketsocks
shoeswatch
socks undershorts pants shoes watch shirt belt tie jacket
Figure 22.7 (a) Professor Bumstead topologically sorts his clothing when getting dressed. Each
directed edge .u; /ETB/ means that garment umust be put on before garment /ETB. The discovery and
ﬁnishing times from a depth-ﬁrst search are shown next to each vertex. (b)T h es a m eg r a p hs h o w n
topologically sorted, with its vertices arranged from left to right in order of decreasing ﬁnishing time.
All directed edges go from left to right.
pants). A directed edge .u; /ETB/ in the dag of Figure 22.7(a) indicates that garment u
must be donned before garment /ETB. A topological sort of this dag therefore gives an
order for getting dressed. Figure 22.7(b) shows the topologically sorted dag as anordering of vertices along a horizontal line such that all directed edges go from leftto right.
The following simple algorithm topologically sorts a dag:
T
OPOLOGICAL -SORT.G/
1 call DFS .G/to compute ﬁnishing times /ETB:ffor each vertex /ETB
2 as each vertex is ﬁnished, insert it onto the front of a linked list3return the linked list of vertices
Figure 22.7(b) shows how the topologically sorted vertices appear in reverse order
of their ﬁnishing times.
We can perform a topological sort in time ‚.VCE/, since depth-ﬁrst search
takes ‚.VCE/time and it takes O.1/ time to insert each of the jVjvertices onto
the front of the linked list.
We prove the correctness of this algorithm using the following key lemma char-
acterizing directed acyclic graphs.614 Chapter 22 Elementary Graph Algorithms
Lemma 22.11
A directed graph Gis acyclic if and only if a depth-ﬁrst search of Gyields no back
edges.
Proof): Suppose that a depth-ﬁrst search produces a back edge .u; /ETB/ .T h e n
vertex /ETBis an ancestor of vertex uin the depth-ﬁrst forest. Thus, Gcontains a path
from /ETBtou, and the back edge .u; /ETB/ completes a cycle.
(: Suppose that Gcontains a cycle c. We show that a depth-ﬁrst search of G
yields a back edge. Let /ETBbe the ﬁrst vertex to be discovered in c,a n dl e t .u; /ETB/ be
the preceding edge in c. At time /ETB:d, the vertices of cform a path of white vertices
from /ETBtou. By the white-path theorem, vertex ubecomes a descendant of /ETBin the
depth-ﬁrst forest. Therefore, .u; /ETB/ is a back edge.
Theorem 22.12
TOPOLOGICAL -SORT produces a topological sort of the directed acyclic graph
provided as its input.
Proof Suppose that DFS is run on a given dag GD.V; E/ to determine ﬁn-
ishing times for its vertices. It sufﬁces to show that for any pair of distinct ver-tices u; /ETB2V,i fGcontains an edge from uto/ETB,t h e n /ETB:f<u : f. Consider any
edge .u; /ETB/ explored by DFS .G/. When this edge is explored, /ETBcannot be gray,
since then /ETBwould be an ancestor of uand.u; /ETB/ would be a back edge, contra-
dicting Lemma 22.11. Therefore, /ETBmust be either white or black. If /ETBis white,
it becomes a descendant of u,a n ds o /ETB:f<u : f.I f/ETBis black, it has already been
ﬁnished, so that /ETB:fhas already been set. Because we are still exploring from u,w e
have yet to assign a timestamp to u:f, and so once we do, we will have /ETB:f<u : f
as well. Thus, for any edge .u; /ETB/ in the dag, we have /ETB:f<u : f, proving the
theorem.
Exercises
22.4-1
Show the ordering of vertices produced by T OPOLOGICAL -SORT when it is run on
the dag of Figure 22.8, under the assumption of Exercise 22.3-2.
22.4-2
Give a linear-time algorithm that takes as input a directed acyclic graph GD
.V; E/ and two vertices sandt, and returns the number of simple paths from s
totinG. For example, the directed acyclic graph of Figure 22.8 contains exactly
four simple paths from vertex pto vertex /ETB:po/ETB,pory/ETB ,posry/ETB ,a n d psry/ETB .
(Your algorithm needs only to count the simple paths, not list them.)22.5 Strongly connected components 615
z y xw v u ts r qp o n m
Figure 22.8 A dag for topological sorting.
22.4-3
Give an algorithm that determines whether or not a given undirected graph GD
.V; E/ contains a cycle. Your algorithm should run in O.V / time, independent
ofjEj.
22.4-4
Prove or disprove: If a directed graph Gcontains cycles, then T OPOLOGICAL -
SORT.G/ produces a vertex ordering that minimizes the number of “bad” edges
that are inconsistent with the ordering produced.
22.4-5
Another way to perform topological sorting on a directed acyclic graph GD
.V; E/ is to repeatedly ﬁnd a vertex of in-degree 0, output it, and remove it and
all of its outgoing edges from the graph. Explain how to implement this idea so
that it runs in time O.VCE/. What happens to this algorithm if Ghas cycles?
22.5 Strongly connected components
We now consider a classic application of depth-ﬁrst search: decomposing a di-
rected graph into its strongly connected components. This section shows how to doso using two depth-ﬁrst searches. Many algorithms that work with directed graphs
begin with such a decomposition. After decomposing the graph into strongly con-
nected components, such algorithms run separately on each one and then combinethe solutions according to the structure of connections among components.
Recall from Appendix B that a strongly connected component of a directed
graph GD.V; E/ is a maximal set of vertices C/DC2Vsuch that for every pair
of vertices uand/ETBinC, we have both u;/ETBand/ETB;u; that is, vertices uand/ETB
are reachable from each other. Figure 22.9 shows an example.616 Chapter 22 Elementary Graph Algorithms
13/14 11/16
12/15 3/41/10
2/78/9
5/6abcd
ef g h
abcd
ef g h
abecd
fg h(c)(b)(a)
Figure 22.9 (a) A directed graph G. Each shaded region is a strongly connected component of G.
Each vertex is labeled with its discovery and ﬁnishing times in a depth-ﬁrst search, and tree edges
are shaded. (b)The graph GT, the transpose of G, with the depth-ﬁrst forest computed in line 3
of S TRONGLY -CONNECTED -COMPONENTS shown and tree edges shaded. Each strongly connected
component corresponds to one depth-ﬁrst tree. Vertices b,c,g,a n d h, which are heavily shaded, are
the roots of the depth-ﬁrst trees produced by the depth-ﬁrst search of GT.(c)The acyclic component
graph GSCCobtained by contracting all edges within each strongly connected component of Gso
that only a single vertex remains in each component.
Our algorithm for ﬁnding strongly connected components of a graph GD
.V; E/ uses the transpose of G, which we deﬁned in Exercise 22.1-3 to be the
graph GTD.V; ET/,w h e r e ETDf.u; /ETB/W./ETB; u/2Eg.T h a t i s , ETconsists of
the edges of Gwith their directions reversed. Given an adjacency-list representa-
tion of G, the time to create GTisO.VCE/. It is interesting to observe that G
andGThave exactly the same strongly connected components: uand/ETBare reach-
able from each other in Gif and only if they are reachable from each other in GT.
Figure 22.9(b) shows the transpose of the graph in Figure 22.9(a), with the stronglyconnected components shaded.22.5 Strongly connected components 617
The following linear-time (i.e., ‚.VCE/-time) algorithm computes the strongly
connected components of a directed graph GD.V; E/ using two depth-ﬁrst
searches, one on Gand one on GT.
STRONGLY -CONNECTED -COMPONENTS .G/
1 call DFS .G/to compute ﬁnishing times u:ffor each vertex u
2 compute GT
3 call DFS .GT/, but in the main loop of DFS, consider the vertices
in order of decreasing u:f(as computed in line 1)
4 output the vertices of each tree in the depth-ﬁrst forest formed in line 3 as a
separate strongly connected component
The idea behind this algorithm comes from a key property of the component
graph GSCCD.VSCC;ESCC/, which we deﬁne as follows. Suppose that G
has strongly connected components C1;C2;:::;C k. The vertex set VSCCis
f/ETB1;/ETB2;:::;/ETB kg, and it contains a vertex /ETBifor each strongly connected compo-
nentCiofG. There is an edge ./ETBi;/ETBj/2ESCCifGcontains a directed edge .x; y/
for some x2Ciand some y2Cj. Looked at another way, by contracting all
edges whose incident vertices are within the same strongly connected componentofG, the resulting graph is G
SCC. Figure 22.9(c) shows the component graph of
the graph in Figure 22.9(a).
The key property is that the component graph is a dag, which the following
lemma implies.
Lemma 22.13
LetCandC0be distinct strongly connected components in directed graph GD
.V; E/ ,l e tu; /ETB2C,l e tu0;/ETB02C0, and suppose that Gcontains a path u;u0.
Then Gcannot also contain a path /ETB0;/ETB.
Proof IfGcontains a path /ETB0;/ETB, then it contains paths u;u0;/ETB0and
/ETB0;/ETB;u. Thus, uand/ETB0are reachable from each other, thereby contradicting
the assumption that CandC0are distinct strongly connected components.
We shall see that by considering vertices in the second depth-ﬁrst search in de-
creasing order of the ﬁnishing times that were computed in the ﬁrst depth-ﬁrstsearch, we are, in essence, visiting the vertices of the component graph (each ofwhich corresponds to a strongly connected component of G) in topologically sorted
order.
Because the S
TRONGLY -CONNECTED -COMPONENTS procedure performs two
depth-ﬁrst searches, there is the potential for ambiguity when we discuss u:d
oru:f. In this section, these values always refer to the discovery and ﬁnishing
times as computed by the ﬁrst call of DFS, in line 1.618 Chapter 22 Elementary Graph Algorithms
We extend the notation for discovery and ﬁnishing times to sets of vertices.
IfU/DC2V, then we deﬁne d.U/Dmin u2Ufu:dgandf. U/Dmax u2Ufu:fg.
That is, d.U/ andf. U/ are the earliest discovery time and latest ﬁnishing time,
respectively, of any vertex in U.
The following lemma and its corollary give a key property relating strongly con-
nected components and ﬁnishing times in the ﬁrst depth-ﬁrst search.
Lemma 22.14
LetCandC0be distinct strongly connected components in directed graph GD
.V; E/ . Suppose that there is an edge .u; /ETB/2E,w h e r e u2Cand/ETB2C0.T h e n
f. C/>f. C0/.
Proof We consider two cases, depending on which strongly connected compo-
nent, CorC0, had the ﬁrst discovered vertex during the depth-ﬁrst search.
Ifd.C/ < d.C0/,l e txbe the ﬁrst vertex discovered in C. At time x:d,a l lv e r -
tices in CandC0are white. At that time, Gcontains a path from xto each vertex
inCconsisting only of white vertices. Because .u; /ETB/2E, for any vertex w2C0,
there is also a path in Gat time x:dfrom xtowconsisting only of white vertices:
x;u!/ETB;w. By the white-path theorem, all vertices in CandC0become
descendants of xin the depth-ﬁrst tree. By Corollary 22.8, xhas the latest ﬁnishing
time of any of its descendants, and so x:fDf. C/>f. C0/.
If instead we have d.C/ > d.C0/,l e tybe the ﬁrst vertex discovered in C0.
At time y:d, all vertices in C0are white and Gcontains a path from yto each
vertex in C0consisting only of white vertices. By the white-path theorem, all ver-
tices in C0become descendants of yin the depth-ﬁrst tree, and by Corollary 22.8,
y:fDf. C0/. At time y:d, all vertices in Care white. Since there is an edge .u; /ETB/
from CtoC0, Lemma 22.13 implies that there cannot be a path from C0toC.
Hence, no vertex in Cis reachable from y. At time y:f, therefore, all vertices in C
are still white. Thus, for any vertex w2C,w eh a v e w:f>y : f, which implies
thatf. C/>f. C0/.
The following corollary tells us that each edge in GTthat goes between different
strongly connected components goes from a component with an earlier ﬁnishing
time (in the ﬁrst depth-ﬁrst search) to a component with a later ﬁnishing time.
Corollary 22.15
LetCandC0be distinct strongly connected components in directed graph GD
.V; E/ . Suppose that there is an edge .u; /ETB/2ET,w h e r e u2Cand/ETB2C0.T h e n
f. C/<f. C0/.22.5 Strongly connected components 619
Proof Since .u; /ETB/2ET,w eh a v e ./ETB; u/2E. Because the strongly con-
nected components of GandGTare the same, Lemma 22.14 implies that
f. C/<f. C0/.
Corollary 22.15 provides the key to understanding why the strongly connected
components algorithm works. Let us examine what happens when we perform thesecond depth-ﬁrst search, which is on G
T. We start with the strongly connected
component Cwhose ﬁnishing time f. C/ is maximum. The search starts from
some vertex x2C, and it visits all vertices in C. By Corollary 22.15, GTcontains
no edges from Cto any other strongly connected component, and so the search
from xwill not visit vertices in any other component. Thus, the tree rooted at x
contains exactly the vertices of C. Having completed visiting all vertices in C,
the search in line 3 selects as a root a vertex from some other strongly connected
component C0whose ﬁnishing time f. C0/is maximum over all components other
thanC. Again, the search will visit all vertices in C0, but by Corollary 22.15,
the only edges in GTfrom C0to any other component must be to C,w h i c hw e
have already visited. In general, when the depth-ﬁrst search of GTin line 3 visits
any strongly connected component, any edges out of that component must be tocomponents that the search already visited. Each depth-ﬁrst tree, therefore, will beexactly one strongly connected component. The following theorem formalizes thisargument.
Theorem 22.16
The S
TRONGLY -CONNECTED -COMPONENTS procedure correctly computes the
strongly connected components of the directed graph Gprovided as its input.
Proof We argue by induction on the number of depth-ﬁrst trees found in the
depth-ﬁrst search of GTin line 3 that the vertices of each tree form a strongly
connected component. The inductive hypothesis is that the ﬁrst ktrees produced
in line 3 are strongly connected components. The basis for the induction, whenkD0, is trivial.
In the inductive step, we assume that each of the ﬁrst kdepth-ﬁrst trees produced
in line 3 is a strongly connected component, and we consider the .kC1/st tree
produced. Let the root of this tree be vertex u,a n dl e t ube in strongly connected
component C. Because of how we choose roots in the depth-ﬁrst search in line 3,
u:fDf. C/ > f. C
0/for any strongly connected component C0other than C
that has yet to be visited. By the inductive hypothesis, at the time that the searchvisits u, all other vertices of Care white. By the white-path theorem, therefore, all
other vertices of Care descendants of uin its depth-ﬁrst tree. Moreover, by the
inductive hypothesis and by Corollary 22.15, any edges in G
Tthat leave Cmust be
to strongly connected components that have already been visited. Thus, no vertex620 Chapter 22 Elementary Graph Algorithms
in any strongly connected component other than Cwill be a descendant of uduring
the depth-ﬁrst search of GT. Thus, the vertices of the depth-ﬁrst tree in GTthat is
rooted at uform exactly one strongly connected component, which completes the
inductive step and the proof.
Here is another way to look at how the second depth-ﬁrst search operates. Con-
sider the component graph .GT/SCCofGT. If we map each strongly connected
component visited in the second depth-ﬁrst search to a vertex of .GT/SCC, the sec-
ond depth-ﬁrst search visits vertices of .GT/SCCin the reverse of a topologically
sorted order. If we reverse the edges of .GT/SCC, we get the graph ..GT/SCC/T.
Because ..GT/SCC/TDGSCC(see Exercise 22.5-4), the second depth-ﬁrst search
visits the vertices of GSCCin topologically sorted order.
Exercises
22.5-1
How can the number of strongly connected components of a graph change if a newedge is added?
22.5-2
Show how the procedure S
TRONGLY -CONNECTED -COMPONENTS works on the
graph of Figure 22.6. Speciﬁcally, show the ﬁnishing times computed in line 1 andthe forest produced in line 3. Assume that the loop of lines 5–7 of DFS considersvertices in alphabetical order and that the adjacency lists are in alphabetical order.
22.5-3
Professor Bacon claims that the algorithm for strongly connected components
would be simpler if it used the original (instead of the transpose) graph in the
second depth-ﬁrst search and scanned the vertices in order of increasing ﬁnishing
times. Does this simpler algorithm always produce correct results?
22.5-4
Prove that for any directed graph G,w eh a v e ..G
T/SCC/TDGSCC.T h a t i s , t h e
transpose of the component graph of GTis the same as the component graph of G.
22.5-5
Give an O.VCE/-time algorithm to compute the component graph of a directed
graph GD.V; E/ . Make sure that there is at most one edge between two vertices
in the component graph your algorithm produces.Problems for Chapter 22 621
22.5-6
Given a directed graph GD.V; E/ , explain how to create another graph G0D
.V; E0/such that (a) G0has the same strongly connected components as G,( b )G0
has the same component graph as G,a n d( c ) E0is as small as possible. Describe a
fast algorithm to compute G0.
22.5-7
A directed graph GD.V; E/ issemiconnected if, for all pairs of vertices u; /ETB2V,
we have u;/ETBor/ETB;u. Give an efﬁcient algorithm to determine whether
or not Gis semiconnected. Prove that your algorithm is correct, and analyze its
running time.
Problems
22-1 Classifying edges by breadth-ﬁrst searchA depth-ﬁrst forest classiﬁes the edges of a graph into tree, back, forward, andcross edges. A breadth-ﬁrst tree can also be used to classify the edges reachable
from the source of the search into the same four categories.
a.Prove that in a breadth-ﬁrst search of an undirected graph, the following prop-
erties hold:
1. There are no back edges and no forward edges.
2. For each tree edge .u; /ETB/ ,w eh a v e /ETB:dDu:dC1.
3. For each cross edge .u; /ETB/ ,w eh a v e /ETB:dDu:dor/ETB:dDu:dC1.
b.Prove that in a breadth-ﬁrst search of a directed graph, the following properties
hold:
1. There are no forward edges.
2. For each tree edge .u; /ETB/ ,w eh a v e /ETB:dDu:dC1.
3. For each cross edge .u; /ETB/ ,w eh a v e /ETB:d/DC4u:dC1.
4. For each back edge .u; /ETB/ ,w eh a v e 0/DC4/ETB:d/DC4u:d.
22-2 Articulation points, bridges, and biconnected components
LetGD.V; E/ be a connected, undirected graph. An articulation point ofGis
a vertex whose removal disconnects G.Abridge ofGis an edge whose removal
disconnects G.Abiconnected component ofGis a maximal set of edges such
that any two edges in the set lie on a common simple cycle. Figure 22.10 illustrates622 Chapter 22 Elementary Graph Algorithms
12
34
56
Figure 22.10 The articulation points, bridges, and biconnected components of a connected, undi-
rected graph for use in Problem 22-2. The articulation points are the heavily shaded vertices, the
bridges are the heavily shaded edges, and the biconnected components are the edges in the shaded
regions, with a bccnumbering shown.
these deﬁnitions. We can determine articulation points, bridges, and biconnected
components using depth-ﬁrst search. Let G/EMD.V; E /EM/be a depth-ﬁrst tree of G.
a.Prove that the root of G/EMis an articulation point of Gif and only if it has at
least two children in G/EM.
b.Let/ETBbe a nonroot vertex of G/EM. Prove that /ETBis an articulation point of Gif and
only if /ETBh a sac h i l d ssuch that there is no back edge from sor any descendant
ofsto a proper ancestor of /ETB.
c.Let
/ETB:lowDmin(
/ETB:d;
w:dW.u; w/ is a back edge for some descendant uof/ETB:
Show how to compute /ETB:lowfor all vertices /ETB2VinO.E/ time.
d.Show how to compute all articulation points in O.E/ time.
e.Prove that an edge of Gis a bridge if and only if it does not lie on any simple
cycle of G.
f.Show how to compute all the bridges of GinO.E/ time.
g.Prove that the biconnected components of Gpartition the nonbridge edges of G.
h.Give an O.E/ -time algorithm to label each edge eofGwith a positive in-
teger e:bccsuch that e:bccDe0:bccif and only if eande0are in the same
biconnected component.Notes for Chapter 22 623
22-3 Euler tour
AnEuler tour of a strongly connected, directed graph GD.V; E/ is a cycle that
traverses each edge of Gexactly once, although it may visit a vertex more than
once.
a.Show that Ghas an Euler tour if and only if in-degree ./ETB/Dout-degree ./ETB/for
each vertex /ETB2V.
b.Describe an O.E/ -time algorithm to ﬁnd an Euler tour of Gif one exists. ( Hint:
Merge edge-disjoint cycles.)
22-4 Reachability
LetGD.V; E/ be a directed graph in which each vertex u2Vis labeled with
a unique integer L.u/ from the setf1 ;2;:::;jVjg. For each vertex u2V,l e t
R.u/Df/ETB2VWu;/ETBgbe the set of vertices that are reachable from u.D e ﬁ n e
min.u/to be the vertex in R.u/ whose label is minimum, i.e., min .u/is the vertex /ETB
such that L./ETB/DminfL.w/Ww2R.u/g.G i v ea n O.VCE/-time algorithm that
computes min .u/for all vertices u2V.
Chapter notes
Even [103] and Tarjan [330] are excellent references for graph algorithms.
Breadth-ﬁrst search was discovered by Moore [260] in the context of ﬁnding
paths through mazes. Lee [226] independently discovered the same algorithm in
the context of routing wires on circuit boards.
Hopcroft and Tarjan [178] advocated the use of the adjacency-list representation
over the adjacency-matrix representation for sparse graphs and were the ﬁrst torecognize the algorithmic importance of depth-ﬁrst search. Depth-ﬁrst search hasbeen widely used since the late 1950s, especially in artiﬁcial intelligence programs.
Tarjan [327] gave a linear-time algorithm for ﬁnding strongly connected compo-
nents. The algorithm for strongly connected components in Section 22.5 is adaptedfrom Aho, Hopcroft, and Ullman [6], who credit it to S. R. Kosaraju (unpublished)and M. Sharir [314]. Gabow [119] also developed an algorithm for strongly con-nected components that is based on contracting cycles and uses two stacks to makeit run in linear time. Knuth [209] was the ﬁrst to give a linear-time algorithm fortopological sorting.23 Minimum Spanning Trees
Electronic circuit designs often need to make the pins of several components elec-
trically equivalent by wiring them together. To interconnect a set of npins, we can
use an arrangement of n/NUL1wires, each connecting two pins. Of all such arrange-
ments, the one that uses the least amount of wire is usually the most desirable.
We can model this wiring problem with a connected, undirected graph GD
.V; E/ ,w h e r e Vis the set of pins, Eis the set of possible interconnections between
pairs of pins, and for each edge .u; /ETB/2E,w eh a v eaw e i g h t w.u;/ETB/ specifying
the cost (amount of wire needed) to connect uand/ETB. W et h e nw i s ht oﬁ n da n
acyclic subset T/DC2Ethat connects all of the vertices and whose total weight
w.T/DX
.u;/ETB/ 2Tw.u;/ETB/
is minimized. Since Tis acyclic and connects all of the vertices, it must form a tree,
which we call a spanning tree since it “spans” the graph G. We call the problem of
determining the tree Ttheminimum-spanning-tree problem .1Figure 23.1 shows
an example of a connected graph and a minimum spanning tree.
In this chapter, we shall examine two algorithms for solving the minimum-
spanning-tree problem: Kruskal’s algorithm and Prim’s algorithm. We can easilymake each of them run in time O.E lgV/using ordinary binary heaps. By using
Fibonacci heaps, Prim’s algorithm runs in time O.ECVlgV/, which improves
over the binary-heap implementation if jVjis much smaller than jEj.
The two algorithms are greedy algorithms, as described in Chapter 16. Each
step of a greedy algorithm must make one of several possible choices. The greedystrategy advocates making the choice that is the best at the moment. Such a strat-egy does not generally guarantee that it will always ﬁnd globally optimal solutions
1The phrase “minimum spanning tree” is a shortened form of the phrase “minimum-weight spanning
tree.” We are not, for example, minimizing the number of edges in T, since all spanning trees have
exactlyjVj/NUL1edges by Theorem B.2.23.1 Growing a minimum spanning tree 625
b
a
hc
gid
fe4
81187
9
1014 4
2 12
76
Figure 23.1 A minimum spanning tree for a connected graph. The weights on edges are shown,
and the edges in a minimum spanning tree are shaded. The total weight of the tree shown is 37.T h i s
minimum spanning tree is not unique: removing the edge .b; c/ and replacing it with the edge .a; h/
yields another spanning tree with weight 37.
to problems. For the minimum-spanning-tree problem, however, we can prove that
certain greedy strategies do yield a spanning tree with minimum weight. Althoughyou can read this chapter independently of Chapter 16, the greedy methods pre-sented here are a classic application of the theoretical notions introduced there.
Section 23.1 introduces a “generic” minimum-spanning-tree method that grows
a spanning tree by adding one edge at a time. Section 23.2 gives two algorithmsthat implement the generic method. The ﬁrst algorithm, due to Kruskal, is similarto the connected-components algorithm from Section 21.1. The second, due toPrim, resembles Dijkstra’s shortest-paths algorithm (Section 24.3).
Because a tree is a type of graph, in order to be precise we must deﬁne a tree in
terms of not just its edges, but its vertices as well. Although this chapter focuses
on trees in terms of their edges, we shall operate with the understanding that the
vertices of a tree Tare those that some edge of Tis incident on.
23.1 Growing a minimum spanning tree
Assume that we have a connected, undirected graph GD.V; E/ with a weight
function wWE! R, and we wish to ﬁnd a minimum spanning tree for G.T h e
two algorithms we consider in this chapter use a greedy approach to the problem,although they differ in how they apply this approach.
This greedy strategy is captured by the following generic method, which grows
the minimum spanning tree one edge at a time. The generic method manages a set
of edges A, maintaining the following loop invariant:
Prior to each iteration, Ais a subset of some minimum spanning tree.
At each step, we determine an edge .u; /ETB/ that we can add to Awithout violating
this invariant, in the sense that A[f.u; /ETB/gis also a subset of a minimum spanning626 Chapter 23 Minimum Spanning Trees
tree. We call such an edge a safe edge forA, since we can add it safely to Awhile
maintaining the invariant.
GENERIC -MST .G; w/
1AD;
2while Adoes not form a spanning tree
3 ﬁnd an edge .u; /ETB/ that is safe for A
4 ADA[f.u; /ETB/g
5return A
We use the loop invariant as follows:
Initialization: After line 1, the set Atrivially satisﬁes the loop invariant.
Maintenance: The loop in lines 2–4 maintains the invariant by adding only safe
edges.
Termination: All edges added to Aare in a minimum spanning tree, and so the
setAreturned in line 5 must be a minimum spanning tree.
The tricky part is, of course, ﬁnding a safe edge in line 3. One must exist, since
when line 3 is executed, the invariant dictates that there is a spanning tree Tsuch
thatA/DC2T. Within the while loop body, Amust be a proper subset of T,a n d
therefore there must be an edge .u; /ETB/2Tsuch that .u; /ETB/62Aand.u; /ETB/ is safe
forA.
In the remainder of this section, we provide a rule (Theorem 23.1) for recogniz-
ing safe edges. The next section describes two algorithms that use this rule to ﬁndsafe edges efﬁciently.
We ﬁrst need some deﬁnitions. A cut.S; V/NULS/of an undirected graph GD
.V; E/ is a partition of V. Figure 23.2 illustrates this notion. We say that an edge
.u; /ETB/2Ecrosses the cut .S; V/NULS/if one of its endpoints is in Sand the other
is inV/NULS. We say that a cut respects a setA
of edges if no edge in Acrosses the
cut. An edge is a light edge crossing a cut if its weight is the minimum of any edge
crossing the cut. Note that there can be more than one light edge crossing a cut inthe case of ties. More generally, we say that an edge is a light edge satisfying a
given property if its weight is the minimum of any edge satisfying the property.
Our rule for recognizing safe edges is given by the following theorem.
Theorem 23.1
LetGD.V; E/ be a connected, undirected graph with a real-valued weight func-
tionwdeﬁned on E.L e t Abe a subset of Ethat is included in some minimum
spanning tree for G,l e t.S; V/NULS/be any cut of Gthat respects A,a n dl e t .u; /ETB/
be a light edge crossing .S; V/NULS/. Then, edge .u; /ETB/ is safe for A.23.1 Growing a minimum spanning tree 627
b
a
hc
gid
fe4
81187
9
10144
2 12
76a
b
d
eh
i
g
c
f8
11
8
7
14
10467
4
921
2
S
(a) (b)V – SS
V – SS
V – S
Figure 23.2 Two ways of viewing a cut .S; V/NULS/of the graph from Figure 23.1. (a)Black
vertices are in the set S, and white vertices are in V/NULS. The edges crossing the cut are those
connecting white vertices with black vertices. The edge .d; c/ is the unique light edge crossing the
cut. A subset Aof the edges is shaded; note that the cut .S; V/NULS/respects A, since no edge of A
crosses the cut. (b)The same graph with the vertices in the set Son the left and the vertices in the
setV/NULSon the right. An edge crosses the cut if it connects a vertex on the left with a vertex on the
right.
Proof LetTbe a minimum spanning tree that includes A, and assume that T
does not contain the light edge .u; /ETB/ , since if it does, we are done. We shall
construct another minimum spanning tree T0that includes A[f.u; /ETB/gby using a
cut-and-paste technique, thereby showing that .u; /ETB/ is a safe edge for A.
The edge .u; /ETB/ forms a cycle with the edges on the simple path pfrom u
to/ETBinT, as Figure 23.3 illustrates. Since uand/ETBare on opposite sides of the
cut.S; V/NULS/, at least one edge in Tlies on the simple path pand also crosses
the cut. Let .x; y/ be any such edge. The edge .x; y/ is not in A, because the cut
respects A.S i n c e .x; y/ is on the unique simple path from uto/ETBinT,r e m o v -
ing.x; y/ breaks Tinto two components. Adding .u; /ETB/ reconnects them to form
a new spanning tree T0DT/NULf.x; y/g[f.u; /ETB/g.
We next show that T0is a minimum spanning tree. Since .u; /ETB/ is a light edge
crossing .S; V/NULS/and.x; y/ also crosses this cut, w.u;/ETB//DC4w.x;y/ . Therefore,
w.T0/Dw.T//NULw.x;y/Cw.u;/ETB/
/DC4w.T/ :628 Chapter 23 Minimum Spanning Trees
y
vux
p
Figure 23.3 The proof of Theorem 23.1. Black vertices are in S, and white vertices are in V/NULS.
The edges in the minimum spanning tree Tare shown, but the edges in the graph Gare not. The
edges in Aare shaded, and .u; /ETB/ is a light edge crossing the cut .S; V/NULS/. The edge .x; y/ is
an edge on the unique simple path pfrom uto/ETBinT. To form a minimum spanning tree T0that
contains .u; /ETB/ , remove the edge .x; y/ from Tand add the edge .u; /ETB/ .
ButTis a minimum spanning tree, so that w.T//DC4w.T0/; thus, T0must be a
minimum spanning tree also.
It remains to show that .u; /ETB/ is actually a safe edge for A.W e h a v e A/DC2T0,
since A/DC2Tand.x; y/62A; thus, A[f.u; /ETB/g/DC2T0. Consequently, since T0is a
minimum spanning tree, .u; /ETB/ is safe for A.
Theorem 23.1 gives us a better understanding of the workings of the G ENERIC -
MST method on a connected graph GD.V; E/ . As the method proceeds, the
setAis always acyclic; otherwise, a minimum spanning tree including Awould
contain a cycle, which is a contradiction. At any point in the execution, the graphG
AD.V; A/ is a forest, and each of the connected components of GAis a tree.
(Some of the trees may contain just one vertex, as is the case, for example, whenthe method begins: Ais empty and the forest contains jVjtrees, one for each
vertex.) Moreover, any safe edge .u; /ETB/ forAconnects distinct components of G
A,
since A[f.u; /ETB/gmust be acyclic.
Thewhile loop in lines 2–4 of G ENERIC -MST executesjVj/NUL1times because
it ﬁnds one of thejVj/NUL1edges of a minimum spanning tree in each iteration.
Initially, when AD;,t h e r ea r ejVjtrees in GA, and each iteration reduces that
number by 1. When the forest contains only a single tree, the method terminates.
The two algorithms in Section 23.2 use the following corollary to Theorem 23.1.23.1 Growing a minimum spanning tree 629
Corollary 23.2
LetGD.V; E/ be a connected, undirected graph with a real-valued weight func-
tionwdeﬁned on E.L e t Abe a subset of Ethat is included in some minimum
spanning tree for G,a n dl e t CD.VC;EC/be a connected component (tree) in the
forest GAD.V; A/ .I f.u; /ETB/ is a light edge connecting Cto some other component
inGA,t h e n .u; /ETB/ is safe for A.
Proof The cut .VC;V/NULVC/respects A,a n d .u; /ETB/ is a light edge for this cut.
Therefore, .u; /ETB/ is safe for A.
Exercises
23.1-1
Let.u; /ETB/ be a minimum-weight edge in a connected graph G. Show that .u; /ETB/
belongs to some minimum spanning tree of G.
23.1-2
Professor Sabatier conjectures the following converse of Theorem 23.1. Let GD
.V; E/ be a connected, undirected graph with a real-valued weight function wde-
ﬁned on E.L e t Abe a subset of Ethat is included in some minimum spanning
tree for G,l e t.S; V/NULS/be any cut of Gthat respects A,a n dl e t .u; /ETB/ be a safe
edge for Acrossing .S; V/NULS/. Then, .u; /ETB/ is a light edge for the cut. Show that
the professor’s conjecture is incorrect by giving a counterexample.
23.1-3
Show that if an edge .u; /ETB/ is contained in some minimum spanning tree, then it is
a light edge crossing some cut of the graph.
23.1-4
Give a simple example of a connected graph such that the set of edges f.u; /ETB/W
there exists a cut .S; V/NULS/such that .u; /ETB/ is a light edge crossing .S; V/NULS/g
does not form a minimum spanning tree.
23.1-5
Letebe a maximum-weight edge on some cycle of connected graph GD.V; E/ .
Prove that there is a minimum spanning tree of G0D.V; E/NULfeg/that is also a
minimum spanning tree of G. That is, there is a minimum spanning tree of Gthat
does not include e.630 Chapter 23 Minimum Spanning Trees
23.1-6
Show that a graph has a unique minimum spanning tree if, for every cut of thegraph, there is a unique light edge crossing the cut. Show that the converse is nottrue by giving a counterexample.
23.1-7
Argue that if all edge weights of a graph are positive, then any subset of edges thatconnects all vertices and has minimum total weight must be a tree. Give an exampleto show that the same conclusion does not follow if we allow some weights to be
nonpositive.
23.1-8
LetTbe a minimum spanning tree of a graph G,a n dl e t Lbe the sorted list of the
edge weights of T. Show that for any other minimum spanning tree T
0ofG,t h e
listLis also the sorted list of edge weights of T0.
23.1-9
LetTbe a minimum spanning tree of a graph GD.V; E/ ,a n dl e t V0b eas u b s e t
ofV.L e t T0be the subgraph of Tinduced by V0,a n dl e t G0be the subgraph of G
induced by V0. Show that if T0is connected, then T0is a minimum spanning tree
ofG0.
23.1-10
Given a graph Gand a minimum spanning tree T, suppose that we decrease the
weight of one of the edges in T. Show that Tis still a minimum spanning tree
forG. More formally, let Tbe a minimum spanning tree for Gwith edge weights
given by weight function w. Choose one edge .x; y/2Tand a positive number k,
and deﬁne the weight function w0by
w0.u; /ETB/D(
w.u;/ETB/ if.u; /ETB/¤.x; y/ ;
w.x;y//NULkif.u; /ETB/D.x; y/ :
Show that Tis a minimum spanning tree for Gwith edge weights given by w0.
23.1-11 ?
Given a graph Gand a minimum spanning tree T, suppose that we decrease the
weight of one of the edges not in T. Give an algorithm for ﬁnding the minimum
spanning tree in the modiﬁed graph.23.2 The algorithms of Kruskal and Prim 631
23.2 The algorithms of Kruskal and Prim
The two minimum-spanning-tree algorithms described in this section elaborate on
the generic method. They each use a speciﬁc rule to determine a safe edge in line 3of G
ENERIC -MST. In Kruskal’s algorithm, the set Ais a forest whose vertices are
all those of the given graph. The safe edge added to Ais always a least-weight
edge in the graph that connects two distinct components. In Prim’s algorithm, the
setAforms a single tree. The safe edge added to Ais always a least-weight edge
connecting the tree to a vertex not in the tree.
Kruskal’s algorithm
Kruskal’s algorithm ﬁnds a safe edge to add to the growing forest by ﬁnding, of all
the edges that connect any two trees in the forest, an edge .u; /ETB/ of least weight.
LetC1andC2denote the two trees that are connected by .u; /ETB/ .S i n c e .u; /ETB/ must
be a light edge connecting C1to some other tree, Corollary 23.2 implies that .u; /ETB/
i sas a f ee d g ef o r C1. Kruskal’s algorithm qualiﬁes as a greedy algorithm because
at each step it adds to the forest an edge of least possible weight.
Our implementation of Kruskal’s algorithm is like the algorithm to compute
connected components from Section 21.1. It uses a disjoint-set data structure to
maintain several disjoint sets of elements. Each set contains the vertices in one tree
of the current forest. The operation F IND-SET.u/returns a representative element
from the set that contains u. Thus, we can determine whether two vertices uand/ETB
belong to the same tree by testing whether F IND-SET.u/equals F IND-SET./ETB/.T o
combine trees, Kruskal’s algorithm calls the U NION procedure.
MST-K RUSKAL .G; w/
1AD;
2foreach vertex /ETB2G:V
3M AKE-SET./ETB/
4 sort the edges of G:Einto nondecreasing order by weight w
5foreach edge .u; /ETB/2G:E, taken in nondecreasing order by weight
6 ifFIND-SET.u/¤FIND-SET./ETB/
7 ADA[f.u; /ETB/g
8U NION .u; /ETB/
9return A
Figure 23.4 shows how Kruskal’s algorithm works. Lines 1–3 initialize the set A
to the empty set and create jVjtrees, one containing each vertex. The forloop in
lines 5–8 examines edges in order of weight, from lowest to highest. The loop632 Chapter 23 Minimum Spanning Trees
b
a
hc
gid
fe4
81187
9
1014 4
2 12
76(a) (b)
(c) (d)
(e)
(g)(f)
(h)b
a
hc
gid
fe4
81187
9
1014 4
2 1762
b
a
hc
gid
fe4
81187
9
1014 4
2 1762b
a
hc
gid
fe4
81187
9
1014 4
2 1762
b
a
hc
gid
fe4
81187
9
1014 4
2 1762b
a
hc
gid
fe4
81187
9
1014 4
2 1762
b
a
hc
gid
fe4
81187
9
1014 4
2 1762b
a
hc
gid
fe4
81187
9
1014 4
2 1762
Figure 23.4 The execution of Kruskal’s algorithm on the graph from Figure 23.1. Shaded edges
belong to the forest Abeing grown. The algorithm considers each edge in sorted order by weight.
An arrow points to the edge under consideration at each step of the algorithm. If the edge joins two
distinct trees in the forest, it is added to the forest, thereby merging the two trees.
checks, for each edge .u; /ETB/ , whether the endpoints uand/ETBbelong to the same
tree. If they do, then the edge .u; /ETB/ cannot be added to the forest without creating
a cycle, and the edge is discarded. Otherwise, the two vertices belong to differenttrees. In this case, line 7 adds the edge .u; /ETB/ toA, and line 8 merges the vertices
in the two trees.23.2 The algorithms of Kruskal and Prim 633
(i) (j)
(k) (l)
(n) (m)b
a
hc
gid
fe4
81187
9
1014 4
2 176b
a
hc
gid
fe4
81187
9
1014 4
2 1762 2
b
a
hc
gid
fe4
81187
9
1014 4
2 1762b
a
hc
gid
fe4
81187
9
1014 4
2 176
b
a
hc
gid
fe4
81187
9
1014 4
2 176b
a
hc
gid
fe4
81187
9
1014 4
2 17622
2
Figure 23.4, continued Further steps in the execution of Kruskal’s algorithm.
The running time of Kruskal’s algorithm for a graph GD.V; E/ depends
on how we implement the disjoint-set data structure. We assume that we usethe disjoint-set-forest implementation of Section 21.3 with the union-by-rank andpath-compression heuristics, since it is the asymptotically fastest implementationknown. Initializing the set Ain line 1 takes O.1/ time, and the time to sort the
edges in line 4 is O.E lgE/. (We will account for the cost of the jVjM
AKE-SET
operations in the forloop of lines 2–3 in a moment.) The forloop of lines 5–8
performs O.E/ FIND-SETand U NION operations on the disjoint-set forest. Along
with thejVjMAKE-SEToperations, these take a total of O..VCE/ ˛.V // time,
where ˛is the very slowly growing function deﬁned in Section 21.4. Because we
assume that Gis connected, we have jEj/NAKjVj/NUL1, and so the disjoint-set opera-
tions take O.E˛.V // time. Moreover, since ˛.jVj/DO.lgV/DO.lgE/,t h et o -
tal running time of Kruskal’s algorithm is O.E lgE/. Observing thatjEj<jVj2,
we have lgjEjDO.lgV/, and so we can restate the running time of Kruskal’s
algorithm as O.E lgV/.634 Chapter 23 Minimum Spanning Trees
Prim’s algorithm
Like Kruskal’s algorithm, Prim’s algorithm is a special case of the generic min-
imum-spanning-tree method from Section 23.1. Prim’s algorithm operates muchlike Dijkstra’s algorithm for ﬁnding shortest paths in a graph, which we shall see inSection 24.3. Prim’s algorithm has the property that the edges in the set Aalways
form a single tree. As Figure 23.5 shows, the tree starts from an arbitrary rootvertex rand grows until the tree spans all the vertices in V. Each step adds to the
treeAa light edge that connects Ato an isolated vertex—one on which no edge
ofAis incident. By Corollary 23.2, this rule adds only edges that are safe for A;
therefore, when the algorithm terminates, the edges in Aform a minimum spanning
tree. This strategy qualiﬁes as greedy since at each step it adds to the tree an edgethat contributes the minimum amount possible to the tree’s weight.
In order to implement Prim’s algorithm efﬁciently, we need a fast way to select
a new edge to add to the tree formed by the edges in A. In the pseudocode below,
the connected graph Gand the root rof the minimum spanning tree to be grown
are inputs to the algorithm. During execution of the algorithm, all vertices thatarenotin the tree reside in a min-priority queue Qbased on a keyattribute. For
each vertex /ETB, the attribute /ETB:keyis the minimum weight of any edge connecting /ETB
to a vertex in the tree; by convention, /ETB:keyD1 if there is no such edge. The
attribute /ETB:/EMnames the parent of /ETBin the tree. The algorithm implicitly maintains
the set Afrom G
ENERIC -MST as
ADf./ETB; /ETB:/EM/W/ETB2V/NULfrg/NULQg:
When the algorithm terminates, the min-priority queue Qis empty; the minimum
spanning tree AforGis thus
ADf./ETB; /ETB:/EM/W/ETB2V/NULfrgg:
MST-P RIM. G ;w;r/
1foreachu2G:V
2 u:keyD1
3 u:/EMDNIL
4r:keyD0
5QDG:V
6while Q¤;
7 uDEXTRACT -MIN.Q/
8 foreach/ETB2G:AdjŒu/c141
9 if/ETB2Qandw.u;/ETB/ < /ETB: key
10 /ETB:/EMDu
11 /ETB:keyDw.u;/ETB/23.2 The algorithms of Kruskal and Prim 635
(a) (b)
(c) (d)
(e) (f)
(g) (h)
(i)b
a
hc
gid
fe4
81187
9
1014 4
2 12
76b
a
hc
gid
fe4
81187
9
1014 4
2 12
76
b
a
hc
gid
fe4
81187
9
1014 4
2 12
76b
a
hc
gid
fe4
81187
9
1014 4
2 1762
b
a
hc
gid
fe4
81187
9
1014 4
2 1762b
a
hc
gid
fe4
81187
9
1014 4
2 1762
b
a
hc
gid
fe4
81187
9
1014 4
2 1762b
a
hc
gid
fe4
81187
9
1014 4
2 1762
b
a
hc
gid
fe4
81187
9
1014 4
2 1762
Figure 23.5 The execution of Prim’s algorithm on the graph from Figure 23.1. The root vertex
isa. Shaded edges are in the tree being grown, and black vertices are in the tree. At each step of
the algorithm, the vertices in the tree determine a cut of the graph, and a light edge crossing the cut
is added to the tree. In the second step, for example, the algorithm has a choice of adding either
edge .b; c/ or edge .a; h/ to the tree since both are light edges crossing the cut.636 Chapter 23 Minimum Spanning Trees
Figure 23.5 shows how Prim’s algorithm works. Lines 1–5 set the key of each
vertex to1(except for the root r, whose key is set to 0so that it will be the
ﬁrst vertex processed), set the parent of each vertex to NIL, and initialize the min-
priority queue Qto contain all the vertices. The algorithm maintains the following
three-part loop invariant:
Prior to each iteration of the while loop of lines 6–11,
1.ADf./ETB; /ETB:/EM/W/ETB2V/NULfrg/NULQg.
2. The vertices already placed into the minimum spanning tree are those in
V/NULQ.
3. For all vertices /ETB2Q,i f/ETB:/EM¤NIL,t h e n /ETB:key<1and/ETB:keyis
the weight of a light edge ./ETB; /ETB:/EM/ connecting /ETBto some vertex already
placed into the minimum spanning tree.
Line 7 identiﬁes a vertex u2Qincident on a light edge that crosses the cut
.V/NULQ; Q/ (with the exception of the ﬁrst iteration, in which uDrdue to line 4).
Removing ufrom the set Qadds it to the set V/NULQof vertices in the tree, thus
adding .u; u:/EM/ toA.T h e forloop of lines 8–11 updates the keyand/EMattributes
of every vertex /ETBadjacent to ubut not in the tree, thereby maintaining the third
part of the loop invariant.
The running time of Prim’s algorithm depends on how we implement the min-
priority queue Q. If we implement Qas a binary min-heap (see Chapter 6), we
can use the B UILD -MIN-HEAP procedure to perform lines 1–5 in O.V / time. The
body of the while loop executesjVjtimes, and since each E XTRACT -MINopera-
tion takes O.lgV/time, the total time for all calls to E XTRACT -MINisO.V lgV/.
Theforloop in lines 8–11 executes O.E/ times altogether, since the sum of the
lengths of all adjacency lists is 2jEj. Within the forloop, we can implement the
test for membership in Qin line 9 in constant time by keeping a bit for each vertex
that tells whether or not it is in Q, and updating the bit when the vertex is removed
from Q. The assignment in line 11 involves an implicit D ECREASE -KEYopera-
tion on the min-heap, which a binary min-heap supports in O.lgV/time. Thus,
the total time for Prim’s algorithm is O.V lgVCElgV/DO.E lgV/,w h i c hi s
asymptotically the same as for our implementation of Kruskal’s algorithm.
We can improve the asymptotic running time of Prim’s algorithm by using Fi-
bonacci heaps. Chapter 19 shows that if a Fibonacci heap holds jVjelements, an
EXTRACT -MINoperation takes O.lgV/amortized time and a D ECREASE -KEY
operation (to implement line 11) takes O.1/ amortized time. Therefore, if we use a
Fibonacci heap to implement the min-priority queue Q, the running time of Prim’s
algorithm improves to O.ECVlgV/.23.2 The algorithms of Kruskal and Prim 637
Exercises
23.2-1
Kruskal’s algorithm can return different spanning trees for the same input graph G,
depending on how it breaks ties when the edges are sorted into order. Show that
for each minimum spanning tree TofG, there is a way to sort the edges of Gin
Kruskal’s algorithm so that the algorithm returns T.
23.2-2
Suppose that we represent the graph GD.V; E/ as an adjacency matrix. Give a
simple implementation of Prim’s algorithm for this case that runs in O.V2/time.
23.2-3
For a sparse graph GD.V; E/ ,w h e r ejEjD‚.V / , is the implementation of
Prim’s algorithm with a Fibonacci heap asymptotically faster than the binary-heapimplementation? What about for a dense graph, where jEjD‚.V
2/?H o w
must the sizesjEjandjVjbe related for the Fibonacci-heap implementation to
be asymptotically faster than the binary-heap implementation?
23.2-4
Suppose that all edge weights in a graph are integers in the range from 1tojVj.
How fast can you make Kruskal’s algorithm run? What if the edge weights areintegers in the range from 1toWfor some constant W?
23.2-5
Suppose that all edge weights in a graph are integers in the range from 1tojVj.
How fast can you make Prim’s algorithm run? What if the edge weights are integersin the range from 1toWfor some constant W?
23.2-6 ?
Suppose that the edge weights in a graph are uniformly distributed over the half-
open interval Œ0; 1/ . Which algorithm, Kruskal’s or Prim’s, can you make run
faster?
23.2-7 ?
Suppose that a graph Ghas a minimum spanning tree already computed. How
quickly can we update the minimum spanning tree if we add a new vertex andincident edges to G?
23.2-8
Professor Borden proposes a new divide-and-conquer algorithm for computing
minimum spanning trees, which goes as follows. Given a graph GD.V; E/ ,
partition the set Vof vertices into two sets V
1andV2such thatjV1jandjV2jdiffer638 Chapter 23 Minimum Spanning Trees
by at most 1.L e t E1be the set of edges that are incident only on vertices in V1,a n d
letE2be the set of edges that are incident only on vertices in V2. Recursively solve
a minimum-spanning-tree problem on each of the two subgraphs G1D.V1;E1/
andG2D.V2;E2/. Finally, select the minimum-weight edge in Ethat crosses the
cut.V1;V2/, and use this edge to unite the resulting two minimum spanning trees
into a single spanning tree.
Either argue that the algorithm correctly computes a minimum spanning tree
ofG, or provide an example for which the algorithm fails.
Problems
23-1 Second-best minimum spanning tree
LetGD.V; E/ be an undirected, connected graph whose weight function is
wWE!R, and suppose thatjEj/NAKjVjand all edge weights are distinct.
We deﬁne a second-best minimum spanning tree as follows. Let Tbe the set
of all spanning trees of G,a n dl e t T0be a minimum spanning tree of G.T h e n
asecond-best minimum spanning tree is a spanning tree Tsuch that w.T/D
min T002T/NULfT0gfw.T00/g.
a.Show that the minimum spanning tree is unique, but that the second-best mini-
mum spanning tree need not be unique.
b.LetTbe the minimum spanning tree of G. Prove that Gcontains edges
.u; /ETB/2Tand.x; y/62Tsuch that T/NULf.u; /ETB/g[f.x; y/gis a second-best
minimum spanning tree of G.
c.LetTbe a spanning tree of Gand, for any two vertices u; /ETB2V,l e tmaxŒu; /ETB/c141
denote an edge of maximum weight on the unique simple path between uand/ETB
inT. Describe an O.V2/-time algorithm that, given T, computes maxŒu; /ETB/c141 for
allu; /ETB2V.
d.Give an efﬁcient algorithm to compute the second-best minimum spanning tree
ofG.
23-2 Minimum spanning tree in sparse graphs
For a very sparse connected graph GD.V; E/ , we can further improve upon the
O.ECVlgV/running time of Prim’s algorithm with Fibonacci heaps by prepro-
cessing Gto decrease the number of vertices before running Prim’s algorithm. In
particular, we choose, for each vertex u, the minimum-weight edge .u; /ETB/ incident
onu, and we put .u; /ETB/ into the minimum spanning tree under construction. WeProblems for Chapter 23 639
then contract all chosen edges (see Section B.4). Rather than contracting these
edges one at a time, we ﬁrst identify sets of vertices that are united into the samenew vertex. Then we create the graph that would have resulted from contractingthese edges one at a time, but we do so by “renaming” edges according to the setsinto which their endpoints were placed. Several edges from the original graph maybe renamed the same as each other. In such a case, only one edge results, and itsweight is the minimum of the weights of the corresponding original edges.
Initially, we set the minimum spanning tree Tbeing constructed to be empty,
and for each edge .u; /ETB/2E, we initialize the attributes .u; /ETB/: origD.u; /ETB/
and.u; /ETB/: cDw.u;/ETB/ .W e u s e t h e orig attribute to reference the edge from the
initial graph that is associated with an edge in the contracted graph. The cattribute
holds the weight of an edge, and as edges are contracted, we update it according tothe above scheme for choosing edge weights. The procedure MST-R
EDUCE takes
inputs GandT, and it returns a contracted graph G0with updated attributes orig0
andc0. The procedure also accumulates edges of Ginto the minimum spanning
treeT.
MST-R EDUCE .G; T /
1foreach/ETB2G:V
2 /ETB:markDFALSE
3M AKE-SET./ETB/
4foreachu2G:V
5 ifu:mark ==FALSE
6 choose /ETB2G:AdjŒu/c141such that .u; /ETB/: cis minimized
7U NION .u; /ETB/
8 TDT[f.u; /ETB/: origg
9 u:markD/ETB:markDTRUE
10G0:VDfFIND-SET./ETB/W/ETB2G:Vg
11G0:ED;
12foreach.x; y/2G:E
13 uDFIND-SET.x/
14 /ETBDFIND-SET.y/
15 if.u; /ETB/62G0:E
16 G0:EDG0:E[f.u; /ETB/g
17 .u; /ETB/: orig0D.x; y/: orig
18 .u; /ETB/:c0D.x; y/: c
19 else if .x; y/: c<. u ;/ETB / : c0
20 .u; /ETB/: orig0D.x; y/: orig
21 .u; /ETB/:c0D.x; y/: c
22 construct adjacency lists G0:AdjforG0
23return G0andT640 Chapter 23 Minimum Spanning Trees
a.LetTbe the set of edges returned by MST-R EDUCE ,a n dl e t Abe the minimum
spanning tree of the graph G0formed by the call MST-P RIM.G0;c0;r/,w h e r e c0
is the weight attribute on the edges of G0:Eandris any vertex in G0:V. Prove
thatT[f.x; y/: orig0W.x; y/2Agis a minimum spanning tree of G.
b.Argue thatjG0:Vj/DC4jVj=2.
c.Show how to implement MST-R EDUCE so that it runs in O.E/ time. ( Hint:
Use simple data structures.)
d.Suppose that we run kphases of MST-R EDUCE , using the output G0produced
by one phase as the input Gto the next phase and accumulating edges in T.
Argue that the overall running time of the kphases is O.kE/ .
e.Suppose that after running kphases of MST-R EDUCE , as in part (d), we run
Prim’s algorithm by calling MST-P RIM.G0;c0;r/,w h e r e G0, with weight at-
tribute c0, is returned by the last phase and ris any vertex in G0:V.S h o wh o w
to pick kso that the overall running time is O.E lg lgV/. Argue that your
choice of kminimizes the overall asymptotic running time.
f.F o rw h a tv a l u e so fjEj(in terms ofjVj) does Prim’s algorithm with preprocess-
ing asymptotically beat Prim’s algorithm without preprocessing?
23-3 Bottleneck spanning tree
Abottleneck spanning tree Tof an undirected graph Gis a spanning tree of G
whose largest edge weight is minimum over all spanning trees of G. We say that
the value of the bottleneck spanning tree is the weight of the maximum-weightedge in T.
a.Argue that a minimum spanning tree is a bottleneck spanning tree.
Part (a) shows that ﬁnding a bottleneck spanning tree is no harder than ﬁnding
a minimum spanning tree. In the remaining parts, we will show how to ﬁnd abottleneck spanning tree in linear time.
b.Give a linear-time algorithm that given a graph Gand an integer b, determines
whether the value of the bottleneck spanning tree is at most b.
c.Use your algorithm for part (b) as a subroutine in a linear-time algorithm for
the bottleneck-spanning-tree problem. ( Hint: You may want to use a subroutine
that contracts sets of edges, as in the MST-R
EDUCE procedure described in
Problem 23-2.)Notes for Chapter 23 641
23-4 Alternative minimum-spanning-tree algorithms
In this problem, we give pseudocode for three different algorithms. Each one takesa connected graph and a weight function as input and returns a set of edges T.F o r
each algorithm, either prove that Tis a minimum spanning tree or prove that Tis
not a minimum spanning tree. Also describe the most efﬁcient implementation ofeach algorithm, whether or not it computes a minimum spanning tree.
a.M
AYBE -MST-A .G; w/
1 sort the edges into nonincreasing order of edge weights w
2TDE
3foreach edge e, taken in nonincreasing order by weight
4 ifT/NULfegis a connected graph
5 TDT/NULfeg
6return T
b.MAYBE -MST-B .G; w/
1TD;
2foreach edge e, taken in arbitrary order
3 ifT[feghas no cycles
4 TDT[feg
5return T
c.MAYBE -MST-C .G; w/
1TD;
2foreach edge e, taken in arbitrary order
3 TDT[feg
4 ifThas a cycle c
5l e t e0be a maximum-weight edge on c
6 TDT/NULfe0g
7return T
Chapter notes
Tarjan [330] surveys the minimum-spanning-tree problem and provides excellent
advanced material. Graham and Hell [151] compiled a history of the minimum-spanning-tree problem.
Tarjan attributes the ﬁrst minimum-spanning-tree algorithm to a 1926 paper by
O. Bor˙ uvka. Bor˙ uvka’s algorithm consists of running O.lgV/iterations of the642 Chapter 23 Minimum Spanning Trees
procedure MST-R EDUCE described in Problem 23-2. Kruskal’s algorithm was
reported by Kruskal [222] in 1956. The algorithm commonly known as Prim’salgorithm was indeed invented by Prim [285], but it was also invented earlier byV. Jarn´ ık in 1930.
The reason underlying why greedy algorithms are effective at ﬁnding minimum
spanning trees is that the set of forests of a graph forms a graphic matroid. (SeeSection 16.4.)
WhenjEjD/DEL.V lgV/, Prim’s algorithm, implemented with Fibonacci heaps,
runs in O.E/ time. For sparser graphs, using a combination of the ideas from
Prim’s algorithm, Kruskal’s algorithm, and Bor˙ uvka’s algorithm, together with ad-
vanced data structures, Fredman and Tarjan [114] give an algorithm that runs inO.E lg
/ETXV/time. Gabow, Galil, Spencer, and Tarjan [120] improved this algo-
rithm to run in O.E lg lg/ETXV/time. Chazelle [60] gives an algorithm that runs
inO.Ey˛.E;V // time, wherey˛.E;V / is the functional inverse of Ackermann’s
function. (See the chapter notes for Chapter 21 for a brief discussion of Acker-
mann’s function and its inverse.) Unlike previous minimum-spanning-tree algo-
rithms, Chazelle’s algorithm does not follow the greedy method.
A related problem is spanning-tree veriﬁcation , in which we are given a graph
GD.V; E/ and a tree T/DC2E, and we wish to determine whether Tis a minimum
spanning tree of G. King [203] gives a linear-time algorithm to verify a spanning
tree, building on earlier work of Koml´ os [215] and Dixon, Rauch, and Tarjan [90].
The above algorithms are all deterministic and fall into the comparison-based
model described in Chapter 8. Karger, Klein, and Tarjan [195] give a randomized
minimum-spanning-tree algorithm that runs in O.VCE/expected time. This
algorithm uses recursion in a manner similar to the linear-time selection algorithmin Section 9.3: a recursive call on an auxiliary problem identiﬁes a subset of theedges E
0that cannot be in any minimum spanning tree. Another recursive call
onE/NULE0then ﬁnds the minimum spanning tree. The algorithm also uses ideas
from Bor˙ uvka’s algorithm and King’s algorithm for spanning-tree veriﬁcation.
Fredman and Willard [116] showed how to ﬁnd a minimum spanning tree in
O.VCE/time using a deterministic algorithm that is not comparison based. Their
algorithm assumes that the data are b-bit integers and that the computer memory
consists of addressable b-bit words.24 Single-Source Shortest Paths
Professor Patrick wishes to ﬁnd the shortest possible route from Phoenix to Indi-
anapolis. Given a road map of the United States on which the distance betweeneach pair of adjacent intersections is marked, how can she determine this shortestroute?
One possible way would be to enumerate all the routes from Phoenix to Indi-
anapolis, add up the distances on each route, and select the shortest. It is easy tosee, however, that even disallowing routes that contain cycles, Professor Patrickwould have to examine an enormous number of possibilities, most of which aresimply not worth considering. For example, a route from Phoenix to Indianapolisthat passes through Seattle is obviously a poor choice, because Seattle is severalhundred miles out of the way.
In this chapter and in Chapter 25, we show how to solve such problems ef-
ﬁciently. In a shortest-paths problem , we are given a weighted, directed graph
GD.V; E/ , with weight function wWE! Rmapping edges to real-valued
weights. The weight w.p/ of path pDh/ETB
0;/ETB1;:::;/ETB kiis the sum of the weights
of its constituent edges:
w.p/DkX
iD1w./ETB i/NUL1;/ETBi/:
We deﬁne the shortest-path weight ı.u;/ETB/ from uto/ETBby
ı.u;/ETB/D(
minfw.p/Wup;/ETBgif there is a path from uto/ETB;
1 otherwise :
Ashortest path from vertex uto vertex /ETBis then deﬁned as any path pwith weight
w.p/Dı.u;/ETB/ .
In the Phoenix-to-Indianapolis example, we can model the road map as a graph:
vertices represent intersections, edges represent road segments between intersec-tions, and edge weights represent road distances. Our goal is to ﬁnd a shortest pathfrom a given intersection in Phoenix to a given intersection in Indianapolis.644 Chapter 24 Single-Source Shortest Paths
Edge weights can represent metrics other than distances, such as time, cost,
penalties, loss, or any other quantity that accumulates linearly along a path andthat we would want to minimize.
The breadth-ﬁrst-search algorithm from Section 22.2 is a shortest-paths algo-
rithm that works on unweighted graphs, that is, graphs in which each edge has unitweight. Because many of the concepts from breadth-ﬁrst search arise in the studyof shortest paths in weighted graphs, you might want to review Section 22.2 beforeproceeding.
Variants
In this chapter, we shall focus on the single-source shortest-paths problem :g i v e n
ag r a p h GD.V; E/ , we want to ﬁnd a shortest path from a given source vertex
s2Vto each vertex /ETB2V. The algorithm for the single-source problem can
solve many other problems, including the following variants.
Single-destination shortest-paths problem: Find a shortest path to a given des-
tination vertex tfrom each vertex /ETB. By reversing the direction of each edge in
the graph, we can reduce this problem to a single-source problem.
Single-pair shortest-path problem: Find a shortest path from uto/ETBfor given
vertices uand/ETB. If we solve the single-source problem with source vertex u,
we solve this problem also. Moreover, all known algorithms for this problemhave the same worst-case asymptotic running time as the best single-sourcealgorithms.
All-pairs shortest-paths problem: Find a shortest path from uto/ETBfor every pair
of vertices uand/ETB. Although we can solve this problem by running a single-
source algorithm once from each vertex, we usually can solve it faster. Addi-tionally, its structure is interesting in its own right. Chapter 25 addresses theall-pairs problem in detail.
Optimal substructure of a shortest path
Shortest-paths algorithms typically rely on the property that a shortest path be-
tween two vertices contains other shortest paths within it. (The Edmonds-Karpmaximum-ﬂow algorithm in Chapter 26 also relies on this property.) Recallthat optimal substructure is one of the key indicators that dynamic programming(Chapter 15) and the greedy method (Chapter 16) might apply. Dijkstra’s algo-rithm, which we shall see in Section 24.3, is a greedy algorithm, and the Floyd-Warshall algorithm, which ﬁnds shortest paths between all pairs of vertices (seeSection 25.2), is a dynamic-programming algorithm. The following lemma states
the optimal-substructure property of shortest paths more precisely.Chapter 24 Single-Source Shortest Paths 645
Lemma 24.1 (Subpaths of shortest paths are shortest paths)
Given a weighted, directed graph GD.V; E/ with weight function wWE!R,
letpDh/ETB0;/ETB1;:::;/ETB kibe a shortest path from vertex /ETB0to vertex /ETBkand, for any
iandjsuch that 0/DC4i/DC4j/DC4k,l e tpijDh/ETBi;/ETBiC1;:::;/ETB jibe the subpath of p
from vertex /ETBito vertex /ETBj. Then, pijis a shortest path from /ETBito/ETBj.
Proof If we decompose path pinto/ETB0p0i;/ETBipij;/ETBjpjk;/ETBk, then we have that
w.p/Dw.p 0i/Cw.p ij/Cw.p jk/. Now, assume that there is a path p0
ijfrom /ETBi
to/ETBjwith weight w.p0
ij/<w . p ij/. Then, /ETB0p0i;/ETBip0
ij;/ETBjpjk;/ETBkis a path from /ETB0
to/ETBkwhose weight w.p 0i/Cw.p0
ij/Cw.p jk/is less than w.p/ , which contradicts
the assumption that pis a shortest path from /ETB0to/ETBk.
Negative-weight edges
Some instances of the single-source shortest-paths problem may include edges
whose weights are negative. If the graph GD.V; E/ contains no negative-
weight cycles reachable from the source s, then for all /ETB2V, the shortest-path
weight ı.s; /ETB/ remains well deﬁned, even if it has a negative value. If the graph
contains a negative-weight cycle reachable from s, however, shortest-path weights
are not well deﬁned. No path from sto a vertex on the cycle can be a short-
est path—we can always ﬁnd a path with lower weight by following the proposed
“shortest” path and then traversing the negative-weight cycle. If there is a negative-weight cycle on some path from sto/ETB,w ed e ﬁ n e ı.s; /ETB/D/NUL1 .
Figure 24.1 illustrates the effect of negative weights and negative-weight cy-
cles on shortest-path weights. Because there is only one path from stoa(the
pathhs; ai), we have ı.s;a/Dw.s;a/D3. Similarly, there is only one path
from stob,a n ds o ı.s; b/Dw.s;a/Cw.a;b/D3C./NUL4/D/NUL1.T h e r e a r e
inﬁnitely many paths from stoc:hs; ci,hs; c; d; ci,hs; c; d; c; d; ci, and so on.
Because the cyclehc;d;cihas weight 6C./NUL3/D3>0 , the shortest path from s
toc
ishs;ci, with weight ı.s; c/Dw.s;c/D5. Similarly, the shortest path from s
todishs;c;di, with weight ı.s; d/Dw.s;c/Cw.c;d/D11. Analogously, there
are inﬁnitely many paths from stoe:hs; ei,hs; e; f; ei,hs; e; f; e; f; ei,a n ds o
on. Because the cycle he; f; eihas weight 3C./NUL6/D/NUL3<0 ,h o w e v e r ,t h e r e
is no shortest path from stoe. By traversing the negative-weight cycle he; f; ei
arbitrarily many times, we can ﬁnd paths from stoewith arbitrarily large negative
weights, and so ı.s; e/D/NUL1 . Similarly, ı.s; f /D/NUL1 . Because gis reachable
from f, we can also ﬁnd paths with arbitrarily large negative weights from stog,
and so ı.s; g/D/NUL1 . Vertices h,i,a n d jalso form a negative-weight cycle. They
are not reachable from s, however, and so ı.s; h/Dı.s; i/Dı.s; j/D1 .646 Chapter 24 Single-Source Shortest Paths
5c
11d 6
–3
–∞e
–∞f 3
–63a
–1b
0s –∞g–4
53
284
7∞h
∞i
2
∞
j–8 3
Figure 24.1 Negative edge weights in a directed graph. The shortest-path weight from source s
appears within each vertex. Because vertices eandfform a negative-weight cycle reachable from s,
they have shortest-path weights of /NUL1. Because vertex gis reachable from a vertex whose shortest-
path weight is/NUL1, it, too, has a shortest-path weight of /NUL1. Vertices such as h,i,a n d jare not
reachable from s, and so their shortest-path weights are 1, even though they lie on a negative-weight
cycle.
Some shortest-paths algorithms, such as Dijkstra’s algorithm, assume that all
edge weights in the input graph are nonnegative, as in the road-map example. Oth-
ers, such as the Bellman-Ford algorithm, allow negative-weight edges in the in-put graph and produce a correct answer as long as no negative-weight cycles arereachable from the source. Typically, if there is such a negative-weight cycle, thealgorithm can detect and report its existence.
Cycles
Can a shortest path contain a cycle? As we have just seen, it cannot contain a
negative-weight cycle. Nor can it contain a positive-weight cycle, since remov-ing the cycle from the path produces a path with the same source and destinationvertices and a lower path weight. That is, if pDh/ETB
0;/ETB1;:::;/ETB kii sap a t ha n d
cDh/ETBi;/ETBiC1;:::;/ETB jiis a positive-weight cycle on this path (so that /ETBiD/ETBjand
w.c/ > 0 ), then the path p0Dh/ETB0;/ETB1;:::;/ETB i;/ETBjC1;/ETBjC2;:::;/ETB kihas weight
w.p0/Dw.p//NULw.c/ < w.p/ ,a n ds o pcannot be a shortest path from /ETB0to/ETBk.
That leaves only 0-weight cycles. We can remove a 0-weight cycle from any
path to produce another path whose weight is the same. Thus, if there is a shortestpath from a source vertex sto a destination vertex /ETBthat contains a 0-weight cycle,
then there is another shortest path from sto/ETBwithout this cycle. As long as a
shortest path has 0-weight cycles, we can repeatedly remove these cycles from the
path until we have a shortest path that is cycle-free. Therefore, without loss of
generality we can assume that when we are ﬁnding shortest paths, they have nocycles, i.e., they are simple paths. Since any acyclic path in a graph GD.V; E/Chapter 24 Single-Source Shortest Paths 647
contains at mostjVjdistinct vertices, it also contains at most jVj/NUL1edges. Thus,
we can restrict our attention to shortest paths of at most jVj/NUL1edges.
Representing shortest paths
We often wish to compute not only shortest-path weights, but the vertices on short-
est paths as well. We represent shortest paths similarly to how we representedbreadth-ﬁrst trees in Section 22.2. Given a graph GD.V; E/ , we maintain for
each vertex /ETB2Vapredecessor /ETB:/EM that is either another vertex or
NIL.T h e
shortest-paths algorithms in this chapter set the /EMattributes so that the chain of pre-
decessors originating at a vertex /ETBruns backwards along a shortest path from sto/ETB.
Thus, given a vertex /ETBfor which /ETB:/EM¤NIL, the procedure P RINT -PATH. G ;s ;/ETB/
from Section 22.2 will print a shortest path from sto/ETB.
In the midst of executing a shortest-paths algorithm, however, the /EMvalues might
not indicate shortest paths. As in breadth-ﬁrst search, we shall be interested in thepredecessor subgraph G
/EMD.V/EM;E/EM/induced by the /EMvalues. Here again, we
deﬁne the vertex set V/EMto be the set of vertices of Gwith non- NILpredecessors,
plus the source s:
V/EMDf/ETB2VW/ETB:/EM¤NILg[fsg:
The directed edge set E/EMis the set of edges induced by the /EMvalues for vertices
inV/EM:
E/EMDf./ETB:/EM; /ETB/2EW/ETB2V/EM/NULfsgg:
We shall prove that the /EMvalues produced by the algorithms in this chapter have
the property that at termination G/EMis a “shortest-paths tree”—informally, a rooted
tree containing a shortest path from the source sto every vertex that is reachable
from s. A shortest-paths tree is like the breadth-ﬁrst tree from Section 22.2, but it
contains shortest paths from the source deﬁned in terms of edge weights instead ofnumbers of edges. To be precise, let GD.V; E/ be a weighted, directed graph
with weight function wWE!R, and assume that Gcontains no negative-weight
cycles reachable from the source vertex s2V, so that shortest paths are well
deﬁned. A shortest-paths tree rooted at sis a directed subgraph G
0D.V0;E0/,
where V0/DC2VandE0/DC2E, such that
1.V0is the set of vertices reachable from sinG,
2.G0forms a rooted tree with root s,a n d
3. for all /ETB2V0, the unique simple path from sto/ETBinG0is a shortest path from s
to/ETBinG.648 Chapter 24 Single-Source Shortest Paths
(a) (b) (c)06
67 2 1 243
53stx
yz39
51 106
67 2 1 243
53stx
yz39
51 106
67 2 1 243
53stx
yz39
51 1
Figure 24.2 (a) A weighted, directed graph with shortest-path weights from source s.(b)The
shaded edges form a shortest-paths tree rooted at the source s.(c)Another shortest-paths tree with
the same root.
Shortest paths are not necessarily unique, and neither are shortest-paths trees. For
example, Figure 24.2 shows a weighted, directed graph and two shortest-paths treeswith the same root.
Relaxation
The algorithms in this chapter use the technique of relaxation . For each vertex
/ETB2V, we maintain an attribute /ETB:d, which is an upper bound on the weight of
a shortest path from source sto/ETB. We call /ETB:dashortest-path estimate .W e
initialize the shortest-path estimates and predecessors by the following ‚.V / -time
procedure:
I
NITIALIZE -SINGLE -SOURCE .G; s/
1foreach vertex /ETB2G:V
2 /ETB:dD1
3 /ETB:/EMDNIL
4s:dD0
After initialization, we have /ETB:/EMDNILfor all /ETB2V,s:dD0,a n d /ETB:dD1 for
/ETB2V/NULfsg.
The process of relaxing an edge .u; /ETB/ consists of testing whether we can im-
prove the shortest path to /ETBfound so far by going through uand, if so, updat-
ing/ETB:dand/ETB:/EM. A relaxation step1may decrease the value of the shortest-path
1
The use of the term is historical. The outcome of a relaxation step can be viewed as a relaxation
of the constraint /ETB:d/DC4u:dCw.u;/ETB/ , which, by the triangle inequality (Lemma 24.10), must be
satisﬁed if u:dDı.s; u/ and/ETB:dDı.s; /ETB/ .T h a ti s ,i f /ETB:d/DC4u:dCw.u;/ETB/ , there is no “pressure”It may seem strange that the term “relaxation” is used for an operation that tightens an upper bound.
so the constraint is “relaxed.” to satisfy this constraint,Chapter 24 Single-Source Shortest Paths 649
uv
592
uv
572RELAX (u,v,w)
(a) (b)uv
562
uv
562RELAX (u,v,w)
Figure 24.3 Relaxing an edge .u; /ETB/ with weight w.u;/ETB/D2. The shortest-path estimate of each
vertex appears within the vertex. (a)Because /ETB:d>u : dCw.u;/ETB/ prior to relaxation, the value
of/ETB:ddecreases. (b)Here, /ETB:d/DC4u:dCw.u;/ETB/ before relaxing the edge, and so the relaxation step
leaves /ETB:dunchanged.
estimate /ETB:dand update /ETB’s predecessor attribute /ETB:/EM. The following code per-
forms a relaxation step on edge .u; /ETB/ inO.1/ time:
RELAX . u ;/ETB;w/
1if/ETB:d>u : dCw.u;/ETB/
2 /ETB:dDu:dCw.u;/ETB/
3 /ETB:/EMDu
Figure 24.3 shows two examples of relaxing an edge, one in which a shortest-path
estimate decreases and one in which no estimate changes.
Each algorithm in this chapter calls I NITIALIZE -SINGLE -SOURCE a n dt h e nr e -
peatedly relaxes edges. Moreover, relaxation is the only means by which shortest-
path estimates and predecessors change. The algorithms in this chapter differ in
how many times they relax each edge and the order in which they relax edges. Dijk-
stra’s algorithm and the shortest-paths algorithm for directed acyclic graphs relaxeach edge exactly once. The Bellman-Ford algorithm relaxes each edge jVj/NUL1
times.
Properties of shortest paths and relaxation
To prove the algorithms in this chapter correct, we shall appeal to several prop-
erties of shortest paths and relaxation. We state these properties here, and Sec-tion 24.5 proves them formally. For your reference, each property stated here in-cludes the appropriate lemma or corollary number from Section 24.5. The latterﬁve of these properties, which refer to shortest-path estimates or the predecessorsubgraph, implicitly assume that the graph is initialized with a call to I
NITIALIZE -
SINGLE -SOURCE .G; s/ and that the only way that shortest-path estimates and the
predecessor subgraph change are by some sequence of relaxation steps.650 Chapter 24 Single-Source Shortest Paths
Triangle inequality (Lemma 24.10)
For any edge .u; /ETB/2E,w eh a v e ı.s; /ETB//DC4ı.s; u/Cw.u;/ETB/ .
Upper-bound property (Lemma 24.11)
We always have /ETB:d/NAKı.s; /ETB/ for all vertices /ETB2V, and once /ETB:dachieves the
value ı.s; /ETB/ , it never changes.
No-path property (Corollary 24.12)
If there is no path from sto/ETB,t h e nw ea l w a y sh a v e /ETB:dDı.s; /ETB/D1 .
Convergence property (Lemma 24.14)
Ifs;u!/ETBis a shortest path in Gfor some u; /ETB2V,a n di f u:dDı.s; u/ at
any time prior to relaxing edge .u; /ETB/ ,t h e n /ETB:dDı.s; /ETB/ at all times afterward.
Path-relaxation property (Lemma 24.15)
IfpDh/ETB0;/ETB1;:::;/ETB kiis a shortest path from sD/ETB0to/ETBk, and we relax the
edges of pin the order ./ETB0;/ETB1/; ./ETB 1;/ETB2/ ;:::;. /ETB k/NUL1;/ETBk/,t h e n /ETBk:dDı.s; /ETB k/.
This property holds regardless of any other relaxation steps that occur, even ifthey are intermixed with relaxations of the edges of p.
Predecessor-subgraph property (Lemma 24.17)
Once /ETB:dDı.s; /ETB/ for all /ETB2V, the predecessor subgraph is a shortest-paths
tree rooted at s.
Chapter outline
Section 24.1 presents the Bellman-Ford algorithm, which solves the single-source
shortest-paths problem in the general case in which edges can have negative weight.The Bellman-Ford algorithm is remarkably simple, and it has the further beneﬁtof detecting whether a negative-weight cycle is reachable from the source. Sec-
tion 24.2 gives a linear-time algorithm for computing shortest paths from a single
source in a directed acyclic graph. Section 24.3 covers Dijkstra’s algorithm, whichhas a lower running time than the Bellman-Ford algorithm but requires the edgeweights to be nonnegative. Section 24.4 shows how we can use the Bellman-Fordalgorithm to solve a special case of linear programming. Finally, Section 24.5proves the properties of shortest paths and relaxation stated above.
We require some conventions for doing arithmetic with inﬁnities. We shall as-
sume that for any real number a¤/NUL1 ,w eh a v e aC1D1C aD1 . Also, to
make our proofs hold in the presence of negative-weight cycles, we shall assumethat for any real number a¤1 ,w eh a v e aC./NUL1/D./NUL1/CaD/NUL1 .
All algorithms in this chapter assume that the directed graph Gis stored in the
adjacency-list representation. Additionally, stored with each edge is its weight, sothat as we traverse each adjacency list, we can determine the edge weights in O.1/
time per edge.24.1 The Bellman-Ford algorithm 651
24.1 The Bellman-Ford algorithm
TheBellman-Ford algorithm solves the single-source shortest-paths problem in
the general case in which edge weights may be negative. Given a weighted, di-rected graph GD.V; E/ with source sand weight function wWE! R,t h e
Bellman-Ford algorithm returns a boolean value indicating whether or not there is
a negative-weight cycle that is reachable from the source. If there is such a cy-
cle, the algorithm indicates that no solution exists. If there is no such cycle, thealgorithm produces the shortest paths and their weights.
The algorithm relaxes edges, progressively decreasing an estimate /ETB:don the
weight of a shortest path from the source sto each vertex /ETB2Vuntil it achieves
the actual shortest-path weight ı.s; /ETB/ . The algorithm returns
TRUE if and only if
the graph contains no negative-weight cycles that are reachable from the source.
BELLMAN -FORD. G ;w;s/
1I NITIALIZE -SINGLE -SOURCE .G; s/
2foriD1tojG:Vj/NUL1
3 foreach edge .u; /ETB/2G:E
4R ELAX . u ;/ETB;w/
5foreach edge .u; /ETB/2G:E
6 if/ETB:d>u : dCw.u;/ETB/
7 return FALSE
8return TRUE
Figure 24.4 shows the execution of the Bellman-Ford algorithm on a graph
with 5vertices. After initializing the dand/EMvalues of all vertices in line 1,
the algorithm makes jVj/NUL1passes over the edges of the graph. Each pass is
one iteration of the forloop of lines 2–4 and consists of relaxing each edge of the
graph once. Figures 24.4(b)–(e) show the state of the algorithm after each of thefour passes over the edges. After making jVj/NUL1passes, lines 5–8 check for a
negative-weight cycle and return the appropriate boolean value. (We’ll see a littlelater why this check works.)
The Bellman-Ford algorithm runs in time O.VE/ , since the initialization in
line 1 takes ‚.V / time, each of thejVj/NUL1passes over the edges in lines 2–4
takes ‚.E/ time, and the forloop of lines 5–7 takes O.E/ time.
To prove the correctness of the Bellman-Ford algorithm, we start by showing that
if there are no negative-weight cycles, the algorithm computes correct shortest-pathweights for all vertices reachable from the source.652 Chapter 24 Single-Source Shortest Paths
(a) (b) (c)
(d)05
97 86
7
(e)tx
s
yz –4  – 3  – 2 2
74
–2205
97 86
7tx
s
yz –4  – 3  – 2 2
74
2205
97 86
7tx
s
yz –4  – 3  – 2 6
74
2205
97 86
7tx
s
yz –4  – 3  – 2 6
7∞
∞205
97 86
7tx
s
yz –4  – 3  – 2 ∞
∞2∞
∞
Figure 24.4 The execution of the Bellman-Ford algorithm. The source is vertex s.T h e dval-
ues appear within the vertices, and shaded edges indicate predecessor values: if edge .u; /ETB/ is
shaded, then /ETB:/EMDu. In this particular example, each pass relaxes the edges in the order
.t; x/; .t; y/; .t; ´/; .x; t/; .y; x/; .y; ´/; .´; x/; .´; s/; .s; t/; .s; y/ .(a)The situation just before the
ﬁrst pass over the edges. (b)–(e) The situation after each successive pass over the edges. The d
and/EMvalues in part (e) are the ﬁnal values. The Bellman-Ford algorithm returns TRUE in this
example.
Lemma 24.2
LetGD.V; E/ be a weighted, directed graph with source sand weight func-
tionwWE! R, and assume that Gcontains no negative-weight cycles that are
reachable from s. Then, after thejVj/NUL1iterations of the forloop of lines 2–4
of B ELLMAN -FORD,w eh a v e /ETB:dDı.s; /ETB/ for all vertices /ETBthat are reachable
from s.
Proof We prove the lemma by appealing to the path-relaxation property. Con-
sider any vertex /ETBthat is reachable from s,a n dl e t pDh/ETB0;/ETB1;:::;/ETB ki,w h e r e
/ETB0Dsand/ETBkD/ETB, be any shortest path from sto/ETB. Because shortest paths are
simple, phas at mostjVj/NUL1edges, and so k/DC4jVj/NUL1. Each of thejVj/NUL1itera-
tions of the forloop of lines 2–4 relaxes all jEjedges. Among the edges relaxed in
theith iteration, for iD1 ;2;:::;k ,i s./ETBi/NUL1;/ETBi/. By the path-relaxation property,
therefore, /ETB:dD/ETBk:dDı.s; /ETB k/Dı.s; /ETB/ .
24.1 The Bellman-Ford algorithm 653
Corollary 24.3
LetGD.V; E/ be a weighted, directed graph with source vertex sand weight
function wWE! R, and assume that Gcontains no negative-weight cycles that
are reachable from s. Then, for each vertex /ETB2V, there is a path from sto/ETBif
and only if B ELLMAN -FORD terminates with /ETB:d<1when it is run on G.
Proof The proof is left as Exercise 24.1-2.
Theorem 24.4 (Correctness of the Bellman-Ford algorithm)
Let B ELLMAN -FORD be run on a weighted, directed graph GD.V; E/ with
source sand weight function wWE!R.I fGcontains no negative-weight cycles
that are reachable from s, then the algorithm returns TRUE ,w eh a v e /ETB:dDı.s; /ETB/
for all vertices /ETB2V, and the predecessor subgraph G/EMis a shortest-paths tree
rooted at s.I fGdoes contain a negative-weight cycle reachable from s, then the
algorithm returns FALSE .
Proof Suppose that graph Gcontains no negative-weight cycles that are reach-
able from the source s. We ﬁrst prove the claim that at termination, /ETB:dDı.s; /ETB/
for all vertices /ETB2V. If vertex /ETBis reachable from s, then Lemma 24.2 proves this
claim. If /ETBis not reachable from s, then the claim follows from the no-path prop-
erty. Thus, the claim is proven. The predecessor-subgraph property, along with theclaim, implies that G
/EMis a shortest-paths tree. Now we use the claim to show that
BELLMAN -FORD returns TRUE . At termination, we have for all edges .u; /ETB/2E,
/ETB:dDı.s; /ETB/
/DC4ı.s; u/Cw.u;/ETB/ (by the triangle inequality)
Du:dCw.u;/ETB/ ;
and so none of the tests in line 6 causes B ELLMAN -FORD to return FALSE .T h e r e -
fore, it returns TRUE .
Now, suppose that graph Gcontains a negative-weight cycle that is reachable
from the source s; let this cycle be cDh/ETB0;/ETB1;:::;/ETB ki,w h e r e /ETB0D/ETBk. Then,
kX
iD1w./ETB i/NUL1;/ETBi/<0: (24.1)
Assume for the purpose of contradiction that the Bellman-Ford algorithm returns
TRUE . Thus, /ETBi:d/DC4/ETBi/NUL1:dCw./ETB i/NUL1;/ETBi/foriD1 ;2;:::;k . Summing the
inequalities around cycle cgives us654 Chapter 24 Single-Source Shortest Paths
kX
iD1/ETBi:d/DC4kX
iD1./ETBi/NUL1:dCw./ETB i/NUL1;/ETBi//
DkX
iD1/ETBi/NUL1:dCkX
iD1w./ETB i/NUL1;/ETBi/:
Since /ETB0D/ETBk, each vertex in cappears exactly once in each of the summationsPk
iD1/ETBi:dandPk
iD1/ETBi/NUL1:d,a n ds o
kX
iD1/ETBi:dDkX
iD1/ETBi/NUL1:d:
Moreover, by Corollary 24.3, /ETBi:dis ﬁnite for iD1 ;2;:::;k . Thus,
0/DC4kX
iD1w./ETB i/NUL1;/ETBi/;
which contradicts inequality (24.1). We conclude that the Bellman-Ford algorithm
returns TRUE if graph Gcontains no negative-weight cycles reachable from the
source, and FALSE otherwise.
Exercises
24.1-1
Run the Bellman-Ford algorithm on the directed graph of Figure 24.4, using ver-tex´as the source. In each pass, relax edges in the same order as in the ﬁgure, and
show the dand/EMvalues after each pass. Now, change the weight of edge .´; x/
to4and run the algorithm again, using sas the source.
24.1-2
Prove Corollary 24.3.
24.1-3
Given a weighted, directed graph GD.V; E/ with no negative-weight cycles,
letmbe the maximum over all vertices /ETB2Vof the minimum number of edges
in a shortest path from the source sto/ETB. (Here, the shortest path is by weight, not
the number of edges.) Suggest a simple change to the Bellman-Ford algorithm that
allows it to terminate in mC1passes, even if mis not known in advance.
24.1-4
Modify the Bellman-Ford algorithm so that it sets /ETB:dto/NUL1 for all vertices /ETBfor
which there is a negative-weight cycle on some path from the source to /ETB.24.2 Single-source shortest paths in directed acyclic graphs 655
24.1-5 ?
LetGD.V; E/ be a weighted, directed graph with weight function wWE! R.
Give an O.VE/ -time algorithm to ﬁnd, for each vertex /ETB2V,t h ev a l u e ı/ETX./ETB/D
min u2Vfı.u;/ETB/g.
24.1-6 ?
Suppose that a weighted, directed graph GD.V; E/ has a negative-weight cycle.
Give an efﬁcient algorithm to list the vertices of one such cycle. Prove that youralgorithm is correct.
24.2 Single-source shortest paths in directed acyclic graphs
By relaxing the edges of a weighted dag (directed acyclic graph) GD.V; E/
according to a topological sort of its vertices, we can compute shortest paths froma single source in ‚.VCE/time. Shortest paths are always well deﬁned in a dag,
since even if there are negative-weight edges, no negative-weight cycles can exist.
The algorithm starts by topologically sorting the dag (see Section 22.4) to im-
pose a linear ordering on the vertices. If the dag contains a path from vertex uto
vertex /ETB,t h e n uprecedes /ETBin the topological sort. We make just one pass over the
vertices in the topologically sorted order. As we process each vertex, we relax eachedge that leaves the vertex.
D
AG-SHORTEST -PATHS . G ;w;s/
1 topologically sort the vertices of G
2I NITIALIZE -SINGLE -SOURCE .G; s/
3foreach vertex u, taken in topologically sorted order
4 foreach vertex /ETB2G:AdjŒu/c141
5R ELAX . u ;/ETB;w/
Figure 24.5 shows the execution of this algorithm.
The running time of this algorithm is easy to analyze. As shown in Section 22.4,
the topological sort of line 1 takes ‚.VCE/time. The call of I NITIALIZE -
SINGLE -SOURCE in line 2 takes ‚.V / time. The forloop of lines 3–5 makes one
iteration per vertex. Altogether, the forloop of lines 4–5 relaxes each edge exactly
once. (We have used an aggregate analysis here.) Because each iteration of theinnerforloop takes ‚.1/ time, the total running time is ‚.VCE/, which is linear
in the size of an adjacency-list representation of the graph.
The following theorem shows that the D
AG-SHORTEST -PATHS procedure cor-
rectly computes the shortest paths.656 Chapter 24 Single-Source Shortest Paths
2∞∞ 051 6
34∞ ∞ ∞7– 1 – 2
2
(a)x t s ry z
2 51 6
347– 1 – 2
2
(c)x t s ry z
2 51 6
347– 1 – 2
2
(e)x t s ry z
2 51 6
347– 1 – 2
2
(g)x t s ry z2 51 6
347– 1 – 2
2
(b)x t s ry z
2 51 6
347– 1 – 2
2
(d)x t s ry z
2 51 6
347– 1 – 2
2
(f)x t s ry z∞ 0 ∞ ∞ 26
∞ 0 26 54
∞ 0 2653∞ 0 265 3∞ 0 2 664∞ ∞ 0 ∞ ∞ ∞
Figure 24.5 The execution of the algorithm for shortest paths in a directed acyclic graph. The
vertices are topologically sorted from left to right. The source vertex is s.T h e dvalues appear
within the vertices, and shaded edges indicate the /EMvalues. (a)The situation before the ﬁrst iteration
of the forloop of lines 3–5. (b)–(g) The situation after each iteration of the forloop of lines 3–5.
The newly blackened vertex in each iteration was used as uin that iteration. The values shown in
part (g) are the ﬁnal values.
Theorem 24.5
If a weighted, directed graph GD.V; E/ has source vertex sand no cycles, then
at the termination of the D AG-SHORTEST -PATHS procedure, /ETB:dDı.s; /ETB/ for all
vertices /ETB2V, and the predecessor subgraph G/EMis a shortest-paths tree.
Proof We ﬁrst show that /ETB:dDı.s; /ETB/ for all vertices /ETB2Vat termina-
tion. If /ETBis not reachable from s,t h e n /ETB:dDı.s; /ETB/D1 by the no-path
property. Now, suppose that /ETBis reachable from s, so that there is a short-
est path pDh/ETB0;/ETB1;:::;/ETB ki,w h e r e /ETB0Dsand/ETBkD/ETB. Because we pro-24.2 Single-source shortest paths in directed acyclic graphs 657
cess the vertices in topologically sorted order, we relax the edges on pin the
order ./ETB0;/ETB1/; ./ETB 1;/ETB2/ ;:::;. /ETB k/NUL1;/ETBk/. The path-relaxation property implies that
/ETBi:dDı.s; /ETB i/at termination for iD0; 1; : : : ; k . Finally, by the predecessor-
subgraph property, G/EMis a shortest-paths tree.
An interesting application of this algorithm arises in determining critical paths
inPERT chart2analysis. Edges represent jobs to be performed, and edge weights
represent the times required to perform particular jobs. If edge .u; /ETB/ enters ver-
tex/ETBand edge ./ETB; x/ leaves /ETB, then job .u; /ETB/ must be performed before job ./ETB; x/ .
A path through this dag represents a sequence of jobs that must be performed in aparticular order. A critical path is alongest path through the dag, corresponding
to the longest time to perform any sequence of jobs. Thus, the weight of a criticalpath provides a lower bound on the total time to perform all the jobs. We can ﬁnd
a critical path by either
/SInegating the edge weights and running D AG-SHORTEST -PATHS ,o r
/SIrunning D AG-SHORTEST -PATHS , with the modiﬁcation that we replace “ 1”
by “/NUL1” in line 2 of I NITIALIZE -SINGLE -SOURCE and “ >”b y“ <”i nt h e
RELAX procedure.
Exercises
24.2-1
Run D AG-SHORTEST -PATHS on the directed graph of Figure 24.5, using vertex r
as the source.
24.2-2
Suppose we change line 3 of D AG-SHORTEST -PATHS to read
3forthe ﬁrstjVj/NUL1vertices, taken in topologically sorted order
Show that the procedure would remain correct.
24.2-3
The PERT chart formulation given above is somewhat unnatural. In a more natu-ral structure, vertices would represent jobs and edges would represent sequencingconstraints; that is, edge .u; /ETB/ would indicate that job umust be performed before
job/ETB. We would then assign weights to vertices, not edges. Modify the D
AG-
SHORTEST -PATHS procedure so that it ﬁnds a longest path in a directed acyclic
graph with weighted vertices in linear time.
2“PERT” is an acronym for “program evaluation and review technique.”658 Chapter 24 Single-Source Shortest Paths
24.2-4
Give an efﬁcient algorithm to count the total number of paths in a directed acyclicgraph. Analyze your algorithm.
24.3 Dijkstra’s algorithm
Dijkstra’s algorithm solves the single-source shortest-paths problem on a weighted,directed graph GD.V; E/ for the case in which all edge weights are nonnegative.
In this section, therefore, we assume that w.u;/ETB//NAK0for each edge .u; /ETB/2E.A s
we shall see, with a good implementation, the running time of Dijkstra’s algorithmis lower than that of the Bellman-Ford algorithm.
Dijkstra’s algorithm maintains a set Sof vertices whose ﬁnal shortest-path
weights from the source shave already been determined. The algorithm repeat-
edly selects the vertex u2V/NULSwith the minimum shortest-path estimate, adds u
toS, and relaxes all edges leaving u. In the following implementation, we use a
min-priority queue Qof vertices, keyed by their dvalues.
D
IJKSTRA . G ;w;s/
1I NITIALIZE -SINGLE -SOURCE .G; s/
2SD;
3QDG:V
4while Q¤;
5 uDEXTRACT -MIN.Q/
6 SDS[fug
7 foreach vertex /ETB2G:AdjŒu/c141
8R ELAX . u ;/ETB;w/
Dijkstra’s algorithm relaxes edges as shown in Figure 24.6. Line 1 initializes
thedand/EMvalues in the usual way, and line 2 initializes the set Sto the empty
set. The algorithm maintains the invariant that QDV/NULSat the start of each
iteration of the while loop of lines 4–8. Line 3 initializes the min-priority queue Q
to contain all the vertices in V;s i n c e SD; at that time, the invariant is true after
line 3. Each time through the while loop of lines 4–8, line 5 extracts a vertex ufrom
QDV/NULSand line 6 adds it to set S, thereby maintaining the invariant. (The ﬁrst
time through this loop, uDs.) Vertex u, therefore, has the smallest shortest-path
estimate of any vertex in V/NULS. Then, lines 7–8 relax each edge .u; /ETB/ leaving u,
thus updating the estimate /ETB:dand the predecessor /ETB:/EM if we can improve the
shortest path to /ETBfound so far by going through u. Observe that the algorithm
never inserts vertices into Qafter line 3 and that each vertex is extracted from Q24.3 Dijkstra’s algorithm 659
0∞∞
∞∞0∞∞1
210
5
(c)10
508
514
7
08
513
70859
70
59
786 4 3 29
7stx
yz
1
210
5
(f)6 4 3 29
7stx
yz1
210
5
(b)6 4 3 29
7stx
yz
1
210
5
(e)6 4 3 29
7stx
yz1
210
5
(a)6 4 3 29
7stx
yz
1
210
5
(d)6 4 3 29
7stx
yz
Figure 24.6 The execution of Dijkstra’s algorithm. The source sis the leftmost vertex. The
shortest-path estimates appear within the vertices, and shaded edges indicate predecessor values.Black vertices are in the set S, and white vertices are in the min-priority queue QDV/NULS.(a)The
situation just before the ﬁrst iteration of the while loop of lines 4–8. The shaded vertex has the mini-
mum dvalue and is chosen as vertex uin line 5. (b)–(f) The situation after each successive iteration
of the while loop. The shaded vertex in each part is chosen as vertex uin line 5 of the next iteration.
Thedvalues and predecessors shown in part (f) are the ﬁnal values.
and added to Sexactly once, so that the while loop of lines 4–8 iterates exactly jVj
times.
Because Dijkstra’s algorithm always chooses the “lightest” or “closest” vertex
inV/NULSto add to set S, we say that it uses a greedy strategy. Chapter 16 explains
greedy strategies in detail, but you need not have read that chapter to understandDijkstra’s algorithm. Greedy strategies do not always yield optimal results in gen-eral, but as the following theorem and its corollary show, Dijkstra’s algorithm doesindeed compute shortest paths. The key is to show that each time it adds a vertex u
to set S,w eh a v e u:dDı.s; u/ .
Theorem 24.6 (Correctness of Dijkstra’s algorithm)
Dijkstra’s algorithm, run on a weighted, directed graph GD.V; E/ with non-
negative weight function wand source s, terminates with u:dDı.s; u/ for all
vertices u2V.660 Chapter 24 Single-Source Shortest Paths
p1Sp2u
ys
x
Figure 24.7 The proof of Theorem 24.6. Set Sis nonempty just before vertex uis added to it. We
decompose a shortest path pfrom source sto vertex uintosp1;x!yp2;u,w h e r e yis the ﬁrst
vertex on the path that is not in Sandx2Simmediately precedes y. Vertices xandyare distinct,
but we may have sDxoryDu.P a t h p2may or may not reenter set S.
Proof We use the following loop invariant:
At the start of each iteration of the while loop of lines 4–8, /ETB:dDı.s; /ETB/
for each vertex /ETB2S.
It sufﬁces to show for each vertex u2V,w eh a v e u:dDı.s; u/ at the time when u
is added to set S. Once we show that u:dDı.s; u/ , we rely on the upper-bound
property to show that the equality holds at all times thereafter.
Initialization: Initially, SD;, and so the invariant is trivially true.
Maintenance: We wish to show that in each iteration, u:dDı.s; u/ for the vertex
added to set S. For the purpose of contradiction, let ube the ﬁrst vertex for
which u:d¤ı.s; u/ when it is added to set S. We shall focus our attention
on the situation at the beginning of the iteration of the while loop in which u
is added to Sand derive the contradiction that u:dDı.s; u/ at that time by
examining a shortest path from stou.W e m u s t h a v e u¤sbecause sis the
ﬁrst vertex added to set Sands:dDı.s; s/D0at that time. Because u¤s,
we also have that S¤; just before uis added to S. There must be some
path from stou, for otherwise u:dDı.s; u/D1 by the no-path property,
which would violate our assumption that u:d¤ı.s; u/ . Because there is at
least one path, there is a shortest path pfrom stou. Prior to adding utoS,
pathpconnects a vertex in S, namely s, to a vertex in V/NULS, namely u.L e tu s
consider the ﬁrst vertex yalong psuch that y2V/NULS,a n dl e t x2Sbey’s
predecessor along p. Thus, as Figure 24.7 illustrates, we can decompose path p
intosp1;x!yp2;u. (Either of paths p1orp2m a yh a v en oe d g e s . )
We claim that y:dDı.s; y/ when uis added to S. To prove this claim, ob-
serve that x2S. Then, because we chose uas the ﬁrst vertex for which
u:d¤ı.s; u/ when it is added to S,w eh a d x:dDı.s; x/ when xwas added24.3 Dijkstra’s algorithm 661
toS. Edge .x; y/ was relaxed at that time, and the claim follows from the
convergence property.
We can now obtain a contradiction to prove that u:dDı.s; u/ . Because y
appears before uon a shortest path from stouand all edge weights are non-
negative (notably those on path p2), we have ı.s; y//DC4ı.s; u/ , and thus
y:dDı.s; y/
/DC4ı.s; u/ (24.2)
/DC4u:d (by the upper-bound property) .
But because both vertices uandywere in V/NULSwhen uwas chosen in line 5,
we have u:d/DC4y:d. Thus, the two inequalities in (24.2) are in fact equalities,
giving
y:dDı.s; y/Dı.s; u/Du:d:
Consequently, u:dDı.s; u/ , which contradicts our choice of u. We conclude
thatu:dDı.s; u/ when uis added to S, and that this equality is maintained at
all times thereafter.
Termination: At termination, QD; which, along with our earlier invariant that
QDV/NULS, implies that SDV. Thus, u:dDı.s; u/ for all vertices u2V.
Corollary 24.7
If we run Dijkstra’s algorithm on a weighted, directed graph GD.V; E/ with
nonnegative weight function wand source s, then at termination, the predecessor
subgraph G/EMis a shortest-paths tree rooted at s.
Proof Immediate from Theorem 24.6 and the predecessor-subgraph property.
Analysis
How fast is Dijkstra’s algorithm? It maintains the min-priority queue Qby call-
ing three priority-queue operations: I NSERT (implicit in line 3), E XTRACT -MIN
(line 5), and D ECREASE -KEY(implicit in R ELAX , which is called in line 8). The
algorithm calls both I NSERT and E XTRACT -MINonce per vertex. Because each
vertex u2Vis added to set Sexactly once, each edge in the adjacency list AdjŒu/c141
is examined in the forloop of lines 7–8 exactly once during the course of the al-
gorithm. Since the total number of edges in all the adjacency lists is jEj,t h i sfor
loop iterates a total of jEjtimes, and thus the algorithm calls D ECREASE -KEYat
mostjEjtimes overall. (Observe once again that we are using aggregate analysis.)
The running time of Dijkstra’s algorithm depends on how we implement the
min-priority queue. Consider ﬁrst the case in which we maintain the min-priority662 Chapter 24 Single-Source Shortest Paths
queue by taking advantage of the vertices being numbered 1 to jVj.W e s i m p l y
store /ETB:din the /ETBth entry of an array. Each I NSERT and D ECREASE -KEYoperation
takes O.1/ time, and each E XTRACT -MINoperation takes O.V / t i m e( s i n c ew e
have to search through the entire array), for a total time of O.V2CE/DO.V2/.
If the graph is sufﬁciently sparse—in particular, EDo.V2=lgV/—we can
improve the algorithm by implementing the min-priority queue with a binary min-heap. (As discussed in Section 6.5, the implementation should make sure thatvertices and corresponding heap elements maintain handles to each other.) Each
E
XTRACT -MINoperation then takes time O.lgV/. As before, there are jVjsuch
operations. The time to build the binary min-heap is O.V / . Each D ECREASE -KEY
operation takes time O.lgV/, and there are still at most jEjsuch operations. The
total running time is therefore O..VCE/lgV/,w h i c hi s O.E lgV/if all vertices
are reachable from the source. This running time improves upon the straightfor-ward O.V
2/-time implementation if EDo.V2=lgV/.
We can in fact achieve a running time of O.V lgVCE/by implementing the
min-priority queue with a Fibonacci heap (see Chapter 19). The amortized cost
of each of thejVjEXTRACT -MINoperations is O.lgV/, and each D ECREASE -
KEYcall, of which there are at most jEj, takes only O.1/ amortized time. His-
torically, the development of Fibonacci heaps was motivated by the observationthat Dijkstra’s algorithm typically makes many more D
ECREASE -KEYcalls than
EXTRACT -MINcalls, so that any method of reducing the amortized time of each
DECREASE -KEYoperation to o.lgV/without increasing the amortized time of
EXTRACT -MINwould yield an asymptotically faster implementation than with bi-
nary heaps.
Dijkstra’s algorithm resembles both breadth-ﬁrst search (see Section 22.2) and
Prim’s algorithm for computing minimum spanning trees (see Section 23.2). It islike breadth-ﬁrst search in that set Scorresponds to the set of black vertices in a
breadth-ﬁrst search; just as vertices in Shave their ﬁnal shortest-path weights, so
do black vertices in a breadth-ﬁrst search have their correct breadth-ﬁrst distances.Dijkstra’s algorithm is like Prim’s algorithm in that both algorithms use a min-priority queue to ﬁnd the “lightest” vertex outside a given set (the set Sin Dijkstra’s
algorithm and the tree being grown in Prim’s algorithm), add this vertex into theset, and adjust the weights of the remaining vertices outside the set accordingly.
Exercises
24.3-1
Run Dijkstra’s algorithm on the directed graph of Figure 24.2, ﬁrst using vertex s
as the source and then using vertex ´as the source. In the style of Figure 24.6,
show the dand/EMvalues and the vertices in set Safter each iteration of the while
loop.24.3 Dijkstra’s algorithm 663
24.3-2
Give a simple example of a directed graph with negative-weight edges for whichDijkstra’s algorithm produces incorrect answers. Why doesn’t the proof of Theo-rem 24.6 go through when negative-weight edges are allowed?
24.3-3
Suppose we change line 4 of Dijkstra’s algorithm to the following.
4whilejQj>1
This change causes the while loop to executejVj/NUL1times instead ofjVjtimes. Is
this proposed algorithm correct?
24.3-4
Professor Gaedel has written a program that he claims implements Dijkstra’s al-gorithm. The program produces /ETB:dand/ETB:/EM for each vertex /ETB2V.G i v e a n
O.VCE/-time algorithm to check the output of the professor’s program. It should
determine whether the dand/EMattributes match those of some shortest-paths tree.
You may assume that all edge weights are nonnegative.
24.3-5
Professor Newman thinks that he has worked out a simpler proof of correctnessfor Dijkstra’s algorithm. He claims that Dijkstra’s algorithm relaxes the edges ofevery shortest path in the graph in the order in which they appear on the path, andtherefore the path-relaxation property applies to every vertex reachable from thesource. Show that the professor is mistaken by constructing a directed graph for
which Dijkstra’s algorithm could relax the edges of a shortest path out of order.
24.3-6
We are given a directed graph GD.V; E/ on which each edge .u; /ETB/2Ehas an
associated value r.u;/ETB/ , which is a real number in the range 0/DC4r.u;/ETB//DC41that
represents the reliability of a communication channel from vertex uto vertex /ETB.
We interpret r.u;/ETB/ as the probability that the channel from uto/ETBwill not fail,
and we assume that these probabilities are independent. Give an efﬁcient algorithmto ﬁnd the most reliable path between two given vertices.
24.3-7
LetGD.V; E/ be a weighted, directed graph with positive weight function
wWE!f1 ;2;:::;Wgfor some positive integer W, and assume that no two ver-
tices have the same shortest-path weights from source vertex s. Now suppose that
we deﬁne an unweighted, directed graph G
0D.V[V0;E0/by replacing each
edge .u; /ETB/2Ewith w.u;/ETB/ unit-weight edges in series. How many vertices
does G0have? Now suppose that we run a breadth-ﬁrst search on G0. Show that664 Chapter 24 Single-Source Shortest Paths
the order in which the breadth-ﬁrst search of G0colors vertices in Vblack is the
same as the order in which Dijkstra’s algorithm extracts the vertices of Vfrom the
priority queue when it runs on G.
24.3-8
LetGD.V; E/ be a weighted, directed graph with nonnegative weight function
wWE!f0; 1; : : : ; Wgfor some nonnegative integer W. Modify Dijkstra’s algo-
rithm to compute the shortest paths from a given source vertex sinO.W VCE/
time.
24.3-9
Modify your algorithm from Exercise 24.3-8 to run in O..VCE/lgW/time.
(Hint: How many distinct shortest-path estimates can there be in V/NULSat any
point in time?)
24.3-10
Suppose that we are given a weighted, directed graph GD.V; E/ in which edges
that leave the source vertex smay have negative weights, all other edge weights
are nonnegative, and there are no negative-weight cycles. Argue that Dijkstra’salgorithm correctly ﬁnds shortest paths from sin this graph.
24.4 Difference constraints and shortest paths
Chapter 29 studies the general linear-programming problem, in which we wish to
optimize a linear function subject to a set of linear inequalities. In this section, weinvestigate a special case of linear programming that we reduce to ﬁnding shortestpaths from a single source. We can then solve the single-source shortest-pathsproblem that results by running the Bellman-Ford algorithm, thereby also solvingthe linear-programming problem.
Linear programming
In the general linear-programming problem ,w ea r eg i v e na n m/STXnmatrix A,
anm-vector b,a n da n n-vector c. W ew i s ht oﬁ n dav e c t o r xofnelements that
maximizes the objective functionP
n
iD1cixisubject to the mconstraints given by
Ax/DC4b.
Although the simplex algorithm, which is the focus of Chapter 29, does not
always run in time polynomial in the size of its input, there are other linear-programming algorithms that do run in polynomial time. We offer here two reasons
to understand the setup of linear-programming problems. First, if we know that we24.4 Difference constraints and shortest paths 665
can cast a given problem as a polynomial-sized linear-programming problem, then
we immediately have a polynomial-time algorithm to solve the problem. Second,faster algorithms exist for many special cases of linear programming. For exam-ple, the single-pair shortest-path problem (Exercise 24.4-4) and the maximum-ﬂowproblem (Exercise 26.1-5) are special cases of linear programming.
Sometimes we don’t really care about the objective function; we just wish to ﬁnd
anyfeasible solution , that is, any vector xthat satisﬁes Ax/DC4b, or to determine
that no feasible solution exists. We shall focus on one such feasibility problem .
Systems of difference constraints
In asystem of difference constraints , each row of the linear-programming matrix A
contains one 1and one/NUL1, and all other entries of Aare0. Thus, the constraints
given by Ax/DC4bare a set of mdifference constraints involving nunknowns, in
which each constraint is a simple linear inequality of the form
x
j/NULxi/DC4bk;
where 1/DC4i;j/DC4n,i¤j,a n d 1/DC4k/DC4m.
For example, consider the problem of ﬁnding a 5-vector xD.xi/that satisﬁes /EOT
1/NUL1000
1000/NUL1
0100/NUL1
/NUL10100
/NUL10010
00/NUL110
00/NUL101
000/NUL11˘
ˇ
x1
x2
x3
x4
x5/CR
/DC4/EOT
0
/NUL1
1
54
/NUL1
/NUL3
/NUL3˘
:
This problem is equivalent to ﬁnding values for the unknowns x1;x2;x3;x4;x5,
satisfying the following 8difference constraints:
x1/NULx2/DC4 0, (24.3)
x1/NULx5/DC4/NUL 1, (24.4)
x2/NULx5/DC4 1, (24.5)
x3/NULx1/DC4 5, (24.6)
x4/NULx1/DC4 4, (24.7)
x4/NULx3/DC4/NUL 1, (24.8)
x5/NULx3/DC4/NUL 3, (24.9)
x5/NULx4/DC4/NUL 3. (24.10)666 Chapter 24 Single-Source Shortest Paths
One solution to this problem is xD./NUL5;/NUL3; 0;/NUL1;/NUL4/, which you can verify di-
rectly by checking each inequality. In fact, this problem has more than one solution.Another is x
0D. 0 ;2;5 ;4;1 / . These two solutions are related: each component
ofx0is5larger than the corresponding component of x. This fact is not mere
coincidence.
Lemma 24.8
LetxD.x1;x2;:::;x n/be a solution to a system Ax/DC4bof difference con-
straints, and let dbe any constant. Then xCdD.x1Cd;x 2Cd;:::;x nCd/
is a solution to Ax/DC4bas well.
Proof For each xiandxj,w eh a v e .xjCd//NUL.xiCd/Dxj/NULxi. Thus, if x
satisﬁes Ax/DC4b, so does xCd.
Systems of difference constraints occur in many different applications. For ex-
ample, the unknowns ximay be times at which events are to occur. Each constraint
states that at least a certain amount of time, or at most a certain amount of time,must elapse between two events. Perhaps the events are jobs to be performed dur-ing the assembly of a product. If we apply an adhesive that takes 2 hours to set at
timex
1and we have to wait until it sets to install a part at time x2, then we have the
constraint that x2/NAKx1C2or, equivalently, that x1/NULx2/DC4/NUL2. Alternatively, we
might require that the part be installed after the adhesive has been applied but nolater than the time that the adhesive has set halfway. In this case, we get the pair ofconstraints x
2/NAKx1andx2/DC4x1C1or, equivalently, x1/NULx2/DC40andx2/NULx1/DC41.
Constraint graphs
We can interpret systems of difference constraints from a graph-theoretic point
of view. In a system Ax/DC4bof difference constraints, we view the m/STXn
linear-programming matrix Aas the transpose of an incidence matrix (see Exer-
cise 22.1-7) for a graph with nvertices and medges. Each vertex /ETBiin the graph,
foriD1 ;2;:::;n , corresponds to one of the nunknown variables xi. Each di-
rected edge in the graph corresponds to one of the minequalities involving two
unknowns.
More formally, given a system Ax/DC4bof difference constraints, the correspond-
ingconstraint graph is a weighted, directed graph GD.V; E/ ,w h e r e
VDf/ETB0;/ETB1;:::;/ETB ng
and
EDf./ETBi;/ETBj/Wxj/NULxi/DC4bkis a constraintg
[f./ETB0;/ETB1/; ./ETB 0;/ETB2/; ./ETB 0;/ETB3/ ;:::;. /ETB 0;/ETBn/g:24.4 Difference constraints and shortest paths 667
0
0
0000 –1
1
5
4
–1–3 –30–5
–3
0 –1–4
v3v2v1
v5
v0
v4
Figure 24.8 The constraint graph corresponding to the system (24.3)–(24.10) of difference con-
straints. The value of ı./ETB0;/ETBi/appears in each vertex /ETBi. One feasible solution to the system is
xD./NUL5;/NUL3; 0;/NUL1;/NUL4/.
The constraint graph contains the additional vertex /ETB0, as we shall see shortly, to
guarantee that the graph has some vertex which can reach all other vertices. Thus,the vertex set Vconsists of a vertex /ETB
ifor each unknown xi, plus an additional
vertex /ETB0. The edge set Econtains an edge for each difference constraint, plus
an edge ./ETB0;/ETBi/for each unknown xi.I fxj/NULxi/DC4bkis a difference constraint,
then the weight of edge ./ETBi;/ETBj/isw./ETB i;/ETBj/Dbk. The weight of each edge leav-
ing/ETB0is0. Figure 24.8 shows the constraint graph for the system (24.3)–(24.10)
of difference constraints.
The following theorem shows that we can ﬁnd a solution to a system of differ-
ence constraints by ﬁnding shortest-path weights in the corresponding constraint
graph.
Theorem 24.9
Given a system Ax/DC4bof difference constraints, let GD.V; E/ be the corre-
sponding constraint graph. If Gcontains no negative-weight cycles, then
xD.ı./ETB 0;/ETB1/; ı./ETB 0;/ETB2/; ı./ETB 0;/ETB3/ ;:::;ı . /ETB 0;/ETBn// (24.11)
is a feasible solution for the system. If Gcontains a negative-weight cycle, then
there is no feasible solution for the system.
Proof We ﬁrst show that if the constraint graph contains no negative-weight
cycles, then equation (24.11) gives a feasible solution. Consider any edge
./ETBi;/ETBj/2E. By the triangle inequality, ı./ETB0;/ETBj//DC4ı./ETB0;/ETBi/Cw./ETB i;/ETBj/or,
equivalently, ı./ETB0;/ETBj//NULı./ETB0;/ETBi//DC4w./ETB i;/ETBj/. Thus, letting xiDı./ETB0;/ETBi/and668 Chapter 24 Single-Source Shortest Paths
xjDı./ETB0;/ETBj/satisﬁes the difference constraint xj/NULxi/DC4w./ETB i;/ETBj/that corre-
sponds to edge ./ETBi;/ETBj/.
Now we show that if the constraint graph contains a negative-weight cycle, then
the system of difference constraints has no feasible solution. Without loss of gen-erality, let the negative-weight cycle be cDh/ETB
1;/ETB2;:::;/ETB ki,w h e r e /ETB1D/ETBk.
(The vertex /ETB0cannot be on cycle c, because it has no entering edges.) Cycle c
corresponds to the following difference constraints:
x2/NULx1/DC4w./ETB 1;/ETB2/;
x3/NULx2/DC4w./ETB 2;/ETB3/;
:::
xk/NUL1/NULxk/NUL2/DC4w./ETB k/NUL2;/ETBk/NUL1/;
xk/NULxk/NUL1/DC4w./ETB k/NUL1;/ETBk/:
We will assume that xhas a solution satisfying each of these kinequalities and then
derive a contradiction. The solution must also satisfy the inequality that results
when we sum the kinequalities together. If we sum the left-hand sides, each
unknown xiis added in once and subtracted out once (remember that /ETB1D/ETBk
implies x1Dxk), so that the left-hand side of the sum is 0. The right-hand side
sums to w.c/ , and thus we obtain 0/DC4w.c/ . But since cis a negative-weight cycle,
w.c/ < 0 , and we obtain the contradiction that 0/DC4w.c/ < 0 .
Solving systems of difference constraints
Theorem 24.9 tells us that we can use the Bellman-Ford algorithm to solve a
system of difference constraints. Because the constraint graph contains edgesfrom the source vertex /ETB
0to all other vertices, any negative-weight cycle in the
constraint graph is reachable from /ETB0. If the Bellman-Ford algorithm returns
TRUE , then the shortest-path weights give a feasible solution to the system. In
Figure 24.8, for example, the shortest-path weights provide the feasible solutionxD./NUL5;/NUL3; 0;/NUL1;/NUL4/, and by Lemma 24.8, xD.d/NUL5; d/NUL3; d; d/NUL1; d/NUL4/
is also a feasible solution for any constant d. If the Bellman-Ford algorithm returns
FALSE , there is no feasible solution to the system of difference constraints.
A system of difference constraints with mconstraints on nunknowns produces
a graph with nC1vertices and nCmedges. Thus, using the Bellman-Ford
algorithm, we can solve the system in O..nC1/.nCm//DO.n2Cnm/ time.
Exercise 24.4-5 asks you to modify the algorithm to run in O.nm/ time, even if m
is much less than n.24.4 Difference constraints and shortest paths 669
Exercises
24.4-1
Find a feasible solution or determine that no feasible solution exists for the follow-ing system of difference constraints:
x
1/NULx2/DC4 1,
x1/NULx4/DC4/NUL 4,
x2/NULx3/DC4 2,
x2/NULx5/DC4 7,
x2/NULx6/DC4 5,
x3/NULx6/DC410,
x4/NULx2/DC4 2,
x5/NULx1/DC4/NUL 1,
x5/NULx4/DC4 3,
x6/NULx3/DC4/NUL 8.
24.4-2
Find a feasible solution or determine that no feasible solution exists for the follow-ing system of difference constraints:
x
1/NULx2/DC4 4,
x1/NULx5/DC4 5,
x2/NULx4/DC4/NUL 6,
x3/NULx2/DC4 1,
x4/NULx1/DC4 3,
x4/NULx3/DC4 5,
x4/NULx5/DC410,
x5/NULx3/DC4/NUL 4,
x5/NULx4/DC4/NUL 8.
24.4-3
Can any shortest-path weight from the new vertex /ETB0in a constraint graph be posi-
tive? Explain.
24.4-4
Express the single-pair shortest-path problem as a linear program.670 Chapter 24 Single-Source Shortest Paths
24.4-5
Show how to modify the Bellman-Ford algorithm slightly so that when we use itto solve a system of difference constraints with minequalities on nunknowns, the
running time is O.nm/ .
24.4-6
Suppose that in addition to a system of difference constraints, we want to handleequality constraints of the form x
iDxjCbk. Show how to adapt the Bellman-
Ford algorithm to solve this variety of constraint system.
24.4-7
Show how to solve a system of difference constraints by a Bellman-Ford-like algo-rithm that runs on a constraint graph without the extra vertex /ETB
0.
24.4-8 ?
LetAx/DC4bb eas y s t e mo f mdifference constraints in nunknowns. Show that the
Bellman-Ford algorithm, when run on the corresponding constraint graph, maxi-mizesP
n
iD1xisubject to Ax/DC4bandxi/DC40for all xi.
24.4-9 ?
Show that the Bellman-Ford algorithm, when run on the constraint graph for a sys-temAx/DC4bof difference constraints, minimizes the quantity .maxfx
ig/NULminfxig/
subject to Ax/DC4b. Explain how this fact might come in handy if the algorithm is
used to schedule construction jobs.
24.4-10
Suppose that every row in the matrix Aof a linear program Ax/DC4bcorresponds to
a difference constraint, a single-variable constraint of the form xi/DC4bk, or a single-
variable constraint of the form /NULxi/DC4bk. Show how to adapt the Bellman-Ford
algorithm to solve this variety of constraint system.
24.4-11
Give an efﬁcient algorithm to solve a system Ax/DC4bof difference constraints
when all of the elements of bare real-valued and all of the unknowns ximust be
integers.
24.4-12 ?
Give an efﬁcient algorithm to solve a system Ax/DC4bof difference constraints
when all of the elements of bare real-valued and a speciﬁed subset of some, but
not necessarily all, of the unknowns ximust be integers.24.5 Proofs of shortest-paths properties 671
24.5 Proofs of shortest-paths properties
Throughout this chapter, our correctness arguments have relied on the triangle
inequality, upper-bound property, no-path property, convergence property, path-relaxation property, and predecessor-subgraph property. We stated these propertieswithout proof at the beginning of this chapter. In this section, we prove them.
The triangle inequality
In studying breadth-ﬁrst search (Section 22.2), we proved as Lemma 22.1 a sim-
ple property of shortest distances in unweighted graphs. The triangle inequality
generalizes the property to weighted graphs.
Lemma 24.10 (Triangle inequality)
LetGD.V; E/ be a weighted, directed graph with weight function wWE! R
and source vertex s. Then, for all edges .u; /ETB/2E,w eh a v e
ı.s; /ETB//DC4ı.s; u/Cw.u;/ETB/ :
Proof Suppose that pis a shortest path from source sto vertex /ETB.T h e n phas
no more weight than any other path from sto/ETB. Speciﬁcally, path phas no more
weight than the particular path that takes a shortest path from source sto vertex u
and then takes edge .u; /ETB/ .
Exercise 24.5-3 asks you to handle the case in which there is no shortest path
from sto/ETB.
Effects of relaxation on shortest-path estimates
The next group of lemmas describes how shortest-path estimates are affected when
we execute a sequence of relaxation steps on the edges of a weighted, directedgraph that has been initialized by I
NITIALIZE -SINGLE -SOURCE .
Lemma 24.11 (Upper-bound property)
LetGD.V; E/ be a weighted, directed graph with weight function wWE! R.
Lets2Vbe the source vertex, and let the graph be initialized by I NITIALIZE -
SINGLE -SOURCE .G; s/ . Then, /ETB:d/NAKı.s; /ETB/ for all /ETB2V, and this invariant is
maintained over any sequence of relaxation steps on the edges of G. Moreover,
once /ETB:dachieves its lower bound ı.s; /ETB/ , it never changes.672 Chapter 24 Single-Source Shortest Paths
Proof We prove the invariant /ETB:d/NAKı.s; /ETB/ for all vertices /ETB2Vby induction
over the number of relaxation steps.
For the basis, /ETB:d/NAKı.s; /ETB/ is certainly true after initialization, since /ETB:dD1
implies /ETB:d/NAKı.s; /ETB/ for all /ETB2V/NULfsg, and since s:dD0/NAKı.s; s/ (note that
ı.s; s/D/NUL1 ifsis on a negative-weight cycle and 0otherwise).
For the inductive step, consider the relaxation of an edge .u; /ETB/ . By the inductive
hypothesis, x:d/NAKı.s; x/ for all x2Vprior to the relaxation. The only dvalue
that may change is /ETB:d. If it changes, we have
/ETB:dDu:dCw.u;/ETB/
/NAKı.s; u/Cw.u;/ETB/ (by the inductive hypothesis)
/NAKı.s; /ETB/ (by the triangle inequality) ,
and so the invariant is maintained.
To see that the value of /ETB:dnever changes once /ETB:dDı.s; /ETB/ , note that having
achieved its lower bound, /ETB:dcannot decrease because we have just shown that
/ETB:d/NAKı.s; /ETB/ , and it cannot increase because relaxation steps do not increase d
values.
Corollary 24.12 (No-path property)Suppose that in a weighted, directed graph GD.V; E/ with weight function
wWE! R, no path connects a source vertex s2Vto a given vertex /ETB2V.
Then, after the graph is initialized by I
NITIALIZE -SINGLE -SOURCE .G; s/ ,w e
have /ETB:dDı.s; /ETB/D1 , and this equality is maintained as an invariant over
any sequence of relaxation steps on the edges of G.
Proof By the upper-bound property, we always have 1D ı.s; /ETB//DC4/ETB:d,a n d
thus/ETB:dD1D ı.s; /ETB/ .
Lemma 24.13
LetGD.V; E/ be a weighted, directed graph with weight function wWE! R,
and let .u; /ETB/2E. Then, immediately after relaxing edge .u; /ETB/ by executing
RELAX . u ;/ETB;w/ ,w eh a v e /ETB:d/DC4u:dCw.u;/ETB/ .
Proof If, just prior to relaxing edge .u; /ETB/ ,w eh a v e /ETB:d>u : dCw.u;/ETB/ ,t h e n
/ETB:dDu:dCw.u;/ETB/ afterward. If, instead, /ETB:d/DC4u:dCw.u;/ETB/ just before
the relaxation, then neither u:dnor/ETB:dchanges, and so /ETB:d/DC4u:dCw.u;/ETB/
afterward.
Lemma 24.14 (Convergence property)LetGD.V; E/ be a weighted, directed graph with weight function wWE!R,
lets2Vbe a source vertex, and let s;u!/ETBbe a shortest path in Gfor24.5 Proofs of shortest-paths properties 673
some vertices u; /ETB2V. Suppose that Gis initialized by I NITIALIZE -SINGLE -
SOURCE .G; s/ and then a sequence of relaxation steps that includes the call
RELAX . u ;/ETB;w/ is executed on the edges of G.I fu:dDı.s; u/ at any time
prior to the call, then /ETB:dDı.s; /ETB/ at all times after the call.
Proof By the upper-bound property, if u:dDı.s; u/ at some point prior to re-
laxing edge .u; /ETB/ , then this equality holds thereafter. In particular, after relaxing
edge .u; /ETB/ ,w eh a v e
/ETB:d/DC4u:dCw.u;/ETB/ (by Lemma 24.13)
Dı.s; u/Cw.u;/ETB/
Dı.s; /ETB/ (by Lemma 24.1) .
By the upper-bound property, /ETB:d/NAKı.s; /ETB/ , from which we conclude that
/ETB:dDı.s; /ETB/ , and this equality is maintained thereafter.
Lemma 24.15 (Path-relaxation property)
LetGD.V; E/ be a weighted, directed graph with weight function wWE! R,
and let s2Vbe a source vertex. Consider any shortest path pDh/ETB0;/ETB1;:::;/ETB ki
from sD/ETB0to/ETBk.I fGis initialized by I NITIALIZE -SINGLE -SOURCE .G; s/ and
then a sequence of relaxation steps occurs that includes, in order, relaxing the edges./ETB
0;/ETB1/; ./ETB 1;/ETB2/ ;:::;. /ETB k/NUL1;/ETBk/,t h e n /ETBk:dDı.s; /ETB k/after these relaxations and
at all times afterward. This property holds no matter what other edge relaxationsoccur, including relaxations that are intermixed with relaxations of the edges of p.
Proof We show by induction that after the ith edge of path pis relaxed, we have
/ETB
i:dDı.s; /ETB i/. For the basis, iD0, and before any edges of phave been relaxed,
we have from the initialization that /ETB0:dDs:dD0Dı.s; s/ . By the upper-bound
property, the value of s:dnever changes after initialization.
For the inductive step, we assume that /ETBi/NUL1:dDı.s; /ETB i/NUL1/,a n dw ee x a m i n e
what happens when we relax edge ./ETBi/NUL1;/ETBi/. By the convergence property, after
relaxing this edge, we have /ETBi:dDı.s; /ETB i/, and this equality is maintained at all
times thereafter.
Relaxation and shortest-paths trees
We now show that once a sequence of relaxations has caused the shortest-path es-
timates to converge to shortest-path weights, the predecessor subgraph G/EMinduced
by the resulting /EMvalues is a shortest-paths tree for G. We start with the follow-
ing lemma, which shows that the predecessor subgraph always forms a rooted tree
whose root is the source.674 Chapter 24 Single-Source Shortest Paths
Lemma 24.16
LetGD.V; E/ be a weighted, directed graph with weight function wWE! R,
lets2Vbe a source vertex, and assume that Gcontains no negative-weight
cycles that are reachable from s. Then, after the graph is initialized by I NITIALIZE -
SINGLE -SOURCE .G; s/ , the predecessor subgraph G/EMforms a rooted tree with
roots, and any sequence of relaxation steps on edges of Gmaintains this property
as an invariant.
Proof Initially, the only vertex in G/EMis the source vertex, and the lemma is triv-
ially true. Consider a predecessor subgraph G/EMthat arises after a sequence of
relaxation steps. We shall ﬁrst prove that G/EMis acyclic. Suppose for the sake of
contradiction that some relaxation step creates a cycle in the graph G/EM.L e tt h ec y -
cle be cDh/ETB0;/ETB1;:::;/ETB ki,w h e r e /ETBkD/ETB0. Then, /ETBi:/EMD/ETBi/NUL1foriD1 ;2;:::;k
and, without loss of generality, we can assume that relaxing edge ./ETBk/NUL1;/ETBk/created
the cycle in G/EM.
We claim that all vertices on cycle care reachable from the source s. Why?
Each vertex on chas a non- NILpredecessor, and so each vertex on cwas assigned
a ﬁnite shortest-path estimate when it was assigned its non- NIL/EMvalue. By the
upper-bound property, each vertex on cycle chas a ﬁnite shortest-path weight,
which implies that it is reachable from s.
We shall examine the shortest-path estimates on cjust prior to the call
RELAX ./ETBk/NUL1;/ETBk;w/ and show that cis a negative-weight cycle, thereby contra-
dicting the assumption that Gcontains no negative-weight cycles that are reachable
from the source. Just before the call, we have /ETBi:/EMD/ETBi/NUL1foriD1 ;2;:::;k/NUL1.
Thus, for iD1 ;2;:::;k/NUL1, the last update to /ETBi:dwas by the assignment
/ETBi:dD/ETBi/NUL1:dCw./ETB i/NUL1;/ETBi/.I f/ETBi/NUL1:dchanged since then, it decreased. Therefore,
just before the call R ELAX ./ETBk/NUL1;/ETBk;w/,w eh a v e
/ETBi:d/NAK/ETBi/NUL1:dCw./ETB i/NUL1;/ETBi/ for all iD1 ;2;:::;k/NUL1: (24.12)
Because /ETBk:/EMis changed by the call, immediately beforehand we also have the
strict inequality
/ETBk:d>/ETB k/NUL1:dCw./ETB k/NUL1;/ETBk/:
Summing this strict inequality with the k/NUL1inequalities (24.12), we obtain the
sum of the shortest-path estimates around cycle c:
kX
iD1/ETBi:d>kX
iD1./ETBi/NUL1:dCw./ETB i/NUL1;/ETBi//
DkX
iD1/ETBi/NUL1:dCkX
iD1w./ETB i/NUL1;/ETBi/:24.5 Proofs of shortest-paths properties 675
sux
yzv
Figure 24.9 Showing that a simple path in G/EMfrom source sto vertex /ETBis unique. If there are two
paths p1(s;u;x!´;/ETB)a n d p2(s;u;y!´;/ETB), where x¤y,t h e n ´:/EMDx
and´:/EMDy, a contradiction.
But
kX
iD1/ETBi:dDkX
iD1/ETBi/NUL1:d;
since each vertex in the cycle cappears exactly once in each summation. This
equality implies
0>kX
iD1w./ETB i/NUL1;/ETBi/:
Thus, the sum of weights around the cycle cis negative, which provides the desired
contradiction.
We have now proven that G/EMis a directed, acyclic graph. To show that it forms
a rooted tree with root s, it sufﬁces (see Exercise B.5-2) to prove that for each
vertex /ETB2V/EM, there is a unique simple path from sto/ETBinG/EM.
We ﬁrst must show that a path from sexists for each vertex in V/EM.T h e v e r -
tices in V/EMare those with non- NIL/EMvalues, plus s. The idea here is to prove by
induction that a path exists from sto all vertices in V/EM. We leave the details as
Exercise 24.5-6.
To complete the proof of the lemma, we must now show that for any vertex
/ETB2V/EM, the graph G/EMcontains at most one simple path from sto/ETB. Suppose other-
wise. That is, suppose that, as Figure 24.9 illustrates, G/EMcontains two simple paths
from sto some vertex /ETB:p1, which we decompose into s;u;x!´;/ETB,
andp2, which we decompose into s;u;y!´;/ETB,w h e r e x¤y(though u
could be sand´could be /ETB). But then, ´:/EMDxand´:/EMDy, which implies
the contradiction that xDy. We conclude that G/EMcontains a unique simple path
from sto/ETB, and thus G/EMforms a rooted tree with root s.
We can now show that if, after we have performed a sequence of relaxation steps,
all vertices have been assigned their true shortest-path weights, then the predeces-sor subgraph G
/EMis a shortest-paths tree.676 Chapter 24 Single-Source Shortest Paths
Lemma 24.17 (Predecessor-subgraph property)
LetGD.V; E/ be a weighted, directed graph with weight function wWE! R,
lets2Vbe a source vertex, and assume that Gcontains no negative-weight cycles
that are reachable from s. Let us call I NITIALIZE -SINGLE -SOURCE .G; s/ and then
execute any sequence of relaxation steps on edges of Gthat produces /ETB:dDı.s; /ETB/
for all /ETB2V. Then, the predecessor subgraph G/EMis a shortest-paths tree rooted
ats.
Proof We must prove that the three properties of shortest-paths trees given on
page 647 hold for G/EM. To show the ﬁrst property, we must show that V/EMis the set
of vertices reachable from s. By deﬁnition, a shortest-path weight ı.s; /ETB/ is ﬁnite
if and only if /ETBis reachable from s, and thus the vertices that are reachable from s
are exactly those with ﬁnite dvalues. But a vertex /ETB2V/NULfsghas been assigned
a ﬁnite value for /ETB:dif and only if /ETB:/EM¤NIL. Thus, the vertices in V/EMare exactly
those reachable from s.
The second property follows directly from Lemma 24.16.
It remains, therefore, to prove the last property of shortest-paths trees: for each
vertex /ETB2V/EM, the unique simple path sp;/ETBinG/EMis a shortest path from sto/ETB
inG.L e t pDh/ETB0;/ETB1;:::;/ETB ki,w h e r e /ETB0Dsand/ETBkD/ETB.F o r iD1 ;2;:::;k ,
we have both /ETBi:dDı.s; /ETB i/and/ETBi:d/NAK/ETBi/NUL1:dCw./ETB i/NUL1;/ETBi/, from which we
conclude w./ETB i/NUL1;/ETBi//DC4ı.s; /ETB i//NULı.s; /ETB i/NUL1/. Summing the weights along path p
yields
w.p/DkX
iD1w./ETB i/NUL1;/ETBi/
/DC4kX
iD1.ı.s; /ETB i//NULı.s; /ETB i/NUL1//
Dı.s; /ETB k//NULı.s; /ETB 0/ (because the sum telescopes)
Dı.s; /ETB k/ (because ı.s; /ETB 0/Dı.s; s/D0).
Thus, w.p//DC4ı.s; /ETB k/.S i n c e ı.s; /ETB k/is a lower bound on the weight of any path
from sto/ETBk, we conclude that w.p/Dı.s; /ETB k/, and thus pis a shortest path
from sto/ETBD/ETBk.
Exercises
24.5-1
Give two shortest-paths trees for the directed graph of Figure 24.2 (on page 648)other than the two shown.24.5 Proofs of shortest-paths properties 677
24.5-2
Give an example of a weighted, directed graph GD.V; E/ with weight function
wWE! Rand source vertex ssuch that Gsatisﬁes the following property: For
every edge .u; /ETB/2E, there is a shortest-paths tree rooted at sthat contains .u; /ETB/
and another shortest-paths tree rooted at sthat does not contain .u; /ETB/ .
24.5-3
Embellish the proof of Lemma 24.10 to handle cases in which shortest-pathweights are1or/NUL1.
24.5-4
LetGD.V; E/ be a weighted, directed graph with source vertex s,a n dl e t G
be initialized by I
NITIALIZE -SINGLE -SOURCE .G; s/ . Prove that if a sequence of
relaxation steps sets s:/EMto a non- NILvalue, then Gcontains a negative-weight
cycle.
24.5-5
LetGD.V; E/ be a weighted, directed graph with no negative-weight edges. Let
s2Vbe the source vertex, and suppose that we allow /ETB:/EMto be the predecessor
of/ETBonanyshortest path to /ETBfrom source sif/ETB2V/NULfsgis reachable from s,
and NILotherwise. Give an example of such a graph Gand an assignment of /EM
values that produces a cycle in G/EM. (By Lemma 24.16, such an assignment cannot
be produced by a sequence of relaxation steps.)
24.5-6
LetGD.V; E/ be a weighted, directed graph with weight function wWE!R
and no negative-weight cycles. Let s2Vbe the source vertex, and let Gbe initial-
ized by I NITIALIZE -SINGLE -SOURCE .G; s/ . Prove that for every vertex /ETB2V/EM,
there exists a path from sto/ETBinG/EMand that this property is maintained as an
invariant over any sequence of relaxations.
24.5-7
LetGD.V; E/ be a weighted, directed graph that contains no negative-weight
cycles. Let s2Vbe the source vertex, and let Gbe initialized by I NITIALIZE -
SINGLE -SOURCE .G; s/ . Prove that there exists a sequence of jVj/NUL1relaxation
steps that produces /ETB:dDı.s; /ETB/ for all /ETB2V.
24.5-8
LetGbe an arbitrary weighted, directed graph with a negative-weight cycle reach-
able from the source vertex s. Show how to construct an inﬁnite sequence of relax-
ations of the edges of Gsuch that every relaxation causes a shortest-path estimate
to change.678 Chapter 24 Single-Source Shortest Paths
Problems
24-1 Yen’s improvement to Bellman-Ford
Suppose that we order the edge relaxations in each pass of the Bellman-Ford al-gorithm as follows. Before the ﬁrst pass, we assign an arbitrary linear order/ETB
1;/ETB2;:::;/ETB jVjto the vertices of the input graph GD.V; E/ . Then, we parti-
tion the edge set EintoEf[Eb,w h e r e EfDf./ETBi;/ETBj/2EWi<jgand
EbDf./ETBi;/ETBj/2EWi>jg. (Assume that Gcontains no self-loops, so that every
edge is in either EforEb.) Deﬁne GfD.V; E f/andGbD.V; E b/.
a.Prove that Gfis acyclic with topological sort h/ETB1;/ETB2;:::;/ETB jVjiand that Gbis
acyclic with topological sort h/ETBjVj;/ETBjVj/NUL1;:::;/ETB 1i.
Suppose that we implement each pass of the Bellman-Ford algorithm in the fol-
lowing way. We visit each vertex in the order /ETB1;/ETB2;:::;/ETB jVj, relaxing edges of Ef
that leave the vertex. We then visit each vertex in the order /ETBjVj;/ETBjVj/NUL1;:::;/ETB 1,
relaxing edges of Ebthat leave the vertex.
b.Prove that with this scheme, if Gcontains no negative-weight cycles that are
reachable from the source vertex s, then after onlydjVj=2epasses over the
edges, /ETB:dDı.s; /ETB/ for all vertices /ETB2V.
c.Does this scheme improve the asymptotic running time of the Bellman-Ford
algorithm?
24-2 Nesting boxes
Ad-dimensional box with dimensions .x1;x2;:::;x d/nests within another box
with dimensions .y1;y2;:::;y d/if there exists a permutation /EMonf1 ;2;:::;dg
such that x/EM.1/<y 1,x/EM.2/<y 2,..., x/EM.d/<y d.
a.Argue that the nesting relation is transitive.
b.Describe an efﬁcient method to determine whether or not one d-dimensional
box nests inside another.
c.Suppose that you are given a set of nd-dimensional boxes fB1;B2;:::;B ng.
Give an efﬁcient algorithm to ﬁnd the longest sequence hBi1;Bi2;:::;B ikiof
boxes such that Bijnests within BijC1forjD1 ;2;:::;k/NUL1. Express the
running time of your algorithm in terms of nandd.Problems for Chapter 24 679
24-3 Arbitrage
Arbitrage is the use of discrepancies in currency exchange rates to transform one
unit of a currency into more than one unit of the same currency. For example,suppose that 1U.S. dollar buys 49Indian rupees, 1Indian rupee buys 2Japanese
yen, and 1Japanese yen buys 0:0107 U.S. dollars. Then, by converting currencies,
a trader can start with 1U.S. dollar and buy 49/STX2/STX0:0107D1:0486 U.S. dollars,
thus turning a proﬁt of 4:86 percent.
Suppose that we are given ncurrencies c
1;c2;:::;c nand an n/STXntable Rof
exchange rates, such that one unit of currency cibuys RŒi;j/c141 units of currency cj.
a.Give an efﬁcient algorithm to determine whether or not there exists a sequence
of currencieshci1;ci2;:::;c ikisuch that
RŒi 1;i2/c141/SOHRŒi 2;i3/c141/SOH/SOH/SOHRŒi k/NUL1;ik/c141/SOHRŒi k;i1/c141>1:
Analyze the running time of your algorithm.
b.Give an efﬁcient algorithm to print out such a sequence if one exists. Analyze
the running time of your algorithm.
24-4 Gabow’s scaling algorithm for single-source shortest paths
Ascaling algorithm solves a problem by initially considering only the highest-
order bit of each relevant input value (such as an edge weight). It then reﬁnes theinitial solution by looking at the two highest-order bits. It progressively looks atmore and more high-order bits, reﬁning the solution each time, until it has exam-ined all bits and computed the correct solution.
In this problem, we examine an algorithm for computing the shortest paths from
a single source by scaling edge weights. We are given a directed graph GD.V; E/
with nonnegative integer edge weights w.L e t WDmax
.u;/ETB/ 2Efw.u;/ETB/g.O u r
goal is to develop an algorithm that runs in O.E lgW/time. We assume that all
vertices are reachable from the source.
The algorithm uncovers the bits in the binary representation of the edge weights
one at a time, from the most signiﬁcant bit to the least signiﬁcant bit. Speciﬁcally,letkDdlg.WC1/ebe the number of bits in the binary representation of W,
and for iD1 ;2;:::;k ,l e tw
i.u; /ETB/D/EOT
w.u;/ETB/=2k/NULi˘
.T h a t i s , wi.u; /ETB/ is the
“scaled-down” version of w.u;/ETB/ g i v e nb yt h e imost signiﬁcant bits of w.u;/ETB/ .
(Thus, wk.u; /ETB/Dw.u;/ETB/ for all .u; /ETB/2E.) For example, if kD5and
w.u;/ETB/D25, which has the binary representation h11001i,t h e n w3.u; /ETB/D
h110iD 6. As another example with kD5,i fw.u;/ETB/Dh00100iD 4,t h e n
w3.u; /ETB/Dh001iD1. Let us deﬁne ıi.u; /ETB/ as the shortest-path weight from
vertex uto vertex /ETBusing weight function wi. Thus, ık.u; /ETB/Dı.u;/ETB/ for all
u; /ETB2V. For a given source vertex s, the scaling algorithm ﬁrst computes the680 Chapter 24 Single-Source Shortest Paths
shortest-path weights ı1.s; /ETB/ for all /ETB2V, then computes ı2.s; /ETB/ for all /ETB2V,
and so on, until it computes ık.s; /ETB/ for all /ETB2V. We assume throughout that
jEj/NAKjVj/NUL1, and we shall see that computing ıifrom ıi/NUL1takes O.E/ time, so
that the entire algorithm takes O.kE/DO.E lgW/time.
a.Suppose that for all vertices /ETB2V,w eh a v e ı.s; /ETB//DC4jEj. Show that we can
compute ı.s; /ETB/ for all /ETB2VinO.E/ time.
b.Show that we can compute ı1.s; /ETB/ for all /ETB2VinO.E/ time.
Let us now focus on computing ıifrom ıi/NUL1.
c.Prove that for iD2;3 ;:::;k , we have either wi.u; /ETB/D2wi/NUL1.u; /ETB/ or
wi.u; /ETB/D2wi/NUL1.u; /ETB/C1. Then, prove that
2ıi/NUL1.s; /ETB//DC4ıi.s; /ETB//DC42ıi/NUL1.s; /ETB/CjVj/NUL1
for all /ETB2V.
d.Deﬁne for iD2;3 ;:::;k and all .u; /ETB/2E,
ywi.u; /ETB/Dwi.u; /ETB/C2ıi/NUL1.s; u//NUL2ıi/NUL1.s; /ETB/ :
Prove that for iD2;3 ;:::;k and all u; /ETB2V, the “reweighted” value ywi.u; /ETB/
of edge .u; /ETB/ is a nonnegative integer.
e.Now, deﬁneyıi.s; /ETB/ as the shortest-path weight from sto/ETBusing the weight
functionywi. Prove that for iD2;3 ;:::;k and all /ETB2V,
ıi.s; /ETB/Dyıi.s; /ETB/C2ıi/NUL1.s; /ETB/
and thatyıi.s; /ETB//DC4jEj.
f.Show how to compute ıi.s; /ETB/ from ıi/NUL1.s; /ETB/ for all /ETB2VinO.E/ time, and
conclude that we can compute ı.s; /ETB/ for all /ETB2VinO.E lgW/time.
24-5 Karp’s minimum mean-weight cycle algorithm
LetGD.V; E/ be a directed graph with weight function wWE! R,a n dl e t
nDjVj.W ed e ﬁ n et h e mean weight of a cycle cDhe1;e2;:::;e kiof edges in E
to be
/SYN.c/D1
kkX
iD1w.e i/:Problems for Chapter 24 681
Let/SYN/ETXDmin c/SYN.c/ ,w h e r e cranges over all directed cycles in G. We call a cycle c
for which /SYN.c/D/SYN/ETXaminimum mean-weight cycle . This problem investigates
an efﬁcient algorithm for computing /SYN/ETX.
Assume without loss of generality that every vertex /ETB2Vis reachable from a
source vertex s2V.L e t ı.s; /ETB/ be the weight of a shortest path from sto/ETB,a n dl e t
ık.s; /ETB/ be the weight of a shortest path from sto/ETBconsisting of exactly kedges.
If there is no path from sto/ETBwith exactly kedges, then ık.s; /ETB/D1 .
a.Show that if /SYN/ETXD0,t h e n Gcontains no negative-weight cycles and ı.s; /ETB/D
min 0/DC4k/DC4n/NUL1ık.s; /ETB/ for all vertices /ETB2V.
b.Show that if /SYN/ETXD0,t h e n
max
0/DC4k/DC4n/NUL1ın.s; /ETB//NULık.s; /ETB/
n/NULk/NAK0
for all vertices /ETB2V.(Hint: Use both properties from part (a).)
c.Letcbe a0-weight cycle, and let uand/ETBbe any two vertices on c. Suppose
that/SYN/ETXD0and that the weight of the simple path from uto/ETBalong the cycle
isx. Prove that ı.s; /ETB/Dı.s; u/Cx.(Hint: The weight of the simple path
from /ETBtoualong the cycle is/NULx.)
d.Show that if /SYN/ETXD0, then on each minimum mean-weight cycle there exists a
vertex /ETBsuch that
max
0/DC4k/DC4n/NUL1ın.s; /ETB//NULık.s; /ETB/
n/NULkD0:
(Hint: Show how to extend a shortest path to any vertex on a minimum mean-
weight cycle along the cycle to make a shortest path to the next vertex on thecycle.)
e.Show that if /SYN
/ETXD0,t h e n
min
/ETB2Vmax
0/DC4k/DC4n/NUL1ın.s; /ETB//NULık.s; /ETB/
n/NULkD0:
f.Show that if we add a constant tto the weight of each edge of G,t h e n /SYN/ETX
increases by t. Use this fact to show that
/SYN/ETXDmin
/ETB2Vmax
0/DC4k/DC4n/NUL1ın.s; /ETB//NULık.s; /ETB/
n/NULk:
g.Give an O.VE/ -time algorithm to compute /SYN/ETX.682 Chapter 24 Single-Source Shortest Paths
24-6 Bitonic shortest paths
A sequence is bitonic if it monotonically increases and then monotonically de-
creases, or if by a circular shift it monotonically increases and then monotonicallydecreases. For example the sequences h1; 4; 6; 8; 3;/NUL2i,h9; 2;/NUL4;/NUL10;/NUL5i,a n d
h1; 2; 3; 4iare bitonic, buth1; 3; 12; 4; 2; 10iis not bitonic. (See Problem 15-3 for
the bitonic euclidean traveling-salesman problem.)
Suppose that we are given a directed graph GD.V; E/ with weight function
wWE!R, where all edge weights are unique, and we wish to ﬁnd single-source
shortest paths from a source vertex s. We are given one additional piece of infor-
mation: for each vertex /ETB2V, the weights of the edges along any shortest path
from sto/ETBform a bitonic sequence.
Give the most efﬁcient algorithm you can to solve this problem, and analyze its
running time.
Chapter notes
Dijkstra’s algorithm [88] appeared in 1959, but it contained no mention of a priorityqueue. The Bellman-Ford algorithm is based on separate algorithms by Bellman
[38] and Ford [109]. Bellman describes the relation of shortest paths to difference
constraints. Lawler [224] describes the linear-time algorithm for shortest paths ina dag, which he considers part of the folklore.
When edge weights are relatively small nonnegative integers, we have more ef-
ﬁcient algorithms to solve the single-source shortest-paths problem. The sequenceof values returned by the E
XTRACT -MINcalls in Dijkstra’s algorithm monoton-
ically increases over time. As discussed in the chapter notes for Chapter 6, in
this case several data structures can implement the various priority-queue opera-
tions more efﬁciently than a binary heap or a Fibonacci heap. Ahuja, Mehlhorn,
Orlin, and Tarjan [8] give an algorithm that runs in O.ECVp
lgW/time on
graphs with nonnegative edge weights, where Wis the largest weight of any edge
in the graph. The best bounds are by Thorup [337], who gives an algorithm thatruns in O.E lg lgV/time, and by Raman [291], who gives an algorithm that runs
inO/NUL
ECVmin˚
.lgV/
1=3C/SI;.lgW/1=4C/SI/TAB/SOH
time. These two algorithms use an
amount of space that depends on the word size of the underlying machine. Al-
though the amount of space used can be unbounded in the size of the input, it canbe reduced to be linear in the size of the input using randomized hashing.
For undirected graphs with integer weights, Thorup [336] gives an O.VCE/-
time algorithm for single-source shortest paths. In contrast to the algorithms men-tioned in the previous paragraph, this algorithm is not an implementation of Dijk-Notes for Chapter 24 683
stra’s algorithm, since the sequence of values returned by E XTRACT -MINcalls
does not monotonically increase over time.
For graphs with negative edge weights, an algorithm due to Gabow and Tar-
jan [122] runs in O.p
VElg.V W // time, and one by Goldberg [137] runs in
O.p
VElgW/time, where WDmax .u;/ETB/ 2Efjw.u;/ETB/jg.
Cherkassky, Goldberg, and Radzik [64] conducted extensive experiments com-
paring various shortest-path algorithms.25 All-Pairs Shortest Paths
In this chapter, we consider the problem of ﬁnding shortest paths between all pairs
of vertices in a graph. This problem might arise in making a table of distances be-tween all pairs of cities for a road atlas. As in Chapter 24, we are given a weighted,directed graph GD.V; E/ with a weight function wWE! Rthat maps edges
to real-valued weights. We wish to ﬁnd, for every pair of vertices u; /ETB2V,a
shortest (least-weight) path from uto/ETB, where the weight of a path is the sum of
the weights of its constituent edges. We typically want the output in tabular form:the entry in u’s row and /ETB’s column should be the weight of a shortest path from u
to/ETB.
We can solve an all-pairs shortest-paths problem by running a single-source
shortest-paths algorithm jVjtimes, once for each vertex as the source. If all
edge weights are nonnegative, we can use Dijkstra’s algorithm. If we usethe linear-array implementation of the min-priority queue, the running time isO.V
3CVE/DO.V3/. The binary min-heap implementation of the min-priority
queue yields a running time of O.VE lgV/, which is an improvement if the graph
is sparse. Alternatively, we can implement the min-priority queue with a Fibonacciheap, yielding a running time of O.V
2lgVCVE/.
If the graph has negative-weight edges, we cannot use Dijkstra’s algorithm. In-
stead, we must run the slower Bellman-Ford algorithm once from each vertex. Theresulting running time is O.V
2E/, which on a dense graph is O.V4/. In this chap-
ter we shall see how to do better. We also investigate the relation of the all-pairsshortest-paths problem to matrix multiplication and study its algebraic structure.
Unlike the single-source algorithms, which assume an adjacency-list represen-
tation of the graph, most of the algorithms in this chapter use an adjacency-matrix representation. (Johnson’s algorithm for sparse graphs, in Section 25.3,uses adjacency lists.) For convenience, we assume that the vertices are numbered1 ;2;:::;jVj, so that the input is an n/STXnmatrix Wrepresenting the edge weights
of an n-vertex directed graph GD.V; E/ .T h a ti s , WD.w
ij/,w h e r eChapter 25 All-Pairs Shortest Paths 685
wijD/c128
0 ifiDj;
the weight of directed edge .i; j / ifi¤jand.i; j /2E;
1 ifi¤jand.i; j /62E:(25.1)
We allow negative-weight edges, but we assume for the time being that the input
graph contains no negative-weight cycles.
The tabular output of the all-pairs shortest-paths algorithms presented in this
chapter is an n/STXnmatrix DD.dij/, where entry dijcontains the weight of a
shortest path from vertex ito vertex j. That is, if we let ı.i;j/ denote the shortest-
path weight from vertex ito vertex j(as in Chapter 24), then dijDı.i;j/ at
termination.
To solve the all-pairs shortest-paths problem on an input adjacency matrix, we
need to compute not only the shortest-path weights but also a predecessor matrix
…D./EMij/,w h e r e /EMijisNILif either iDjor there is no path from itoj,
and otherwise /EMijis the predecessor of jon some shortest path from i.J u s t a s
the predecessor subgraph G/EMfrom Chapter 24 is a shortest-paths tree for a given
source vertex, the subgraph induced by the ith row of the …matrix should be a
shortest-paths tree with root i. For each vertex i2V,w ed e ﬁ n et h e predecessor
subgraph ofGforiasG/EM;iD.V/EM;i;E/EM;i/,w h e r e
V/EM;iDfj2VW/EMij¤NILg[fig
and
E/EM;iDf./EMij;j/Wj2V/EM;i/NULfigg:
IfG/EM;iis a shortest-paths tree, then the following procedure, which is a modiﬁed
version of the P RINT -PATH procedure from Chapter 22, prints a shortest path from
vertex ito vertex j.
PRINT -ALL-PAIRS -SHORTEST -PATH. … ;i;j/
1ifi==j
2 print i
3elseif /EMij==NIL
4 print “no path from” i“to”j“exists”
5elsePRINT -ALL-PAIRS -SHORTEST -PATH. … ;i;/EM ij/
6 print j
In order to highlight the essential features of the all-pairs algorithms in this chapter,
we won’t cover the creation and properties of predecessor matrices as extensivelyas we dealt with predecessor subgraphs in Chapter 24. Some of the exercises cover
the basics.686 Chapter 25 All-Pairs Shortest Paths
Chapter outline
Section 25.1 presents a dynamic-programming algorithm based on matrix multi-
plication to solve the all-pairs shortest-paths problem. Using the technique of “re-peated squaring,” we can achieve a running time of ‚.V
3lgV/. Section 25.2 gives
another dynamic-programming algorithm, the Floyd-Warshall algorithm, whichruns in time ‚.V
3/. Section 25.2 also covers the problem of ﬁnding the tran-
sitive closure of a directed graph, which is related to the all-pairs shortest-pathsproblem. Finally, Section 25.3 presents Johnson’s algorithm, which solves the all-pairs shortest-paths problem in O.V
2lgVCVE/ time and is a good choice for
large, sparse graphs.
Before proceeding, we need to establish some conventions for adjacency-matrix
representations. First, we shall generally assume that the input graph GD.V; E/
hasnvertices, so that nDjVj. Second, we shall use the convention of denoting
matrices by uppercase letters, such as W,L,o rD, and their individual elements
by subscripted lowercase letters, such as wij,lij,o rdij. Some matrices will have
parenthesized superscripts, as in L.m/D/NUL
l.m/
ij/SOH
orD.m/D/NUL
d.m/
ij/SOH
, to indicate
iterates. Finally, for a given n/STXnmatrix A, we shall assume that the value of nis
stored in the attribute A:rows .
25.1 Shortest paths and matrix multiplication
This section presents a dynamic-programming algorithm for the all-pairs shortest-
paths problem on a directed graph GD.V; E/ . Each major loop of the dynamic
program will invoke an operation that is very similar to matrix multiplication, sothat the algorithm will look like repeated matrix multiplication. We shall start bydeveloping a ‚.V
4/-time algorithm for the all-pairs shortest-paths problem and
then improve its running time to ‚.V3lgV/.
Before proceeding, let us brieﬂy recap the steps given in Chapter 15 for devel-
oping a dynamic-programming algorithm.
1. Characterize the structure of an optimal solution.
2. Recursively deﬁne the value of an optimal solution.
3. Compute the value of an optimal solution in a bottom-up fashion.
We reserve the fourth step—constructing an optimal solution from computed in-
formation—for the exercises.25.1 Shortest paths and matrix multiplication 687
The structure of a shortest path
We start by characterizing the structure of an optimal solution. For the all-pairs
shortest-paths problem on a graph GD.V; E/ , we have proven (Lemma 24.1)
that all subpaths of a shortest path are shortest paths. Suppose that we representthe graph by an adjacency matrix WD.w
ij/. Consider a shortest path pfrom
vertex ito vertex j, and suppose that pcontains at most medges. Assuming that
there are no negative-weight cycles, mis ﬁnite. If iDj,t h e n phas weight 0
and no edges. If vertices iandjare distinct, then we decompose path pinto
ip0
;k!j, where path p0now contains at most m/NUL1edges. By Lemma 24.1,
p0is a shortest path from itok,a n ds o ı.i;j/Dı.i;k/Cwkj.
A recursive solution to the all-pairs shortest-paths problem
Now, let l.m/
ijbe the minimum weight of any path from vertex ito vertex jthat
contains at most medges. When mD0, there is a shortest path from itojwith
no edges if and only if iDj. Thus,
l.0/
ijD(
0 ifiDj;
1 ifi¤j:
Form/NAK1, we compute l.m/
ijas the minimum of l.m/NUL1/
ij (the weight of a shortest
path from itojconsisting of at most m/NUL1edges) and the minimum weight of any
path from itojconsisting of at most medges, obtained by looking at all possible
predecessors kofj. Thus, we recursively deﬁne
l.m/
ijDmin/DLE
l.m/NUL1/
ij ;min
1/DC4k/DC4n˚
l.m/NUL1/
ikCwkj/TAB/DC1
D min
1/DC4k/DC4n˚
l.m/NUL1/
ikCwkj/TAB
: (25.2)
The latter equality follows since wjjD0for all j.
What are the actual shortest-path weights ı.i;j/ ? If the graph contains
no negative-weight cycles, then for every pair of vertices iandjfor which
ı.i;j/ <1, there is a shortest path from itojthat is simple and thus contains at
most n/NUL1edges. A path from vertex ito vertex jwith more than n/NUL1edges
cannot have lower weight than a shortest path from itoj. The actual shortest-path
weights are therefore given by
ı.i;j/Dl.n/NUL1/
ijDl.n/
ijDl.nC1/
ijD/SOH/SOH/SOH : (25.3)688 Chapter 25 All-Pairs Shortest Paths
Computing the shortest-path weights bottom up
Taking as our input the matrix WD.wij/, we now compute a series of matrices
L.1/;L.2/;:::;L.n/NUL1/, where for mD1 ;2;:::;n/NUL1,w eh a v e L.m/D/NUL
l.m/
ij/SOH
.
The ﬁnal matrix L.n/NUL1/contains the actual shortest-path weights. Observe that
l.1/
ijDwijfor all vertices i;j2V,a n ds o L.1/DW.
The heart of the algorithm is the following procedure, which, given matrices
L.m/NUL1/andW, returns the matrix L.m/. That is, it extends the shortest paths com-
puted so far by one more edge.
EXTEND -SHORTEST -PATHS .L; W /
1nDL:rows
2l e t L0D/NUL
l0
ij/SOH
be a new n/STXnmatrix
3foriD1ton
4 forjD1ton
5 l0
ijD1
6 forkD1ton
7 l0
ijDmin.l0
ij;likCwkj/
8return L0
The procedure computes a matrix L0D.l0
ij/, which it returns at the end. It does so
by computing equation (25.2) for all iandj,u s i n g LforL.m/NUL1/andL0forL.m/.
(It is written without the superscripts to make its input and output matrices inde-pendent of m.) Its running time is ‚.n
3/due to the three nested forloops.
Now we can see the relation to matrix multiplication. Suppose we wish to com-
pute the matrix product CDA/SOHBof two n/STXnmatrices AandB. Then, for
i;jD1 ;2;:::;n , we compute
cijDnX
kD1aik/SOHbkj: (25.4)
Observe that if we make the substitutions
l.m/NUL1/!a;
w!b;
l.m/!c;
min!C ;
C!/SOH
in equation (25.2), we obtain equation (25.4). Thus, if we make these changes to
EXTEND -SHORTEST -PATHS and also replace1(the identity for min) by 0(the25.1 Shortest paths and matrix multiplication 689
identity forC), we obtain the same ‚.n3/-time procedure for multiplying square
matrices that we saw in Section 4.2:
SQUARE -MATRIX -MULTIPLY .A; B/
1nDA:rows
2l e t Cb ean e w n/STXnmatrix
3foriD1ton
4 forjD1ton
5 cijD0
6 forkD1ton
7 cijDcijCaik/SOHbkj
8return C
Returning to the all-pairs shortest-paths problem, we compute the shortest-path
weights by extending shortest paths edge by edge. Letting A/SOHBdenote the ma-
trix “product” returned by E XTEND -SHORTEST -PATHS .A; B/ , we compute the se-
quence of n/NUL1matrices
L.1/D L.0//SOHWDW;
L.2/D L.1//SOHWDW2;
L.3/D L.2//SOHWDW3;
:::
L.n/NUL1/DL.n/NUL2//SOHWDWn/NUL1:
As we argued above, the matrix L.n/NUL1/DWn/NUL1contains the shortest-path weights.
The following procedure computes this sequence in ‚.n4/time.
SLOW -ALL-PAIRS -SHORTEST -PATHS .W /
1nDW:rows
2L.1/DW
3formD2ton/NUL1
4l e t L.m/b ean e w n/STXnmatrix
5 L.m/DEXTEND -SHORTEST -PATHS .L.m/NUL1/;W/
6return L.n/NUL1/
Figure 25.1 shows a graph and the matrices L.m/computed by the procedure
SLOW -ALL-PAIRS -SHORTEST -PATHS .
Improving the running time
Our goal, however, is not to compute alltheL.m/matrices: we are interested
only in matrix L.n/NUL1/. Recall that in the absence of negative-weight cycles, equa-690 Chapter 25 All-Pairs Shortest Paths
2
1 3
5 434
82
671–4 –5
L.1/D/EOT03 81/NUL 4
1 01 17
1 4011
21/NUL 501
111 60˘
L.2/D/EOT038 2 /NUL4
30/NUL41 7
1 40 5 1 1
2/NUL1/NUL50/NUL2
81 16 0˘
L.3/D/EOT03/NUL32/NUL4
30/NUL41/NUL1
740 5 1 1
2/NUL1/NUL50/NUL2
851 60˘
L.4/D/EOT01/NUL32/NUL4
30/NUL41/NUL1
740 53
2/NUL1/NUL50/NUL2
851 60˘
Figure 25.1 A directed graph and the sequence of matrices L.m/computed by S LOW -ALL-PAIRS -
SHORTEST -PAT HS . You might want to verify that L.5/,d e ﬁ n e da s L.4//SOHW, equals L.4/, and thus
L.m/DL.4/for all m/NAK4.
tion (25.3) implies L.m/DL.n/NUL1/for all integers m/NAKn/NUL1. Just as tradi-
tional matrix multiplication is associative, so is matrix multiplication deﬁned by
the E XTEND -SHORTEST -PATHS procedure (see Exercise 25.1-4). Therefore, we
can compute L.n/NUL1/with onlydlg.n/NUL1/ematrix products by computing the se-
quence
L.1/D W;
L.2/D W2DW/SOHW;
L.4/D W4DW2/SOHW2
L.8/D W8DW4/SOHW4;
:::
L.2dlg.n/NUL1/e/DW2dlg.n/NUL1/eDW2dlg.n/NUL1/e/NUL1/SOHW2dlg.n/NUL1/e/NUL1:
Since 2dlg.n/NUL1/e/NAKn/NUL1, the ﬁnal product L.2dlg.n/NUL1/e/is equal to L.n/NUL1/.
The following procedure computes the above sequence of matrices by using this
technique of repeated squaring .25.1 Shortest paths and matrix multiplication 691
12
35 –1 2123
456–4 –8 10 7
Figure 25.2 A weighted, directed graph for use in Exercises 25.1-1, 25.2-1, and 25.3-1.
FASTER -ALL-PAIRS -SHORTEST -PATHS .W /
1nDW:rows
2L.1/DW
3mD1
4while m<n/NUL1
5l e t L.2m/be a new n/STXnmatrix
6 L.2m/DEXTEND -SHORTEST -PATHS .L.m/;L.m//
7 mD2m
8return L.m/
In each iteration of the while loop of lines 4–7, we compute L.2m/D/NUL
L.m//SOH2,
starting with mD1. At the end of each iteration, we double the value
ofm. The ﬁnal iteration computes L.n/NUL1/by actually computing L.2m/for some
n/NUL1/DC42m < 2n/NUL2. By equation (25.3), L.2m/DL.n/NUL1/. The next time the test
in line 4 is performed, mhas been doubled, so now m/NAKn/NUL1, the test fails, and
the procedure returns the last matrix it computed.
Because each of the dlg.n/NUL1/ematrix products takes ‚.n3/time, F ASTER -
ALL-PAIRS -SHORTEST -PATHS runs in ‚.n3lgn/time. Observe that the code
is tight, containing no elaborate data structures, and the constant hidden in the
‚-notation is therefore small.
Exercises
25.1-1
Run S LOW -ALL-PAIRS -SHORTEST -PATHS on the weighted, directed graph of
Figure 25.2, showing the matrices that result for each iteration of the loop. Then
do the same for F ASTER -ALL-PAIRS -SHORTEST -PATHS .
25.1-2
Why do we require that wiiD0for all 1/DC4i/DC4n?692 Chapter 25 All-Pairs Shortest Paths
25.1-3
What does the matrix
L.0/D/NUL
01 1 /SOH/SOH/SOH 1
1 01 /SOH/SOH/SOH 1
11 0/SOH/SOH/SOH 1
:::::::::::::::
111/SOH /SOH /SOH 0/SOH
used in the shortest-paths algorithms correspond to in regular matrix multiplica-
tion?
25.1-4
Show that matrix multiplication deﬁned by E XTEND -SHORTEST -PATHS is asso-
ciative.
25.1-5
Show how to express the single-source shortest-paths problem as a product of ma-trices and a vector. Describe how evaluating this product corresponds to a Bellman-Ford-like algorithm (see Section 24.1).
25.1-6
Suppose we also wish to compute the vertices on shortest paths in the algorithms ofthis section. Show how to compute the predecessor matrix …from the completed
matrix Lof shortest-path weights in O.n
3/time.
25.1-7
We can also compute the vertices on shortest paths as we compute the shortest-
path weights. Deﬁne /EM.m/
ijas the predecessor of vertex jon any minimum-weight
path from itojthat contains at most medges. Modify the E XTEND -SHORTEST -
PATHS and S LOW -ALL-PAIRS -SHORTEST -PATHS procedures to compute the ma-
trices ….1/;….2/;:::;….n/NUL1/as the matrices L.1/;L.2/;:::;L.n/NUL1/are computed.
25.1-8
The F ASTER -ALL-PAIRS -SHORTEST -PATHS procedure, as written, requires us to
storedlg.n/NUL1/ematrices, each with n2elements, for a total space requirement of
‚.n2lgn/. Modify the procedure to require only ‚.n2/space by using only two
n/STXnmatrices.
25.1-9
Modify F ASTER -ALL-PAIRS -SHORTEST -PATHS so that it can determine whether
the graph contains a negative-weight cycle.25.2 The Floyd-Warshall algorithm 693
25.1-10
Give an efﬁcient algorithm to ﬁnd the length (number of edges) of a minimum-length negative-weight cycle in a graph.
25.2 The Floyd-Warshall algorithm
In this section, we shall use a different dynamic-programming formulation to solvethe all-pairs shortest-paths problem on a directed graph GD.V; E/ . The result-
ing algorithm, known as the Floyd-Warshall algorithm , runs in ‚.V
3/time. As
before, negative-weight edges may be present, but we assume that there are nonegative-weight cycles. As in Section 25.1, we follow the dynamic-programmingprocess to develop the algorithm. After studying the resulting algorithm, wepresent a similar method for ﬁnding the transitive closure of a directed graph.
The structure of a shortest path
In the Floyd-Warshall algorithm, we characterize the structure of a shortest path
differently from how we characterized it in Section 25.1. The Floyd-Warshall algo-rithm considers the intermediate vertices of a shortest path, where an intermediate
vertex of a simple path pDh/ETB
1;/ETB2;:::;/ETB liis any vertex of pother than /ETB1or/ETBl,
that is, any vertex in the set f/ETB2;/ETB3;:::;/ETB l/NUL1g.
The Floyd-Warshall algorithm relies on the following observation. Under our
assumption that the vertices of GareVDf1 ;2;:::;ng, let us consider a subset
f1 ;2;:::;kgof vertices for some k. For any pair of vertices i;j2V, consider all
paths from itojwhose intermediate vertices are all drawn from f1 ;2;:::;kg,a n d
letpbe a minimum-weight path from among them. (Path pis simple.) The Floyd-
Warshall algorithm exploits a relationship between path pand shortest paths from i
tojwith all intermediate vertices in the set f1 ;2;:::;k/NUL1g. The relationship
depends on whether or not kis an intermediate vertex of path p.
/SIIfkis not an intermediate vertex of path p, then all intermediate vertices of
path pare in the setf1 ;2;:::;k/NUL1g. Thus, a shortest path from vertex i
to vertex jwith all intermediate vertices in the set f1 ;2;:::;k/NUL1gis also a
shortest path from itojwith all intermediate vertices in the set f1 ;2;:::;kg.
/SIIfkis an intermediate vertex of path p, then we decompose pintoip1;kp2;j,
as Figure 25.3 illustrates. By Lemma 24.1, p1is a shortest path from itok
with all intermediate vertices in the set f1 ;2;:::;kg. In fact, we can make a
slightly stronger statement. Because vertex kis not an intermediate vertex of
pathp1, all intermediate vertices of p1are in the setf1 ;2;:::;k/NUL1g.T h e r e -694 Chapter 25 All-Pairs Shortest Paths
ik
jp1 p2
p: all intermediate vertices in f1 ;2;:::;kgall intermediate vertices in f1 ;2;:::;k/NUL1g all intermediate vertices in f1 ;2;:::;k/NUL1g
Figure 25.3 Path pis a shortest path from vertex ito vertex j,a n d kis the highest-numbered
intermediate vertex of p.P a t h p1, the portion of path pfrom vertex ito vertex k, has all intermediate
vertices in the setf1 ;2;:::;k/NUL1g. The same holds for path p2from vertex kto vertex j.
fore, p1is a shortest path from itokwith all intermediate vertices in the set
f1 ;2;:::;k/NUL1g. Similarly, p2is a shortest path from vertex kto vertex jwith
all intermediate vertices in the set f1 ;2;:::;k/NUL1g.
A recursive solution to the all-pairs shortest-paths problem
Based on the above observations, we deﬁne a recursive formulation of shortest-
path estimates that differs from the one in Section 25.1. Let d.k/
ijbe the weight
of a shortest path from vertex ito vertex jfor which all intermediate vertices
are in the setf1 ;2;:::;kg.W h e n kD0, a path from vertex ito vertex jwith
no intermediate vertex numbered higher than 0has no intermediate vertices at all.
Such a path has at most one edge, and hence d.0/
ijDwij. Following the above
discussion, we deﬁne d.k/
ijrecursively by
d.k/
ijD(
wij ifkD0;
min/NUL
d.k/NUL1/
ij;d.k/NUL1/
ikCd.k/NUL1/
kj/SOH
ifk/NAK1:(25.5)
Because for any path, all intermediate vertices are in the set f1 ;2;:::;ng,t h em a -
trixD.n/D/NUL
d.n/
ij/SOH
gives the ﬁnal answer: d.n/
ijDı.i;j/ for all i;j2V.
Computing the shortest-path weights bottom up
Based on recurrence (25.5), we can use the following bottom-up procedure to com-
pute the values d.k/
ijin order of increasing values of k. Its input is an n/STXnmatrix W
deﬁned as in equation (25.1). The procedure returns the matrix D.n/of shortest-
path weights.25.2 The Floyd-Warshall algorithm 695
FLOYD -WARSHALL .W /
1nDW:rows
2D.0/DW
3forkD1ton
4l e t D.k/D/NUL
d.k/
ij/SOH
b ean e w n/STXnmatrix
5 foriD1ton
6 forjD1ton
7 d.k/
ijDmin/NUL
d.k/NUL1/
ij;d.k/NUL1/
ikCd.k/NUL1/
kj/SOH
8return D.n/
Figure 25.4 shows the matrices D.k/computed by the Floyd-Warshall algorithm
for the graph in Figure 25.1.
The running time of the Floyd-Warshall algorithm is determined by the triply
nested forloops of lines 3–7. Because each execution of line 7 takes O.1/ time,
the algorithm runs in time ‚.n3/. As in the ﬁnal algorithm in Section 25.1, the
code is tight, with no elaborate data structures, and so the constant hidden in the
‚-notation is small. Thus, the Floyd-Warshall algorithm is quite practical for even
moderate-sized input graphs.
Constructing a shortest path
There are a variety of different methods for constructing shortest paths in the Floyd-
Warshall algorithm. One way is to compute the matrix Dof shortest-path weights
and then construct the predecessor matrix …from the Dmatrix. Exercise 25.1-6
asks you to implement this method so that it runs in O.n3/time. Given the pre-
decessor matrix …,t h eP RINT -ALL-PAIRS -SHORTEST -PATH procedure will print
the vertices on a given shortest path.
Alternatively, we can compute the predecessor matrix …while the algorithm
computes the matrices D.k/. Speciﬁcally, we compute a sequence of matrices
….0/;….1/;:::;….n/,w h e r e …D….n/and we deﬁne /EM.k/
ijas the predecessor of
vertex jon a shortest path from vertex iwith all intermediate vertices in the set
f1 ;2;:::;kg.
We can give a recursive formulation of /EM.k/
ij.W h e n kD0, a shortest path from i
tojhas no intermediate vertices at all. Thus,
/EM.0/
ijD(
NIL ifiDjorwijD1 ;
i ifi¤jandwij<1:(25.6)
Fork/NAK1, if we take the path i;k;j,w h e r e k¤j, then the predecessor
ofjwe choose is the same as the predecessor of jwe chose on a shortest path
from kwith all intermediate vertices in the set f1 ;2;:::;k/NUL1g. Otherwise, we696 Chapter 25 All-Pairs Shortest Paths
D.0/D/EOT03 81/NUL 4
1 01 17
1 4011
21/NUL 501
111 60˘
….0/D/EOT
NIL 11 NIL 1
NIL NIL NIL 22
NIL 3 NIL NIL NIL
4 NIL 4 NIL NIL
NIL NIL NIL 5 NIL˘
D.1/D/EOT03 81/NUL 4
1 01 17
1 4011
25/NUL50/NUL2
111 60˘
….1/D/EOT
NIL 11 NIL 1
NIL NIL NIL 22
NIL 3 NIL NIL NIL
414 NIL 1
NIL NIL NIL 5 NIL˘
D.2/D/EOT03 8 4/NUL4
1 01 17
1 40 5 1 1
25/NUL50/NUL2
111 60˘
….2/D/EOT
NIL 1121
NIL NIL NIL 22
NIL 3 NIL 22
414 NIL 1
NIL NIL NIL 5 NIL˘
D.3/D/EOT038 4 /NUL4
1 01 17
1 40 5 1 1
2/NUL1/NUL50/NUL2
111 60˘
….3/D/EOT
NIL 1121
NIL NIL NIL 22
NIL 3 NIL 22
434 NIL 1
NIL NIL NIL 5 NIL˘
D.4/D/EOT03/NUL14/NUL4
30/NUL41/NUL1
740 53
2/NUL1/NUL50/NUL2
851 60˘
….4/D/EOT
NIL 1421
4 NIL 421
43 NIL 21
434 NIL 1
4345 NIL˘
D.5/D/EOT01/NUL32/NUL4
30/NUL41/NUL1
740 53
2/NUL1/NUL50/NUL2
851 60˘
….5/D/EOT
NIL 3451
4 NIL 421
43 NIL 21
434 NIL 1
4345 NIL˘
Figure 25.4 The sequence of matrices D.k/and….k/computed by the Floyd-Warshall algorithm
for the graph in Figure 25.1.25.2 The Floyd-Warshall algorithm 697
choose the same predecessor of jthat we chose on a shortest path from iwith all
intermediate vertices in the set f1 ;2;:::;k/NUL1g. Formally, for k/NAK1,
/EM.k/
ijD(
/EM.k/NUL1/
ij ifd.k/NUL1/
ij/DC4d.k/NUL1/
ikCd.k/NUL1/
kj;
/EM.k/NUL1/
kjifd.k/NUL1/
ij >d.k/NUL1/
ikCd.k/NUL1/
kj:(25.7)
We leave the incorporation of the ….k/matrix computations into the F LOYD -
WARSHALL procedure as Exercise 25.2-3. Figure 25.4 shows the sequence of ….k/
matrices that the resulting algorithm computes for the graph of Figure 25.1. The
exercise also asks for the more difﬁcult task of proving that the predecessor sub-
graph G/EM;iis a shortest-paths tree with root i. Exercise 25.2-7 asks for yet another
way to reconstruct shortest paths.
Transitive closure of a directed graph
Given a directed graph GD.V; E/ with vertex set VDf1 ;2;:::;ng, we might
wish to determine whether Gcontains a path from itojfor all vertex pairs
i;j2V.W ed e ﬁ n et h e transitive closure ofGas the graph G/ETXD.V; E/ETX/,w h e r e
E/ETXDf.i; j /Wthere is a path from vertex ito vertex jinGg:
One way to compute the transitive closure of a graph in ‚.n3/time is to assign
a weight of 1to each edge of Eand run the Floyd-Warshall algorithm. If there is a
path from vertex ito vertex j,w eg e t dij<n. Otherwise, we get dijD1 .
There is another, similar way to compute the transitive closure of Gin‚.n3/
time that can save time and space in practice. This method substitutes the logicaloperations_(logical OR) and^(logical AND) for the arithmetic operations min
andCin the Floyd-Warshall algorithm. For i;j;kD1 ;2;:::;n ,w ed e ﬁ n e t
.k/
ijto
be1if there exists a path in graph Gfrom vertex ito vertex jwith all intermediate
vertices in the setf1 ;2;:::;kg,a n d 0otherwise. We construct the transitive closure
G/ETXD.V; E/ETX/by putting edge .i; j / intoE/ETXif and only if t.n/
ijD1. A recursive
deﬁnition of t.k/
ij, analogous to recurrence (25.5), is
t.0/
ijD(
0ifi¤jand.i; j /62E;
1ifiDjor.i; j /2E;
and for k/NAK1,
t.k/
ijDt.k/NUL1/
ij_/NUL
t.k/NUL1/
ik^t.k/NUL1/
kj/SOH
: (25.8)
As in the Floyd-Warshall algorithm, we compute the matrices T.k/D/NUL
t.k/
ij/SOH
in
order of increasing k.698 Chapter 25 All-Pairs Shortest Paths
1 2
4 3
T.0/D/NUL1000
0111
0110
1011/SOH
T.1/D/NUL1000
0111
0110
1011/SOH
T.2/D/NUL1000
0111
0111
1011/SOH
T.3/D/NUL1000
01110111
1111/SOH
T.4/D/NUL1000
11111111
1111/SOH
Figure 25.5 A directed graph and the matrices T.k/computed by the transitive-closure algorithm.
TRANSITIVE -CLOSURE .G/
1nDjG:Vj
2l e t T.0/D/NUL
t.0/
ij/SOH
b ean e w n/STXnmatrix
3foriD1ton
4 forjD1ton
5 ifi==jor.i; j /2G:E
6 t.0/
ijD1
7 elset.0/
ijD0
8forkD1ton
9l e t T.k/D/NUL
t.k/
ij/SOH
be a new n/STXnmatrix
10 foriD1ton
11 forjD1ton
12 t.k/
ijDt.k/NUL1/
ij_/NUL
t.k/NUL1/
ik^t.k/NUL1/
kj/SOH
13return T.n/
Figure 25.5 shows the matrices T.k/computed by the T RANSITIVE -CLOSURE
procedure on a sample graph. The T RANSITIVE -CLOSURE procedure, like the
Floyd-Warshall algorithm, runs in ‚.n3/time. On some computers, though, log-
ical operations on single-bit values execute faster than arithmetic operations on
integer words of data. Moreover, because the direct transitive-closure algorithm
uses only boolean values rather than integer values, its space requirement is less25.2 The Floyd-Warshall algorithm 699
than the Floyd-Warshall algorithm’s by a factor corresponding to the size of a word
of computer storage.
Exercises
25.2-1
Run the Floyd-Warshall algorithm on the weighted, directed graph of Figure 25.2.Show the matrix D
.k/that results for each iteration of the outer loop.
25.2-2
Show how to compute the transitive closure using the technique of Section 25.1.
25.2-3
Modify the F LOYD -WARSHALL procedure to compute the ….k/matrices according
to equations (25.6) and (25.7). Prove rigorously that for all i2V, the predecessor
subgraph G/EM;iis a shortest-paths tree with root i.(Hint: To show that G/EM;iis
acyclic, ﬁrst show that /EM.k/
ijDlimplies d.k/
ij/NAKd.k/
ilCwlj, according to the
deﬁnition of /EM.k/
ij. Then, adapt the proof of Lemma 24.16.)
25.2-4
As it appears above, the Floyd-Warshall algorithm requires ‚.n3/space, since we
compute d.k/
ijfori;j;kD1 ;2;:::;n . Show that the following procedure, which
simply drops all the superscripts, is correct, and thus only ‚.n2/space is required.
FLOYD -WARSHALL0.W /
1nDW:rows
2DDW
3forkD1ton
4 foriD1ton
5 forjD1ton
6 dijDmin.dij;dikCdkj/
7return D
25.2-5
Suppose that we modify the way in which equation (25.7) handles equality:
/EM.k/
ijD(
/EM.k/NUL1/
ij ifd.k/NUL1/
ij <d.k/NUL1/
ikCd.k/NUL1/
kj;
/EM.k/NUL1/
kjifd.k/NUL1/
ij/NAKd.k/NUL1/
ikCd.k/NUL1/
kj:
Is this alternative deﬁnition of the predecessor matrix …correct?700 Chapter 25 All-Pairs Shortest Paths
25.2-6
How can we use the output of the Floyd-Warshall algorithm to detect the presenceof a negative-weight cycle?
25.2-7
Another way to reconstruct shortest paths in the Floyd-Warshall algorithm uses
values /RS
.k/
ijfori;j;kD1 ;2;:::;n ,w h e r e /RS.k/
ijis the highest-numbered interme-
diate vertex of a shortest path from itojin which all intermediate vertices are
in the setf1 ;2;:::;kg. Give a recursive formulation for /RS.k/
ij, modify the F LOYD -
WARSHALL procedure to compute the /RS.k/
ijvalues, and rewrite the P RINT -ALL-
PAIRS -SHORTEST -PATH procedure to take the matrix ˆD/NUL
/RS.n/
ij/SOH
as an input.
How is the matrix ˆlike the stable in the matrix-chain multiplication problem of
Section 15.2?
25.2-8
Give an O.VE/ -time algorithm for computing the transitive closure of a directed
graph GD.V; E/ .
25.2-9
Suppose that we can compute the transitive closure of a directed acyclic graph inf.jVj;jEj/time, where fis a monotonically increasing function of jVjandjEj.
Show that the time to compute the transitive closure G
/ETXD.V; E/ETX/of a general
directed graph GD.V; E/ is then f.jVj;jEj/CO.VCE/ETX/.
25.3 Johnson’s algorithm for sparse graphs
Johnson’s algorithm ﬁnds shortest paths between all pairs in O.V2lgVCVE/
time. For sparse graphs, it is asymptotically faster than either repeated squaring ofmatrices or the Floyd-Warshall algorithm. The algorithm either returns a matrix of
shortest-path weights for all pairs of vertices or reports that the input graph contains
a negative-weight cycle. Johnson’s algorithm uses as subroutines both Dijkstra’salgorithm and the Bellman-Ford algorithm, which Chapter 24 describes.
Johnson’s algorithm uses the technique of reweighting , which works as follows.
If all edge weights win a graph GD.V; E/ are nonnegative, we can ﬁnd short-
est paths between all pairs of vertices by running Dijkstra’s algorithm once fromeach vertex; with the Fibonacci-heap min-priority queue, the running time of thisall-pairs algorithm is O.V
2lgVCVE/.I fGhas negative-weight edges but no
negative-weight cycles, we simply compute a new set of nonnegative edge weights25.3 Johnson’s algorithm for sparse graphs 701
that allows us to use the same method. The new set of edge weights ywmust satisfy
two important properties:
1. For all pairs of vertices u; /ETB2V,ap a t h pis a shortest path from uto/ETBusing
weight function wif and only if pis also a shortest path from uto/ETBusing
weight functionyw.
2. For all edges .u; /ETB/ , the new weightyw.u;/ETB/ is nonnegative.
As we shall see in a moment, we can preprocess Gto determine the new weight
functionywinO.VE/ time.
Preserving shortest paths by reweighting
The following lemma shows how easily we can reweight the edges to satisfy the
ﬁrst property above. We use ıto denote shortest-path weights derived from weight
function wandyıto denote shortest-path weights derived from weight function yw.
Lemma 25.1 (Reweighting does not change shortest paths)
Given a weighted, directed graph GD.V; E/ with weight function wWE! R,
lethWV! Rbe any function mapping vertices to real numbers. For each edge
.u; /ETB/2E,d e ﬁ n e
yw.u;/ETB/Dw.u;/ETB/Ch.u//NULh./ETB/ : (25.9)
LetpDh/ETB0;/ETB1;:::;/ETB kibe any path from vertex /ETB0to vertex /ETBk.T h e n pis a
shortest path from /ETB0to/ETBkwith weight function wif and only if it is a shortest path
with weight function yw.T h a ti s , w.p/Dı./ETB0;/ETBk/if and only ifyw.p/Dyı./ETB0;/ETBk/.
Furthermore, Ghas a negative-weight cycle using weight function wif and only
ifGhas a negative-weight cycle using weight function yw.
Proof We start by showing that
yw.p/Dw.p/Ch./ETB 0//NULh./ETB k/: (25.10)
We have
yw.p/DkX
iD1yw./ETB i/NUL1;/ETBi/
DkX
iD1.w./ETB i/NUL1;/ETBi/Ch./ETB i/NUL1//NULh./ETB i//
DkX
iD1w./ETB i/NUL1;/ETBi/Ch./ETB 0//NULh./ETB k/ (because the sum telescopes)
Dw.p/Ch./ETB 0//NULh./ETB k/:702 Chapter 25 All-Pairs Shortest Paths
Therefore, any path pfrom /ETB0to/ETBkhasyw.p/Dw.p/Ch./ETB 0//NULh./ETB k/.B e -
cause h./ETB 0/andh./ETB k/do not depend on the path, if one path from /ETB0to/ETBkis
shorter than another using weight function w, then it is also shorter using yw. Thus,
w.p/Dı./ETB0;/ETBk/if and only ifyw.p/Dyı./ETB0;/ETBk/.
Finally, we show that Ghas a negative-weight cycle using weight function wif
and only if Ghas a negative-weight cycle using weight function yw. Consider any
cycle cDh/ETB0;/ETB1;:::;/ETB ki,w h e r e /ETB0D/ETBk. By equation (25.10),
yw.c/Dw.c/Ch./ETB 0//NULh./ETB k/
Dw.c/ ;
and thus chas negative weight using wif and only if it has negative weight us-
ingyw.
Producing nonnegative weights by reweighting
Our next goal is to ensure that the second property holds: we want yw.u;/ETB/ to be
nonnegative for all edges .u; /ETB/2E. Given a weighted, directed graph GD
.V; E/ with weight function wWE! R,w em a k ean e wg r a p h G0D.V0;E0/,
where V0DV[fsgfor some new vertex s62VandE0DE[f.s; /ETB/W/ETB2Vg.
We extend the weight function wso that w.s;/ETB/D0for all /ETB2V. Note that
because shas no edges that enter it, no shortest paths in G0, other than those with
source s, contain s. Moreover, G0has no negative-weight cycles if and only if G
has no negative-weight cycles. Figure 25.6(a) shows the graph G0corresponding
to the graph Gof Figure 25.1.
Now suppose that GandG0have no negative-weight cycles. Let us deﬁne
h./ETB/Dı.s; /ETB/ for all /ETB2V0. By the triangle inequality (Lemma 24.10),
we have h./ETB//DC4h.u/Cw.u;/ETB/ for all edges .u; /ETB/2E0. Thus, if we de-
ﬁne the new weights ywby reweighting according to equation (25.9), we have
yw.u;/ETB/Dw.u;/ETB/Ch.u//NULh./ETB//NAK0, and we have satisﬁed the second property.
Figure 25.6(b) shows the graph G0from Figure 25.6(a) with reweighted edges.
Computing all-pairs shortest paths
Johnson’s algorithm to compute all-pairs shortest paths uses the Bellman-Ford al-
gorithm (Section 24.1) and Dijkstra’s algorithm (Section 24.3) as subroutines. It
assumes implicitly that the edges are stored in adjacency lists. The algorithm re-
turns the usualjVj/STXjVjmatrix DDdij,w h e r e dijDı.i;j/ , or it reports that
the input graph contains a negative-weight cycle. As is typical for an all-pairs
shortest-paths algorithm, we assume that the vertices are numbered from 1tojVj.25.3 Johnson’s algorithm for sparse graphs 703
2
1
5434
82
67 10
0
0
0
00
0
2/1
2/–3
2/2 0/–42/3 0/–4
0/1 2/–12/70/4
0/5 2/3
2/20/–1
0/–5
2/–24/82/5
2/1
2/6(a)
(c)(b)–4
–4–1
–5
–532
1
5440
132
210 05
1
0
4
00
00
–4–1
–5
03
2
1
544 0
132
210 0003
(d)2
1
5440
132
210 0003
(e)2
1
544 0
132
210 0003
(f)2
1
544 0
132
210 0003
(g)2
1
544 0
132
210 0003
0/0 0/00/00/0
0/00 0
Figure 25.6 Johnson’s all-pairs shortest-paths algorithm run on the graph of Figure 25.1. Ver-
tex numbers appear outside the vertices. (a)The graph G0with the original weight function w.
The new vertex sis black. Within each vertex /ETBish./ETB/Dı.s; /ETB/ .(b)After reweighting each
edge .u; /ETB/ with weight function yw.u;/ETB/Dw.u;/ETB/Ch.u//NULh./ETB/.(c)–(g) The result of running
is black, and shaded edges are in the shortest-paths tree computed by the algorithm. Within each
vertex /ETBare the valuesyı.u;/ETB/ andı.u; /ETB/ , separated by a slash. The value du/ETBDı.u; /ETB/ is equal to
yı. /ETB/Ch./ETB//NULh.u/Dijkstra’s algorithm on each vertex of Gusing weight function wy. In each part, the source vertex u
. u;704 Chapter 25 All-Pairs Shortest Paths
JOHNSON .G; w/
1 compute G0,w h e r e G0:VDG:V[fsg,
G0:EDG:E[f.s; /ETB/W/ETB2G:Vg,a n d
w.s;/ETB/D0for all /ETB2G:V
2ifBELLMAN -FORD.G0;w;s/ ==FALSE
3 print “the input graph contains a negative-weight cycle”
4else for each vertex /ETB2G0:V
5 set h./ETB/ to the value of ı.s; /ETB/
computed by the Bellman-Ford algorithm
6 foreach edge .u; /ETB/2G0:E
7yw.u;/ETB/Dw.u;/ETB/Ch.u//NULh./ETB/
8l e t DD.du/ETB/be a new n/STXnmatrix
9 foreach vertex u2G:V
10 run D IJKSTRA .G;yw;u/ to computeyı.u;/ETB/ for all /ETB2G:V
11 foreach vertex /ETB2G:V
12 du/ETBDyı.u;/ETB/Ch./ETB//NULh.u/
13 return D
This code simply performs the actions we speciﬁed earlier. Line 1 produces G0.
Line 2 runs the Bellman-Ford algorithm on G0with weight function wand source
vertex s.I fG0, and hence G, contains a negative-weight cycle, line 3 reports the
problem. Lines 4–12 assume that G0contains no negative-weight cycles. Lines 4–5
seth./ETB/ to the shortest-path weight ı.s; /ETB/ computed by the Bellman-Ford algo-
rithm for all /ETB2V0. Lines 6–7 compute the new weights yw. For each pair of ver-
ticesu; /ETB2V,t h eforloop of lines 9–12 computes the shortest-path weight yı.u;/ETB/
by calling Dijkstra’s algorithm once from each vertex in V. Line 12 stores in
matrix entry du/ETBthe correct shortest-path weight ı.u;/ETB/ , calculated using equa-
tion (25.10). Finally, line 13 returns the completed Dmatrix. Figure 25.6 depicts
the execution of Johnson’s algorithm.
If we implement the min-priority queue in Dijkstra’s algorithm by a Fibonacci
heap, Johnson’s algorithm runs in O.V2lgVCVE/ time. The simpler binary min-
heap implementation yields a running time of O.VE lgV/, which is still asymp-
totically faster than the Floyd-Warshall algorithm if the graph is sparse.
Exercises
25.3-1
Use Johnson’s algorithm to ﬁnd the shortest paths between all pairs of vertices inthe graph of Figure 25.2. Show the values of handywcomputed by the algorithm.Problems for Chapter 25 705
25.3-2
What is the purpose of adding the new vertex stoV, yielding V0?
25.3-3
Suppose that w.u;/ETB//NAK0for all edges .u; /ETB/2E. What is the relationship
between the weight functions wandyw?
25.3-4
Professor Greenstreet claims that there is a simpler way to reweight edges thanthe method used in Johnson’s algorithm. Letting w
/ETXDmin .u;/ETB/ 2Efw.u;/ETB/g,j u s t
deﬁneyw.u;/ETB/Dw.u;/ETB//NULw/ETXfor all edges .u; /ETB/2E. What is wrong with the
professor’s method of reweighting?
25.3-5
Suppose that we run Johnson’s algorithm on a directed graph Gwith weight func-
tionw. Show that if Gcontains a 0-weight cycle c,t h e nyw.u;/ETB/D0for every
edge .u; /ETB/ inc.
25.3-6
Professor Michener claims that there is no need to create a new source vertex inline 1 of J
OHNSON . He claims that instead we can just use G0DGand let sbe any
vertex. Give an example of a weighted, directed graph Gfor which incorporating
the professor’s idea into J OHNSON causes incorrect answers. Then show that if G
is strongly connected (every vertex is reachable from every other vertex), the resultsreturned by J
OHNSON with the professor’s modiﬁcation are correct.
Problems
25-1 Transitive closure of a dynamic graphSuppose that we wish to maintain the transitive closure of a directed graph GD
.V; E/ as we insert edges into E. That is, after each edge has been inserted, we
want to update the transitive closure of the edges inserted so far. Assume that thegraph Ghas no edges initially and that we represent the transitive closure as a
boolean matrix.
a.Show how to update the transitive closure G
/ETXD.V; E/ETX/of a graph GD.V; E/
inO.V2/time when a new edge is added to G.
b.Give an example of a graph Gand an edge esuch that /DEL.V2/time is required
to update the transitive closure after the insertion of eintoG, no matter what
algorithm is used.706 Chapter 25 All-Pairs Shortest Paths
c.Describe an efﬁcient algorithm for updating the transitive closure as edges are
inserted into the graph. For any sequence of ninsertions, your algorithm should
run in total timePn
iD1tiDO.V3/,w h e r e tiis the time to update the transitive
closure upon inserting the ith edge. Prove that your algorithm attains this time
bound.
25-2 Shortest paths in /SI-dense graphs
Ag r a p h GD.V; E/ is/SI-dense ifjEjD‚.V1C/SI/for some constant /SIin the
range 0</SI/DC41.B y u s i n g d-ary min-heaps (see Problem 6-2) in shortest-paths
algorithms on /SI-dense graphs, we can match the running times of Fibonacci-heap-
based algorithms without using as complicated a data structure.
a.What are the asymptotic running times for I NSERT ,E XTRACT -MIN,a n d
DECREASE -KEY,a saf u n c t i o no f dand the number nof elements in a d-ary
min-heap? What are these running times if we choose dD‚.n˛/for some
constant 0<˛/DC41? Compare these running times to the amortized costs of
these operations for a Fibonacci heap.
b.Show how to compute shortest paths from a single source on an /SI-dense directed
graph GD.V; E/ with no negative-weight edges in O.E/ time. ( Hint: Pickd
a saf u n c t i o no f /SI.)
c.Show how to solve the all-pairs shortest-paths problem on an /SI-dense directed
graph GD.V; E/ with no negative-weight edges in O.VE/ time.
d.Show how to solve the all-pairs shortest-paths problem in O.VE/ time on an
/SI-dense directed graph GD.V; E/ that may have negative-weight edges but
has no negative-weight cycles.
Chapter notes
Lawler [224] has a good discussion of the all-pairs shortest-paths problem, al-though he does not analyze solutions for sparse graphs. He attributes the matrix-multiplication algorithm to the folklore. The Floyd-Warshall algorithm is due toFloyd [105], who based it on a theorem of Warshall [349] that describes how to
compute the transitive closure of boolean matrices. Johnson’s algorithm is taken
from [192].
Several researchers have given improved algorithms for computing shortest
paths via matrix multiplication. Fredman [111] shows how to solve the all-pairs shortest paths problem using O.V
5=2/comparisons between sums of edgeNotes for Chapter 25 707
weights and obtains an algorithm that runs in O.V3.lg lgV=lgV/1=3/time, which
is slightly better than the running time of the Floyd-Warshall algorithm. Han [159]reduced the running time to O.V
3.lg lgV=lgV/5=4/. Another line of research
demonstrates that we can apply algorithms for fast matrix multiplication (see thechapter notes for Chapter 4) to the all-pairs shortest paths problem. Let O.n
!/be
the running time of the fastest algorithm for multiplying n/STXnmatrices; currently
! < 2:376 [78]. Galil and Margalit [123, 124] and Seidel [308] designed algo-
rithms that solve the all-pairs shortest paths problem in undirected, unweighted
graphs in .V!p.V // time, where p.n/ denotes a particular function that is poly-
logarithmically bounded in n. In dense graphs, these algorithms are faster than
theO.VE/ time needed to perform jVjbreadth-ﬁrst searches. Several researchers
have extended these results to give algorithms for solving the all-pairs shortestpaths problem in undirected graphs in which the edge weights are integers in therangef1 ;2;:::;Wg. The asymptotically fastest such algorithm, by Shoshan and
Zwick [316], runs in time O.W V
!p.V W // .
Karger, Koller, and Phillips [196] and independently McGeoch [247] have given
a time bound that depends on E/ETX, the set of edges in Ethat participate in some
shortest path. Given a graph with nonnegative edge weights, their algorithms run in
O.VE/ETXCV2lgV/time and improve upon running Dijkstra’s algorithm jVjtimes
whenjE/ETXjDo.E/ .
Baswana, Hariharan, and Sen [33] examined decremental algorithms for main-
taining all-pairs shortest paths and transitive-closure information. Decremen-
tal algorithms allow a sequence of intermixed edge deletions and queries; by
comparison, Problem 25-1, in which edges are inserted, asks for an incremen-tal algorithm. The algorithms by Baswana, Hariharan, and Sen are randomizedand, when a path exists, their transitive-closure algorithm can fail to report itwith probability 1=n
cfor an arbitrary c>0 . The query times are O.1/ with
high probability. For transitive closure, the amortized time for each update isO.V
4=3lg1=3V/. For all-pairs shortest paths, the update times depend on the
queries. For queries just giving the shortest-path weights, the amortized time perupdate is O.V
3=Elg2V/. To report the actual shortest path, the amortized up-
date time is min .O.V3=2p
lgV/ ;O . V3=Elg2V/ /. Demetrescu and Italiano [84]
showed how to handle update and query operations when edges are both insertedand deleted, as long as each given edge has a bounded range of possible valuesdrawn from the real numbers.
Aho, Hopcroft, and Ullman [5] deﬁned an algebraic structure known as a “closed
semiring,” which serves as a general framework for solving path problems in di-
rected graphs. Both the Floyd-Warshall algorithm and the transitive-closure algo-rithm from Section 25.2 are instantiations of an all-pairs algorithm based on closedsemirings. Maggs and Plotkin [240] showed how to ﬁnd minimum spanning treesusing a closed semiring.26 Maximum Flow
Just as we can model a road map as a directed graph in order to ﬁnd the shortest
path from one point to another, we can also interpret a directed graph as a “ﬂownetwork” and use it to answer questions about material ﬂows. Imagine a mate-rial coursing through a system from a source, where the material is produced, toa sink, where it is consumed. The source produces the material at some steadyrate, and the sink consumes the material at the same rate. The “ﬂow” of the mate-rial at any point in the system is intuitively the rate at which the material moves.Flow networks can model many problems, including liquids ﬂowing through pipes,parts through assembly lines, current through electrical networks, and informationthrough communication networks.
We can think of each directed edge in a ﬂow network as a conduit for the mate-
rial. Each conduit has a stated capacity, given as a maximum rate at which the ma-terial can ﬂow through the conduit, such as 200gallons of liquid per hour through
ap i p eo r 20amperes of electrical current through a wire. Vertices are conduit
junctions, and other than the source and sink, material ﬂows through the vertices
without collecting in them. In other words, the rate at which material enters a ver-tex must equal the rate at which it leaves the vertex. We call this property “ﬂowconservation,” and it is equivalent to Kirchhoff’s current law when the material iselectrical current.
In the maximum-ﬂow problem, we wish to compute the greatest rate at which
we can ship material from the source to the sink without violating any capacityconstraints. It is one of the simplest problems concerning ﬂow networks and, aswe shall see in this chapter, this problem can be solved by efﬁcient algorithms.Moreover, we can adapt the basic techniques used in maximum-ﬂow algorithms tosolve other network-ﬂow problems.
This chapter presents two general methods for solving the maximum-ﬂow prob-
lem. Section 26.1 formalizes the notions of ﬂow networks and ﬂows, formallydeﬁning the maximum-ﬂow problem. Section 26.2 describes the classical method
of Ford and Fulkerson for ﬁnding maximum ﬂows. An application of this method,26.1 Flow networks 709
ﬁnding a maximum matching in an undirected bipartite graph, appears in Sec-
tion 26.3. Section 26.4 presents the push-relabel method, which underlies many ofthe fastest algorithms for network-ﬂow problems. Section 26.5 covers the “relabel-to-front” algorithm, a particular implementation of the push-relabel method thatruns in time O.V
3/. Although this algorithm is not the fastest algorithm known,
it illustrates some of the techniques used in the asymptotically fastest algorithms,and it is reasonably efﬁcient in practice.
26.1 Flow networks
In this section, we give a graph-theoretic deﬁnition of ﬂow networks, discuss theirproperties, and deﬁne the maximum-ﬂow problem precisely. We also introducesome helpful notation.
Flow networks and ﬂows
Aﬂow network GD.V; E/ is a directed graph in which each edge .u; /ETB/2E
has a nonnegative capacity c.u;/ETB//NAK0. We further require that if Econtains an
edge .u; /ETB/ , then there is no edge ./ETB; u/ in the reverse direction. (We shall see
shortly how to work around this restriction.) If .u; /ETB/62E, then for convenience
we deﬁne c.u;/ETB/D0, and we disallow self-loops. We distinguish two vertices
in a ﬂow network: a source sand asinkt. For convenience, we assume that each
vertex lies on some path from the source to the sink. That is, for each vertex /ETB2V,
the ﬂow network contains a path s;/ETB;t. The graph is therefore connected
and, since each vertex other than shas at least one entering edge, jEj/NAKjVj/NUL1.
Figure 26.1 shows an example of a ﬂow network.
We are now ready to deﬁne ﬂows more formally. Let GD.V; E/ be a ﬂow
network with a capacity function c.L e t sbe the source of the network, and let tbe
the sink. A ﬂow inGis a real-valued function fWV/STXV! Rthat satisﬁes the
following two properties:
Capacity constraint: For all u; /ETB2V, we require 0/DC4
f. u ;/ETB//DC4c.u;/ETB/ .
Flow conservation: For all u2V/NULfs; tg, we require
X
/ETB2Vf. /ETB;u /DX
/ETB2Vf. u ;/ETB/:
When .u; /ETB/62E, there can be no ﬂow from uto/ETB,a n d f. u ;/ETB/D0.710 Chapter 26 Maximum Flow
st1612
20794
13
144Edmonton
CalgarySaskatoon
ReginaVancouver Winnipeg
st11/1612/12
15/207/7 4/91/4
8/13
11/144/4
(a) (b)v1 v1
v2 v2v3 v3
v4 v4
Figure 26.1 (a) A ﬂow network GD.V; E/ for the Lucky Puck Company’s trucking problem.
The Vancouver factory is the source s, and the Winnipeg warehouse is the sink t. The company ships
pucks through intermediate cities, but only c.u;/ETB/ crates per day can go from city uto city /ETB. Each
edge is labeled with its capacity. (b)Aﬂ o w finGwith valuejfjD19. Each edge .u; /ETB/ is labeled
byf. u ;/ETB/ = c. u ;/ETB/ . The slash notation merely separates the ﬂow and capacity; it does not indicate
division.
We call the nonnegative quantity f. u ;/ETB/ the ﬂow from vertex uto vertex /ETB.T h e
valuejfjof a ﬂow fis deﬁned as
jfjDX
/ETB2Vf. s ;/ETB//NULX
/ETB2Vf. /ETB;s/; (26.1)
that is, the total ﬂow out of the source minus the ﬂow into the source. (Here, the j/SOHj
notation denotes ﬂow value, not absolute value or cardinality.) Typically, a ﬂow
network will not have any edges into the source, and the ﬂow into the source, givenby the summationP
/ETB2Vf. /ETB;s/ , will be 0. We include it, however, because when
we introduce residual networks later in this chapter, the ﬂow into the source willbecome signiﬁcant. In the maximum-ﬂow problem , we are given a ﬂow network G
with source sand sink t, and we wish to ﬁnd a ﬂow of maximum value.
Before seeing an example of a network-ﬂow problem, let us brieﬂy explore the
deﬁnition of ﬂow and the two ﬂow properties. The capacity constraint simply
says that the ﬂow from one vertex to another must be nonnegative and must not
exceed the given capacity. The ﬂow-conservation property says that the total ﬂow
into a vertex other than the source or sink must equal the total ﬂow out of that
vertex—informally, “ﬂow in equals ﬂow out.”
An example of ﬂow
A ﬂow network can model the trucking problem shown in Figure 26.1(a). The
Lucky Puck Company has a factory (source s) in Vancouver that manufactures
hockey pucks, and it has a warehouse (sink t) in Winnipeg that stocks them. Lucky26.1 Flow networks 711
st1612
20794
13
144
(a) (b)v1
v2v3
v410st1612
20794
13
144v1
v2v3
v4v′10
10
Figure 26.2 Converting a network with antiparallel edges to an equivalent one with no antiparallel
edges. (a)A ﬂow network containing both the edges ./ETB1;/ETB2/and./ETB2;/ETB1/.(b)An equivalent network
with no antiparallel edges. We add the new vertex /ETB0, and we replace edge ./ETB1;/ETB2/by the pair of
edges ./ETB1;/ETB0/and./ETB0;/ETB2/, both with the same capacity as ./ETB1;/ETB2/.
Puck leases space on trucks from another ﬁrm to ship the pucks from the factory
to the warehouse. Because the trucks travel over speciﬁed routes (edges) betweencities (vertices) and have a limited capacity, Lucky Puck can ship at most c.u;/ETB/
crates per day between each pair of cities uand/ETBin Figure 26.1(a). Lucky Puck
has no control over these routes and capacities, and so the company cannot alter
the ﬂow network shown in Figure 26.1(a). They need to determine the largest
number pof crates per day that they can ship and then to produce this amount, since
there is no point in producing more pucks than they can ship to their warehouse.Lucky Puck is not concerned with how long it takes for a given puck to get fromthe factory to the warehouse; they care only that pcrates per day leave the factory
andpcrates per day arrive at the warehouse.
We can model the “ﬂow” of shipments with a ﬂow in this network because the
number of crates shipped per day from one city to another is subject to a capacityconstraint. Additionally, the model must obey ﬂow conservation, for in a steadystate, the rate at which pucks enter an intermediate city must equal the rate at whichthey leave. Otherwise, crates would accumulate at intermediate cities.
Modeling problems with antiparallel edges
Suppose that the trucking ﬁrm offered Lucky Puck the opportunity to lease space
for 10 crates in trucks going from Edmonton to Calgary. It would seem natural toadd this opportunity to our example and form the network shown in Figure 26.2(a).This network suffers from one problem, however: it violates our original assump-tion that if an edge ./ETB
1;/ETB2/2E,t h e n ./ETB2;/ETB1/62E. We call the two edges ./ETB1;/ETB2/
and./ETB2;/ETB1/antiparallel . Thus, if we wish to model a ﬂow problem with antipar-
allel edges, we must transform the network into an equivalent one containing no712 Chapter 26 Maximum Flow
antiparallel edges. Figure 26.2(b) displays this equivalent network. We choose
one of the two antiparallel edges, in this case ./ETB1;/ETB2/, and split it by adding a new
vertex /ETB0and replacing edge ./ETB1;/ETB2/with the pair of edges ./ETB1;/ETB0/and./ETB0;/ETB2/.
We also set the capacity of both new edges to the capacity of the original edge.The resulting network satisﬁes the property that if an edge is in the network, thereverse edge is not. Exercise 26.1-1 asks you to prove that the resulting network isequivalent to the original one.
Thus, we see that a real-world ﬂow problem might be most naturally modeled
by a network with antiparallel edges. It will be convenient to disallow antipar-
allel edges, however, and so we have a straightforward way to convert a networkcontaining antiparallel edges into an equivalent one with no antiparallel edges.
Networks with multiple sources and sinks
A maximum-ﬂow problem may have several sources and sinks, rather than just
one of each. The Lucky Puck Company, for example, might actually have a setofmfactoriesfs
1;s2;:::;s mgand a set of nwarehousesft1;t2;:::;t ng,a ss h o w n
in Figure 26.3(a). Fortunately, this problem is no harder than ordinary maximumﬂow.
We can reduce the problem of determining a maximum ﬂow in a network with
multiple sources and multiple sinks to an ordinary maximum-ﬂow problem. Fig-
ure 26.3(b) shows how to convert the network from (a) to an ordinary ﬂow network
with only a single source and a single sink. We add a supersource sand add a
directed edge .s; s
i/with capacity c.s;s i/D1 for each iD1 ;2;:::;m .W ea l s o
create a new supersink tand add a directed edge .ti;t/with capacity c.ti;t/D1
for each iD1 ;2;:::;n . Intuitively, any ﬂow in the network in (a) corresponds to
a ﬂow in the network in (b), and vice versa. The single source ssimply provides
as much ﬂow as desired for the multiple sources si, and the single sink tlikewise
consumes as much ﬂow as desired for the multiple sinks ti. Exercise 26.1-2 asks
you to prove formally that the two problems are equivalent.
Exercises
26.1-1
Show that splitting an edge in a ﬂow network yields an equivalent network. Moreformally, suppose that ﬂow network Gcontains edge .u; /ETB/ , and we create a new
ﬂow network G
0by creating a new vertex xand replacing .u; /ETB/ by new edges
.u; x/ and.x; /ETB/ withc.u;x/Dc.x;/ETB/Dc.u;/ETB/ . Show that a maximum ﬂow
inG0has the same value as a maximum ﬂow in G.26.1 Flow networks 713
10
(a)12
5
8
14
7
11
23
15
6
20
13
1810
12
5
8
14
7
11
23
15
6
20
13
18∞∞∞∞
∞
∞
∞∞s1 s1
s2 s2
s3 s3
s4 s4
s5 s5t1 t1
t2 t2
t3 t3
(b)st
Figure 26.3 Converting a multiple-source, multiple- sink maximum-ﬂow problem into a problem
with a single source and a single sink. (a)A ﬂow network with ﬁve sources SDfs1;s2;s3;s4;s5g
and three sinks TDft1;t2;t3g.(b)An equivalent single-source, single-sink ﬂow network. We add
a supersource sand an edge with inﬁnite capacity from sto each of the multiple sources. We also
add a supersink tand an edge with inﬁnite capacity from each of the multiple sinks to t.
26.1-2
Extend the ﬂow properties and deﬁnitions to the multiple-source, multiple-sinkproblem. Show that any ﬂow in a multiple-source, multiple-sink ﬂow networkcorresponds to a ﬂow of identical value in the single-source, single-sink networkobtained by adding a supersource and a supersink, and vice versa.
26.1-3
Suppose that a ﬂow network GD.V; E/ violates the assumption that the network
contains a path s;/ETB;tfor all vertices /ETB2V.L e t ube a vertex for which there
is no path s;u;t. Show that there must exist a maximum ﬂow finGsuch
thatf. u ;/ETB/Df. /ETB;u /D0for all vertices /ETB2V.714 Chapter 26 Maximum Flow
26.1-4
Letfbe a ﬂow in a network, and let ˛be a real number. The scalar ﬂow product ,
denoted ˛f, is a function from V/STXVtoRdeﬁned by
.˛f /.u; /ETB/D˛/SOHf. u ;/ETB/:
Prove that the ﬂows in a network form a convex set . That is, show that if f1andf2
are ﬂows, then so is ˛f1C.1/NUL˛/f 2for all ˛in the range 0/DC4˛/DC41.
26.1-5
State the maximum-ﬂow problem as a linear-programming problem.
26.1-6
Professor Adam has two children who, unfortunately, dislike each other. The prob-lem is so severe that not only do they refuse to walk to school together, but in facteach one refuses to walk on any block that the other child has stepped on that day.The children have no problem with their paths crossing at a corner. Fortunatelyboth the professor’s house and the school are on corners, but beyond that he is notsure if it is going to be possible to send both of his children to the same school.The professor has a map of his town. Show how to formulate the problem of de-termining whether both his children can go to the same school as a maximum-ﬂowproblem.
26.1-7
Suppose that, in addition to edge capacities, a ﬂow network has vertex capacities .
That is each vertex /ETBhas a limit l./ETB/ on how much ﬂow can pass though /ETB.S h o w
how to transform a ﬂow network GD.V; E/ with vertex capacities into an equiv-
alent ﬂow network G
0D.V0;E0/without vertex capacities, such that a maximum
ﬂow in G0has the same value as a maximum ﬂow in G. How many vertices and
edges does G0have?
26.2 The Ford-Fulkerson method
This section presents the Ford-Fulkerson method for solving the maximum-ﬂow
problem. We call it a “method” rather than an “algorithm” because it encompassesseveral implementations with differing running times. The Ford-Fulkerson methoddepends on three important ideas that transcend the method and are relevant tomany ﬂow algorithms and problems: residual networks, augmenting paths, andcuts. These ideas are essential to the important max-ﬂow min-cut theorem (The-orem 26.6), which characterizes the value of a maximum ﬂow in terms of cuts of26.2 The Ford-Fulkerson method 715
the ﬂow network. We end this section by presenting one speciﬁc implementation
of the Ford-Fulkerson method and analyzing its running time.
The Ford-Fulkerson method iteratively increases the value of the ﬂow. We start
with f. u ;/ETB/D0for all u; /ETB2V, giving an initial ﬂow of value 0. At each
iteration, we increase the ﬂow value in Gby ﬁnding an “augmenting path” in an
associated “residual network” Gf. Once we know the edges of an augmenting
path in Gf, we can easily identify speciﬁc edges in Gfor which we can change
the ﬂow so that we increase the value of the ﬂow. Although each iteration of the
Ford-Fulkerson method increases the value of the ﬂow, we shall see that the ﬂow
on any particular edge of Gmay increase or decrease; decreasing the ﬂow on some
edges may be necessary in order to enable an algorithm to send more ﬂow from thesource to the sink. We repeatedly augment the ﬂow until the residual network hasno more augmenting paths. The max-ﬂow min-cut theorem will show that upontermination, this process yields a maximum ﬂow.
F
ORD-FULKERSON -METHOD . G ;s ;t/
1 initialize ﬂow fto0
2while there exists an augmenting path pin the residual network Gf
3 augment ﬂow falong p
4return f
In order to implement and analyze the Ford-Fulkerson method, we need to intro-
duce several additional concepts.
Residual networks
Intuitively, given a ﬂow network Gand a ﬂow f, the residual network Gfconsists
of edges with capacities that represent how we can change the ﬂow on edges of G.
An edge of the ﬂow network can admit an amount of additional ﬂow equal to theedge’s capacity minus the ﬂow on that edge. If that value is positive, we placethat edge into G
fwith a “residual capacity” of cf.u; /ETB/Dc.u;/ETB//NULf. u ;/ETB/ .
The only edges of Gthat are in Gfare those that can admit more ﬂow; those
edges .u; /ETB/ whose ﬂow equals their capacity have cf.u; /ETB/D0, and they are not
inGf.
The residual network Gfmay also contain edges that are not in G,h o w e v e r .
As an algorithm manipulates the ﬂow, with the goal of increasing the total ﬂow, itmight need to decrease the ﬂow on a particular edge. In order to represent a pos-sible decrease of a positive ﬂow f. u ;/ETB/ on an edge in G, we place an edge ./ETB; u/
intoG
fwith residual capacity cf./ETB; u/Df. u ;/ETB/ —that is, an edge that can admit
ﬂow in the opposite direction to .u; /ETB/ , at most canceling out the ﬂow on .u; /ETB/ .
These reverse edges in the residual network allow an algorithm to send back ﬂow716 Chapter 26 Maximum Flow
it has already sent along an edge. Sending ﬂow back along an edge is equiva-
lent to decreasing the ﬂow on the edge, which is a necessary operation in many
algorithms.
More formally, suppose that we have a ﬂow network GD.V; E/ with source s
and sink t.L e t fbe a ﬂow in G, and consider a pair of vertices u; /ETB2V.W e
deﬁne the residual capacity cf.u; /ETB/ by
cf.u; /ETB/D/c128
c.u;/ETB//NULf. u ;/ETB/ if.u; /ETB/2E;
f. /ETB;u / if./ETB; u/2E;
0 otherwise :(26.2)
Because of our assumption that .u; /ETB/2Eimplies ./ETB; u/62E, exactly one case in
equation (26.2) applies to each ordered pair of vertices.
As an example of equation (26.2), if c.u;/ETB/D16andf. u ;/ETB/D11,t h e nw e
can increase f. u ;/ETB/ by up to cf.u; /ETB/D5units before we exceed the capacity
constraint on edge .u; /ETB/ . We also wish to allow an algorithm to return up to 11
units of ﬂow from /ETBtou, and hence cf./ETB; u/D11.
Given a ﬂow network GD.V; E/ and a ﬂow f,t h eresidual network ofG
induced by fisGfD.V; E f/,w h e r e
EfDf.u; /ETB/2V/STXVWcf.u; /ETB/ > 0g: (26.3)
That is, as promised above, each edge of the residual network, or residual edge ,
can admit a ﬂow that is greater than 0. Figure 26.4(a) repeats the ﬂow network G
and ﬂow fof Figure 26.1(b), and Figure 26.4(b) shows the corresponding residual
network Gf. The edges in Efare either edges in Eor their reversals, and thus
jEfj/DC42jEj:
Observe that the residual network Gfis similar to a ﬂow network with capacities
given by cf. It does not satisfy our deﬁnition of a ﬂow network because it may
contain both an edge .u; /ETB/ and its reversal ./ETB; u/ . Other than this difference, a
residual network has the same properties as a ﬂow network, and we can deﬁne a
ﬂow in the residual network as one that satisﬁes the deﬁnition of a ﬂow, but with
respect to capacities cfin the network Gf.
A ﬂow in a residual network provides a roadmap for adding ﬂow to the original
ﬂow network. If fis a ﬂow in Gandf0is a ﬂow in the corresponding residual
network Gf,w ed e ﬁ n e f"f0,t h eaugmentation of ﬂow fbyf0, to be a function
from V/STXVtoR,d e ﬁ n e db y
.f"f0/.u; /ETB/D(
f. u ;/ETB/Cf0.u; /ETB//NULf0./ETB; u/ if.u; /ETB/2E;
0 otherwise :(26.4)26.2 The Ford-Fulkerson method 717
915
st512
57531
8
114
st11/1612/12
19/207/7 91/4
12/13
11/144/4(b)
(c)11
5
34st11/1612/12
15/207/7 4/91/4
8/13
11/144/4
(d)19
st512
1731
12
11411
1
3v1
v1v1
v1v2
v2v2
v2v3
v3v3
v3v4
v4v4
v4(a)
Figure 26.4 (a) The ﬂow network Gand ﬂow fof Figure 26.1(b). (b)The residual network Gf
with augmenting path pshaded; its residual capacity is cf.p/Dcf./ETB2;/ETB3/D4. Edges with
residual capacity equal to 0,s u c ha s ./ETB1;/ETB3/, are not shown, a convention we follow in the remainder
of this section. (c)The ﬂow in Gthat results from augmenting along path pby its residual capacity 4.
Edges carrying no ﬂow, such as ./ETB3;/ETB2/, are labeled only by their capacity, another convention we
follow throughout. (d)The residual network induced by the ﬂow in (c).
The intuition behind this deﬁnition follows the deﬁnition of the residual network.
We increase the ﬂow on .u; /ETB/ byf0.u; /ETB/ but decrease it by f0./ETB; u/ because
pushing ﬂow on the reverse edge in the residual network signiﬁes decreasing theﬂow in the original network. Pushing ﬂow on the reverse edge in the residualnetwork is also known as cancellation . For example, if we send 5crates of hockey
pucks from uto/ETBand send 2crates from /ETBtou, we could equivalently (from the
perspective of the ﬁnal result) just send 3creates from uto/ETBand none from /ETBtou.
Cancellation of this type is crucial for any maximum-ﬂow algorithm.
Lemma 26.1
LetGD.V; E/ be a ﬂow network with source sand sink t,a n dl e t fb eaﬂ o w
inG.L e t G
fbe the residual network of Ginduced by f,a n dl e t f0be a ﬂow
inGf. Then the function f"f0deﬁned in equation (26.4) is a ﬂow in Gwith
valuejf"f0jDjfjCjf0j.
Proof We ﬁrst verify that f"f0obeys the capacity constraint for each edge in E
and ﬂow conservation at each vertex in V/NULfs; tg.718 Chapter 26 Maximum Flow
For the capacity constraint, ﬁrst observe that if .u; /ETB/2E,t h e n cf./ETB; u/D
f. u ;/ETB/ . Therefore, we have f0./ETB; u//DC4cf./ETB; u/Df. u ;/ETB/ , and hence
.f"f0/.u; /ETB/Df. u ;/ETB/Cf0.u; /ETB//NULf0./ETB; u/ (by equation (26.4))
/NAKf. u ;/ETB/Cf0.u; /ETB//NULf. u ;/ETB/ (because f0./ETB; u//DC4f. u ;/ETB/ )
Df0.u; /ETB/
/NAK0:
In addition,.f"f
0/.u; /ETB/
Df. u ;/ETB/Cf0.u; /ETB//NULf0./ETB; u/ (by equation (26.4))
/DC4f. u ;/ETB/Cf0.u; /ETB/ (because ﬂows are nonnegative)
/DC4f. u ;/ETB/Ccf.u; /ETB/ (capacity constraint)
Df. u ;/ETB/Cc.u;/ETB//NULf. u ;/ETB/ (deﬁnition of cf)
Dc.u;/ETB/ :
For ﬂow conservation, because both fandf0obey ﬂow conservation, we have
that for all u2V/NULfs; tg,X
/ETB2V.f"f0/.u; /ETB/DX
/ETB2V.f .u; /ETB/Cf0.u; /ETB//NULf0./ETB; u//
DX
/ETB2Vf. u ;/ETB/CX
/ETB2Vf0.u; /ETB//NULX
/ETB2Vf0./ETB; u/
DX
/ETB2Vf. /ETB;u /CX
/ETB2Vf0./ETB; u//NULX
/ETB2Vf0.u; /ETB/
DX
/ETB2V.f ./ETB; u/Cf0./ETB; u//NULf0.u; /ETB//
DX
/ETB2V.f"f0/./ETB; u/ ;
where the third line follows from the second by ﬂow conservation.
Finally, we compute the value of f"f0. Recall that we disallow antiparallel
edges in G(but not in Gf), and hence for each vertex /ETB2V, we know that there
can be an edge .s; /ETB/ or./ETB; s/ , but never both. We deﬁne V1Df/ETBW.s; /ETB/2Eg
to be the set of vertices with edges from s,a n d V2Df/ETBW./ETB; s/2Egto be the
set of vertices with edges to s.W e h a v e V1[V2/DC2Vand, because we disallow
antiparallel edges, V1\V2D;. We now compute
jf"f0jDX
/ETB2V.f"f0/. s ;/ETB//NULX
/ETB2V.f"f0/. /ETB;s/
DX
/ETB2V1.f"f0/. s ;/ETB//NULX
/ETB2V2.f"f0/. /ETB;s/; (26.5)26.2 The Ford-Fulkerson method 719
where the second line follows because .f"f0/.w; x/ is0if.w; x/62E.W en o w
apply the deﬁnition of f"f0to equation (26.5), and then reorder and group terms
to obtain
jf"f0j
DX
/ETB2V1.f .s; /ETB/Cf0.s; /ETB//NULf0./ETB; s///NULX
/ETB2V2.f ./ETB; s/Cf0./ETB; s//NULf0.s; /ETB//
DX
/ETB2V1f. s ;/ETB/CX
/ETB2V1f0.s; /ETB//NULX
/ETB2V1f0./ETB; s/
/NULX
/ETB2V2f. /ETB;s//NULX
/ETB2V2f0./ETB; s/CX
/ETB2V2f0.s; /ETB/
DX
/ETB2V1f. s ;/ETB//NULX
/ETB2V2f. /ETB;s/
CX
/ETB2V1f0.s; /ETB/CX
/ETB2V2f0.s; /ETB//NULX
/ETB2V1f0./ETB; s//NULX
/ETB2V2f0./ETB; s/
DX
/ETB2V1f. s ;/ETB//NULX
/ETB2V2f. /ETB;s/CX
/ETB2V1[V2f0.s; /ETB//NULX
/ETB2V1[V2f0./ETB; s/ : (26.6)
In equation (26.6), we can extend all four summations to sum over V, since each
additional term has value 0. (Exercise 26.2-1 asks you to prove this formally.) We
thus have
jf"f0jDX
/ETB2Vf. s ;/ETB//NULX
/ETB2Vf. /ETB;s/CX
/ETB2Vf0.s; /ETB//NULX
/ETB2Vf0./ETB; s/ (26.7)
DjfjCjf0j:
Augmenting paths
Given a ﬂow network GD.V; E/ and a ﬂow f,a naugmenting path pis a
simple path from stotin the residual network Gf. By the deﬁnition of the resid-
ual network, we may increase the ﬂow on an edge .u; /ETB/ of an augmenting path
by up to cf.u; /ETB/ without violating the capacity constraint on whichever of .u; /ETB/
and./ETB; u/ is in the original ﬂow network G.
The shaded path in Figure 26.4(b) is an augmenting path. Treating the residual
network Gfin the ﬁgure as a ﬂow network, we can increase the ﬂow through each
edge of this path by up to 4units without violating a capacity constraint, since the
smallest residual capacity on this path is cf./ETB2;/ETB3/D4. We call the maximum
amount by which we can increase the ﬂow on each edge in an augmenting path p
theresidual capacity ofp,g i v e nb y
cf.p/Dminfcf.u; /ETB/W.u; /ETB/ is on pg:720 Chapter 26 Maximum Flow
The following lemma, whose proof we leave as Exercise 26.2-7, makes the above
argument more precise.
Lemma 26.2
LetGD.V; E/ be a ﬂow network, let fbe a ﬂow in G,a n dl e t pbe an augmenting
path in Gf. Deﬁne a function fpWV/STXV!Rby
fp.u; /ETB/D(
cf.p/ if.u; /ETB/ is on p;
0 otherwise :(26.8)
Then, fpi saﬂ o wi n Gfwith valuejfpjDcf.p/ > 0 .
The following corollary shows that if we augment fbyfp, we get another ﬂow
inGwhose value is closer to the maximum. Figure 26.4(c) shows the result of
augmenting the ﬂow ffrom Figure 26.4(a) by the ﬂow fpin Figure 26.4(b), and
Figure 26.4(d) shows the ensuing residual network.
Corollary 26.3
LetGD.V; E/ be a ﬂow network, let fbe a ﬂow in G,a n dl e t pbe an
augmenting path in Gf.L e t fpbe deﬁned as in equation (26.8), and suppose
that we augment fbyfp. Then the function f"fpi saﬂ o wi n Gwith value
jf"fpjDjfjCjfpj>jfj.
Proof Immediate from Lemmas 26.1 and 26.2.
Cuts of ﬂow networks
The Ford-Fulkerson method repeatedly augments the ﬂow along augmenting paths
until it has found a maximum ﬂow. How do we know that when the algorithmterminates, we have actually found a maximum ﬂow? The max-ﬂow min-cut theo-rem, which we shall prove shortly, tells us that a ﬂow is maximum if and only if itsresidual network contains no augmenting path. To prove this theorem, though, wemust ﬁrst explore the notion of a cut of a ﬂow network.
Acut.S; T / of ﬂow network GD.V; E/ is a partition of VintoSand
TDV/NULSsuch that s2Sandt2T. (This deﬁnition is similar to the def-
inition of “cut” that we used for minimum spanning trees in Chapter 23, exceptthat here we are cutting a directed graph rather than an undirected graph, and weinsist that s2Sandt2T.) Iffis a ﬂow, then the net ﬂow f. S;T/ across the
cut.S; T / is deﬁned to be
f. S;T/DX
u2SX
/ETB2Tf. u ;/ETB//NULX
u2SX
/ETB2Tf. /ETB;u /: (26.9)26.2 The Ford-Fulkerson method 721
s t11/1612/12
15/207/7 4/91/4
8/13
11/144/4
STv4v3 v1
v2
Figure 26.5 Ac u t .S; T / in the ﬂow network of Figure 26.1(b), where SDfs; /ETB1;/ETB2gand
TDf/ETB3;/ETB4;tg. The vertices in Sare black, and the vertices in Tare white. The net ﬂow
across .S; T / isf. S;T/D19, and the capacity is c.S;T /D26.
Thecapacity of the cut .S; T / is
c.S;T/DX
u2SX
/ETB2Tc.u;/ETB/ : (26.10)
Aminimum cut of a network is a cut whose capacity is minimum over all cuts of
the network.
The asymmetry between the deﬁnitions of ﬂow and capacity of a cut is inten-
tional and important. For capacity, we count only the capacities of edges goingfrom StoT, ignoring edges in the reverse direction. For ﬂow, we consider the
ﬂow going from StoTminus the ﬂow going in the reverse direction from TtoS.
The reason for this difference will become clear later in this section.
Figure 26.5 shows the cut .fs; /ETB
1;/ETB2g;f/ETB3;/ETB4;tg/in the ﬂow network of Fig-
ure 26.1(b). The net ﬂow across this cut is
f. /ETB 1;/ETB3/Cf. /ETB 2;/ETB4//NULf. /ETB 3;/ETB2/D12C11/NUL4
D19 ;
and the capacity of this cut is
c./ETB 1;/ETB3/Cc./ETB 2;/ETB4/D12C14
D26 :
The following lemma shows that, for a given ﬂow f, the net ﬂow across any cut
is the same, and it equals jfj, the value of the ﬂow.
Lemma 26.4
Letfbe a ﬂow in a ﬂow network Gwith source sand sink t,a n dl e t .S; T / be any
cut of G. Then the net ﬂow across .S; T / isf. S;T/Djfj.722 Chapter 26 Maximum Flow
Proof We can rewrite the ﬂow-conservation condition for any node u2V/NULfs; tg
as
X
/ETB2Vf. u ;/ETB//NULX
/ETB2Vf. /ETB;u /D0: (26.11)
Taking the deﬁnition of jfjfrom equation (26.1) and adding the left-hand side of
equation (26.11), which equals 0, summed over all vertices in S/NULfsg,g i v e s
jfjDX
/ETB2Vf. s ;/ETB//NULX
/ETB2Vf. /ETB;s/CX
u2S/NULfsg X
/ETB2Vf. u ;/ETB//NULX
/ETB2Vf. /ETB;u /!
:
Expanding the right-hand summation and regrouping terms yields
jfjDX
/ETB2Vf. s ;/ETB//NULX
/ETB2Vf. /ETB;s/CX
u2S/NULfsgX
/ETB2Vf. u ;/ETB//NULX
u2S/NULfsgX
/ETB2Vf. /ETB;u /
DX
/ETB2V 
f. s ;/ETB/CX
u2S/NULfsgf. u ;/ETB/!
/NULX
/ETB2V 
f. /ETB;s/CX
u2S/NULfsgf. /ETB;u /!
DX
/ETB2VX
u2Sf. u ;/ETB//NULX
/ETB2VX
u2Sf. /ETB;u /:
Because VDS[TandS\TD;, we can split each summation over Vinto
summations over SandTto obtain
jfjDX
/ETB2SX
u2Sf. u ;/ETB/CX
/ETB2TX
u2Sf. u ;/ETB//NULX
/ETB2SX
u2Sf. /ETB;u //NULX
/ETB2TX
u2Sf. /ETB;u /
DX
/ETB2TX
u2Sf. u ;/ETB//NULX
/ETB2TX
u2Sf. /ETB;u /
C X
/ETB2SX
u2Sf. u ;/ETB//NULX
/ETB2SX
u2Sf. /ETB;u /!
:
The two summations within the parentheses are actually the same, since for all
vertices x;y2V, the term f. x;y/ appears once in each summation. Hence, these
summations cancel, and we have
jfjDX
u2SX
/ETB2Tf. u ;/ETB//NULX
u2SX
/ETB2Tf. /ETB;u /
Df. S;T/:
A corollary to Lemma 26.4 shows how we can use cut capacities to bound the
value of a ﬂow.26.2 The Ford-Fulkerson method 723
Corollary 26.5
The value of any ﬂow fin a ﬂow network Gis bounded from above by the capacity
of any cut of G.
Proof Let.S; T / be any cut of Gand let fbe any ﬂow. By Lemma 26.4 and the
capacity constraint,
jfjDf. S;T/
DX
u2SX
/ETB2Tf. u ;/ETB//NULX
u2SX
/ETB2Tf. /ETB;u /
/DC4X
u2SX
/ETB2Tf. u ;/ETB/
/DC4X
u2SX
/ETB2Tc.u;/ETB/
Dc.S;T/ :
Corollary 26.5 yields the immediate consequence that the value of a maximum
ﬂow in a network is bounded from above by the capacity of a minimum cut ofthe network. The important max-ﬂow min-cut theorem, which we now state andprove, says that the value of a maximum ﬂow is in fact equal to the capacity of a
minimum cut.
Theorem 26.6 (Max-ﬂow min-cut theorem)
Iffis a ﬂow in a ﬂow network GD.V; E/ with source sand sink t, then the
following conditions are equivalent:
1.fis a maximum ﬂow in G.
2. The residual network G
fcontains no augmenting paths.
3.jfjDc.S;T/ for some cut .S; T / ofG.
Proof .1/).2/: Suppose for the sake of contradiction that fi sam a x i m u m
ﬂow in Gbut that Gfhas an augmenting path p. Then, by Corollary 26.3, the
ﬂow found by augmenting fbyfp,w h e r e fpis given by equation (26.8), is a ﬂow
inGwith value strictly greater than jfj, contradicting the assumption that fis a
maximum ﬂow.
.2/).3/: Suppose that Gfhas no augmenting path, that is, that Gfcontains
no path from stot.D e ﬁ n e
SDf/ETB2VWthere exists a path from sto/ETBinGfg
andTDV/NULS. The partition .S; T / i sac u t :w eh a v e s2Strivially and t62S
because there is no path from stotinGf. Now consider a pair of vertices724 Chapter 26 Maximum Flow
u2Sand/ETB2T.I f .u; /ETB/2E,w em u s th a v e f. u ;/ETB/Dc.u;/ETB/ ,s i n c e
otherwise .u; /ETB/2Ef, which would place /ETBin set S.I f./ETB; u/2E,w em u s t
have f. /ETB;u /D0, because otherwise cf.u; /ETB/Df. /ETB;u / would be positive and
we would have .u; /ETB/2Ef, which would place /ETBinS. Of course, if neither .u; /ETB/
nor./ETB; u/ is inE,t h e n f. u ;/ETB/Df. /ETB;u /D0. We thus have
f. S;T/DX
u2SX
/ETB2Tf. u ;/ETB//NULX
/ETB2TX
u2Sf. /ETB;u /
DX
u2SX
/ETB2Tc.u;/ETB//NULX
/ETB2TX
u2S0
Dc.S;T/ :
By Lemma 26.4, therefore, jfjDf. S;T/Dc.S;T/ .
.3/).1/: By Corollary 26.5, jfj/DC4c.S;T/ for all cuts .S; T / . The condition
jfjDc.S;T/ thus implies that fi sam a x i m u mﬂ o w .
The basic Ford-Fulkerson algorithm
In each iteration of the Ford-Fulkerson method, we ﬁnd some augmenting path p
and use pto modify the ﬂow f. As Lemma 26.2 and Corollary 26.3 suggest, we
replace fbyf"fp, obtaining a new ﬂow whose value is jfjCjfpj. The follow-
ing implementation of the method computes the maximum ﬂow in a ﬂow networkGD.V; E/ by updating the ﬂow attribute .u; /ETB/: ffor each edge .u; /ETB/2E.
1
If.u; /ETB/62E, we assume implicitly that .u; /ETB/: fD0. We also assume that we
are given the capacities c.u;/ETB/ along with the ﬂow network, and c.u;/ETB/D0
if.u; /ETB/62E. We compute the residual capacity cf.u; /ETB/ in accordance with the
formula (26.2). The expression cf.p/in the code is just a temporary variable that
stores the residual capacity of the path p.
FORD-FULKERSON . G ;s ;t/
1foreach edge .u; /ETB/2G:E
2 .u; /ETB/: fD0
3while there exists a path pfrom stotin the residual network Gf
4 cf.p/Dminfcf.u; /ETB/W.u; /ETB/ is inpg
5 foreach edge .u; /ETB/ inp
6 if.u; /ETB/2E
7 .u; /ETB/: fD.u; /ETB/: fCcf.p/
8 else./ETB; u/: fD./ETB; u/: f/NULcf.p/
1Recall from Section 22.1 that we represent an attribute ffor edge .u; /ETB/ with the same style of
notation— .u; /ETB/: f—that we use for an attribute of any other object.26.2 The Ford-Fulkerson method 725
The F ORD-FULKERSON algorithm simply expands on the F ORD-FULKERSON -
METHOD pseudocode given earlier. Figure 26.6 shows the result of each iteration
in a sample run. Lines 1–2 initialize the ﬂow fto0.T h e while loop of lines 3–8
repeatedly ﬁnds an augmenting path pinGfand augments ﬂow falong pby
the residual capacity cf.p/. Each residual edge in path pis either an edge in the
original network or the reversal of an edge in the original network. Lines 6–8update the ﬂow in each case appropriately, adding ﬂow when the residual edge isan original edge and subtracting it otherwise. When no augmenting paths exist, the
ﬂowfi sam a x i m u mﬂ o w .
Analysis of Ford-Fulkerson
The running time of F
ORD-FULKERSON depends on how we ﬁnd the augmenting
pathpin line 3. If we choose it poorly, the algorithm might not even terminate: the
value of the ﬂow will increase with successive augmentations, but it need not evenconverge to the maximum ﬂow value.
2If we ﬁnd the augmenting path by using a
breadth-ﬁrst search (which we saw in Section 22.2), however, the algorithm runs inpolynomial time. Before proving this result, we obtain a simple bound for the casein which we choose the augmenting path arbitrarily and all capacities are integers.
In practice, the maximum-ﬂow problem often arises with integral capacities. If
the capacities are rational numbers, we can apply an appropriate scaling transfor-mation to make them all integral. If f
/ETXdenotes a maximum ﬂow in the transformed
network, then a straightforward implementation of F ORD-FULKERSON executes
thewhile loop of lines 3–8 at most jf/ETXjtimes, since the ﬂow value increases by at
least one unit in each iteration.
We can perform the work done within the while loop efﬁciently if we implement
the ﬂow network GD.V; E/ with the right data structure and ﬁnd an augmenting
path by a linear-time algorithm. Let us assume that we keep a data structure cor-
responding to a directed graph G0D.V; E0/,w h e r e E0Df.u; /ETB/W.u; /ETB/2Eor
./ETB; u/2Eg. Edges in the network Ga r ea l s oe d g e si n G0, and therefore we can
easily maintain capacities and ﬂows in this data structure. Given a ﬂow fonG,
the edges in the residual network Gfconsist of all edges .u; /ETB/ ofG0such that
cf.u; /ETB/ > 0 ,w h e r e cfconforms to equation (26.2). The time to ﬁnd a path in
a residual network is therefore O.VCE0/DO.E/ if we use either depth-ﬁrst
search or breadth-ﬁrst search. Each iteration of the while loop thus takes O.E/
time, as does the initialization in lines 1–2, making the total running time of the
FORD-FULKERSON algorithm O.Ejf/ETXj/.
2The Ford-Fulkerson method might fail to terminate only if edge capacities are irrational numbers.726 Chapter 26 Maximum Flow
12
4
4 4/4 4v1
416410st1612
20794
13
144v1
st4/164/12
2074/9
13
4/144/4
st
754
4v18
4
1320v1
st4/168/12
4/2074/9
4/13
4/144/4
410st
758
4v14
9v1
st8/168/12
8/2079
4/13
4/144/4v2 v2
v2 v2
v2 v2v3 v3
v3 v3
v3 v3v4 v4
v4 v4
v4 v4(b)(a)
(c)12
4 444
4
Figure 26.6 The execution of the basic Ford-Fulkerson algorithm. (a)–(e) Successive iterations of
thewhile loop. The left side of each part shows the residual network Gffrom line 3 with a shaded
augmenting path p. The right side of each part shows the new ﬂow fthat results from augmenting f
byfp. The residual network in (a) is the input network G.
When the capacities are integral and the optimal ﬂow value jf/ETXjis small, the
running time of the Ford-Fulkerson algorithm is good. Figure 26.7(a) shows an ex-ample of what can happen on a simple ﬂow network for which jf
/ETXjis large. A max-
imum ﬂow in this network has value 2,000,000: 1,000,000 units of ﬂow traversethe path s!u!t, and another 1,000,000 units traverse the path s!/ETB!t.I f
the ﬁrst augmenting path found by F
ORD-FULKERSON iss!u!/ETB!t,s h o w n
in Figure 26.7(a), the ﬂow has value 1after the ﬁrst iteration. The resulting resid-
ual network appears in Figure 26.7(b). If the second iteration ﬁnds the augment-ing path s!/ETB!u!t, as shown in Figure 26.7(b), the ﬂow then has value 2.
Figure 26.7(c) shows the resulting residual network. We can continue, choosingthe augmenting path s!u!/ETB!tin the odd-numbered iterations and the aug-
menting path s!/ETB!u!tin the even-numbered iterations. We would perform
a total of 2,000,000 augmentations, increasing the ﬂow value by only 1unit in each.26.2 The Ford-Fulkerson method 727
4
12
11
211
28
89
4
49844
98st127
44v1
st8/168/12
15/207/7 9
11/13
11/144/4v1
10
19
st12
17
114 3v2v3 v3
v3v4v4
v4(d)
(f)
4
984415
st57
114v1
st12/1612/12
19/207/7 9
11/13
11/144/4v1
3v2v3 v3
v4v4(e)
4v2
v2
v1
v28
8
Figure 26.6, continued (f) The residual network at the last while loop test. It has no augmenting
paths, and the ﬂow fshown in (e) is therefore a maximum ﬂow. The value of the maximum ﬂow
found is 23.
The Edmonds-Karp algorithm
We can improve the bound on F ORD-FULKERSON by ﬁnding the augmenting
pathpin line 3 with a breadth-ﬁrst search. That is, we choose the augmenting
path as a shortest path from stotin the residual network, where each edge has
unit distance (weight). We call the Ford-Fulkerson method so implemented theEdmonds-Karp algorithm . We now prove that the Edmonds-Karp algorithm runs
inO.VE
2/time.
The analysis depends on the distances to vertices in the residual network Gf.
The following lemma uses the notation ıf.u; /ETB/ for the shortest-path distance
from uto/ETBinGf, where each edge has unit distance.
Lemma 26.7
If the Edmonds-Karp algorithm is run on a ﬂow network GD.V; E/ with source s
and sink t, then for all vertices /ETB2V/NULfs; tg, the shortest-path distance ıf.s; /ETB/
in the residual network Gfincreases monotonically with each ﬂow augmentation.728 Chapter 26 Maximum Flow
1999,999
999,999
1st1,000,0001,000,000 1
1,000,000
1,000,000999,999
11999,999u
vst1,000,000 1
1,000,000u
v999,999
1999,999
1st
1u
v
(a) (b) (c)
Figure 26.7 (a) A ﬂow network for which F ORD-FULKERSON can take ‚.Ejf/ETXj/time,
where f/ETXis a maximum ﬂow, shown here with jf/ETXjD2,000,000. The shaded path is an aug-
menting path with residual capacity 1.(b)The resulting residual network, with another augmenting
path whose residual capacity is 1.(c)The resulting residual network.
Proof We will suppose that for some vertex /ETB2V/NULfs; tg, there is a ﬂow aug-
mentation that causes the shortest-path distance from sto/ETBto decrease, and then
we will derive a contradiction. Let fbe the ﬂow just before the ﬁrst augmentation
that decreases some shortest-path distance, and let f0be the ﬂow just afterward.
Let/ETBbe the vertex with the minimum ıf0.s; /ETB/ whose distance was decreased by
the augmentation, so that ıf0.s; /ETB/ < ı f.s; /ETB/ .L e t pDs;u!/ETBbe a shortest
path from sto/ETBinGf0,s ot h a t .u; /ETB/2Ef0and
ıf0.s; u/Dıf0.s; /ETB//NUL1: (26.12)
Because of how we chose /ETB, we know that the distance of vertex ufrom the source s
did not decrease, i.e.,
ıf0.s; u//NAKıf.s; u/ : (26.13)
We claim that .u; /ETB/62Ef. Why? If we had .u; /ETB/2Ef,t h e nw ew o u l da l s oh a v e
ıf.s; /ETB//DC4ıf.s; u/C1(by Lemma 24.10, the triangle inequality)
/DC4ıf0.s; u/C1(by inequality (26.13))
Dıf0.s; /ETB/ (by equation (26.12)) ,
which contradicts our assumption that ıf0.s; /ETB/ < ı f.s; /ETB/ .
How can we have .u; /ETB/62Efand.u; /ETB/2Ef0? The augmentation must
have increased the ﬂow from /ETBtou. The Edmonds-Karp algorithm always aug-
ments ﬂow along shortest paths, and therefore the shortest path from stouinGf
has./ETB; u/ as its last edge. Therefore,
ıf.s; /ETB/Dıf.s; u//NUL1
/DC4ıf0.s; u//NUL1(by inequality (26.13))
Dıf0.s; /ETB//NUL2(by equation (26.12)) ,26.2 The Ford-Fulkerson method 729
which contradicts our assumption that ıf0.s; /ETB/ < ı f.s; /ETB/ . We conclude that our
assumption that such a vertex /ETBexists is incorrect.
The next theorem bounds the number of iterations of the Edmonds-Karp algo-
rithm.
Theorem 26.8
If the Edmonds-Karp algorithm is run on a ﬂow network GD.V; E/ with source s
and sink t, then the total number of ﬂow augmentations performed by the algorithm
isO.VE/ .
Proof We say that an edge .u; /ETB/ in a residual network Gfiscritical on an aug-
menting path pif the residual capacity of pis the residual capacity of .u; /ETB/ ,t h a t
is, ifcf.p/Dcf.u; /ETB/ . After we have augmented ﬂow along an augmenting path,
any critical edge on the path disappears from the residual network. Moreover, atleast one edge on any augmenting path must be critical. We will show that each ofthejEjedges can become critical at most jVj=2times.
Letuand/ETBbe vertices in Vthat are connected by an edge in E. Since augment-
ing paths are shortest paths, when .u; /ETB/ is critical for the ﬁrst time, we have
ı
f.s; /ETB/Dıf.s; u/C1:
Once the ﬂow is augmented, the edge .u; /ETB/ disappears from the residual network.
It cannot reappear later on another augmenting path until after the ﬂow from uto/ETB
is decreased, which occurs only if ./ETB; u/ appears on an augmenting path. If f0is
the ﬂow in Gwhen this event occurs, then we have
ıf0.s; u/Dıf0.s; /ETB/C1:
Since ıf.s; /ETB//DC4ıf0.s; /ETB/ by Lemma 26.7, we have
ıf0.s; u/Dıf0.s; /ETB/C1
/NAKıf.s; /ETB/C1
Dıf.s; u/C2:
Consequently, from the time .u; /ETB/ becomes critical to the time when it next
becomes critical, the distance of ufrom the source increases by at least 2.T h e
distance of ufrom the source is initially at least 0. The intermediate vertices on a
shortest path from stoucannot contain s,u,o rt(since .u; /ETB/ on an augmenting
path implies that u¤t). Therefore, until ubecomes unreachable from the source,
if ever, its distance is at most jVj/NUL2. Thus, after the ﬁrst time that .u; /ETB/ becomes
critical, it can become critical at most .jVj/NUL2/=2DjVj=2/NUL1times more, for a
total of at mostjVj=2times. Since there are O.E/ pairs of vertices that can have an
edge between them in a residual network, the total number of critical edges during730 Chapter 26 Maximum Flow
the entire execution of the Edmonds-Karp algorithm is O.VE/ . Each augmenting
path has at least one critical edge, and hence the theorem follows.
Because we can implement each iteration of F ORD-FULKERSON inO.E/ time
when we ﬁnd the augmenting path by breadth-ﬁrst search, the total running time ofthe Edmonds-Karp algorithm is O.VE
2/. We shall see that push-relabel algorithms
can yield even better bounds. The algorithm of Section 26.4 gives a method forachieving an O.V
2E/running time, which forms the basis for the O.V3/-time
algorithm of Section 26.5.
Exercises
26.2-1
Prove that the summations in equation (26.6) equal the summations in equa-tion (26.7).
26.2-2
In Figure 26.1(b), what is the ﬂow across the cut .fs; /ETB
2;/ETB4g;f/ETB1;/ETB3;tg/?W h a ti s
the capacity of this cut?
26.2-3
Show the execution of the Edmonds-Karp algorithm on the ﬂow network of Fig-ure 26.1(a).
26.2-4
In the example of Figure 26.6, what is the minimum cut corresponding to the max-imum ﬂow shown? Of the augmenting paths appearing in the example, which one
cancels ﬂow?
26.2-5
Recall that the construction in Section 26.1 that converts a ﬂow network with mul-tiple sources and sinks into a single-source, single-sink network adds edges withinﬁnite capacity. Prove that any ﬂow in the resulting network has a ﬁnite valueif the edges of the original network with multiple sources and sinks have ﬁnitecapacity.
26.2-6
Suppose that each source s
iin a ﬂow network with multiple sources and sinks
produces exactly piunits of ﬂow, so thatP
/ETB2Vf. s i;/ETB/Dpi. Suppose also
that each sink tjconsumes exactly qjunits, so thatP
/ETB2Vf. /ETB;t j/Dqj,w h e r eP
ipiDP
jqj. Show how to convert the problem of ﬁnding a ﬂow fthat obeys26.2 The Ford-Fulkerson method 731
these additional constraints into the problem of ﬁnding a maximum ﬂow in a single-
source, single-sink ﬂow network.
26.2-7
Prove Lemma 26.2.
26.2-8
Suppose that we redeﬁne the residual network to disallow edges into s. Argue that
the procedure F ORD-FULKERSON still correctly computes a maximum ﬂow.
26.2-9
Suppose that both fandf0are ﬂows in a network Gand we compute ﬂow f"f0.
Does the augmented ﬂow satisfy the ﬂow conservation property? Does it satisfy
the capacity constraint?
26.2-10
Show how to ﬁnd a maximum ﬂow in a network GD.V; E/ by a sequence of at
mostjEjaugmenting paths. ( Hint: Determine the paths after ﬁnding the maximum
ﬂow.)
26.2-11
Theedge connectivity of an undirected graph is the minimum number kof edges
that must be removed to disconnect the graph. For example, the edge connectivityof a tree is 1, and the edge connectivity of a cyclic chain of vertices is 2.S h o w
how to determine the edge connectivity of an undirected graph GD.V; E/ by
running a maximum-ﬂow algorithm on at most jVjﬂow networks, each having
O.V / vertices and O.E/ edges.
26.2-12
Suppose that you are given a ﬂow network G,a n d Ghas edges entering the
source s.L e t fbe a ﬂow in Gin which one of the edges ./ETB; s/ entering the source
hasf. /ETB;s/D1. Prove that there must exist another ﬂow f
0withf0./ETB; s/D0
such thatjfjDjf0j.G i v ea n O.E/ -time algorithm to compute f0,g i v e n f,a n d
assuming that all edge capacities are integers.
26.2-13
Suppose that you wish to ﬁnd, among all minimum cuts in a ﬂow network Gwith
integral capacities, one that contains the smallest number of edges. Show how tomodify the capacities of Gto create a new ﬂow network G
0in which any minimum
cut in G0is a minimum cut with the smallest number of edges in G.732 Chapter 26 Maximum Flow
26.3 Maximum bipartite matching
Some combinatorial problems can easily be cast as maximum-ﬂow problems. The
multiple-source, multiple-sink maximum-ﬂow problem from Section 26.1 gave usone example. Some other combinatorial problems seem on the surface to have littleto do with ﬂow networks, but can in fact be reduced to maximum-ﬂow problems.
This section presents one such problem: ﬁnding a maximum matching in a bipartite
graph. In order to solve this problem, we shall take advantage of an integralityproperty provided by the Ford-Fulkerson method. We shall also see how to usethe Ford-Fulkerson method to solve the maximum-bipartite-matching problem onag r a p h GD.V; E/ inO.VE/ time.
The maximum-bipartite-matching problem
Given an undirected graph GD.V; E/ ,amatching is a subset of edges M/DC2E
such that for all vertices /ETB2V, at most one edge of Mis incident on /ETB.W e
say that a vertex /ETB2Vismatched by the matching Mif some edge in Mis
incident on /ETB; otherwise, /ETBisunmatched .Amaximum matching is a matching
of maximum cardinality, that is, a matching Msuch that for any matching M
0,
we havejMj/NAKjM0j. In this section, we shall restrict our attention to ﬁnding
maximum matchings in bipartite graphs: graphs in which the vertex set can be
partitioned into VDL[R,w h e r e LandRare disjoint and all edges in E
go between LandR. We further assume that every vertex in Vhas at least one
incident edge. Figure 26.8 illustrates the notion of a matching in a bipartite graph.
The problem of ﬁnding a maximum matching in a bipartite graph has many
practical applications. As an example, we might consider matching a set Lof ma-
chines with a set Rof tasks to be performed simultaneously. We take the presence
of edge .u; /ETB/ inEto mean that a particular machine u2Lis capable of per-
forming a particular task /ETB2R. A maximum matching provides work for as many
machines as possible.
Finding a maximum bipartite matching
We can use the Ford-Fulkerson method to ﬁnd a maximum matching in an undi-
rected bipartite graph GD.V; E/ in time polynomial in jVjandjEj. The trick is
to construct a ﬂow network in which ﬂows correspond to matchings, as shown inFigure 26.8(c). We deﬁne the corresponding ﬂow network G
0D.V0;E0/for the
bipartite graph Gas follows. We let the source sand sink tbe new vertices not
inV,a n dw el e t V0DV[fs; tg. If the vertex partition of GisVDL[R,t h e26.3 Maximum bipartite matching 733
LR LRst
(a) (c)LR
(b)
Figure 26.8 A bipartite graph GD.V; E/ with vertex partition VDL[R.(a)A matching
with cardinality 2, indicated by shaded edges. (b)A maximum matching with cardinality 3.(c)The
corresponding ﬂow network G0with a maximum ﬂow shown. Each edge has unit capacity. Shaded
edges have a ﬂow of 1, and all other edges carry no ﬂow. The shaded edges from LtoRcorrespond
to those in the maximum matching from (b).
directed edges of G0are the edges of E, directed from LtoR, along withjVjnew
directed edges:
E0Df.s; u/Wu2Lg[f.u; /ETB/W.u; /ETB/2Eg[f./ETB; t/W/ETB2Rg:
To complete the construction, we assign unit capacity to each edge in E0.S i n c e
each vertex in Vhas at least one incident edge, jEj/NAKjVj=2. Thus,jEj/DC4jE0jD
jEjCjVj/DC43jEj,a n ds ojE0jD‚.E/ .
The following lemma shows that a matching in Gcorresponds directly to a ﬂow
inG’s corresponding ﬂow network G0. We say that a ﬂow fon a ﬂow network
GD.V; E/ isinteger-valued iff. u ;/ETB/ is an integer for all .u; /ETB/2V/STXV.
Lemma 26.9
LetGD.V; E/ be a bipartite graph with vertex partition VDL[R,a n dl e t
G0D.V0;E0/be its corresponding ﬂow network. If Mis a matching in G,t h e n
there is an integer-valued ﬂow finG0with valuejfjDjMj. Conversely, if f
is an integer-valued ﬂow in G0, then there is a matching MinGwith cardinality
jMjDjfj.
Proof We ﬁrst show that a matching MinGcorresponds to an integer-valued
ﬂowfinG0.D e ﬁ n e fas follows. If .u; /ETB/2M,t h e n f. s ;u /Df. u ;/ETB/D
f. /ETB;t/D1. For all other edges .u; /ETB/2E0,w ed e ﬁ n e f. u ;/ETB/D0.I t i s s i m p l e
to verify that fsatisﬁes the capacity constraint and ﬂow conservation.734 Chapter 26 Maximum Flow
Intuitively, each edge .u; /ETB/2Mcorresponds to one unit of ﬂow in G0that
traverses the path s!u!/ETB!t. Moreover, the paths induced by edges in M
are vertex-disjoint, except for sandt. The net ﬂow across cut .L[fsg;R[ftg/
is equal tojMj; thus, by Lemma 26.4, the value of the ﬂow is jfjDjMj.
To prove the converse, let fbe an integer-valued ﬂow in G0,a n dl e t
MDf.u; /ETB/Wu2L; /ETB2R;andf. u ;/ETB/>0g:
Each vertex u2Lhas only one entering edge, namely .s; u/ , and its capacity
is1. Thus, each u2Lhas at most one unit of ﬂow entering it, and if one unit of
ﬂow does enter, by ﬂow conservation, one unit of ﬂow must leave. Furthermore,since fis integer-valued, for each u2L, the one unit of ﬂow can enter on at most
one edge and can leave on at most one edge. Thus, one unit of ﬂow enters uif and
only if there is exactly one vertex /ETB2Rsuch that f. u ;/ETB/D1, and at most one
edge leaving each u2Lcarries positive ﬂow. A symmetric argument applies to
each/ETB2R. The set Mis therefore a matching.
To see thatjMjDjfj, observe that for every matched vertex u2L,w eh a v e
f. s ;u /D1, and for every edge .u; /ETB/2E/NULM,w eh a v e f. u ;/ETB/D0. Conse-
quently, f. L[fsg;R[ftg/, the net ﬂow across cut .L
[fsg;R[ftg/, is equal
tojMj. Applying Lemma 26.4, we have that jfjDf. L[fsg;R[ftg/DjMj.
Based on Lemma 26.9, we would like to conclude that a maximum matching
in a bipartite graph Gcorresponds to a maximum ﬂow in its corresponding ﬂow
network G0, and we can therefore compute a maximum matching in Gby running
a maximum-ﬂow algorithm on G0. The only hitch in this reasoning is that the
maximum-ﬂow algorithm might return a ﬂow in G0for which some f. u ;/ETB/ is
not an integer, even though the ﬂow value jfjmust be an integer. The following
theorem shows that if we use the Ford-Fulkerson method, this difﬁculty cannotarise.
Theorem 26.10 (Integrality theorem)
If the capacity function ctakes on only integral values, then the maximum ﬂow f
produced by the Ford-Fulkerson method has the property that jfjis an integer.
Moreover, for all vertices uand/ETB,t h ev a l u eo f f. u ;/ETB/ is an integer.
Proof The proof is by induction on the number of iterations. We leave it as
Exercise 26.3-2.
We can now prove the following corollary to Lemma 26.9.26.3 Maximum bipartite matching 735
Corollary 26.11
The cardinality of a maximum matching Min a bipartite graph Gequals the value
of a maximum ﬂow fin its corresponding ﬂow network G0.
Proof We use the nomenclature from Lemma 26.9. Suppose that Mi sam a x -
imum matching in Gand that the corresponding ﬂow finG0is not maximum.
Then there is a maximum ﬂow f0inG0such thatjf0j>jfj. Since the ca-
pacities in G0are integer-valued, by Theorem 26.10, we can assume that f0is
integer-valued. Thus, f0corresponds to a matching M0inGwith cardinality
jM0jDjf0j>jfjDjMj, contradicting our assumption that Mis a maximum
matching. In a similar manner, we can show that if fis a maximum ﬂow in G0, its
corresponding matching is a maximum matching on G.
Thus, given a bipartite undirected graph G, we can ﬁnd a maximum matching by
creating the ﬂow network G0, running the Ford-Fulkerson method, and directly ob-
taining a maximum matching Mfrom the integer-valued maximum ﬂow ffound.
Since any matching in a bipartite graph has cardinality at most min .L; R/DO.V / ,
the value of the maximum ﬂow in G0isO.V / . We can therefore ﬁnd a maximum
matching in a bipartite graph in time O.VE0/DO.VE/ ,s i n c ejE0jD‚.E/ .
Exercises
26.3-1
Run the Ford-Fulkerson algorithm on the ﬂow network in Figure 26.8(c) and showthe residual network after each ﬂow augmentation. Number the vertices in Ltop
to bottom from 1to5and in Rtop to bottom from 6to9. For each iteration, pick
the augmenting path that is lexicographically smallest.
26.3-2
Prove Theorem 26.10.
26.3-3
LetGD.V; E/ be a bipartite graph with vertex partition VDL[R,a n dl e t G
0
be its corresponding ﬂow network. Give a good upper bound on the length of any
augmenting path found in G0during the execution of F ORD-FULKERSON .
26.3-4 ?
Aperfect matching is a matching in which every vertex is matched. Let GD
.V; E/ be an undirected bipartite graph with vertex partition VDL[R,w h e r e
jLjDjRj.F o ra n y X/DC2V,d e ﬁ n et h e neighborhood ofXas
N.X/Dfy2VW.x; y/2Efor some x2Xg;736 Chapter 26 Maximum Flow
that is, the set of vertices adjacent to some member of X. Prove Hall’s theorem :
there exists a perfect matching in Gif and only ifjAj/DC4jN.A/jfor every subset
A/DC2L.
26.3-5 ?
We say that a bipartite graph GD.V; E/ ,w h e r e VDL[R,i sd-regular if every
vertex /ETB2Vhas degree exactly d.E v e r y d-regular bipartite graph has jLjDjRj.
Prove that every d-regular bipartite graph has a matching of cardinality jLjby
arguing that a minimum cut of the corresponding ﬂow network has capacity jLj.
?26.4 Push-relabel algorithms
In this section, we present the “push-relabel” approach to computing maximum
ﬂows. To date, many of the asymptotically fastest maximum-ﬂow algorithms are
push-relabel algorithms, and the fastest actual implementations of maximum-ﬂow
algorithms are based on the push-relabel method. Push-relabel methods also efﬁ-ciently solve other ﬂow problems, such as the minimum-cost ﬂow problem. Thissection introduces Goldberg’s “generic” maximum-ﬂow algorithm, which has asimple implementation that runs in O.V
2E/time, thereby improving upon the
O.VE2/bound of the Edmonds-Karp algorithm. Section 26.5 reﬁnes the generic
algorithm to obtain another push-relabel algorithm that runs in O.V3/time.
Push-relabel algorithms work in a more localized manner than the Ford-
Fulkerson method. Rather than examine the entire residual network to ﬁnd an aug-menting path, push-relabel algorithms work on one vertex at a time, looking onlyat the vertex’s neighbors in the residual network. Furthermore, unlike the Ford-Fulkerson method, push-relabel algorithms do not maintain the ﬂow-conservationproperty throughout their execution. They do, however, maintain a preﬂow ,w h i c h
is a function fWV/STXV!Rthat satisﬁes the capacity constraint and the following
relaxation of ﬂow conservation:X
/ETB2Vf. /ETB;u //NULX
/ETB2Vf. u ;/ETB//NAK0
for all vertices u2V/NULfsg. That is, the ﬂow into a vertex may exceed the ﬂow
out. We call the quantity
e.u/DX
/ETB2Vf. /ETB;u //NULX
/ETB2Vf. u ;/ETB/ (26.14)
theexcess ﬂow into vertex u. The excess at a vertex is the amount by which the
ﬂow in exceeds the ﬂow out. We say that a vertex u2V/NULfs; tgisoverﬂowing if
e.u/ > 0 .26.4 Push-relabel algorithms 737
We shall begin this section by describing the intuition behind the push-relabel
method. We shall then investigate the two operations employed by the method:“pushing” preﬂow and “relabeling” a vertex. Finally, we shall present a genericpush-relabel algorithm and analyze its correctness and running time.
Intuition
You can understand the intuition behind the push-relabel method in terms of ﬂuid
ﬂows: we consider a ﬂow network GD.V; E/ to be a system of interconnected
pipes of given capacities. Applying this analogy to the Ford-Fulkerson method,we might say that each augmenting path in the network gives rise to an additionalstream of ﬂuid, with no branch points, ﬂowing from the source to the sink. TheFord-Fulkerson method iteratively adds more streams of ﬂow until no more can be
added.
The generic push-relabel algorithm has a rather different intuition. As before,
directed edges correspond to pipes. Vertices, which are pipe junctions, have twointeresting properties. First, to accommodate excess ﬂow, each vertex has an out-ﬂow pipe leading to an arbitrarily large reservoir that can accumulate ﬂuid. Second,each vertex, its reservoir, and all its pipe connections sit on a platform whose heightincreases as the algorithm progresses.
Vertex heights determine how ﬂow is pushed: we push ﬂow only downhill, that
is, from a higher vertex to a lower vertex. The ﬂow from a lower vertex to a higher
vertex may be positive, but operations that push ﬂow push it only downhill. Weﬁx the height of the source at jVjand the height of the sink at 0. All other vertex
heights start at 0and increase with time. The algorithm ﬁrst sends as much ﬂow as
possible downhill from the source toward the sink. The amount it sends is exactlyenough to ﬁll each outgoing pipe from the source to capacity; that is, it sends the
capacity of the cut .s; V/NULfsg/. When ﬂow ﬁrst enters an intermediate vertex, it
collects in the vertex’s reservoir. From there, we eventually push it downhill.
We may eventually ﬁnd that the only pipes that leave a vertex uand are not
already saturated with ﬂow connect to vertices that are on the same level as uor
are uphill from u. In this case, to rid an overﬂowing vertex uof its excess ﬂow, we
must increase its height—an operation called “relabeling” vertex u. We increase
its height to one unit more than the height of the lowest of its neighbors to whichit has an unsaturated pipe. After a vertex is relabeled, therefore, it has at least oneoutgoing pipe through which we can push more ﬂow.
Eventually, all the ﬂow that can possibly get through to the sink has arrived there.
No more can arrive, because the pipes obey the capacity constraints; the amount ofﬂow across any cut is still limited by the capacity of the cut. To make the preﬂowa “legal” ﬂow, the algorithm then sends the excess collected in the reservoirs ofoverﬂowing vertices back to the source by continuing to relabel vertices to above738 Chapter 26 Maximum Flow
the ﬁxed heightjVjof the source. As we shall see, once we have emptied all the
reservoirs, the preﬂow is not only a “legal” ﬂow, it is also a maximum ﬂow.
The basic operations
From the preceding discussion, we see that a push-relabel algorithm performs two
basic operations: pushing ﬂow excess from a vertex to one of its neighbors andrelabeling a vertex. The situations in which these operations apply depend on theheights of vertices, which we now deﬁne precisely.
LetGD.V; E/ be a ﬂow network with source sand sink t,a n dl e t fbe a
preﬂow in G. A function hWV! Nis aheight function
3ifh.s/DjVj,
h.t/D0,a n d
h.u//DC4h./ETB/C1
for every residual edge .u; /ETB/2Ef. We immediately obtain the following lemma.
Lemma 26.12
LetGD.V; E/ be a ﬂow network, let fbe a preﬂow in G,a n dl e t hbe a height
function on V. For any two vertices u; /ETB2V,i fh.u/ > h./ETB/C1,t h e n .u; /ETB/ is
not an edge in the residual network.
The push operationThe basic operation P
USH.u; /ETB/ applies if uis an overﬂowing vertex, cf.u; /ETB/ > 0 ,
andh.u/Dh./ETB/C1. The pseudocode below updates the preﬂow fand the excess
ﬂows for uand/ETB. It assumes that we can compute residual capacity cf.u; /ETB/ in
constant time given candf. We maintain the excess ﬂow stored at a vertex uas
the attribute u:eand the height of uas the attribute u:h. The expression /c129f.u; /ETB/
is a temporary variable that stores the amount of ﬂow that we can push from uto/ETB.
3In the literature, a height function is typically called a “distance function,” and the height of a vertex
is called a “distance label.” We use the term “height” because it is more suggestive of the intuitionbehind the algorithm. We retain the use of the term “relabel” to refer to the operation that increases
the height of a vertex. The height of a vertex is related to its distance from the sink t, as would be
found in a breadth-ﬁrst search of the transpose G
T.26.4 Push-relabel algorithms 739
PUSH.u; /ETB/
1//Applies when :uis overﬂowing, cf.u; /ETB/ > 0 ,a n d u:hD/ETB:hC1.
2//Action: Push /c129f.u; /ETB/Dmin.u:e;cf.u; /ETB// units of ﬂow from uto/ETB.
3/c129f.u; /ETB/Dmin.u:e;cf.u; /ETB//
4if.u; /ETB/2E
5 .u; /ETB/: fD.u; /ETB/: fC/c129f.u; /ETB/
6else./ETB; u/: fD./ETB; u/: f/NUL/c129f.u; /ETB/
7u:eDu:e/NUL/c129f.u; /ETB/
8/ETB:eD/ETB:eC/c129f.u; /ETB/
The code for P USH operates as follows. Because vertex uhas a positive excess u:e
and the residual capacity of .u; /ETB/ is positive, we can increase the ﬂow from uto/ETB
by/c129f.u; /ETB/Dmin.u:e;cf.u; /ETB// without causing u:eto become negative or the
capacity c.u;/ETB/ to be exceeded. Line 3 computes the value /c129f.u; /ETB/ , and lines 4–6
update f. Line 5 increases the ﬂow on edge .u; /ETB/ , because we are pushing ﬂow
over a residual edge that is also an original edge. Line 6 decreases the ﬂow onedge ./ETB; u/ , because the residual edge is actually the reverse of an edge in the
original network. Finally, lines 7–8 update the excess ﬂows into vertices uand/ETB.
Thus, if fis a preﬂow before P
USH is called, it remains a preﬂow afterward.
Observe that nothing in the code for P USH depends on the heights of uand/ETB,
yet we prohibit it from being invoked unless u:hD/ETB:hC1. Thus, we push excess
ﬂow downhill only by a height differential of 1. By Lemma 26.12, no residual
edges exist between two vertices whose heights differ by more than 1, and thus,
as long as the attribute his indeed a height function, we would gain nothing by
allowing ﬂow to be pushed downhill by a height differential of more than 1.
We call the operation P USH.u; /ETB/ apush from uto/ETB. If a push operation ap-
plies to some edge .u; /ETB/ leaving a vertex u, we also say that the push operation
applies to u.I ti sa saturating push if edge .u; /ETB/ in the residual network becomes
saturated (cf.u; /ETB/D0afterward); otherwise, it is a nonsaturating push .I f a n
edge becomes saturated, it disappears from the residual network. A simple lemmacharacterizes one result of a nonsaturating push.
Lemma 26.13
After a nonsaturating push from uto/ETB, the vertex uis no longer overﬂowing.
Proof Since the push was nonsaturating, the amount of ﬂow /c129
f.u; /ETB/ actually
pushed must equal u:eprior to the push. Since u:eis reduced by this amount, it
becomes 0after the push.
740 Chapter 26 Maximum Flow
The relabel operation
The basic operation R ELABEL .u/applies if uis overﬂowing and if u:h/DC4/ETB:hfor
all edges .u; /ETB/2Ef. In other words, we can relabel an overﬂowing vertex uif
for every vertex /ETBfor which there is residual capacity from uto/ETB, ﬂow cannot be
pushed from uto/ETBbecause /ETBis not downhill from u. (Recall that by deﬁnition,
neither the source snor the sink tcan be overﬂowing, and so sandtare ineligible
for relabeling.)
RELABEL .u/
1//Applies when: uis overﬂowing and for all /ETB2Vsuch that .u; /ETB/2Ef,
we have u:h/DC4/ETB:h.
2//Action: Increase the height of u.
3u:hD1Cminf/ETB:hW.u; /ETB/2Efg
When we call the operation R ELABEL .u/, we say that vertex uisrelabeled .N o t e
that when uis relabeled, Efmust contain at least one edge that leaves u,s ot h a t
the minimization in the code is over a nonempty set. This property follows fromthe assumption that uis overﬂowing, which in turn tells us that
u:eDX
/ETB2Vf. /ETB;u //NULX
/ETB2Vf. u ;/ETB/>0:
Since all ﬂows are nonnegative, we must therefore have at least one vertex /ETBsuch
that./ETB; u/: f>0.B u t t h e n , cf.u; /ETB/ > 0 , which implies that .u; /ETB/2Ef.T h e
operation R ELABEL .u/thus gives uthe greatest height allowed by the constraints
on height functions.
The generic algorithm
The generic push-relabel algorithm uses the following subroutine to create an ini-
tial preﬂow in the ﬂow network.
INITIALIZE -PREFLOW .G; s/
1foreach vertex /ETB2G:V
2 /ETB:hD0
3 /ETB:eD0
4foreach edge .u; /ETB/2G:E
5 .u; /ETB/: fD0
6s:hDjG:Vj
7foreach vertex /ETB2s:Adj
8 .s; /ETB/: fDc.s;/ETB/
9 /ETB:eDc.s;/ETB/
10 s:eDs:e/NULc.s;/ETB/26.4 Push-relabel algorithms 741
INITIALIZE -PREFLOW creates an initial preﬂow fdeﬁned by
.u; /ETB/: fD(
c.u;/ETB/ ifuDs;
0 otherwise :(26.15)
That is, we ﬁll to capacity each edge leaving the source s, and all other edges carry
no ﬂow. For each vertex /ETBadjacent to the source, we initially have /ETB:eDc.s;/ETB/ ,
and we initialize s:eto the negative of the sum of these capacities. The generic
algorithm also begins with an initial height function h,g i v e nb y
u:hD(
jVjifuDs;
0 otherwise :(26.16)
Equation (26.16) deﬁnes a height function because the only edges .u; /ETB/ for which
u:h>/ETB : hC1are those for which uDs, and those edges are saturated, which
means that they are not in the residual network.
Initialization, followed by a sequence of push and relabel operations, executed
in no particular order, yields the G ENERIC -PUSH-RELABEL algorithm:
GENERIC -PUSH-RELABEL .G/
1I NITIALIZE -PREFLOW .G; s/
2while there exists an applicable push or relabel operation
3 select an applicable push or relabel operation and perform it
The following lemma tells us that as long as an overﬂowing vertex exists, at least
one of the two basic operations applies.
Lemma 26.14 (An overﬂowing vertex can be either pushed or relabeled)
LetGD.V; E/ be a ﬂow network with source sand sink t,l e tfb eap r e ﬂ o w ,
and let hbe any height function for f.I fuis any overﬂowing vertex, then either a
push or relabel operation applies to it.
Proof For any residual edge .u; /ETB/ ,w eh a v e h.u//DC4h./ETB/C1because his a
height function. If a push operation does not apply to an overﬂowing vertex u,
then for all residual edges .u; /ETB/ ,w em u s th a v e h.u/ < h./ETB/C1, which implies
h.u//DC4h./ETB/. Thus, a relabel operation applies to u.
Correctness of the push-relabel method
To show that the generic push-relabel algorithm solves the maximum-ﬂow prob-
lem, we shall ﬁrst prove that if it terminates, the preﬂow fis a maximum ﬂow.
We shall later prove that it terminates. We start with some observations about theheight function h.742 Chapter 26 Maximum Flow
Lemma 26.15 (Vertex heights never decrease)
During the execution of the G ENERIC -PUSH-RELABEL procedure on a ﬂow net-
work GD.V; E/ , for each vertex u2V, the height u:hnever decreases. More-
over, whenever a relabel operation is applied to a vertex u, its height u:hincreases
by at least 1.
Proof Because vertex heights change only during relabel operations, it sufﬁces
to prove the second statement of the lemma. If vertex uis about to be rela-
beled, then for all vertices /ETBsuch that .u; /ETB/2Ef,w eh a v e u:h/DC4/ETB:h. Thus,
u:h<1Cminf/ETB:hW.u; /ETB/2Efg, and so the operation must increase u:h.
Lemma 26.16
LetGD.V; E/ be a ﬂow network with source sand sink t. Then the execution of
GENERIC -PUSH-RELABEL onGmaintains the attribute has a height function.
Proof The proof is by induction on the number of basic operations performed.
Initially, his a height function, as we have already observed.
We claim that if his a height function, then an operation R ELABEL .u/leaves h
a height function. If we look at a residual edge .u; /ETB/2Efthat leaves u,t h e n
the operation R ELABEL .u/ensures that u:h/DC4/ETB:hC1afterward. Now consider
a residual edge .w; u/ that enters u. By Lemma 26.15, w:h/DC4u:hC1before the
operation R ELABEL .u/implies w:h<u : hC1afterward. Thus, the operation
RELABEL .u/leaves ha height function.
Now, consider an operation P USH.u; /ETB/ . This operation may add the edge ./ETB; u/
toEf, and it may remove .u; /ETB/ from Ef. In the former case, we have
/ETB:hDu:h/NUL1<u : hC1,a n ds o hremains a height function. In the latter case,
removing .u; /ETB/ from the residual network removes the corresponding constraint,
andhagain remains a height function.
The following lemma gives an important property of height functions.
Lemma 26.17
LetGD.V; E/ be a ﬂow network with source sand sink t,l e tfb eap r e ﬂ o w
inG,a n dl e t hbe a height function on V. Then there is no path from the source s
to the sink tin the residual network Gf.
Proof Assume for the sake of contradiction that Gfcontains a path pfrom stot,
where pDh/ETB0;/ETB1;:::;/ETB ki,/ETB0Ds,a n d /ETBkDt. Without loss of generality, p
is a simple path, and so k<jVj.F o r iD0; 1; : : : ; k/NUL1, edge ./ETBi;/ETBiC1/2Ef.
Because his a height function, h./ETB i//DC4h./ETB iC1/C1foriD0; 1; : : : ; k/NUL1.C o m -
bining these inequalities over path pyields h.s//DC4h.t/Ck. But because h.t/D0,26.4 Push-relabel algorithms 743
we have h.s//DC4k<jVj, which contradicts the requirement that h.s/DjVjin a
height function.
We are now ready to show that if the generic push-relabel algorithm terminates,
the preﬂow it computes is a maximum ﬂow.
Theorem 26.18 (Correctness of the generic push-relabel algorithm)
If the algorithm G ENERIC -PUSH-RELABEL terminates when run on a ﬂow net-
work GD.V; E/ with source sand sink t, then the preﬂow fit computes is a
maximum ﬂow for G.
Proof We use the following loop invariant:
Each time the while loop test in line 2 in G ENERIC -PUSH-RELABEL is
executed, fis a preﬂow.
Initialization: INITIALIZE -PREFLOW makes fap r e ﬂ o w .
Maintenance: The only operations within the while loop of lines 2–3 are push and
relabel. Relabel operations affect only height attributes and not the ﬂow values;hence they do not affect whether fis a preﬂow. As argued on page 739, if fis
a preﬂow prior to a push operation, it remains a preﬂow afterward.
Termination: At termination, each vertex in V/NULfs; tgmust have an excess of 0,
because by Lemma 26.14 and the invariant that fis always a preﬂow, there are
no overﬂowing vertices. Therefore, fis a ﬂow. Lemma 26.16 shows that his
a height function at termination, and thus Lemma 26.17 tells us that there is nopath from stotin the residual network G
f. By the max-ﬂow min-cut theorem
(Theorem 26.6), therefore, fi sam a x i m u mﬂ o w .
Analysis of the push-relabel method
To show that the generic push-relabel algorithm indeed terminates, we shall bound
the number of operations it performs. We bound separately each of the three typesof operations: relabels, saturating pushes, and nonsaturating pushes. With knowl-
edge of these bounds, it is a straightforward problem to construct an algorithm that
runs in O.V
2E/time. Before beginning the analysis, however, we prove an im-
portant lemma. Recall that we allow edges into the source in the residual network.
Lemma 26.19
LetGD.V; E/ be a ﬂow network with source sand sink t,a n dl e t fb eap r e ﬂ o w
inG. Then, for any overﬂowing vertex x, there is a simple path from xtosin the
residual network Gf.744 Chapter 26 Maximum Flow
Proof For an overﬂowing vertex x,l e tUDf/ETBWthere exists a simple path from x
to/ETBinGfg, and suppose for the sake of contradiction that s62U.L e t
 UDV/NULU.
We take the deﬁnition of excess from equation (26.14), sum over all vertices
inU, and note that VDU[
U, to obtainX
u2Ue.u/
DX
u2U X
/ETB2Vf. /ETB;u //NULX
/ETB2Vf. u ;/ETB/!
DX
u2U  X
/ETB2Uf. /ETB;u /CX
/ETB2
Uf. /ETB;u /!
/NUL X
/ETB2Uf. u ;/ETB/CX
/ETB2
Uf. u ;/ETB/!!
DX
u2UX
/ETB2Uf. /ETB;u /CX
u2UX
/ETB2
Uf. /ETB;u //NULX
u2UX
/ETB2Uf. u ;/ETB//NULX
u2UX
/ETB2
Uf. u ;/ETB/
DX
u2UX
/ETB2
Uf. /ETB;u //NULX
u2UX
/ETB2
Uf. u ;/ETB/:
We know that the quantityP
u2Ue.u/ must be positive because e.x/ > 0 ,x2U,
all vertices other than shave nonnegative excess, and, by assumption, s62U. Thus,
we haveX
u2UX
/ETB2
Uf. /ETB;u //NULX
u2UX
/ETB2
Uf. u ;/ETB/>0: (26.17)
All edge ﬂows are nonnegative, and so for equation (26.17) to hold, we must haveP
u2UP
/ETB2
Uf. /ETB;u / > 0 . Hence, there must exist at least one pair of vertices
u02Uand/ETB02
Uwith f. /ETB0;u0/>0 .B u t , i f f. /ETB0;u0/>0 , there must be a
residual edge .u0;/ETB0/, which means that there is a simple path from xto/ETB0(the
pathx;u0!/ETB0), thus contradicting the deﬁnition of U.
The next lemma bounds the heights of vertices, and its corollary bounds the
number of relabel operations that are performed in total.
Lemma 26.20
LetGD.V; E/ be a ﬂow network with source sand sink t. At any time during
the execution of G ENERIC -PUSH-RELABEL onG,w eh a v e u:h/DC42jVj/NUL1for all
vertices u2V.
Proof The heights of the source sand the sink tnever change because these
vertices are by deﬁnition not overﬂowing. Thus, we always have s:hDjVjand
t:hD0, both of which are no greater than 2jVj/NUL1.
Now consider any vertex u2V/NULfs; tg. Initially, u:hD0/DC42jVj/NUL1.W es h a l l
show that after each relabeling operation, we still have u:h/DC42jVj/NUL1.W h e n uis26.4 Push-relabel algorithms 745
relabeled, it is overﬂowing, and Lemma 26.19 tells us that there is a simple path p
from utosinGf.L e tpDh/ETB0;/ETB1;:::;/ETB ki,w h e r e /ETB0Du,/ETBkDs,a n d k/DC4jVj/NUL1
because pis simple. For iD0; 1; : : : ; k/NUL1,w eh a v e ./ETBi;/ETBiC1/2Ef,a n d
therefore, by Lemma 26.16, /ETBi:h/DC4/ETBiC1:hC1. Expanding these inequalities over
pathpyields u:hD/ETB0:h/DC4/ETBk:hCk/DC4s:hC.jVj/NUL1/D2jVj/NUL1.
Corollary 26.21 (Bound on relabel operations)
LetGD.V; E/ be a ﬂow network with source sand sink t. Then, during the
execution of G ENERIC -PUSH-RELABEL onG, the number of relabel operations is
at most 2jVj/NUL1per vertex and at most .2jVj/NUL1/.jVj/NUL2/ < 2jVj2overall.
Proof Only thejVj/NUL2vertices in V/NULfs; tgmay be relabeled. Let u2V/NULfs; tg.
The operation R ELABEL .u/increases u:h.T h e v a l u e o f u:his initially 0and by
Lemma 26.20, it grows to at most 2jVj/NUL1. Thus, each vertex u2V/NULfs; tg
is relabeled at most 2jVj/NUL1times, and the total number of relabel operations
performed is at most .2jVj/NUL1/.jVj/NUL2/ < 2jVj2.
Lemma 26.20 also helps us to bound the number of saturating pushes.
Lemma 26.22 (Bound on saturating pushes)
During the execution of G ENERIC -PUSH-RELABEL on any ﬂow network GD
.V; E/ , the number of saturating pushes is less than 2jVjjEj.
Proof For any pair of vertices u; /ETB2V, we will count the saturating pushes
from uto/ETBand from /ETBtoutogether, calling them the saturating pushes between u
and/ETB. If there are any such pushes, at least one of .u; /ETB/ and./ETB; u/ is actually
an edge in E. Now, suppose that a saturating push from uto/ETBhas occurred.
At that time, /ETB:hDu:h/NUL1. In order for another push from uto/ETBto occur
later, the algorithm must ﬁrst push ﬂow from /ETBtou, which cannot happen until
/ETB:hDu:hC1.S i n c e u:hnever decreases, in order for /ETB:hDu:hC1,t h e
value of /ETB:hmust increase by at least 2. Likewise, u:hmust increase by at least 2
between saturating pushes from /ETBtou. Heights start at 0and, by Lemma 26.20,
never exceed 2jVj/NUL1, which implies that the number of times any vertex can have
its height increase by 2is less thanjVj. Since at least one of u:hand/ETB:hmust
increase by 2between any two saturating pushes between uand/ETB, there are fewer
than2jVjsaturating pushes between uand/ETB. Multiplying by the number of edges
gives a bound of less than 2jVjjEjon the total number of saturating pushes.
The following lemma bounds the number of nonsaturating pushes in the generic
push-relabel algorithm.746 Chapter 26 Maximum Flow
Lemma 26.23 (Bound on nonsaturating pushes)
During the execution of G ENERIC -PUSH-RELABEL on any ﬂow network GD
.V; E/ , the number of nonsaturating pushes is less than 4jVj2.jVjCjEj/.
Proof Deﬁne a potential function ˆDP
/ETBWe./ETB/>0 /ETB:h. Initially, ˆD0,a n dt h e
value of ˆmay change after each relabeling, saturating push, and nonsaturating
push. We will bound the amount that saturating pushes and relabelings can con-tribute to the increase of ˆ. Then we will show that each nonsaturating push must
decrease ˆby at least 1, and will use these bounds to derive an upper bound on the
number of nonsaturating pushes.
Let us examine the two ways in which ˆmight increase. First, relabeling a
vertex uincreases ˆby less than 2jVj, since the set over which the sum is taken is
the same and the relabeling cannot increase u’s height by more than its maximum
possible height, which, by Lemma 26.20, is at most 2jVj/NUL1. Second, a saturating
push from a vertex uto a vertex /ETBincreases ˆby less than 2jVj, since no heights
change and only vertex /ETB, whose height is at most 2jVj/NUL1, can possibly become
overﬂowing.
Now we show that a nonsaturating push from uto/ETBdecreases ˆby at least 1.
Why? Before the nonsaturating push, uwas overﬂowing, and /ETBmay or may not
have been overﬂowing. By Lemma 26.13, uis no longer overﬂowing after the
push. In addition, unless /ETBis the source, it may or may not be overﬂowing after
the push. Therefore, the potential function ˆhas decreased by exactly u:h,a n di t
has increased by either 0or/ETB:h.S i n c e u:h/NUL
/ETB:hD1, the net effect is that the
potential function has decreased by at least 1.
Thus, during the course of the algorithm, the total amount of increase in ˆis
due to relabelings and saturated pushes, and Corollary 26.21 and Lemma 26.22constrain the increase to be less than .2jVj/.2jVj
2/C.2jVj/.2jVjjEj/D
4jVj2.jVjCjEj/.S i n c e ˆ/NAK0, the total amount of decrease, and therefore the
total number of nonsaturating pushes, is less than 4jVj2.jVjCjEj/.
Having bounded the number of relabelings, saturating pushes, and nonsatu-
rating push, we have set the stage for the following analysis of the G ENERIC -
PUSH-RELABEL procedure, and hence of any algorithm based on the push-relabel
method.
Theorem 26.24
During the execution of G ENERIC -PUSH-RELABEL on any ﬂow network GD
.V; E/ , the number of basic operations is O.V2E/.
Proof Immediate from Corollary 26.21 and Lemmas 26.22 and 26.23.
26.4 Push-relabel algorithms 747
Thus, the algorithm terminates after O.V2E/operations. All that remains is
to give an efﬁcient method for implementing each operation and for choosing anappropriate operation to execute.
Corollary 26.25
There is an implementation of the generic push-relabel algorithm that runs inO.V
2E/time on any ﬂow network GD.V; E/ .
Proof Exercise 26.4-2 asks you to show how to implement the generic algorithm
with an overhead of O.V / per relabel operation and O.1/ per push. It also asks
you to design a data structure that allows you to pick an applicable operation inO.1/ time. The corollary then follows.
Exercises
26.4-1
Prove that, after the procedure I NITIALIZE -PREFLOW .G; s/ terminates, we have
s:e/DC4/NULjf/ETXj,w h e r e f/ETXis a maximum ﬂow for G.
26.4-2
Show how to implement the generic push-relabel algorithm using O.V / time per
relabel operation, O.1/ time per push, and O.1/ time to select an applicable oper-
ation, for a total time of O.V2E/.
26.4-3
Prove that the generic push-relabel algorithm spends a total of only O.VE/ time
in performing all the O.V2/relabel operations.
26.4-4
Suppose that we have found a maximum ﬂow in a ﬂow network GD.V; E/ using
a push-relabel algorithm. Give a fast algorithm to ﬁnd a minimum cut in G.
26.4-5
Give an efﬁcient push-relabel algorithm to ﬁnd a maximum matching in a bipartitegraph. Analyze your algorithm.
26.4-6
Suppose that all edge capacities in a ﬂow network GD.V; E/ are in the set
f1 ;2;:::;kg. Analyze the running time of the generic push-relabel algorithm in
terms ofjVj,jEj,a n d k.(Hint: How many times can each edge support a nonsat-
urating push before it becomes saturated?)748 Chapter 26 Maximum Flow
26.4-7
Show that we could change line 6 of I NITIALIZE -PREFLOW to
6s:hDjG:Vj/NUL2
without affecting the correctness or asymptotic performance of the generic push-
relabel algorithm.
26.4-8
Letıf.u; /ETB/ be the distance (number of edges) from uto/ETBin the residual net-
work Gf. Show that the G ENERIC -PUSH-RELABEL procedure maintains the
properties that u:h<jVjimplies u:h/DC4ıf.u; t/ and that u:h/NAKjVjimplies
u:h/NULjVj/DC4ıf.u; s/ .
26.4-9 ?
As in the previous exercise, let ıf.u; /ETB/ be the distance from uto/ETBin the residual
network Gf. Show how to modify the generic push-relabel algorithm to maintain
the property that u:h<jVjimplies u:hDıf.u; t/ and that u:h/NAKjVjimplies
u:h/NULjVjDıf.u; s/ . The total time that your implementation dedicates to main-
taining this property should be O.VE/ .
26.4-10
Show that the number of nonsaturating pushes executed by the G ENERIC -PUSH-
RELABEL procedure on a ﬂow network GD.V; E/ is at most 4jVj2jEjfor
jVj/NAK4.
?26.5 The relabel-to-front algorithm
The push-relabel method allows us to apply the basic operations in any order at
all. By choosing the order carefully and managing the network data structure efﬁ-
ciently, however, we can solve the maximum-ﬂow problem faster than the O.V2E/
bound given by Corollary 26.25. We shall now examine the relabel-to-front algo-rithm, a push-relabel algorithm whose running time is O.V
3/, which is asymptot-
ically at least as good as O.V2E/, and even better for dense networks.
The relabel-to-front algorithm maintains a list of the vertices in the network.
Beginning at the front, the algorithm scans the list, repeatedly selecting an over-ﬂowing vertex uand then “discharging” it, that is, performing push and relabel
operations until uno longer has a positive excess. Whenever we relabel a ver-
tex, we move it to the front of the list (hence the name “relabel-to-front”) and thealgorithm begins its scan anew.26.5 The relabel-to-front algorithm 749
The correctness and analysis of the relabel-to-front algorithm depend on the
notion of “admissible” edges: those edges in the residual network through whichﬂow can be pushed. After proving some properties about the network of admissibleedges, we shall investigate the discharge operation and then present and analyze therelabel-to-front algorithm itself.
Admissible edges and networks
IfGD.V; E/ is a ﬂow network with source sand sink t,fis a preﬂow in G,a n d h
is a height function, then we say that .u; /ETB/ is anadmissible edge ifc
f.u; /ETB/ > 0
andh.u/Dh./ETB/C1. Otherwise, .u; /ETB/ isinadmissible .T h eadmissible network
isGf;hD.V; E f;h/,w h e r e Ef;his the set of admissible edges.
The admissible network consists of those edges through which we can push ﬂow.
The following lemma shows that this network is a directed acyclic graph (dag).
Lemma 26.26 (The admissible network is acyclic)
IfGD.V; E/ is a ﬂow network, fis a preﬂow in G,a n d his a height function
onG, then the admissible network Gf;hD.V; E f;h/is acyclic.
Proof The proof is by contradiction. Suppose that Gf;hcontains a cycle pD
h/ETB0;/ETB1;:::;/ETB ki,w h e r e /ETB0D/ETBkandk>0 . Since each edge in pis admissible, we
have h./ETB i/NUL1/Dh./ETB i/C1foriD1 ;2;:::;k . Summing around the cycle gives
kX
iD1h./ETB i/NUL1/DkX
iD1.h./ETB i/C1/
DkX
iD1h./ETB i/Ck:
Because each vertex in cycle pappears once in each of the summations, we derive
the contradiction that 0Dk.
The next two lemmas show how push and relabel operations change the admis-
sible network.
Lemma 26.27
LetGD.V; E/ be a ﬂow network, let fb eap r e ﬂ o wi n G, and suppose that the
attribute his a height function. If a vertex uis overﬂowing and .u; /ETB/ is an ad-
missible edge, then P USH.u; /ETB/ applies. The operation does not create any new
admissible edges, but it may cause .u; /ETB/ to become inadmissible.750 Chapter 26 Maximum Flow
Proof By the deﬁnition of an admissible edge, we can push ﬂow from uto/ETB.
Since uis overﬂowing, the operation P USH.u; /ETB/ applies. The only new residual
edge that pushing ﬂow from uto/ETBcan create is ./ETB; u/ .S i n c e /ETB:hDu:h/NUL1,
edge ./ETB; u/ cannot become admissible. If the operation is a saturating push, then
cf.u; /ETB/D0afterward and .u; /ETB/ becomes inadmissible.
Lemma 26.28
LetGD.V; E/ be a ﬂow network, let fb eap r e ﬂ o wi n G, and suppose that
the attribute his a height function. If a vertex uis overﬂowing and there are no
admissible edges leaving u,t h e nR ELABEL .u/applies. After the relabel operation,
there is at least one admissible edge leaving u, but there are no admissible edges
entering u.
Proof Ifuis overﬂowing, then by Lemma 26.14, either a push or a relabel op-
eration applies to it. If there are no admissible edges leaving u,t h e nn oﬂ o w
can be pushed from uand so R ELABEL .u/applies. After the relabel operation,
u:hD1Cminf/ETB:hW.u; /ETB/2Efg. Thus, if /ETBis a vertex that realizes the mini-
mum in this set, the edge .u; /ETB/ becomes admissible. Hence, after the relabel, there
is at least one admissible edge leaving u.
To show that no admissible edges enter uafter a relabel operation, suppose that
there is a vertex /ETBsuch that ./ETB; u/ is admissible. Then, /ETB:hDu:hC1after the
relabel, and so /ETB:h>u : hC1just before the relabel. But by Lemma 26.12, no
residual edges exist between vertices whose heights differ by more than 1.M o r e -
over, relabeling a vertex does not change the residual network. Thus, ./ETB; u/ is not
in the residual network, and hence it cannot be in the admissible network.
Neighbor lists
Edges in the relabel-to-front algorithm are organized into “neighbor lists.” Given
a ﬂow network GD.V; E/ ,t h eneighbor list u:Nfor a vertex u2Vis a singly
linked list of the neighbors of uinG. Thus, vertex /ETBappears in the list u:Nif
.u; /ETB/2Eor./ETB; u/2E. The neighbor list u:Ncontains exactly those vertices /ETB
for which there may be a residual edge .u; /ETB/ . The attribute u:N:head points to
the ﬁrst vertex in u:N,a n d /ETB:next-neighbor points to the vertex following /ETBin a
neighbor list; this pointer is NILif/ETBis the last vertex in the neighbor list.
The relabel-to-front algorithm cycles through each neighbor list in an arbitrary
order that is ﬁxed throughout the execution of the algorithm. For each vertex u,
the attribute u:current points to the vertex currently under consideration in u:N.
Initially, u:current is set to u:N:head .26.5 The relabel-to-front algorithm 751
Discharging an overﬂowing vertex
An overﬂowing vertex uisdischarged by pushing all of its excess ﬂow through
admissible edges to neighboring vertices, relabeling uas necessary to cause edges
leaving uto become admissible. The pseudocode goes as follows.
DISCHARGE .u/
1while u:e>0
2 /ETBDu:current
3 if/ETB==NIL
4R ELABEL .u/
5 u:currentDu:N:head
6 elseif cf.u; /ETB/ > 0 andu:h==/ETB:hC1
7P USH.u; /ETB/
8 elseu:currentD/ETB:next-neighbor
Figure 26.9 steps through several iterations of the while loop of lines 1–8, which
executes as long as vertex uhas positive excess. Each iteration performs exactly
one of three actions, depending on the current vertex /ETBin the neighbor list u:N.
1. If /ETBisNIL, then we have run off the end of u:N. Line 4 relabels vertex u,
and then line 5 resets the current neighbor of uto be the ﬁrst one in u:N.
(Lemma 26.29 below states that the relabel operation applies in this situation.)
2. If /ETBis non- NIL and.u; /ETB/ is an admissible edge (determined by the test in
line 6), then line 7 pushes some (or possibly all) of u’s excess to vertex /ETB.
3. If /ETBis non- NILbut.u; /ETB/ is inadmissible, then line 8 advances u:current one
position further in the neighbor list u:N.
Observe that if D ISCHARGE is called on an overﬂowing vertex u, then the last
action performed by D ISCHARGE must be a push from u. Why? The procedure
terminates only when u:ebecomes zero, and neither the relabel operation nor ad-
vancing the pointer u:current affects the value of u:e.
We must be sure that when P USH or R ELABEL is called by D ISCHARGE ,t h e
operation applies. The next lemma proves this fact.
Lemma 26.29
If D ISCHARGE calls P USH.u; /ETB/ in line 7, then a push operation applies to .u; /ETB/ .
If D ISCHARGE calls R ELABEL .u/in line 4, then a relabel operation applies to u.
Proof The tests in lines 1 and 6 ensure that a push operation occurs only if the
operation applies, which proves the ﬁrst statement in the lemma.752 Chapter 26 Maximum Flow
s
–265
432106
x
0y
19z
05/5 814/14s
x
z
s
–265
432106
x
0y
19z
0814/14s
xz
5/5
s
–265
432106
x
0y
11z
88/814/14
5/5s
x
zs
x
z123
s
x
z4
5
s
xz6
s
xz7
s
xz8
s
xz9(a)
(b)
(c)
Figure 26.9 Discharging a vertex y. It takes 15iterations of the while loop of D ISCHARGE to push
all the excess ﬂow from y. Only the neighbors of yand edges of the ﬂow network that enter or leave y
are shown. In each part of the ﬁgure, the number inside each vertex is its excess at the beginning of
the ﬁrst iteration shown in the part, and each vertex is shown at its height throughout the part. The
neighbor list y:Nat the beginning of each iteration appears on the right, with the iteration number
on top. The shaded neighbor is y:current .(a)Initially, there are 19units of excess to push from y,
andy:currentDs. Iterations 1, 2, and 3 just advance y:current , since there are no admissible edges
leaving y. I ni t e r a t i o n4 , y:currentDNIL(shown by the shading being below the neighbor list),
and so yis relabeled and y:current is reset to the head of the neighbor list. (b)After relabeling,
vertex yhas height 1. In iterations 5 and 6, edges .y; s/ and.y; x/ are found to be inadmissible, but
iteration 7 pushes 8units of excess ﬂow from yto´. Because of the push, y:current does not advance
in this iteration. (c)Because the push in iteration 7 saturated edge .y; ´/ , it is found inadmissible in
iteration 8. In iteration 9, y:currentDNIL,a n ds ov e r t e x yis again relabeled and y:current is reset.26.5 The relabel-to-front algorithm 753
s
–265
432106
x
5y
6
z
858/814/14s
–265
432106
x
0y
11
z
88/814/14
5/5
s
–265
432106
x
5y
6
z
88/814/14
5
s
–205
432106
x
5y
0
z
858/88/14s
xz10
s
xz11
s
xz12
s
xz13
s
xz14
s
xz15
(f)(d)
(e)
(g)
Figure 26.9, continued (d) In iteration 10, .y; s/ is inadmissible, but iteration 11 pushes 5units
of excess ﬂow from ytox.(e)Because y:current did not advance in iteration 11, iteration 12
ﬁnds .y; x/ to be inadmissible. Iteration 13 ﬁnds .y; ´/ inadmissible, and iteration 14 relabels ver-
texyand resets y:current .(f)Iteration 15 pushes 6units of excess ﬂow from ytos.(g)Vertex y
now has no excess ﬂow, and D ISCHARGE terminates. In this example, D ISCHARGE both starts and
ﬁnishes with the current pointer at the head of the neighbor list, but in general this need not be the
case.754 Chapter 26 Maximum Flow
To prove the second statement, according to the test in line 1 and Lemma 26.28,
we need only show that all edges leaving uare inadmissible. If a call to
DISCHARGE .u/starts with the pointer u:current at the head of u’s neighbor list
and ﬁnishes with it off the end of the list, then all of u’s outgoing edges are in-
admissible and a relabel operation applies. It is possible, however, that during acall to D
ISCHARGE .u/, the pointer u:current traverses only part of the list be-
fore the procedure returns. Calls to D ISCHARGE on other vertices may then oc-
cur, but u:current will continue moving through the list during the next call to
DISCHARGE .u/. We now consider what happens during a complete pass through
the list, which begins at the head of u:Nand ﬁnishes with u:currentDNIL.O n c e
u:current reaches the end of the list, the procedure relabels uand begins a new
pass. For the u:current pointer to advance past a vertex /ETB2u:Nduring a pass, the
edge .u; /ETB/ must be deemed inadmissible by the test in line 6. Thus, by the time
the pass completes, every edge leaving uhas been determined to be inadmissible
at some time during the pass. The key observation is that at the end of the pass,
every edge leaving uis still inadmissible. Why? By Lemma 26.27, pushes cannot
create any admissible edges, regardless of which vertex the ﬂow is pushed from.
Thus, any admissible edge must be created by a relabel operation. But the vertex u
is not relabeled during the pass, and by Lemma 26.28, any other vertex /ETBthat is
relabeled during the pass (resulting from a call of D ISCHARGE ./ETB/) has no entering
admissible edges after relabeling. Thus, at the end of the pass, all edges leaving u
remain inadmissible, which completes the proof.
The relabel-to-front algorithm
In the relabel-to-front algorithm, we maintain a linked list Lconsisting of all ver-
tices in V/NULfs; tg. A key property is that the vertices in Lare topologically sorted
according to the admissible network, as we shall see in the loop invariant that fol-
lows. (Recall from Lemma 26.26 that the admissible network is a dag.)
The pseudocode for the relabel-to-front algorithm assumes that the neighbor
listsu:Nhave already been created for each vertex u. It also assumes that u:next
points to the vertex that follows uin list Land that, as usual, u:nextDNILifuis
the last vertex in the list.26.5 The relabel-to-front algorithm 755
RELABEL -TO-FRONT . G ;s ;t/
1I NITIALIZE -PREFLOW .G; s/
2LDG:V/NULfs; tg, in any order
3foreach vertex u2G:V/NULfs; tg
4 u:currentDu:N:head
5uDL:head
6while u¤NIL
7 old-heightDu:h
8D ISCHARGE .u/
9 ifu:h>old-height
10 move uto the front of list L
11 uDu:next
The relabel-to-front algorithm works as follows. Line 1 initializes the preﬂow
and heights to the same values as in the generic push-relabel algorithm. Line 2initializes the list Lto contain all potentially overﬂowing vertices, in any order.
Lines 3–4 initialize the current pointer of each vertex uto the ﬁrst vertex in u’s
neighbor list.
As Figure 26.10 illustrates, the while loop of lines 6–11 runs through the list L,
discharging vertices. Line 5 makes it start with the ﬁrst vertex in the list. Eachtime through the loop, line 8 discharges a vertex u.I fuwas relabeled by the
D
ISCHARGE procedure, line 10 moves it to the front of list L. We can determine
whether uwas relabeled by comparing its height before the discharge operation,
saved into the variable old-height in line 7, with its height afterward, in line 9.
Line 11 makes the next iteration of the while loop use the vertex following uin
listL. If line 10 moved uto the front of the list, the vertex used in the next iteration
is the one following uin its new position in the list.
To show that R ELABEL -TO-FRONT computes a maximum ﬂow, we shall show
that it is an implementation of the generic push-relabel algorithm. First, ob-serve that it performs push and relabel operations only when they apply, sinceLemma 26.29 guarantees that D
ISCHARGE performs them only when they apply.
It remains to show that when R ELABEL -TO-FRONT terminates, no basic opera-
tions apply. The remainder of the correctness argument relies on the followingloop invariant:
At each test in line 6 of R
ELABEL -TO-FRONT , list Lis a topological sort
of the vertices in the admissible network Gf;hD.V; E f;h/, and no vertex
before uin the list has excess ﬂow.
Initialization: Immediately after I NITIALIZE -PREFLOW has been run, s:hDjVj
and/ETB:hD0for all /ETB2V/NULfsg.S i n c ejVj/NAK2(because Vcontains at756 Chapter 26 Maximum Flow
s
–265
432106
x
12y
14z
0t
058 1 0
71614/1412/12L:xyz
N:s
y
z
ts
xzx
y
t(a)
s
–265
432106
x
0y
19z
0t
75/5 81 014/14 12/12L:xyz
N:s
y
z
ts
xzx
y
t(b)
77/16
s
–205
432106
x
5y
0
z
8t
758/8
108/14
12/12L: x yz
N: s
yz
ts
x
zx
y
t(c)
77/16
Figure 26.10 The action of R ELABEL -TO-FRONT .(a)A ﬂow network just before the ﬁrst iteration
of the while loop. Initially, 26units of ﬂow leave source s. On the right is shown the initial list
LDhx;y;´i, where initially uDx. Under each vertex in list Lis its neighbor list, with the current
neighbor shaded. Vertex xis discharged. It is relabeled to height 1,5units of excess ﬂow are pushed
toy,a n dt h e 7remaining units of excess are pushed to the sink t. Because xis relabeled, it moves
to the head of L, which in this case does not change the structure of L.(b)After x, the next vertex
inLthat is discharged is y. Figure 26.9 shows the detailed action of discharging yin this situation.
Because yis relabeled, it is moved to the head of L.(c)Vertex xnow follows yinL, and so it is
again discharged, pushing all 5units of excess ﬂow to t. Because vertex xis not relabeled in this
discharge operation, it remains in place in list L.26.5 The relabel-to-front algorithm 757
s
–205
432106
x
0y0
z
8t
1258/8
108/14
12/12L: x yz
N: s
y
z
ts
xzx
y
t(d)
712/16
s
–205
432106
x
0y0
z
0t
2058/8
8/108/14
12/12L: x y z
N: s
yz
ts
x
zx
y
t(e)
12/167
Figure 26.10, continued (d) Since vertex ´follows vertex xinL, it is discharged. It is relabeled
to height 1and all 8units of excess ﬂow are pushed to t. Because ´is relabeled, it moves to the
front of L.(e)Vertex ynow follows vertex ´inLand is therefore discharged. But because yhas no
excess, D ISCHARGE immediately returns, and yremains in place in L.V e r t e x xis then discharged.
Because it, too, has no excess, D ISCHARGE again returns, and xremains in place in L.RELABEL -
TO-FRONT has reached the end of list Land terminates. There are no overﬂowing vertices, and the
preﬂow is a maximum ﬂow.
least sandt), no edge can be admissible. Thus, Ef;hD;, and any ordering of
V/NULfs; tgis a topological sort of Gf;h.
Because uis initially the head of the list L, there are no vertices before it and
so there are none before it with excess ﬂow.
Maintenance: To see that each iteration of the while loop maintains the topolog-
ical sort, we start by observing that the admissible network is changed only by
push and relabel operations. By Lemma 26.27, push operations do not cause
edges to become admissible. Thus, only relabel operations can create admissi-ble edges. After a vertex uis relabeled, however, Lemma 26.28 states that there
are no admissible edges entering ubut there may be admissible edges leaving u.
Thus, by moving uto the front of L, the algorithm ensures that any admissible
edges leaving usatisfy the topological sort ordering.758 Chapter 26 Maximum Flow
To see that no vertex preceding uinLhas excess ﬂow, we denote the vertex
that will be uin the next iteration by u0. The vertices that will precede u0in the
next iteration include the current u(due to line 11) and either no other vertices
(ifuis relabeled) or the same vertices as before (if uis not relabeled). When u
is discharged, it has no excess ﬂow afterward. Thus, if uis relabeled during
the discharge, no vertices preceding u0have excess ﬂow. If uis not relabeled
during the discharge, no vertices before it on the list acquired excess ﬂow duringthis discharge, because Lremained topologically sorted at all times during the
discharge (as just pointed out, admissible edges are created only by relabeling,
not pushing), and so each push operation causes excess ﬂow to move only tovertices further down the list (or to sort). Again, no vertices preceding u
0have
excess ﬂow.
Termination: When the loop terminates, uis just past the end of L,a n ds ot h e
loop invariant ensures that the excess of every vertex is 0. Thus, no basic oper-
ations apply.
Analysis
We shall now show that R ELABEL -TO-FRONT runs in O.V3/time on any ﬂow
network GD.V; E/ . Since the algorithm is an implementation of the generic
push-relabel algorithm, we shall take advantage of Corollary 26.21, which pro-
vides an O.V / bound on the number of relabel operations executed per vertex and
anO.V2/bound on the total number of relabel operations overall. In addition, Ex-
ercise 26.4-3 provides an O.VE/ bound on the total time spent performing relabel
operations, and Lemma 26.22 provides an O.VE/ bound on the total number of
saturating push operations.
Theorem 26.30
The running time of R ELABEL -TO-FRONT on any ﬂow network GD.V; E/
isO.V3/.
Proof Let us consider a “phase” of the relabel-to-front algorithm to be the time
between two consecutive relabel operations. There are O.V2/phases, since there
areO.V2/relabel operations. Each phase consists of at most jVjcalls to D IS-
CHARGE , which we can see as follows. If D ISCHARGE does not perform a re-
label operation, then the next call to D ISCHARGE is further down the list L,a n d
the length of Lis less thanjVj.I f D ISCHARGE does perform a relabel, the next
call to D ISCHARGE belongs to a different phase. Since each phase contains at
mostjVjcalls to D ISCHARGE and there are O.V2/phases, the number of times
DISCHARGE is called in line 8 of R ELABEL -TO-FRONT isO.V3/. Thus, the total26.5 The relabel-to-front algorithm 759
work performed by the while loop in R ELABEL -TO-FRONT , excluding the work
performed within D ISCHARGE ,i sa tm o s t O.V3/.
We must now bound the work performed within D ISCHARGE during the ex-
ecution of the algorithm. Each iteration of the while loop within D ISCHARGE
performs one of three actions. We shall analyze the total amount of work involved
in performing each of these actions.
We start with relabel operations (lines 4–5). Exercise 26.4-3 provides an O.VE/
time bound on all the O.V2/relabels that are performed.
Now, suppose that the action updates the u:current pointer in line 8. This action
occurs O.degree .u// times each time a vertex uis relabeled, and O.V/SOHdegree .u//
times overall for the vertex. For all vertices, therefore, the total amount of workdone in advancing pointers in neighbor lists is O.VE/ by the handshaking lemma
(Exercise B.4-1).
The third type of action performed by D
ISCHARGE is a push operation (line 7).
We already know that the total number of saturating push operations is O.VE/ .
Observe that if a nonsaturating push is executed, D ISCHARGE immediately returns,
since the push reduces the excess to 0. Thus, there can be at most one nonsaturating
push per call to D ISCHARGE .A sw eh a v eo b s e r v e d ,D ISCHARGE is called O.V3/
times, and thus the total time spent performing nonsaturating pushes is O.V3/.
The running time of R ELABEL -TO-FRONT is therefore O.V3CVE/,w h i c h
isO.V3/.
Exercises
26.5-1
Illustrate the execution of R ELABEL -TO-FRONT in the manner of Figure 26.10 for
the ﬂow network in Figure 26.1(a). Assume that the initial ordering of vertices in L
ish/ETB1;/ETB2;/ETB3;/ETB4iand that the neighbor lists are
/ETB1:NDh s; /ETB 2;/ETB3i;
/ETB2:NDh s; /ETB 1;/ETB3;/ETB4i;
/ETB3:NDh /ETB1;/ETB2;/ETB4;ti;
/ETB4:NDh /ETB2;/ETB3;ti:
26.5-2 ?
We would like to implement a push-relabel algorithm in which we maintain a ﬁrst-in, ﬁrst-out queue of overﬂowing vertices. The algorithm repeatedly discharges thevertex at the head of the queue, and any vertices that were not overﬂowing beforethe discharge but are overﬂowing afterward are placed at the end of the queue.After the vertex at the head of the queue is discharged, it is removed. When the760 Chapter 26 Maximum Flow
queue is empty, the algorithm terminates. Show how to implement this algorithm
to compute a maximum ﬂow in O.V3/time.
26.5-3
Show that the generic algorithm still works if R ELABEL updates u:hby sim-
ply computing u:hDu:hC1. How would this change affect the analysis of
RELABEL -TO-FRONT ?
26.5-4 ?
Show that if we always discharge a highest overﬂowing vertex, we can make the
push-relabel method run in O.V3/time.
26.5-5
Suppose that at some point in the execution of a push-relabel algorithm, there existsan integer 0<k/DC4jVj/NUL1for which no vertex has /ETB:hDk. Show that all
vertices with /ETB:h>k are on the source side of a minimum cut. If such a kexists,
thegap heuristic updates every vertex /ETB2V/NULfsgfor which /ETB:h>k, to set
/ETB:hDmax./ETB:h;jVjC1/. Show that the resulting attribute his a height function.
(The gap heuristic is crucial in making implementations of the push-relabel methodperform well in practice.)
Problems
26-1 Escape problemAnn/STXngrid is an undirected graph consisting of nrows and ncolumns of vertices,
as shown in Figure 26.11. We denote the vertex in the ith row and the jth column
by.i; j / . All vertices in a grid have exactly four neighbors, except for the boundary
vertices, which are the points .i; j / for which iD1,iDn,jD1,o rjDn.
Given m/DC4n
2starting points .x1;y1/; .x 2;y2/ ;:::;. x m;ym/in the grid, the
escape problem is to determine whether or not there are mvertex-disjoint paths
from the starting points to any mdifferent points on the boundary. For example,
the grid in Figure 26.11(a) has an escape, but the grid in Figure 26.11(b) does not.
a.Consider a ﬂow network in which vertices, as well as edges, have capacities.
That is, the total positive ﬂow entering any given vertex is subject to a capacity
constraint. Show that determining the maximum ﬂow in a network with edge
and vertex capacities can be reduced to an ordinary maximum-ﬂow problem ona ﬂow network of comparable size.Problems for Chapter 26 761
(a) (b)
Figure 26.11 Grids for the escape problem. Starting points are black, and other grid vertices are
white. (a)A grid with an escape, shown by shaded paths. (b)A grid with no escape.
b.Describe an efﬁcient algorithm to solve the escape problem, and analyze its
running time.
26-2 Minimum path cover
Apath cover of a directed graph GD.V; E/ is a set Pof vertex-disjoint paths
such that every vertex in Vis included in exactly one path in P. Paths may start
and end anywhere, and they may be of any length, including 0.Aminimum path
cover ofGis a path cover containing the fewest possible paths.
a.Give an efﬁcient algorithm to ﬁnd a minimum path cover of a directed acyclic
graph GD.V; E/ .(Hint: Assuming that VDf1 ;2;:::;ng, construct the
graph G0D.V0;E0/,w h e r e
V0Dfx0;x1;:::;x ng[fy0;y1;:::;y ng;
E0Df.x0;xi/Wi2Vg[f.yi;y0/Wi2Vg[f.xi;yj/W.i; j /2Eg;
and run a maximum-ﬂow algorithm.)
b.Does your algorithm work for directed graphs that contain cycles? Explain.
26-3 Algorithmic consulting
Professor Gore wants to open up an algorithmic consulting company. He has iden-tiﬁed nimportant subareas of algorithms (roughly corresponding to different por-
tions of this textbook), which he represents by the set ADfA
1;A2;:::;A ng.I n
each subarea Ak, he can hire an expert in that area for ckdollars. The consulting
company has lined up a set JDfJ1;J2;:::;J mgof potential jobs. In order to
perform job Ji, the company needs to have hired experts in a subset Ri/DC2Aof762 Chapter 26 Maximum Flow
subareas. Each expert can work on multiple jobs simultaneously. If the company
chooses to accept job Ji, it must have hired experts in all subareas in Ri, and it will
take in revenue of pidollars.
Professor Gore’s job is to determine which subareas to hire experts in and which
jobs to accept in order to maximize the net revenue, which is the total income fromjobs accepted minus the total cost of employing the experts.
Consider the following ﬂow network G. It contains a source vertex s, vertices
A
1;A2;:::;A n, vertices J1;J2;:::;J m, and a sink vertex t.F o r kD1 ;2:::;n ,
the ﬂow network contains an edge .s; A k/with capacity c.s;A k/Dck,a n d
foriD1 ;2;:::;m , the ﬂow network contains an edge .Ji;t/with capacity
c.J i;t/Dpi.F o r kD1 ;2;:::;n andiD1 ;2;:::;m ,i fAk2Ri,t h e n G
contains an edge .Ak;Ji/with capacity c.A k;Ji/D1 .
a.Show that if Ji2Tfor a ﬁnite-capacity cut .S; T / ofG,t h e n Ak2Tfor each
Ak2Ri.
b.Show how to determine the maximum net revenue from the capacity of a mini-
mum cut of Gand the given pivalues.
c.Give an efﬁcient algorithm to determine which jobs to accept and which experts
to hire. Analyze the running time of your algorithm in terms of m,n,a n d
rDPm
iD1jRij.
26-4 Updating maximum ﬂow
LetGD.V; E/ be a ﬂow network with source s,s i n k t, and integer capacities.
Suppose that we are given a maximum ﬂow in G.
a.Suppose that we increase the capacity of a single edge .u; /ETB/2Eby1.G i v e
anO.VCE/-time algorithm to update the maximum ﬂow.
b.Suppose that we decrease the capacity of a single edge .u; /ETB/2Eby1.G i v e
anO.VCE/-time algorithm to update the maximum ﬂow.
26-5 Maximum ﬂow by scaling
LetGD.V; E/ be a ﬂow network with source s,s i n k t, and an integer capac-
ityc.u;/ETB/ on each edge .u; /ETB/2E.L e t CDmax .u;/ETB/ 2Ec.u;/ETB/ .
a.Argue that a minimum cut of Ghas capacity at most CjEj.
b.For a given number K, show how to ﬁnd an augmenting path of capacity at
leastKinO.E/ time, if such a path exists.Problems for Chapter 26 763
We can use the following modiﬁcation of F ORD-FULKERSON -METHOD to com-
pute a maximum ﬂow in G:
MAX-FLOW -BY-SCALING . G ;s ;t/
1CDmax .u;/ETB/ 2Ec.u;/ETB/
2 initialize ﬂow fto0
3KD2blgCc
4while K/NAK1
5 while there exists an augmenting path pof capacity at least K
6 augment ﬂow falong p
7 KDK=2
8return f
c.Argue that M AX-FLOW -BY-SCALING returns a maximum ﬂow.
d.Show that the capacity of a minimum cut of the residual network Gfis at most
2KjEjeach time line 4 is executed.
e.Argue that the inner while loop of lines 5–6 executes O.E/ times for each value
ofK.
f.Conclude that M AX-FLOW -BY-SCALING can be implemented so that it runs
inO.E2lgC/time.
26-6 The Hopcroft-Karp bipartite matching algorithm
In this problem, we describe a faster algorithm, due to Hopcroft and Karp, for
ﬁnding a maximum matching in a bipartite graph. The algorithm runs in O.p
VE /
time. Given an undirected, bipartite graph GD.V; E/ ,w h e r e VDL[Rand
all edges have exactly one endpoint in L,l e tMbe a matching in G. We say that
as i m p l ep a t h PinGis anaugmenting path with respect to Mif it starts at an
unmatched vertex in L, ends at an unmatched vertex in R, and its edges belong
alternately to MandE/NULM. (This deﬁnition of an augmenting path is related
to, but different from, an augmenting path in a ﬂow network.) In this problem,we treat a path as a sequence of edges, rather than as a sequence of vertices. Ashortest augmenting path with respect to a matching Mis an augmenting path
with a minimum number of edges.
Given two sets AandB,t h esymmetric difference A˚Bis deﬁned as .A/NULB/[
.B/NULA/, that is, the elements that are in exactly one of the two sets.764 Chapter 26 Maximum Flow
a.Show that if Mis a matching and Pis an augmenting path with respect to M,
then the symmetric difference M˚Pis a matching andjM˚PjDjMjC1.
Show that if P1;P2;:::;P kare vertex-disjoint augmenting paths with respect
toM, then the symmetric difference M˚.P1[P2[/SOH/SOH/SOH[ Pk/is a matching
with cardinalityjMjCk.
The general structure of our algorithm is the following:
HOPCROFT -KARP.G/
1MD;
2repeat
3l e t PDfP1;P2;:::;P kgbe a maximal set of vertex-disjoint
shortest augmenting paths with respect to M
4 MDM˚.P1[P2[/SOH/SOH/SOH[ Pk/
5until P==;
6return M
The remainder of this problem asks you to analyze the number of iterations in
the algorithm (that is, the number of iterations in the repeat loop) and to describe
an implementation of line 3.
b.Given two matchings MandM/ETXinG, show that every vertex in the graph
G0D.V; M˚M/ETX/has degree at most 2. Conclude that G0is a disjoint
union of simple paths or cycles. Argue that edges in each such simple path
or cycle belong alternately to MorM/ETX. Prove that ifjMj/DC4jM/ETXj,t h e n
M˚M/ETXcontains at leastjM/ETXj/NULjMjvertex-disjoint augmenting paths with
respect to M.
Letlbe the length of a shortest augmenting path with respect to a matching M,a n d
letP1;P2;:::;P kbe a maximal set of vertex-disjoint augmenting paths of length l
with respect to M.L e tM0DM˚.P1[/SOH/SOH/SOH[ Pk/, and suppose that Pi sas h o r t e s t
augmenting path with respect to M0.
c.Show that if Pis vertex-disjoint from P1;P2;:::;P k,t h e n Phas more than l
edges.
d.Now suppose that Pis not vertex-disjoint from P1;P2;:::;P k.L e t Abe the
set of edges .M˚M0/˚P. Show that AD.P1[P2[/SOH/SOH/SOH[ Pk/˚Pand
thatjAj/NAK.kC1/l. Conclude that Phas more than ledges.
e.Prove that if a shortest augmenting path with respect to Mhasledges, the size
of the maximum matching is at most jMjCjVj=.lC1/.Notes for Chapter 26 765
f.Show that the number of repeat loop iterations in the algorithm is at
most 2p
jVj.(Hint: By how much can Mgrow after iteration numberp
jVj?)
g.Give an algorithm that runs in O.E/ time to ﬁnd a maximal set of vertex-
disjoint shortest augmenting paths P1;P2;:::;P kfor a given matching M.
Conclude that the total running time of H OPCROFT -KARP isO.p
VE / .
Chapter notes
Ahuja, Magnanti, and Orlin [7], Even [103], Lawler [224], Papadimitriou and Stei-
glitz [271], and Tarjan [330] are good references for network ﬂow and related algo-
rithms. Goldberg, Tardos, and Tarjan [139] also provide a nice survey of algorithmsfor network-ﬂow problems, and Schrijver [304] has written an interesting reviewof historical developments in the ﬁeld of network ﬂows.
The Ford-Fulkerson method is due to Ford and Fulkerson [109], who originated
the formal study of many of the problems in the area of network ﬂow, includingthe maximum-ﬂow and bipartite-matching problems. Many early implementationsof the Ford-Fulkerson method found augmenting paths using breadth-ﬁrst search;Edmonds and Karp [102], and independently Dinic [89], proved that this strategyyields a polynomial-time algorithm. A related idea, that of using “blocking ﬂows,”was also ﬁrst developed by Dinic [89]. Karzanov [202] ﬁrst developed the idea ofpreﬂows. The push-relabel method is due to Goldberg [136] and Goldberg and Tar-jan [140]. Goldberg and Tarjan gave an O.V
3/-time algorithm that uses a queue to
maintain the set of overﬂowing vertices, as well as an algorithm that uses dynamic
trees to achieve a running time of O.VE lg.V2=EC2//. Several other researchers
have developed push-relabel maximum-ﬂow algorithms. Ahuja and Orlin [9] andAhuja, Orlin, and Tarjan [10] gave algorithms that used scaling. Cheriyan andMaheshwari [62] proposed pushing ﬂow from the overﬂowing vertex of maximumheight. Cheriyan and Hagerup [61] suggested randomly permuting the neighborlists, and several researchers [14, 204, 276] developed clever derandomizations ofthis idea, leading to a sequence of faster algorithms. The algorithm of King, Rao,and Tarjan [204] is the fastest such algorithm and runs in O.VE log
E=.V lgV/V/
time.
The asymptotically fastest algorithm to date for the maximum-ﬂow problem, by
Goldberg and Rao [138], runs in time O.min.V2=3;E1=2/Elg.V2=EC2/lgC/,
where CDmax .u;/ETB/ 2Ec.u;/ETB/ . This algorithm does not use the push-relabel
method but instead is based on ﬁnding blocking ﬂows. All previous maximum-ﬂow algorithms, including the ones in this chapter, use some notion of distance
(the push-relabel algorithms use the analogous notion of height), with a length of 1766 Chapter 26 Maximum Flow
assigned implicitly to each edge. This new algorithm takes a different approach and
assigns a length of 0to high-capacity edges and a length of 1to low-capacity edges.
Informally, with respect to these lengths, shortest paths from the source to the sinktend have high capacity, which means that fewer iterations need be performed.
In practice, push-relabel algorithms currently dominate augmenting-path or
linear-programming based algorithms for the maximum-ﬂow problem. A studyby Cherkassky and Goldberg [63] underscores the importance of using two heuris-tics when implementing a push-relabel algorithm. The ﬁrst heuristic is to peri-
odically perform a breadth-ﬁrst search of the residual network in order to obtain
more accurate height values. The second heuristic is the gap heuristic, described inExercise 26.5-5. Cherkassky and Goldberg conclude that the best choice of push-relabel variants is the one that chooses to discharge the overﬂowing vertex with themaximum height.
The best algorithm to date for maximum bipartite matching, discovered by
Hopcroft and Karp [176], runs in O.p
VE / time and is described in Problem 26-6.
The book by Lov´ asz and Plummer [239] is an excellent reference on matching
problems.VII Selected TopicsIntroduction
This part contains a selection of algorithmic topics that extend and complement
earlier material in this book. Some chapters introduce new models of computation
such as circuits or parallel computers. Others cover specialized domains such ascomputational geometry or number theory. The last two chapters discuss some ofthe known limitations to the design of efﬁcient algorithms and introduce techniquesfor coping with those limitations.
Chapter 27 presents an algorithmic model for parallel computing based on dy-
namic multithreading. The chapter introduces the basics of the model, showing
how to quantify parallelism in terms of the measures of work and span. It then
investigates several interesting multithreaded algorithms, including algorithms for
matrix multiplication and merge sorting.
Chapter 28 studies efﬁcient algorithms for operating on matrices. It presents
two general methods—LU decomposition and LUP decomposition—for solvinglinear equations by Gaussian elimination in O.n
3/time. It also shows that matrix
inversion and matrix multiplication can be performed equally fast. The chapter
concludes by showing how to compute a least-squares approximate solution when
a set of linear equations has no exact solution.
Chapter 29 studies linear programming, in which we wish to maximize or mini-
mize an objective, given limited resources and competing constraints. Linear pro-gramming arises in a variety of practical application areas. This chapter covers howto formulate and solve linear programs. The solution method covered is the sim-plex algorithm, which is the oldest algorithm for linear programming. In contrastto many algorithms in this book, the simplex algorithm does not run in polynomialtime in the worst case, but it is fairly efﬁcient and widely used in practice.770 Part VII Selected Topics
Chapter 30 studies operations on polynomials and shows how to use a well-
known signal-processing technique—the f ast Fourier transf orm (FFT)—to multi-
ply two degree- npolynomials in O.n lgn/time. It also investigates efﬁcient im-
plementations of the FFT, including a parallel circuit.
Chapter 31 presents number-theoretic algorithms. After reviewing elementary
number theory, it presents Euclid’s algorithm for computing greatest common di-visors. Next, it studies algorithms for solving modular linear equations and forraising one number to a power modulo another number. Then, it explores an impor-
tant application of number-theoretic algorithms: the RSA public-key cryptosystem.
This cryptosystem can be used not only to encrypt messages so that an adversarycannot read them, but also to provide digital signatures. The chapter then presentsthe Miller-Rabin randomized primality test, with which we can ﬁnd large primesefﬁciently—an essential requirement for the RSA system. Finally, the chapter cov-ers Pollard’s “rho” heuristic for factoring integers and discusses the state of the artof integer factorization.
Chapter 32 studies the problem of ﬁnding all occurrences of a given pattern
string in a given text string, a problem that arises frequently in text-editing pro-
grams. After examining the naive approach, the chapter presents an elegant ap-
proach due to Rabin and Karp. Then, after showing an efﬁcient solution basedon ﬁnite automata, the chapter presents the Knuth-Morris-Pratt algorithm, whichmodiﬁes the automaton-based algorithm to save space by cleverly preprocessingthe pattern.
Chapter 33 considers a few problems in computational geometry. After dis-
cussing basic primitives of computational geometry, the chapter shows how to usea “sweeping” method to efﬁciently determine whether a set of line segments con-tains any intersections. Two clever algorithms for ﬁnding the convex hull of a set ofpoints—Graham’s scan and Jarvis’s march—also illustrate the power of sweepingmethods. The chapter closes with an efﬁcient algorithm for ﬁnding the closest pairfrom among a given set of points in the plane.
Chapter 34 concerns NP-complete problems. Many interesting computational
problems are NP-complete, but no polynomial-time algorithm is known for solvingany of them. This chapter presents techniques for determining when a problem isNP-complete. Several classic problems are proved to be NP-complete: determiningwhether a graph has a hamiltonian cycle, determining whether a boolean formulais satisﬁable, and determining whether a given set of numbers has a subset thatadds up to a given target value. The chapter also proves that the famous traveling-
salesman problem is NP-complete.
Chapter 35 shows how to ﬁnd approximate solutions to NP-complete problems
efﬁciently by using approximation algorithms. For some NP-complete problems,approximate solutions that are near optimal are quite easy to produce, but for otherseven the best approximation algorithms known work progressively more poorly asPart VII Selected Topics 771
the problem size increases. Then, there are some problems for which we can invest
increasing amounts of computation time in return for increasingly better approx-imate solutions. This chapter illustrates these possibilities with the vertex-coverproblem (unweighted and weighted versions), an optimization version of 3-CNFsatisﬁability, the traveling-salesman problem, the set-covering problem, and thesubset-sum problem.27 Multithreaded Algorithms
The vast majority of algorithms in this book are serial algorithms suitable for
running on a uniprocessor computer in which only one instruction executes at atime. In this chapter, we shall extend our algorithmic model to encompass parallel
algorithms , which can run on a multiprocessor computer that permits multiple
instructions to execute concurrently. In particular, we shall explore the elegantmodel of dynamic multithreaded algorithms, which are amenable to algorithmicdesign and analysis, as well as to efﬁcient implementation in practice.
Parallel computers—computers with multiple processing units—have become
increasingly common, and they span a wide range of prices and performance. Rela-
tively inexpensive desktop and laptop chip multiprocessors contain a single multi-
core integrated-circuit chip that houses multiple processing “cores,” each of which
is a full-ﬂedged processor that can access a common memory. At an intermedi-ate price/performance point are clusters built from individual computers—oftensimple PC-class machines—with a dedicated network interconnecting them. The
highest-priced machines are supercomputers, which often use a combination of
custom architectures and custom networks to deliver the highest performance interms of instructions executed per second.
Multiprocessor computers have been around, in one form or another, for
decades. Although the computing community settled on the random-access ma-chine model for serial computing early on in the history of computer science, nosingle model for parallel computing has gained as wide acceptance. A major rea-son is that vendors have not agreed on a single architectural model for parallelcomputers. For example, some parallel computers feature shared memory ,w h e r e
each processor can directly access any location of memory. Other parallel com-puters employ distributed memory , where each processor’s memory is private, and
an explicit message must be sent between processors in order for one processor toaccess the memory of another. With the advent of multicore technology, however,every new laptop and desktop machine is now a shared-memory parallel computer,Chapter 27 Multithreaded Algorithms 773
and the trend appears to be toward shared-memory multiprocessing. Although time
will tell, that is the approach we shall take in this chapter.
One common means of programming chip multiprocessors and other shared-
memory parallel computers is by using static threading , which provides a software
abstraction of “virtual processors,” or threads , sharing a common memory. Each
thread maintains an associated program counter and can execute code indepen-dently of the other threads. The operating system loads a thread onto a processorfor execution and switches it out when another thread needs to run. Although the
operating system allows programmers to create and destroy threads, these opera-
tions are comparatively slow. Thus, for most applications, threads persist for theduration of a computation, which is why we call them “static.”
Unfortunately, programming a shared-memory parallel computer directly using
static threads is difﬁcult and error-prone. One reason is that dynamically parti-tioning the work among the threads so that each thread receives approximatelythe same load turns out to be a complicated undertaking. For any but the sim-plest of applications, the programmer must use complex communication protocolsto implement a scheduler to load-balance the work. This state of affairs has ledtoward the creation of concurrency platforms , which provide a layer of software
that coordinates, schedules, and manages the parallel-computing resources. Someconcurrency platforms are built as runtime libraries, but others provide full-ﬂedgedparallel languages with compiler and runtime support.
Dynamic multithreaded programming
One important class of concurrency platform is dynamic multithreading ,w h i c hi s
the model we shall adopt in this chapter. Dynamic multithreading allows program-mers to specify parallelism in applications without worrying about communication
protocols, load balancing, and other vagaries of static-thread programming. The
concurrency platform contains a scheduler, which load-balances the computationautomatically, thereby greatly simplifying the programmer’s chore. Although thefunctionality of dynamic-multithreading environments is still evolving, almost allsupport two features: nested parallelism and parallel loops. Nested parallelismallows a subroutine to be “spawned,” allowing the caller to proceed while thespawned subroutine is computing its result. A parallel loop is like an ordinaryforloop, except that the iterations of the loop can execute concurrently.
These two features form the basis of the model for dynamic multithreading that
we shall study in this chapter. A key aspect of this model is that the programmerneeds to specify only the logical parallelism within a computation, and the threadswithin the underlying concurrency platform schedule and load-balance the compu-tation among themselves. We shall investigate multithreaded algorithms written for774 Chapter 27 Multithreaded Algorithms
this model, as well how the underlying concurrency platform can schedule compu-
tations efﬁciently.
Our model for dynamic multithreading offers several important advantages:
/SIIt is a simple extension of our serial programming model. We can describe amultithreaded algorithm by adding to our pseudocode just three “concurrency”keywords: parallel ,spawn ,a n d sync . Moreover, if we delete these concur-
rency keywords from the multithreaded pseudocode, the resulting text is serialpseudocode for the same problem, which we call the “serialization” of the mul-tithreaded algorithm.
/SIIt provides a theoretically clean way to quantify parallelism based on the no-tions of “work” and “span.”
/SIMany multithreaded algorithms involving nested parallelism follow naturallyfrom the divide-and-conquer paradigm. Moreover, just as serial divide-and-conquer algorithms lend themselves to analysis by solving recurrences, so do
multithreaded algorithms.
/SIThe model is faithful to how parallel-computing practice is evolving. A grow-
ing number of concurrency platforms support one variant or another of dynamicmultithreading, including Cilk [51, 118], Cilk++ [71], OpenMP [59], Task Par-allel Library [230], and Threading Building Blocks [292].
Section 27.1 introduces the dynamic multithreading model and presents the met-
rics of work, span, and parallelism, which we shall use to analyze multithreadedalgorithms. Section 27.2 investigates how to multiply matrices with multithread-ing, and Section 27.3 tackles the tougher problem of multithreading merge sort.
27.1 The basics of dynamic multithreading
We shall begin our exploration of dynamic multithreading using the example ofcomputing Fibonacci numbers recursively. Recall that the Fibonacci numbers aredeﬁned by recurrence (3.22):
F
0D0;
F1D1;
FiDFi/NUL1CFi/NUL2 fori/NAK2:
Here is a simple, recursive, serial algorithm to compute the nth Fibonacci number:27.1 The basics of dynamic multithreading 775
FIB.0/
FIB.0/ FIB.0/ FIB.0/
FIB.0/FIB.1/ FIB.1/
FIB.1/FIB.1/
FIB.1/ FIB.1/ FIB.1/
FIB.1/FIB.2/
FIB.2/ FIB.2/ FIB.2/
FIB.2/FIB.3/ FIB.3/
FIB.3/FIB.4/
FIB.4/FIB.5/FIB.6/
Figure 27.1 The tree of recursive procedure instances when computing F IB.6/. Each instance of
FIBwith the same argument does the same work to produce the same result, providing an inefﬁcient
but interesting way to compute Fibonacci numbers.
FIB.n/
1ifn/DC41
2 return n
3elsexDFIB.n/NUL1/
4 yDFIB.n/NUL2/
5 return xCy
You would not really want to compute large Fibonacci numbers this way, be-
cause this computation does much repeated work. Figure 27.1 shows the tree ofrecursive procedure instances that are created when computing F
6. For example,
a call to F IB.6/recursively calls F IB.5/and then F IB.4/. But, the call to F IB.5/
also results in a call to F IB.4/. Both instances of F IB.4/return the same result
(F4D3). Since the F IBprocedure does not memoize, the second call to F IB.4/
replicates the work that the ﬁrst call performs.
LetT .n/ denote the running time of F IB.n/.S i n c eF IB.n/contains two recur-
sive calls plus a constant amount of extra work, we obtain the recurrence
T .n/DT. n/NUL1/CT. n/NUL2/C‚.1/ :
This recurrence has solution T .n/D‚.F n/, which we can show using the substi-
tution method. For an inductive hypothesis, assume that T .n//DC4aFn/NULb,w h e r e
a>1 andb>0 are constants. Substituting, we obtain776 Chapter 27 Multithreaded Algorithms
T .n//DC4.aF n/NUL1/NULb/C.aF n/NUL2/NULb/C‚.1/
Da.F n/NUL1CFn/NUL2//NUL2bC‚.1/
DaFn/NULb/NUL.b/NUL‚.1//
/DC4aFn/NULb
if we choose blarge enough to dominate the constant in the ‚.1/ . We can then
choose alarge enough to satisfy the initial condition. The analytical bound
T .n/D‚./RSn/; (27.1)
where /RSD.1Cp
5/=2 is the golden ratio, now follows from equation (3.25).
Since Fngrows exponentially in n, this procedure is a particularly slow way to
compute Fibonacci numbers. (See Problem 31-3 for much faster ways.)
Although the F IBprocedure is a poor way to compute Fibonacci numbers, it
makes a good example for illustrating key concepts in the analysis of multithreadedalgorithms. Observe that within F
IB.n/, the two recursive calls in lines 3 and 4 to
FIB.n/NUL1/and F IB.n/NUL2/, respectively, are independent of each other: they could
be called in either order, and the computation performed by one in no way affectsthe other. Therefore, the two recursive calls can run in parallel.
We augment our pseudocode to indicate parallelism by adding the concurrency
keywords spawn andsync . Here is how we can rewrite the F
IBprocedure to use
dynamic multithreading:
P-F IB.n/
1ifn/DC41
2 return n
3elsexDspawn P-F IB.n/NUL1/
4 yDP-F IB.n/NUL2/
5 sync
6 return xCy
Notice that if we delete the concurrency keywords spawn andsync from P-F IB,
the resulting pseudocode text is identical to F IB(other than renaming the procedure
in the header and in the two recursive calls). We deﬁne the serialization of a mul-
tithreaded algorithm to be the serial algorithm that results from deleting the multi-threaded keywords: spawn ,sync , and when we examine parallel loops, parallel .
Indeed, our multithreaded pseudocode has the nice property that a serialization isalways ordinary serial pseudocode to solve the same problem.
Nested parallelism occurs when the keyword spawn precedes a procedure call,
as in line 3. The semantics of a spawn differs from an ordinary procedure call inthat the procedure instance that executes the spawn—the parent —may continue
to execute in parallel with the spawned subroutine—its child —instead of waiting27.1 The basics of dynamic multithreading 777
for the child to complete, as would normally happen in a serial execution. In this
case, while the spawned child is computing P-F IB.n/NUL1/, the parent may go on
to compute P-F IB.n/NUL2/in line 4 in parallel with the spawned child. Since the
P-F IBprocedure is recursive, these two subroutine calls themselves create nested
parallelism, as do their children, thereby creating a potentially vast tree of subcom-putations, all executing in parallel.
The keyword spawn does not say, however, that a procedure must execute con-
currently with its spawned children, only that it may. The concurrency keywords
express the logical parallelism of the computation, indicating which parts of the
computation may proceed in parallel. At runtime, it is up to a scheduler to deter-
mine which subcomputations actually run concurrently by assigning them to avail-able processors as the computation unfolds. We shall discuss the theory behindschedulers shortly.
A procedure cannot safely use the values returned by its spawned children until
after it executes a sync statement, as in line 5. The keyword sync indicates that
the procedure must wait as necessary for all its spawned children to complete be-
fore proceeding to the statement after the sync .I n t h e P - F
IBprocedure, a sync
is required before the return statement in line 6 to avoid the anomaly that would
occur if xandywere summed before xwas computed. In addition to explicit
synchronization provided by the sync statement, every procedure executes a sync
implicitly before it returns, thus ensuring that all its children terminate before itdoes.
A model for multithreaded execution
It helps to think of a multithreaded computation —the set of runtime instruc-
tions executed by a processor on behalf of a multithreaded program—as a directed
acyclic graph GD.V; E/ , called a computation dag . As an example, Figure 27.2
shows the computation dag that results from computing P-F
IB.4/. Conceptually,
the vertices in Vare instructions, and the edges in Erepresent dependencies be-
tween instructions, where .u; /ETB/2Emeans that instruction umust execute before
instruction /ETB. For convenience, however, if a chain of instructions contains no
parallel control (no spawn ,sync ,o rreturn from a spawn—via either an explicit
return statement or the return that happens implicitly upon reaching the end of
a procedure), we may group them into a single strand , each of which represents
one or more instructions. Instructions involving parallel control are not includedin strands, but are represented in the structure of the dag. For example, if a strandhas two successors, one of them must have been spawned, and a strand with mul-tiple predecessors indicates the predecessors joined because of a sync statement.
Thus, in the general case, the set Vforms the set of strands, and the set Eof di-
rected edges represents dependencies between strands induced by parallel control.778 Chapter 27 Multithreaded Algorithms
P-FIB(1) P-FIB(0)P-FIB(3)P-FIB(4)
P-FIB(1)P-FIB(1)
P-FIB(0)P-FIB(2)
P-FIB(2)
Figure 27.2 A directed acyclic graph representing the computation of P-F IB.4/. Each circle rep-
resents one strand, with black circles representing either base cases or the part of the procedure
(instance) up to the spawn of P-F IB.n/NUL1/in line 3, shaded circles representing the part of the pro-
cedure that calls P-F IB.n/NUL2/in line 4 up to the sync in line 5, where it suspends until the spawn of
P-F IB.n/NUL1/returns, and white circles representing the part of the procedure after the sync where
it sums xandyup to the point where it returns the result. Each group of strands belonging to the
same procedure is surrounded by a rounded rectangle, lightly shaded for spawned procedures and
heavily shaded for called procedures. Spawn edges and call edges point downward, continuation
edges point horizontally to the right, and return edges point upward. Assuming that each strand takesunit time, the work equals 17time units, since there are 17strands, and the span is 8time units, since
the critical path—shown with shaded edges—contains 8strands.
IfGhas a directed path from strand uto strand /ETB, we say that the two strands are
(logically) in series . Otherwise, strands uand/ETBare(logically) in parallel .
We can picture a multithreaded computation as a dag of strands embedded in a
tree of procedure instances. For example, Figure 27.1 shows the tree of procedureinstances for P-F
IB.6/without the detailed structure showing strands. Figure 27.2
zooms in on a section of that tree, showing the strands that constitute each proce-dure. All directed edges connecting strands run either within a procedure or alongundirected edges in the procedure tree.
We can classify the edges of a computation dag to indicate the kind of dependen-
cies between the various strands. A continuation edge .u; u
0/, drawn horizontally
in Figure 27.2, connects a strand uto its successor u0within the same procedure
instance. When a strand uspawns a strand /ETB, the dag contains a spawn edge .u; /ETB/ ,
which points downward in the ﬁgure. Call edges , representing normal procedure
calls, also point downward. Strand uspawning strand /ETBdiffers from ucalling /ETB
in that a spawn induces a horizontal continuation edge from uto the strand u0fol-27.1 The basics of dynamic multithreading 779
lowing uin its procedure, indicating that u0is free to execute at the same time
as/ETB, whereas a call induces no such edge. When a strand ureturns to its calling
procedure and xis the strand immediately following the next sync in the calling
procedure, the computation dag contains return edge .u; x/ , which points upward.
A computation starts with a single initial strand —the black vertex in the procedure
labeled P-F IB.4/in Figure 27.2—and ends with a single ﬁnal strand —the white
vertex in the procedure labeled P-F IB.4/.
We shall study the execution of multithreaded algorithms on an ideal paral-
lel computer , which consists of a set of processors and a sequentially consistent
shared memory. Sequential consistency means that the shared memory, which mayin reality be performing many loads and stores from the processors at the sametime, produces the same results as if at each step, exactly one instruction from oneof the processors is executed. That is, the memory behaves as if the instructionswere executed sequentially according to some global linear order that preserves theindividual orders in which each processor issues its own instructions. For dynamicmultithreaded computations, which are scheduled onto processors automaticallyby the concurrency platform, the shared memory behaves as if the multithreadedcomputation’s instructions were interleaved to produce a linear order that preservesthe partial order of the computation dag. Depending on scheduling, the orderingcould differ from one run of the program to another, but the behavior of any exe-cution can be understood by assuming that the instructions are executed in somelinear order consistent with the computation dag.
In addition to making assumptions about semantics, the ideal-parallel-computer
model makes some performance assumptions. Speciﬁcally, it assumes that eachprocessor in the machine has equal computing power, and it ignores the cost ofscheduling. Although this last assumption may sound optimistic, it turns out thatfor algorithms with sufﬁcient “parallelism” (a term we shall deﬁne precisely in amoment), the overhead of scheduling is generally minimal in practice.
Performance measures
We can gauge the theoretical efﬁciency of a multithreaded algorithm by using two
metrics: “work” and “span.” The work of a multithreaded computation is the total
time to execute the entire computation on one processor. In other words, the workis the sum of the times taken by each of the strands. For a computation dag inwhich each strand takes unit time, the work is just the number of vertices in thedag. The span is the longest time to execute the strands along any path in the dag.
Again, for a dag in which each strand takes unit time, the span equals the number ofvertices on a longest or critical path in the dag. (Recall from Section 24.2 that we
can ﬁnd a critical path in a dag GD.V; E/ in‚.VCE/time.) For example, the
computation dag of Figure 27.2 has 17vertices in all and 8vertices on its critical780 Chapter 27 Multithreaded Algorithms
path, so that if each strand takes unit time, its work is 17time units and its span
is8time units.
The actual running time of a multithreaded computation depends not only on
its work and its span, but also on how many processors are available and howthe scheduler allocates strands to processors. To denote the running time of amultithreaded computation on Pprocessors, we shall subscript by P. For example,
we might denote the running time of an algorithm on Pprocessors by T
P.T h e
work is the running time on a single processor, or T1. The span is the running time
if we could run each strand on its own processor—in other words, if we had an
unlimited number of processors—and so we denote the span by T1.
The work and span provide lower bounds on the running time TPof a multi-
threaded computation on Pprocessors:
/SIIn one step, an ideal parallel computer with Pprocessors can do at most P
units of work, and thus in TPtime, it can perform at most PT Pwork. Since the
total work to do is T1,w eh a v e PT P/NAKT1. Dividing by Pyields the work law :
TP/NAKT1=P : (27.2)
/SIAP-processor ideal parallel computer cannot run any faster than a machine
with an unlimited number of processors. Looked at another way, a machine
with an unlimited number of processors can emulate a P-processor machine by
using just Pof its processors. Thus, the span law follows:
TP/NAKT1: (27.3)
We deﬁne the speedup of a computation on Pprocessors by the ratio T1=TP,
which says how many times faster the computation is on Pprocessors than
on1processor. By the work law, we have TP/NAKT1=P, which implies that
T1=TP/DC4P. Thus, the speedup on Pprocessors can be at most P. When the
speedup is linear in the number of processors, that is, when T1=TPD‚.P / ,t h e
computation exhibits linear speedup ,a n dw h e n T1=TPDP,w eh a v e perfect
linear speedup .
The ratio T1=T1of the work to the span gives the parallelism of the multi-
threaded computation. We can view the parallelism from three perspectives. As aratio, the parallelism denotes the average amount of work that can be performed inparallel for each step along the critical path. As an upper bound, the parallelismgives the maximum possible speedup that can be achieved on any number of pro-cessors. Finally, and perhaps most important, the parallelism provides a limit onthe possibility of attaining perfect linear speedup. Speciﬁcally, once the number ofprocessors exceeds the parallelism, the computation cannot possibly achieve per-fect linear speedup. To see this last point, suppose that P> T
1=T1, in which case27.1 The basics of dynamic multithreading 781
the span law implies that the speedup satisﬁes T1=TP/DC4T1=T1<P . Moreover,
if the number Pof processors in the ideal parallel computer greatly exceeds the
parallelism—that is, if P/GST1=T1—then T1=TP/FSP, so that the speedup is
much less than the number of processors. In other words, the more processors weuse beyond the parallelism, the less perfect the speedup.
As an example, consider the computation P-F
IB.4/in Figure 27.2, and assume
that each strand takes unit time. Since the work is T1D17and the span is T1D8,
the parallelism is T1=T1D17=8D2:125 . Consequently, achieving much more
than double the speedup is impossible, no matter how many processors we em-
ploy to execute the computation. For larger input sizes, however, we shall see that
P-F IB.n/exhibits substantial parallelism.
We deﬁne the (parallel) slackness of a multithreaded computation executed
on an ideal parallel computer with Pprocessors to be the ratio .T1=T1/=PD
T1=.P T 1/, which is the factor by which the parallelism of the computation ex-
ceeds the number of processors in the machine. Thus, if the slackness is less than 1,
we cannot hope to achieve perfect linear speedup, because T1=.P T 1/<1 and the
span law imply that the speedup on Pprocessors satisﬁes T1=TP/DC4T1=T1<P .
Indeed, as the slackness decreases from 1toward 0, the speedup of the computation
diverges further and further from perfect linear speedup. If the slackness is greaterthan1, however, the work per processor is the limiting constraint. As we shall see,
as the slackness increases from 1, a good scheduler can achieve closer and closer
to perfect linear speedup.
Scheduling
Good performance depends on more than just minimizing the work and span. The
strands must also be scheduled efﬁciently onto the processors of the parallel ma-
chine. Our multithreaded programming model provides no way to specify which
strands to execute on which processors. Instead, we rely on the concurrency plat-form’s scheduler to map the dynamically unfolding computation to individual pro-cessors. In practice, the scheduler maps the strands to static threads, and the op-erating system schedules the threads on the processors themselves, but this extralevel of indirection is unnecessary for our understanding of scheduling. We canjust imagine that the concurrency platform’s scheduler maps strands to processorsdirectly.
A multithreaded scheduler must schedule the computation with no advance
knowledge of when strands will be spawned or when they will complete—it mustoperate on-line . Moreover, a good scheduler operates in a distributed fashion,
where the threads implementing the scheduler cooperate to load-balance the com-putation. Provably good on-line, distributed schedulers exist, but analyzing themis complicated.782 Chapter 27 Multithreaded Algorithms
Instead, to keep our analysis simple, we shall investigate an on-line centralized
scheduler, which knows the global state of the computation at any given time. Inparticular, we shall analyze greedy schedulers , which assign as many strands to
processors as possible in each time step. If at least Pstrands are ready to execute
during a time step, we say that the step is a complete step , and a greedy scheduler
assigns any Pof the ready strands to processors. Otherwise, fewer than Pstrands
are ready to execute, in which case we say that the step is an incomplete step ,a n d
the scheduler assigns each ready strand to its own processor.
From the work law, the best running time we can hope for on Pprocessors
isT
PDT1=P, and from the span law the best we can hope for is TPDT1.
The following theorem shows that greedy scheduling is provably good in that itachieves the sum of these two lower bounds as an upper bound.
Theorem 27.1
On an ideal parallel computer with Pprocessors, a greedy scheduler executes a
multithreaded computation with work T
1and span T1in time
TP/DC4T1=PCT1: (27.4)
Proof We start by considering the complete steps. In each complete step, the
Pprocessors together perform a total of Pwork. Suppose for the purpose of
contradiction that the number of complete steps is strictly greater than bT1=Pc.
Then, the total work of the complete steps is at least
P/SOH.bT1=PcC1/DPbT1=PcCP
DT1/NUL.T1modP/CP(by equation (3.8))
>T 1 (by inequality (3.9)) .
Thus, we obtain the contradiction that the Pprocessors would perform more work
than the computation requires, which allows us to conclude that the number ofcomplete steps is at most bT
1=Pc.
Now, consider an incomplete step. Let Gbe the dag representing the entire
computation, and without loss of generality, assume that each strand takes unittime. (We can replace each longer strand by a chain of unit-time strands.) Let G
0
be the subgraph of Gthat has yet to be executed at the start of the incomplete step,
and let G00be the subgraph remaining to be executed after the incomplete step. A
longest path in a dag must necessarily start at a vertex with in-degree 0. Since an
incomplete step of a greedy scheduler executes all strands with in-degree 0inG0,
the length of a longest path in G00must be 1less than the length of a longest path
inG0. In other words, an incomplete step decreases the span of the unexecuted dag
by1. Hence, the number of incomplete steps is at most T1.
Since each step is either complete or incomplete, the theorem follows.
27.1 The basics of dynamic multithreading 783
The following corollary to Theorem 27.1 shows that a greedy scheduler always
performs well.
Corollary 27.2
The running time TPof any multithreaded computation scheduled by a greedy
scheduler on an ideal parallel computer with Pprocessors is within a factor of 2
of optimal.
Proof LetT/ETX
Pbe the running time produced by an optimal scheduler on a machine
withPprocessors, and let T1andT1be the work and span of the computation,
respectively. Since the work and span laws—inequalities (27.2) and (27.3)—giveusT
/ETX
P/NAKmax.T1=P; T 1/, Theorem 27.1 implies that
TP/DC4T1=PCT1
/DC42/SOHmax.T1=P; T 1/
/DC42T/ETX
P:
The next corollary shows that, in fact, a greedy scheduler achieves near-perfect
linear speedup on any multithreaded computation as the slackness grows.
Corollary 27.3
LetTPbe the running time of a multithreaded computation produced by a greedy
scheduler on an ideal parallel computer with Pprocessors, and let T1andT1be
the work and span of the computation, respectively. Then, if P/FST1=T1,w e
have TP/EMT1=P, or equivalently, a speedup of approximately P.
Proof If we suppose that P/FST1=T1,t h e nw ea l s oh a v e T1/FST1=P,a n d
hence Theorem 27.1 gives us TP/DC4T1=PCT1/EMT1=P. Since the work
law (27.2) dictates that TP/NAKT1=P, we conclude that TP/EMT1=P, or equiva-
lently, that the speedup is T1=TP/EMP.
The/FSsymbol denotes “much less,” but how much is “much less”? As a rule
of thumb, a slackness of at least 10—that is, 10times more parallelism than pro-
cessors—generally sufﬁces to achieve good speedup. Then, the span term in thegreedy bound, inequality (27.4), is less than 10% of the work-per-processor term,
which is good enough for most engineering situations. For example, if a computa-
tion runs on only 10 or 100 processors, it doesn’t make sense to value parallelism
of, say 1,000,000 over parallelism of 10,000, even with the factor of 100 differ-ence. As Problem 27-2 shows, sometimes by reducing extreme parallelism, wecan obtain algorithms that are better with respect to other concerns and which stillscale up well on reasonable numbers of processors.784 Chapter 27 Multithreaded Algorithms
A
(a) (b)BA
B
Work: T1.A[B/DT1.A/CT1.B/
Span: T1.A[B/DT1.A/CT1.B/Work: T1.A[B/DT1.A/CT1.B/
Span: T1.A[B/Dmax.T1.A/; T 1.B/)
Figure 27.3 The work and span of composed subcomputations. (a)When two subcomputations
are joined in series, the work of the composition is the sum of their work, and the span of the
composition is the sum of their spans. (b)When two subcomputations are joined in parallel, the
work of the composition remains the sum of their work, but the span of the composition is only themaximum of their spans.
Analyzing multithreaded algorithms
We now have all the tools we need to analyze multithreaded algorithms and provide
good bounds on their running times on various numbers of processors. Analyzingthe work is relatively straightforward, since it amounts to nothing more than ana-lyzing the running time of an ordinary serial algorithm—namely, the serializationof the multithreaded algorithm—which you should already be familiar with, sincethat is what most of this textbook is about! Analyzing the span is more interesting,but generally no harder once you get the hang of it. We shall investigate the basicideas using the P-F
IBprogram.
Analyzing the work T1.n/of P-F IB.n/poses no hurdles, because we’ve already
done it. The original F IBprocedure is essentially the serialization of P-F IB,a n d
hence T1.n/DT .n/D‚./RSn/from equation (27.1).
Figure 27.3 illustrates how to analyze the span. If two subcomputations are
joined in series, their spans add to form the span of their composition, whereas
if they are joined in parallel, the span of their composition is the maximum of the
spans of the two subcomputations. For P-F IB.n/, the spawned call to P-F IB.n/NUL1/
in line 3 runs in parallel with the call to P-F IB.n/NUL2/in line 4. Hence, we can
express the span of P-F IB.n/as the recurrence
T1.n/Dmax.T1.n/NUL1/; T 1.n/NUL2//C‚.1/
DT1.n/NUL1/C‚.1/ ;
which has solution T1.n/D‚.n/ .
The parallelism of P-F IB.n/isT1.n/=T 1.n/D‚./RSn=n/, which grows dra-
matically as ngets large. Thus, on even the largest parallel computers, a modest27.1 The basics of dynamic multithreading 785
value for nsufﬁces to achieve near perfect linear speedup for P-F IB.n/, because
this procedure exhibits considerable parallel slackness.
Parallel loops
Many algorithms contain loops all of whose iterations can operate in parallel. As
we shall see, we can parallelize such loops using the spawn andsync keywords,
but it is much more convenient to specify directly that the iterations of such loopscan run concurrently. Our pseudocode provides this functionality via the parallel
concurrency keyword, which precedes the forkeyword in a forloop statement.
As an example, consider the problem of multiplying an n/STXnmatrix AD.a
ij/
by an n-vector xD.xj/. The resulting n-vector yD.yi/is given by the equation
yiDnX
jD1aijxj;
foriD1 ;2;:::;n . We can perform matrix-vector multiplication by computing all
the entries of yin parallel as follows:
MAT-VEC.A; x/
1nDA:rows
2l e t ybe a new vector of length n
3parallel for iD1ton
4 yiD0
5parallel for iD1ton
6 forjD1ton
7 yiDyiCaijxj
8return y
In this code, the parallel for keywords in lines 3 and 5 indicate that the itera-
tions of the respective loops may be run concurrently. A compiler can implementeachparallel for loop as a divide-and-conquer subroutine using nested parallelism.
For example, the parallel for loop in lines 5–7 can be implemented with the call
M
AT-VEC-MAIN-LOOP. A ;x;y;n;1 ;n / , where the compiler produces the auxil-
iary subroutine M AT-VEC-MAIN-LOOP as follows:786 Chapter 27 Multithreaded Algorithms
1,1 2,2 3,3 4,4 5,5 6,6 7,7 8,81,2 3,4 5,6 7,81,4 5,81,8
Figure 27.4 A dag representing the computation of M AT-VEC-MAIN-LOOP. A ;x;y;8 ;1 ;8 / .T h e
two numbers within each rounded rectangle give t he values of the last two parameters ( iandi0in
the procedure header) in the invocation (spawn or call) of the procedure. The black circles repre-
sent strands corresponding to either the base case or the part of the procedure up to the spawn of
MAT-VEC-MAIN-LOOP in line 5; the shaded circles represent strands corresponding to the part of
the procedure that calls M AT-VEC-MAIN-LOOP in line 6 up to the sync in line 7, where it suspends
until the spawned subroutine in line 5 returns; and the white circles represent strands corresponding
to the (negligible) part of the procedure after the sync up to the point where it returns.
MAT-VEC-MAIN-LOOP. A ;x;y;n;i;i0/
1ifi==i0
2 forjD1ton
3 yiDyiCaijxj
4elsemidDb.iCi0/=2c
5 spawn MAT-VEC-MAIN-LOOP. A ;x;y;n;i; mid/
6M AT-VEC-MAIN-LOOP. A ;x;y;n; midC1; i0/
7 sync
This code recursively spawns the ﬁrst half of the iterations of the loop to execute
in parallel with the second half of the iterations and then executes a sync , thereby
creating a binary tree of execution where the leaves are individual loop iterations,as shown in Figure 27.4.
To calculate the work T
1.n/of M AT-VECon an n/STXnmatrix, we simply compute
the running time of its serialization, which we obtain by replacing the parallel for
loops with ordinary forloops. Thus, we have T1.n/D‚.n2/, because the qua-
dratic running time of the doubly nested loops in lines 5–7 dominates. This analysis27.1 The basics of dynamic multithreading 787
seems to ignore the overhead for recursive spawning in implementing the parallel
loops, however. In fact, the overhead of recursive spawning does increase the workof a parallel loop compared with that of its serialization, but not asymptotically.To see why, observe that since the tree of recursive procedure instances is a fullbinary tree, the number of internal nodes is 1fewer than the number of leaves (see
Exercise B.5-3). Each internal node performs constant work to divide the iterationrange, and each leaf corresponds to an iteration of the loop, which takes at leastconstant time ( ‚.n/ time in this case). Thus, we can amortize the overhead of re-
cursive spawning against the work of the iterations, contributing at most a constant
factor to the overall work.
As a practical matter, dynamic-multithreading concurrency platforms sometimes
coarsen the leaves of the recursion by executing several iterations in a single leaf,
either automatically or under programmer control, thereby reducing the overheadof recursive spawning. This reduced overhead comes at the expense of also reduc-ing the parallelism, however, but if the computation has sufﬁcient parallel slack-ness, near-perfect linear speedup need not be sacriﬁced.
We must also account for the overhead of recursive spawning when analyzing the
span of a parallel-loop construct. Since the depth of recursive calling is logarithmic
in the number of iterations, for a parallel loop with niterations in which the ith
iteration has span iter
1.i/, the span is
T1.n/D‚.lgn/Cmax
1/DC4i/DC4niter 1.i/ :
For example, for M AT-VECon an n/STXnmatrix, the parallel initialization loop in
lines 3–4 has span ‚.lgn/, because the recursive spawning dominates the constant-
time work of each iteration. The span of the doubly nested loops in lines 5–7is‚.n/ , because each iteration of the outer parallel for loop contains niterations
of the inner (serial) forloop. The span of the remaining code in the procedure
is constant, and thus the span is dominated by the doubly nested loops, yieldingan overall span of ‚.n/ for the whole procedure. Since the work is ‚.n
2/,t h e
parallelism is ‚.n2/=‚.n/D‚.n/ . (Exercise 27.1-6 asks you to provide an
implementation with even more parallelism.)
Race conditions
A multithreaded algorithm is deterministic if it always does the same thing on the
same input, no matter how the instructions are scheduled on the multicore com-puter. It is nondeterministic if its behavior might vary from run to run. Often, a
multithreaded algorithm that is intended to be deterministic fails to be, because itcontains a “determinacy race.”
Race conditions are the bane of concurrency. Famous race bugs include the
Therac-25 radiation therapy machine, which killed three people and injured sev-788 Chapter 27 Multithreaded Algorithms
eral others, and the North American Blackout of 2003, which left over 50 million
people without power. These pernicious bugs are notoriously hard to ﬁnd. You canrun tests in the lab for days without a failure only to discover that your softwaresporadically crashes in the ﬁeld.
Adeterminacy race occurs when two logically parallel instructions access the
same memory location and at least one of the instructions performs a write. Thefollowing procedure illustrates a race condition:
R
ACE-EXAMPLE ./
1xD0
2parallel for iD1to2
3 xDxC1
4 print x
After initializing xto0in line 1, R ACE-EXAMPLE creates two parallel strands,
each of which increments xin line 3. Although it might seem that R ACE-
EXAMPLE should always print the value 2(its serialization certainly does), it could
instead print the value 1. Let’s see how this anomaly might occur.
When a processor increments x, the operation is not indivisible, but is composed
of a sequence of instructions:
1. Read xfrom memory into one of the processor’s registers.
2. Increment the value in the register.
3. Write the value in the register back into xin memory.
Figure 27.5(a) illustrates a computation dag representing the execution of R ACE-
EXAMPLE , with the strands broken down to individual instructions. Recall that
since an ideal parallel computer supports sequential consistency, we can view theparallel execution of a multithreaded algorithm as an interleaving of instructionsthat respects the dependencies in the dag. Part (b) of the ﬁgure shows the valuesin an execution of the computation that elicits the anomaly. The value xis stored
in memory, and r
1andr2are processor registers. In step 1, one of the processors
setsxto0. In steps 2 and 3, processor 1 reads xfrom memory into its register r1
and increments it, producing the value 1inr1. At that point, processor 2 comes
into the picture, executing instructions 4–6. Processor 2 reads xfrom memory into
register r2; increments it, producing the value 1inr2; and then stores this value
intox, setting xto1. Now, processor 1 resumes with step 7, storing the value 1
inr1intox, which leaves the value of xunchanged. Therefore, step 8 prints the
value 1, rather than 2, as the serialization would print.
We can see what has happened. If the effect of the parallel execution were that
processor 1 executed all its instructions before processor 2, the value 2would be27.1 The basics of dynamic multithreading 789
incrr1 3r1 =x 2
x = r1 7incrr2 5r2 =  x 4
x = r2 6x = 0 1
print x 8
(a)step xr1 r2
1
2345670000011–011111–––0111
(b)
Figure 27.5 Illustration of the determinacy race in R ACE-EXAMPLE .(a)A computation dag show-
ing the dependencies among individual instructions. The processor registers are r1and r2. Instruc-
tions unrelated to the race, such as the implementation of loop control, are omitted. (b)An execution
sequence that elicits the bug, showing the values of xin memory and registers r1and r2for each
step in the execution sequence.
printed. Conversely, if the effect were that processor 2 executed all its instructions
before processor 1, the value 2would still be printed. When the instructions of the
two processors execute at the same time, however, it is possible, as in this exampleexecution, that one of the updates to xis lost.
Of course, many executions do not elicit the bug. For example, if the execution
order were h1; 2; 3; 7; 4; 5; 6; 8 iorh1; 4; 5; 6; 2; 3; 7; 8 i, we would get the cor-
rect result. That’s the problem with determinacy races. Generally, most orderings
produce correct results—such as any in which the instructions on the left executebefore the instructions on the right, or vice versa. But some orderings generateimproper results when the instructions interleave. Consequently, races can be ex-
tremely hard to test for. You can run tests for days and never see the bug, only to
experience a catastrophic system crash in the ﬁeld when the outcome is critical.
Although we can cope with races in a variety of ways, including using mutual-
exclusion locks and other methods of synchronization, for our purposes, we shallsimply ensure that strands that operate in parallel are independent :t h e y h a v e n o
determinacy races among them. Thus, in a parallel for construct, all the iterations
should be independent. Between a spawn and the corresponding sync , the code
of the spawned child should be independent of the code of the parent, including
code executed by additional spawned or called children. Note that arguments to aspawned child are evaluated in the parent before the actual spawn occurs, and thusthe evaluation of arguments to a spawned subroutine is in series with any accessesto those arguments after the spawn.790 Chapter 27 Multithreaded Algorithms
As an example of how easy it is to generate code with races, here is a faulty
implementation of multithreaded matrix-vector multiplication that achieves a spanof‚.lgn/by parallelizing the inner forloop:
M
AT-VEC-WRONG .A; x/
1nDA:rows
2l e t ybe a new vector of length n
3parallel for iD1ton
4 yiD0
5parallel for iD1ton
6 parallel for jD1ton
7 yiDyiCaijxj
8return y
This procedure is, unfortunately, incorrect due to races on updating yiin line 7,
which executes concurrently for all nvalues of j. Exercise 27.1-6 asks you to give
a correct implementation with ‚.lgn/span.
A multithreaded algorithm with races can sometimes be correct. As an exam-
ple, two parallel threads might store the same value into a shared variable, and itwouldn’t matter which stored the value ﬁrst. Generally, however, we shall considercode with races to be illegal.
A chess lesson
We close this section with a true story that occurred during the development of
the world-class multithreaded chess-playing program ?Socrates [80], although the
timings below have been simpliﬁed for exposition. The program was prototypedon a32-processor computer but was ultimately to run on a supercomputer with 512
processors. At one point, the developers incorporated an optimization into the pro-
gram that reduced its running time on an important benchmark on the 32-processor
machine from T
32D65seconds to T0
32D40seconds. Yet, the developers used
the work and span performance measures to conclude that the optimized version,which was faster on 32processors, would actually be slower than the original ver-
sion on 512processsors. As a result, they abandoned the “optimization.”
Here is their analysis. The original version of the program had work T
1D2048
seconds and span T1D1second. If we treat inequality (27.4) as an equation,
TPDT1=PCT1, and use it as an approximation to the running time on Ppro-
cessors, we see that indeed T32D2048=32C1D65. With the optimization, the
work became T0
1D1024 seconds and the span became T0
1D8seconds. Again
using our approximation, we get T0
32D1024=32C8D40.
The relative speeds of the two versions switch when we calculate the running
times on 512processors, however. In particular, we have T512D2048=512C1D527.1 The basics of dynamic multithreading 791
seconds, and T0
512D1024=512C8D10seconds. The optimization that sped up
the program on 32processors would have made the program twice as slow on 512
processors! The optimized version’s span of 8, which was not the dominant term in
the running time on 32processors, became the dominant term on 512processors,
nullifying the advantage from using more processors.
The moral of the story is that work and span can provide a better means of
extrapolating performance than can measured running times.
Exercises
27.1-1
Suppose that we spawn P-F IB.n/NUL2/in line 4 of P-F IB, rather than calling it
as is done in the code. What is the impact on the asymptotic work, span, andparallelism?
27.1-2
Draw the computation dag that results from executing P-F
IB.5/. Assuming that
each strand in the computation takes unit time, what are the work, span, and par-allelism of the computation? Show how to schedule the dag on 3processors using
greedy scheduling by labeling each strand with the time step in which it is executed.
27.1-3
Prove that a greedy scheduler achieves the following time bound, which is slightlystronger than the bound proven in Theorem 27.1:
T
P/DC4T1/NULT1
PCT1: (27.5)
27.1-4
Construct a computation dag for which one execution of a greedy scheduler cantake nearly twice the time of another execution of a greedy scheduler on the samenumber of processors. Describe how the two executions would proceed.
27.1-5
Professor Karan measures her deterministic multithreaded algorithm on 4,10,
and64processors of an ideal parallel computer using a greedy scheduler. She
claims that the three runs yielded T
4D80seconds, T10D42seconds, and
T64D10seconds. Argue that the professor is either lying or incompetent. ( Hint:
Use the work law (27.2), the span law (27.3), and inequality (27.5) from Exer-cise 27.1-3.)792 Chapter 27 Multithreaded Algorithms
27.1-6
Give a multithreaded algorithm to multiply an n/STXnmatrix by an n-vector that
achieves ‚.n2=lgn/parallelism while maintaining ‚.n2/work.
27.1-7
Consider the following multithreaded pseudocode for transposing an n/STXnmatrix A
in place:
P-T RANSPOSE .A/
1nDA:rows
2parallel for jD2ton
3 parallel for iD1toj/NUL1
4 exchange aijwithaji
Analyze the work, span, and parallelism of this algorithm.
27.1-8
Suppose that we replace the parallel for loop in line 3 of P-T RANSPOSE (see Ex-
ercise 27.1-7) with an ordinary forloop. Analyze the work, span, and parallelism
of the resulting algorithm.
27.1-9
For how many processors do the two versions of the chess programs run equally
fast, assuming that TPDT1=PCT1?
27.2 Multithreaded matrix multiplication
In this section, we examine how to multithread matrix multiplication, a problem
whose serial running time we studied in Section 4.2. We’ll look at multithreaded
algorithms based on the standard triply nested loop, as well as divide-and-conquer
algorithms.
Multithreaded matrix multiplication
The ﬁrst algorithm we study is the straighforward algorithm based on parallelizing
the loops in the procedure S QUARE -MATRIX -MULTIPLY on page 75:27.2 Multithreaded matrix multiplication 793
P-S QUARE -MATRIX -MULTIPLY .A; B/
1 nDA:rows
2l e t Cb ean e w n/STXnmatrix
3parallel for iD1ton
4 parallel for jD1ton
5 cijD0
6 for kD1ton
7 cijDcijCaik/SOHbkj
8return C
To analyze this algorithm, observe that since the serialization of the algorithm is
just S QUARE -MATRIX -MULTIPLY , the work is therefore simply T1.n/D‚.n3/,
the same as the running time of S QUARE -MATRIX -MULTIPLY . The span is
T1.n/D ‚.n/ , because it follows a path down the tree of recursion for the
parallel for loop starting in line 3, then down the tree of recursion for the parallel
forloop starting in line 4, and then executes all niterations of the ordinary forloop
starting in line 6, resulting in a total span of ‚.lgn/C‚.lgn/C‚.n/ D‚.n/ .
Thus, the parallelism is ‚.n3/=‚.n/ D‚.n2/. Exercise 27.2-3 asks you to par-
allelize the inner loop to obtain a parallelism of ‚.n3=lgn/, which you cannot do
straightforwardly using parallel for , because you would create races.
A divide-and-conquer mu ltithreaded algorithm for matrix multiplication
As we learned in Section 4.2, we can multiply n/STXnmatrices serially in time
‚.nlg7/DO.n2:81/using Strassen’s divide-and-conquer strategy, which motivates
us to look at multithreading such an algorithm. We begin, as we did in Section 4.2,
with multithreading a simpler divide-and-conquer algorithm.
Recall from page 77 that the S QUARE -MATRIX -MULTIPLY -RECURSIVE proce-
dure, which multiplies two n/STXnmatrices Aand Bto produce the n/STXnmatrix C,
relies on partitioning each of the three matrices into four n=2/STXn=2submatrices:
AD/DC2A11 A12
A21 A22/DC3
;B D/DC2B11 B12
B21 B22/DC3
;C D/DC2C11 C12
C21 C22/DC3
:
Then, we can write the matrix product as
/DC2C11 C12
C21 C22/DC3
D/DC2A11 A12
A21 A22/DC3/DC2B11 B12
B21 B22/DC3
D/DC2A11B11 A11B12
A21B11 A21B12/DC3
C/DC2A12B21 A12B22
A22B21 A22B22/DC3
: (27.6)
Thus, to multiply two n/STXnmatrices, we perform eight multiplications of n=2/STXn=2
matrices and one addition of n/STXnmatrices. The following pseudocode implements794 Chapter 27 Multithreaded Algorithms
this divide-and-conquer strategy using nested parallelism. Unlike the S QUARE -
MATRIX -MULTIPLY -RECURSIVE procedure on which it is based, P-M ATRIX -
MULTIPLY -RECURSIVE takes the output matrix as a parameter to avoid allocating
matrices unnecessarily.
P-M ATRIX -MULTIPLY -RECURSIVE . C;A ;B/
1nDA:rows
2ifn==1
3 c11Da11b11
4elseletTb ean e w n/STXnmatrix
5 partition A,B,C,a n d Tinton=2/STXn=2submatrices
A11;A12;A21;A22;B11;B12;B21;B22;C11;C12;C21;C22;
andT11;T12;T21;T22; respectively
6 spawn P-M ATRIX -MULTIPLY -RECURSIVE .C11;A11;B11/
7 spawn P-M ATRIX -MULTIPLY -RECURSIVE .C12;A11;B12/
8 spawn P-M ATRIX -MULTIPLY -RECURSIVE .C21;A21;B11/
9 spawn P-M ATRIX -MULTIPLY -RECURSIVE .C22;A21;B12/
10 spawn P-M ATRIX -MULTIPLY -RECURSIVE .T11;A12;B21/
11 spawn P-M ATRIX -MULTIPLY -RECURSIVE .T12;A12;B22/
12 spawn P-M ATRIX -MULTIPLY -RECURSIVE .T21;A22;B21/
13 P-M ATRIX -MULTIPLY -RECURSIVE .T22;A22;B22/
14 sync
15 parallel for iD1ton
16 parallel for jD1ton
17 cijDcijCtij
Line 3 handles the base case, where we are multiplying 1/STX1matrices. We handle
the recursive case in lines 4–17. We allocate a temporary matrix Tin line 4, and
line 5 partitions each of the matrices A,B,C,a n d Tinton=2/STXn=2submatrices.
(As with S QUARE -MATRIX -MULTIPLY -RECURSIVE on page 77, we gloss over
the minor issue of how to use index calculations to represent submatrix sectionsof a matrix.) The recursive call in line 6 sets the submatrix C
11to the submatrix
product A11B11,s ot h a t C11equals the ﬁrst of the two terms that form its sum in
equation (27.6). Similarly, lines 7–9 set C12,C21,a n d C22to the ﬁrst of the two
terms that equal their sums in equation (27.6). Line 10 sets the submatrix T11to
the submatrix product A12B21,s ot h a t T11equals the second of the two terms that
form C11’s sum. Lines 11–13 set T12,T21,a n d T22to the second of the two terms
that form the sums of C12,C21,a n d C22, respectively. The ﬁrst seven recursive
calls are spawned, and the last one runs in the main strand. The sync statement in
line 14 ensures that all the submatrix products in lines 6–13 have been computed,27.2 Multithreaded matrix multiplication 795
after which we add the products from TintoCin using the doubly nested parallel
forloops in lines 15–17.
We ﬁrst analyze the work M1.n/of the P-M ATRIX -MULTIPLY -RECURSIVE
procedure, echoing the serial running-time analysis of its progenitor S QUARE -
MATRIX -MULTIPLY -RECURSIVE . In the recursive case, we partition in ‚.1/ time,
perform eight recursive multiplications of n=2/STXn=2matrices, and ﬁnish up with
the‚.n2/work from adding two n/STXnmatrices. Thus, the recurrence for the
work M1.n/is
M1.n/D8M 1.n=2/C‚.n2/
D‚.n3/
by case 1 of the master theorem. In other words, the work of our multithreaded al-
gorithm is asymptotically the same as the running time of the procedure S QUARE -
MATRIX -MULTIPLY in Section 4.2, with its triply nested loops.
To determine the span M1.n/of P-M ATRIX -MULTIPLY -RECURSIVE ,w eﬁ r s t
observe that the span for partitioning is ‚.1/ , which is dominated by the ‚.lgn/
span of the doubly nested parallel for loops in lines 15–17. Because the eight
parallel recursive calls all execute on matrices of the same size, the maximum span
for any recursive call is just the span of any one. Hence, the recurrence for the
spanM1.n/of P-M ATRIX -MULTIPLY -RECURSIVE is
M1.n/DM1.n=2/C‚.lgn/ : (27.7)
This recurrence does not fall under any of the cases of the master theorem, but
it does meet the condition of Exercise 4.6-2. By Exercise 4.6-2, therefore, thesolution to recurrence (27.7) is M
1.n/D‚.lg2n/.
Now that we know the work and span of P-M ATRIX -MULTIPLY -RECURSIVE ,
we can compute its parallelism as M1.n/=M 1.n/D‚.n3=lg2n/, which is very
high.
Multithreading Strassen’s method
To multithread Strassen’s algorithm, we follow the same general outline as on
page 79, only using nested parallelism:
1. Divide the input matrices AandBand output matrix Cinton=2/STXn=2 sub-
matrices, as in equation (27.6). This step takes ‚.1/ w o r ka n ds p a nb yi n d e x
calculation.
2. Create 10matrices S1;S2;:::;S 10, each of which is n=2/STXn=2and is the sum
or difference of two matrices created in step 1. We can create all 10matrices
with‚.n2/work and ‚.lgn/span by using doubly nested parallel for loops.796 Chapter 27 Multithreaded Algorithms
3. Using the submatrices created in step 1 and the 10matrices created in
step 2, recursively spawn the computation of seven n=2/STXn=2matrix products
P1;P2;:::;P 7.
4. Compute the desired submatrices C11;C12;C21;C22of the result matrix Cby
adding and subtracting various combinations of the Pimatrices, once again
using doubly nested parallel for loops. We can compute all four submatrices
with‚.n2/work and ‚.lgn/span.
To analyze this algorithm, we ﬁrst observe that since the serialization is the
same as the original serial algorithm, the work is just the running time of theserialization, namely, ‚.n
lg7/.A s f o r P - M ATRIX -MULTIPLY -RECURSIVE ,w e
can devise a recurrence for the span. In this case, seven recursive calls exe-cute in parallel, but since they all operate on matrices of the same size, we ob-tain the same recurrence (27.7) as we did for P-M
ATRIX -MULTIPLY -RECURSIVE ,
which has solution ‚.lg2n/. Thus, the parallelism of multithreaded Strassen’s
method is ‚.nlg7=lg2n/, which is high, though slightly less than the parallelism
of P-M ATRIX -MULTIPLY -RECURSIVE .
Exercises
27.2-1
Draw the computation dag for computing P-S QUARE -MATRIX -MULTIPLY on2/STX2
matrices, labeling how the vertices in your diagram correspond to strands in the
execution of the algorithm. Use the convention that spawn and call edges point
downward, continuation edges point horizontally to the right, and return edgespoint upward. Assuming that each strand takes unit time, analyze the work, span,and parallelism of this computation.
27.2-2
Repeat Exercise 27.2-1 for P-M
ATRIX -MULTIPLY -RECURSIVE .
27.2-3
Give pseudocode for a multithreaded algorithm that multiplies two n/STXnmatrices
with work ‚.n3/but span only ‚.lgn/. Analyze your algorithm.
27.2-4
Give pseudocode for an efﬁcient multithreaded algorithm that multiplies a p/STXq
matrix by a q/STXrmatrix. Your algorithm should be highly parallel even if any of
p,q,a n d rare1. Analyze your algorithm.27.3 Multithreaded merge sort 797
27.2-5
Give pseudocode for an efﬁcient multithreaded algorithm that transposes an n/STXn
matrix in place by using divide-and-conquer to divide the matrix recursively intofourn=2/STXn=2submatrices. Analyze your algorithm.
27.2-6
Give pseudocode for an efﬁcient multithreaded implementation of the Floyd-Warshall algorithm (see Section 25.2), which computes shortest paths between allpairs of vertices in an edge-weighted graph. Analyze your algorithm.
27.3 Multithreaded merge sort
We ﬁrst saw serial merge sort in Section 2.3.1, and in Section 2.3.2 we analyzed itsrunning time and showed it to be ‚.n lgn/. Because merge sort already uses the
divide-and-conquer paradigm, it seems like a terriﬁc candidate for multithreadingusing nested parallelism. We can easily modify the pseudocode so that the ﬁrstrecursive call is spawned:
M
ERGE -SORT0. A ;p;r/
1ifp<r
2 qDb.pCr/=2c
3 spawn MERGE -SORT0. A ;p;q/
4M ERGE -SORT0.A; qC1; r/
5 sync
6M ERGE . A ;p;q;r/
Like its serial counterpart, M ERGE -SORT0sorts the subarray AŒp : : r/c141 . After the
two recursive subroutines in lines 3 and 4 have completed, which is ensured by thesync statement in line 5, M
ERGE -SORT0calls the same M ERGE procedure as on
page 31.
Let us analyze M ERGE -SORT0. To do so, we ﬁrst need to analyze M ERGE .R e -
call that its serial running time to merge nelements is ‚.n/ . Because M ERGE is
serial, both its work and its span are ‚.n/ . Thus, the following recurrence charac-
terizes the work MS0
1.n/of M ERGE -SORT0onnelements:
MS0
1.n/D2MS0
1.n=2/C‚.n/
D‚.n lgn/ ;798 Chapter 27 Multithreaded Algorithms
…… …
… …merge merge copyp1 q1 r1 p2 q2r2
p3 q3 r3AT
xx
/DC4x/DC4x< x
/NAKx/NAKx /NAKx
Figure 27.6 The idea behind the multithreaded merging of two sorted subarrays TŒ p 1::r1/c141
andTŒ p 2::r2/c141into the subarray AŒp3::r3/c141. Letting xDTŒ q1/c141be the median of TŒ p 1::r1/c141andq2
be the place in TŒ p 2::r2/c141such that xwould fall between TŒ q2/NUL1/c141andTŒ q2/c141, every element in
subarrays TŒ p 1::q1/NUL1/c141andTŒ p 2::q2/NUL1/c141(lightly shaded) is less than or equal to x,a n de v e r y
element in the subarrays TŒ q1C1::r 1/c141andTŒ q2C1::r 2/c141(heavily shaded) is at least x.T om e r g e ,
we compute the index q3where xbelongs in AŒp3::r3/c141,c o p y xintoAŒq3/c141, and then recursively
merge TŒ p 1::q1/NUL1/c141withTŒ p 2::q2/NUL1/c141intoAŒp3::q3/NUL1/c141andTŒ q1C1::r 1/c141withTŒ q2::r2/c141
intoAŒq3C1::r 3/c141.
which is the same as the serial running time of merge sort. Since the two recursive
calls of M ERGE -SORT0can run in parallel, the span MS0
1is given by the recurrence
MS0
1.n/DMS0
1.n=2/C‚.n/
D‚.n/ :
Thus, the parallelism of M ERGE -SORT0comes to MS0
1.n/=MS0
1.n/D‚.lgn/,
which is an unimpressive amount of parallelism. To sort 10 million elements, forexample, it might achieve linear speedup on a few processors, but it would notscale up effectively to hundreds of processors.
You probably have already ﬁgured out where the parallelism bottleneck is in
this multithreaded merge sort: the serial M
ERGE procedure. Although merging
might initially seem to be inherently serial, we can, in fact, fashion a multithreaded
version of it by using nested parallelism.
Our divide-and-conquer strategy for multithreaded merging, which is illus-
trated in Figure 27.6, operates on subarrays of an array T. Suppose that we
are merging the two sorted subarrays TŒ p 1::r1/c141of length n1Dr1/NULp1C1
andTŒ p 2::r2/c141of length n2Dr2/NULp2C1into another subarray AŒp 3::r3/c141,o f
length n3Dr3/NULp3C1Dn1Cn2. Without loss of generality, we make the sim-
plifying assumption that n1/NAKn2.
We ﬁrst ﬁnd the middle element xDTŒ q 1/c141of the subarray TŒ p 1::r1/c141,
where q1Db.p1Cr1/=2c. Because the subarray is sorted, xi sam e d i a n
ofTŒ p 1::r1/c141: every element in TŒ p 1::q 1/NUL1/c141is no more than x,a n de v e r ye l -
ement in TŒ q 1C1::r 1/c141is no less than x. We then use binary search to ﬁnd the27.3 Multithreaded merge sort 799
index q2in the subarray TŒ p 2::r2/c141so that the subarray would still be sorted if we
inserted xbetween TŒ q 2/NUL1/c141andTŒ q 2/c141.
We next merge the original subarrays TŒ p 1::r1/c141andTŒ p 2::r2/c141intoAŒp 3::r3/c141
as follows:
1. Set q3Dp3C.q1/NULp1/C.q2/NULp2/.
2. Copy xintoAŒq 3/c141.
3. Recursively merge TŒ p 1::q 1/NUL1/c141withTŒ p 2::q 2/NUL1/c141, and place the result into
the subarray AŒp 3::q 3/NUL1/c141.
4. Recursively merge TŒ q 1C1::r 1/c141withTŒ q 2::r2/c141, and place the result into the
subarray AŒq 3C1::r 3/c141.
When we compute q3, the quantity q1/NULp1is the number of elements in the subarray
TŒ p 1::q 1/NUL1/c141, and the quantity q2/NULp2is the number of elements in the subarray
TŒ p 2::q 2/NUL1/c141. Thus, their sum is the number of elements that end up before xin
the subarray AŒp 3::r3/c141.
The base case occurs when n1Dn2D0, in which case we have no work
to do to merge the two empty subarrays. Since we have assumed that the sub-array TŒ p
1::r1/c141is at least as long as TŒ p 2::r2/c141,t h a ti s , n1/NAKn2, we can check
for the base case by just checking whether n1D0. We must also ensure that the
recursion properly handles the case when only one of the two subarrays is empty,
which, by our assumption that n1/NAKn2, must be the subarray TŒ p 2::r2/c141.
Now, let’s put these ideas into pseudocode. We start with the binary search,
which we express serially. The procedure B INARY -SEARCH . x;T;p;r/ takes a
keyxand a subarray TŒ p::r/c141 , and it returns one of the following:
/SIIfTŒ p::r/c141 is empty ( r<p ), then it returns the index p.
/SIIfx/DC4TŒ p/c141 , and hence less than or equal to all the elements of TŒ p::r/c141 ,t h e n
it returns the index p.
/SIIfx>TŒ p /c141 , then it returns the largest index qin the range p<q/DC4rC1such
thatTŒ q/NUL1/c141 < x .
Here is the pseudocode:
BINARY -SEARCH . x;T;p;r/
1lowDp
2highDmax.p; rC1/
3while low<high
4 midDb.lowChigh/=2c
5 ifx/DC4TŒmid/c141
6 highDmid
7 elselowDmidC1
8return high800 Chapter 27 Multithreaded Algorithms
The call B INARY -SEARCH . x;T;p;r/ takes ‚.lgn/serial time in the worst case,
where nDr/NULpC1is the size of the subarray on which it runs. (See Exer-
cise 2.3-5.) Since B INARY -SEARCH is a serial procedure, its worst-case work and
span are both ‚.lgn/.
We are now prepared to write pseudocode for the multithreaded merging pro-
cedure itself. Like the M ERGE procedure on page 31, the P-M ERGE procedure
assumes that the two subarrays to be merged lie within the same array. Un-like M
ERGE ,h o w e v e r ,P - M ERGE does not assume that the two subarrays to
be merged are adjacent within the array. (That is, P-M ERGE does not require
thatp2Dr1C1.) Another difference between M ERGE and P-M ERGE is that
P-M ERGE takes as an argument an output subarray Ainto which the merged val-
ues should be stored. The call P-M ERGE .T; p 1;r1;p2;r2;A ;p 3/merges the sorted
subarrays TŒ p 1::r1/c141andTŒ p 2::r2/c141into the subarray AŒp 3::r3/c141,w h e r e r3D
p3C.r1/NULp1C1/C.r2/NULp2C1//NUL1Dp3C.r1/NULp1/C.r2/NULp2/C1and
is not provided as an input.
P-M ERGE .T; p 1;r1;p2;r2;A ;p 3/
1n1Dr1/NULp1C1
2n2Dr2/NULp2C1
3ifn1<n 2 //ensure that n1/NAKn2
4 exchange p1withp2
5 exchange r1withr2
6 exchange n1withn2
7ifn1==0 //both empty?
8 return
9elseq1Db.p1Cr1/=2c
10 q2DBINARY -SEARCH .T Œq 1/c141; T; p 2;r2/
11 q3Dp3C.q1/NULp1/C.q2/NULp2/
12 AŒq 3/c141DTŒ q 1/c141
13 spawn P-M ERGE .T; p 1;q1/NUL1; p 2;q2/NUL1; A; p 3/
14 P-M ERGE .T; q 1C1; r1;q2;r2;A ;q 3C1/
15 sync
The P-M ERGE procedure works as follows. Lines 1–2 compute the lengths n1
andn2of the subarrays TŒ p 1::r1/c141andTŒ p 2::r2/c141, respectively. Lines 3–6 en-
force the assumption that n1/NAKn2. Line 7 tests for the base case, where the
subarray TŒ p 1::r1/c141is empty (and hence so is TŒ p 2::r2/c141), in which case we sim-
ply return. Lines 9–15 implement the divide-and-conquer strategy. Line 9 com-putes the midpoint of TŒ p
1::r1/c141, and line 10 ﬁnds the point q2inTŒ p 2::r2/c141such
that all elements in TŒ p 2::q 2/NUL1/c141are less than TŒ q 1/c141(which corresponds to x)
and all the elements in TŒ q 2::p 2/c141are at least as large as TŒ q 1/c141.L i n e 1 1 c o m -27.3 Multithreaded merge sort 801
putes the index q3of the element that divides the output subarray AŒp 3::r3/c141into
AŒp 3::q 3/NUL1/c141andAŒq 3C1::r 3/c141, and then line 12 copies TŒ q 1/c141directly into AŒq 3/c141.
Then, we recurse using nested parallelism. Line 13 spawns the ﬁrst subproblem,
while line 14 calls the second subproblem in parallel. The sync statement in line 15
ensures that the subproblems have completed before the procedure returns. (Sinceevery procedure implicitly executes a sync before returning, we could have omitted
thesync statement in line 15, but including it is good coding practice.) There
is some cleverness in the coding to ensure that when the subarray TŒ p
2::r2/c141is
empty, the code operates correctly. The way it works is that on each recursive call,
a median element of TŒ p 1::r1/c141is placed into the output subarray, until TŒ p 1::r1/c141
itself ﬁnally becomes empty, triggering the base case.
Analysis of multithreaded merging
We ﬁrst derive a recurrence for the span PM 1.n/of P-M ERGE , where the two
subarrays contain a total of nDn1Cn2elements. Because the spawn in line 13 and
the call in line 14 operate logically in parallel, we need examine only the costlier ofthe two calls. The key is to understand that in the worst case, the maximum numberof elements in either of the recursive calls can be at most 3n=4 , which we see as
follows. Because lines 3–6 ensure that n
2/DC4n1, it follows that n2D2n2=2/DC4
.n1Cn2/=2Dn=2. In the worst case, one of the two recursive calls merges
bn1=2celements of TŒ p 1::r1/c141with all n2elements of TŒ p 2::r2/c141, and hence the
number of elements involved in the call is
bn1=2cCn2/DC4n1=2Cn2=2Cn2=2
D.n1Cn2/=2Cn2=2
/DC4n=2Cn=4
D3n=4 :
Adding in the ‚.lgn/cost of the call to B INARY -SEARCH in line 10, we obtain
the following recurrence for the worst-case span:
PM 1.n/DPM 1.3n=4/C‚.lgn/ : (27.8)
(For the base case, the span is ‚.1/ , since lines 1–8 execute in constant time.)
This recurrence does not fall under any of the cases of the master theorem, but itmeets the condition of Exercise 4.6-2. Therefore, the solution to recurrence (27.8)isPM
1.n/D‚.lg2n/.
We now analyze the work PM 1.n/of P-M ERGE onnelements, which turns out
to be ‚.n/ . Since each of the nelements must be copied from array Tto array A,
we have PM 1.n/D/DEL.n/ . Thus, it remains only to show that PM 1.n/DO.n/ .
We shall ﬁrst derive a recurrence for the worst-case work. The binary search in
line 10 costs ‚.lgn/in the worst case, which dominates the other work outside802 Chapter 27 Multithreaded Algorithms
of the recursive calls. For the recursive calls, observe that although the recursive
calls in lines 13 and 14 might merge different numbers of elements, together thetwo recursive calls merge at most nelements (actually n/NUL1elements, since TŒ q
1/c141
does not participate in either recursive call). Moreover, as we saw in analyzing thespan, a recursive call operates on at most 3n=4 elements. We therefore obtain the
recurrence
PM
1.n/DPM 1.˛n/CPM 1..1/NUL˛/n/CO.lgn/ ; (27.9)
where ˛lies in the range 1=4/DC4˛/DC43=4, and where we understand that the actual
value of ˛may vary for each level of recursion.
We prove that recurrence (27.9) has solution PM 1DO.n/ via the substitution
method. Assume that PM 1.n//DC4c1n/NULc2lgnfor some positive constants c1andc2.
Substituting gives us
PM 1.n//DC4.c1˛n/NULc2lg.˛n//C.c1.1/NUL˛/n/NULc2lg..1/NUL˛/n//C‚.lgn/
Dc1.˛C.1/NUL˛//n/NULc2.lg.˛n/Clg..1/NUL˛/n//C‚.lgn/
Dc1n/NULc2.lg˛ClgnClg.1/NUL˛/Clgn/C‚.lgn/
Dc1n/NULc2lgn/NUL.c2.lgnClg.˛.1/NUL˛////NUL‚.lgn//
/DC4c1n/NULc2lgn;
since we can choose c2large enough that c2.lgnClg.˛.1/NUL˛/// dominates the
‚.lgn/term. Furthermore, we can choose c1large enough to satisfy the base
conditions of the recurrence. Since the work PM 1.n/of P-M ERGE is both /DEL.n/
andO.n/ ,w eh a v e PM 1.n/D‚.n/ .
The parallelism of P-M ERGE isPM 1.n/=PM 1.n/D‚.n= lg2n/.
Multithreaded merge sort
Now that we have a nicely parallelized multithreaded merging procedure, we can
incorporate it into a multithreaded merge sort. This version of merge sort is similarto the M
ERGE -SORT0procedure we saw earlier, but unlike M ERGE -SORT0, it takes
as an argument an output subarray B, which will hold the sorted result. In par-
ticular, the call P-M ERGE -SORT. A ;p;r ;B;s/ sorts the elements in AŒp : : r/c141 and
stores them in BŒs::sCr/NULp/c141.27.3 Multithreaded merge sort 803
P-M ERGE -SORT. A ;p;r ;B;s/
1nDr/NULpC1
2ifn==1
3 BŒs/c141DAŒp/c141
4elseletTŒ 1::n /c141 b ean e wa r r a y
5 qDb.pCr/=2c
6 q0Dq/NULpC1
7 spawn P-M ERGE -SORT. A ;p;q;T;1 /
8P - M ERGE -SORT.A; qC1; r; T; q0C1/
9 sync
10 P-M ERGE . T;1 ;q0;q0C1; n; B; s/
After line 1 computes the number nof elements in the input subarray AŒp : : r/c141 ,
lines 2–3 handle the base case when the array has only 1element. Lines 4–6 set
up for the recursive spawn in line 7 and call in line 8, which operate in parallel. In
particular, line 4 allocates a temporary array Twithnelements to store the results
of the recursive merge sorting. Line 5 calculates the index qofAŒp : : r/c141 to divide
the elements into the two subarrays AŒp : : q/c141 andAŒqC1::r/c141 that will be sorted
recursively, and line 6 goes on to compute the number q0of elements in the ﬁrst
subarray AŒp : : q/c141 , which line 8 uses to determine the starting index in Tof where
to store the sorted result of AŒqC1::r/c141 . At that point, the spawn and recursive
call are made, followed by the sync in line 9, which forces the procedure to wait
until the spawned procedure is done. Finally, line 10 calls P-M ERGE to merge
the sorted subarrays, now in TŒ 1::q0/c141andTŒ q0C1::n /c141 , into the output subarray
BŒ s::sCr/NULp/c141.
Analysis of multithreaded merge sort
We start by analyzing the work PMS 1.n/of P-M ERGE -SORT, which is consider-
ably easier than analyzing the work of P-M ERGE . Indeed, the work is given by the
recurrence
PMS 1.n/D2PMS 1.n=2/CPM 1.n/
D2PMS 1.n=2/C‚.n/ :
This recurrence is the same as the recurrence (4.4) for ordinary M ERGE -SORT
from Section 2.3.1 and has solution PMS 1.n/D‚.n lgn/by case 2 of the master
theorem.
We now derive and analyze a recurrence for the worst-case span PMS 1.n/.B e -
cause the two recursive calls to P-M ERGE -SORT on lines 7 and 8 operate logically
in parallel, we can ignore one of them, obtaining the recurrence804 Chapter 27 Multithreaded Algorithms
PMS 1.n/DPMS 1.n=2/CPM 1.n/
DPMS 1.n=2/C‚.lg2n/ : (27.10)
As for recurrence (27.8), the master theorem does not apply to recurrence (27.10),
but Exercise 4.6-2 does. The solution is PMS 1.n/D‚.lg3n/, and so the span of
P-M ERGE -SORT is‚.lg3n/.
Parallel merging gives P-M ERGE -SORT a signiﬁcant parallelism advantage over
MERGE -SORT0. Recall that the parallelism of M ERGE -SORT0, which calls the se-
rial M ERGE procedure, is only ‚.lgn/.F o rP - M ERGE -SORT, the parallelism is
PMS 1.n/=PMS 1.n/D‚.n lgn/=‚. lg3n/
D‚.n= lg2n/ ;
which is much better both in theory and in practice. A good implementation in
practice would sacriﬁce some parallelism by coarsening the base case in order to
reduce the constants hidden by the asymptotic notation. The straightforward way
to coarsen the base case is to switch to an ordinary serial sort, perhaps quicksort,
when the size of the array is sufﬁciently small.
Exercises
27.3-1
Explain how to coarsen the base case of P-M ERGE .
27.3-2
Instead of ﬁnding a median element in the larger subarray, as P-M ERGE does, con-
sider a variant that ﬁnds a median element of all the elements in the two sortedsubarrays using the result of Exercise 9.3-8. Give pseudocode for an efﬁcientmultithreaded merging procedure that uses this median-ﬁnding procedure. Ana-lyze your algorithm.
27.3-3
Give an efﬁcient multithreaded algorithm for partitioning an array around a pivot,as is done by the P
ARTITION procedure on page 171. You need not partition the ar-
ray in place. Make your algorithm as parallel as possible. Analyze your algorithm.(Hint: You may need an auxiliary array and may need to make more than one pass
over the input elements.)
27.3-4
Give a multithreaded version of R
ECURSIVE -FFT on page 911. Make your imple-
mentation as parallel as possible. Analyze your algorithm.Problems for Chapter 27 805
27.3-5 ?
Give a multithreaded version of R ANDOMIZED -SELECT on page 216. Make your
implementation as parallel as possible. Analyze your algorithm. ( Hint: Use the
partitioning algorithm from Exercise 27.3-3.)
27.3-6 ?
Show how to multithread S ELECT from Section 9.3. Make your implementation as
parallel as possible. Analyze your algorithm.
Problems
27-1 Implementing parallel loops using nested parallelismConsider the following multithreaded algorithm for performing pairwise additiononn-element arrays AŒ1 : : n/c141 andBŒ 1::n /c141 , storing the sums in CŒ 1::n /c141 :
S
UM-ARRAYS . A ;B;C/
1parallel for iD1toA:length
2 CŒ i/c141DAŒi/c141CBŒi/c141
a.Rewrite the parallel loop in S UM-ARRAYS using nested parallelism ( spawn
andsync ) in the manner of M AT-VEC-MAIN-LOOP. Analyze the parallelism
of your implementation.
Consider the following alternative implementation of the parallel loop, which
contains a value grain -sizeto be speciﬁed:
SUM-ARRAYS0. A ;B;C/
1nDA:length
2grain -sizeD‹ //to be determined
3rDdn=grain -sizee
4forkD0tor/NUL1
5 spawn ADD-SUBARRAY . A ;B;C;k/SOHgrain -sizeC1;
min..kC1//SOHgrain -size; n//
6sync
ADD-SUBARRAY . A ;B;C;i;j/
1forkDitoj
2 CŒ k/c141DAŒk/c141CBŒk/c141806 Chapter 27 Multithreaded Algorithms
b.Suppose that we set grain -sizeD1. What is the parallelism of this implemen-
tation?
c.Give a formula for the span of S UM-ARRAYS0in terms of nandgrain -size.
Derive the best value for grain -sizeto maximize parallelism.
27-2 Saving temporary space in matrix multiplication
The P-M ATRIX -MULTIPLY -RECURSIVE procedure has the disadvantage that it
must allocate a temporary matrix Tof size n/STXn, which can adversely affect the
constants hidden by the ‚-notation. The P-M ATRIX -MULTIPLY -RECURSIVE pro-
cedure does have high parallelism, however. For example, ignoring the constantsin the ‚-notation, the parallelism for multiplying 1000/STX1000 matrices comes to
approximately 1000
3=102D107, since lg 1000/EM10. Most parallel computers
have far fewer than 10million processors.
a.Describe a recursive multithreaded algorithm that eliminates the need for the
temporary matrix Tat the cost of increasing the span to ‚.n/ .(Hint: Com-
puteCDCCABfollowing the general strategy of P-M ATRIX -MULTIPLY -
RECURSIVE , but initialize Cin parallel and insert a sync in a judiciously cho-
sen location.)
b.Give and solve recurrences for the work and span of your implementation.
c.Analyze the parallelism of your implementation. Ignoring the constants in the
‚-notation, estimate the parallelism on 1000/STX1000 matrices. Compare with
the parallelism of P-M ATRIX -MULTIPLY -RECURSIVE .
27-3 Multithreaded matrix algorithms
a.Parallelize the LU-D ECOMPOSITION procedure on page 821 by giving pseu-
docode for a multithreaded version of this algorithm. Make your implementa-tion as parallel as possible, and analyze its work, span, and parallelism.
b.Do the same for LUP-D
ECOMPOSITION on page 824.
c.Do the same for LUP-S OLVE on page 817.
d.Do the same for a multithreaded algorithm based on equation (28.13) for in-
verting a symmetric positive-deﬁnite matrix.Problems for Chapter 27 807
27-4 Multithreading reductions and preﬁx computations
A˝-reduction of an array xŒ1::n/c141 ,w h e r e˝is an associative operator, is the value
yDxŒ1/c141˝xŒ2/c141˝/SOH/SOH/SOH˝ xŒn/c141 :
The following procedure computes the ˝-reduction of a subarray xŒ i::j/c141 serially.
REDUCE . x;i;j/
1yDxŒi/c141
2forkDiC1toj
3 yDy˝xŒk/c141
4return y
a.Use nested parallelism to implement a multithreaded algorithm P-R EDUCE ,
which performs the same function with ‚.n/ work and ‚.lgn/span. Analyze
your algorithm.
A related problem is that of computing a ˝-preﬁx computation , sometimes
called a ˝-scan , on an array xŒ1::n/c141 ,w h e r e˝is once again an associative op-
erator. The˝-scan produces the array yŒ 1::n /c141 given by
yŒ1/c141DxŒ1/c141 ;
yŒ2/c141DxŒ1/c141˝xŒ2/c141 ;
yŒ3/c141DxŒ1/c141˝xŒ2/c141˝xŒ3/c141 ;
:::
yŒn/c141DxŒ1/c141˝xŒ2/c141˝xŒ3/c141˝/SOH/SOH/SOH˝ xŒn/c141 ;
that is, all preﬁxes of the array x“summed” using the ˝operator. The following
serial procedure SCAN performs a˝-preﬁx computation:
SCAN.x/
1nDx:length
2l e t yŒ 1::n /c141 be a new array
3yŒ1/c141DxŒ1/c141
4foriD2ton
5 yŒi/c141DyŒi/NUL1/c141˝xŒi/c141
6return y
Unfortunately, multithreading S CAN is not straightforward. For example, changing
theforloop to a parallel for loop would create races, since each iteration of the
loop body depends on the previous iteration. The following procedure P-S CAN-1
performs the˝-preﬁx computation in parallel, albeit inefﬁciently:808 Chapter 27 Multithreaded Algorithms
P-S CAN-1.x/
1nDx:length
2l e t yŒ 1::n /c141 b ean e wa r r a y
3P - S CAN-1-A UX. x;y;1 ;n /
4return y
P-S CAN-1-A UX. x;y;i;j/
1parallel for lDitoj
2 yŒl/c141DP-R EDUCE . x;1 ;l/
b.Analyze the work, span, and parallelism of P-S CAN-1.
By using nested parallelism, we can obtain a more efﬁcient ˝-preﬁx computa-
tion:
P-S CAN-2.x/
1nDx:length
2l e t yŒ 1::n /c141 b ean e wa r r a y
3P - S CAN-2-A UX. x;y;1 ;n /
4return y
P-S CAN-2-A UX. x;y;i;j/
1ifi==j
2 yŒi/c141DxŒi/c141
3elsekDb.iCj/ =2c
4 spawn P-S CAN-2-A UX. x;y;i;k/
5P - S CAN-2-A UX. x;y;kC1; j /
6 sync
7 parallel for lDkC1toj
8 yŒl/c141DyŒk/c141˝yŒl/c141
c.Argue that P-S CAN-2 is correct, and analyze its work, span, and parallelism.
We can improve on both P-S CAN-1 and P-S CAN-2 by performing the ˝-preﬁx
computation in two distinct passes over the data. On the ﬁrst pass, we gather the
terms for various contiguous subarrays of xinto a temporary array t, and on the
second pass we use the terms in tto compute the ﬁnal result y. The following
pseudocode implements this strategy, but certain expressions have been omitted:Problems for Chapter 27 809
P-S CAN-3.x/
1nDx:length
2l e t yŒ 1::n /c141 andtŒ1::n/c141 be new arrays
3yŒ1/c141DxŒ1/c141
4ifn>1
5P - S CAN-UP. x;t;2;n /
6P - S CAN-DOWN.xŒ1/c141; x; t; y; 2; n/
7return y
P-S CAN-UP. x;t;i;j/
1ifi==j
2 return xŒi/c141
3else
4 kDb.iCj/ =2c
5 tŒk/c141Dspawn P-S CAN-UP. x;t;i;k/
6 rightDP-S CAN-UP. x;t;kC1; j /
7 sync
8 return
 //ﬁll in the blank
P-S CAN-DOWN. /ETB;x;t;y;i;j/
1ifi==j
2 yŒi/c141D/ETB˝xŒi/c141
3else
4 kDb.iCj/ =2c
5 spawn P-S CAN-DOWN.
 ;x;t;y;i;k/ //ﬁll in the blank
6P - S CAN-DOWN.
 ;x;t;y;kC1; j / //ﬁll in the blank
7 sync
d.Fill in the three missing expressions in line 8 of P-S CAN-UPand lines 5 and 6
of P-S CAN-DOWN . Argue that with expressions you supplied, P-S CAN-3 is
correct. ( Hint: Prove that the value /ETBpassed to P-S CAN-DOWN. /ETB;x;t;y;i;j/
satisﬁes /ETBDxŒ1/c141˝xŒ2/c141˝/SOH/SOH/SOH˝ xŒi/NUL1/c141.)
e.Analyze the work, span, and parallelism of P-S CAN-3.
27-5 Multithreading a simple stencil calculation
Computational science is replete with algorithms that require the entries of an arrayto be ﬁlled in with values that depend on the values of certain already computedneighboring entries, along with other information that does not change over thecourse of the computation. The pattern of neighboring entries does not changeduring the computation and is called a stencil . For example, Section 15.4 presents810 Chapter 27 Multithreaded Algorithms
a stencil algorithm to compute a longest common subsequence, where the value in
entry cŒi;j/c141 depends only on the values in cŒi/NUL1; j /c141,cŒi;j/NUL1/c141,a n d cŒi/NUL1; j/NUL1/c141,
as well as the elements xiandyjwithin the two sequences given as inputs. The
input sequences are ﬁxed, but the algorithm ﬁlls in the two-dimensional array cso
that it computes entry cŒi;j/c141 after computing all three entries cŒi/NUL1; j /c141,cŒi;j/NUL1/c141,
andcŒi/NUL1; j/NUL1/c141.
In this problem, we examine how to use nested parallelism to multithread a
simple stencil calculation on an n/STXnarray Ain which, of the values in A,t h e
value placed into entry AŒi; j /c141 depends only on values in AŒi0;j0/c141,w h e r e i0/DC4i
andj0/DC4j(and of course, i0¤iorj0¤j). In other words, the value in an
entry depends only on values in entries that are above it and/or to its left, alongwith static information outside of the array. Furthermore, we assume throughoutthis problem that once we have ﬁlled in the entries upon which AŒi; j /c141 depends, we
can ﬁll in AŒi; j /c141 in‚.1/ time (as in the LCS-L
ENGTH procedure of Section 15.4).
We can partition the n/STXnarray Ainto four n=2/STXn=2subarrays as follows:
AD/DC2A11A12
A21A22/DC3
: (27.11)
Observe now that we can ﬁll in subarray A11recursively, since it does not depend
on the entries of the other three subarrays. Once A11is complete, we can continue
to ﬁll in A12andA21recursively in parallel, because although they both depend
onA11, they do not depend on each other. Finally, we can ﬁll in A22recursively.
a.Give multithreaded pseudocode that performs this simple stencil calculation
using a divide-and-conquer algorithm S IMPLE -STENCIL based on the decom-
position (27.11) and the discussion above. (Don’t worry about the details of the
base case, which depends on the speciﬁc stencil.) Give and solve recurrencesfor the work and span of this algorithm in terms of n. What is the parallelism?
b.Modify your solution to part (a) to divide an n/STXnarray into nine n=3/STXn=3
subarrays, again recursing with as much parallelism as possible. Analyze this
algorithm. How much more or less parallelism does this algorithm have com-
pared with the algorithm from part (a)?
c.Generalize your solutions to parts (a) and (b) as follows. Choose an integer
b/NAK2.D i v i d ea n n/STXnarray into b
2subarrays, each of size n=b/STXn=b, recursing
with as much parallelism as possible. In terms of nandb, what are the work,
span, and parallelism of your algorithm? Argue that, using this approach, theparallelism must be o.n/ for any choice of b/NAK2.(Hint: For this last argument,
show that the exponent of nin the parallelism is strictly less than 1for any
choice of b/NAK2.)Notes for Chapter 27 811
d.Give pseudocode for a multithreaded algorithm for this simple stencil calcu-
lation that achieves ‚.n= lgn/parallelism. Argue using notions of work and
span that the problem, in fact, has ‚.n/ inherent parallelism. As it turns out,
the divide-and-conquer nature of our multithreaded pseudocode does not let usachieve this maximal parallelism.
27-6 Randomized multithreaded algorithms
Just as with ordinary serial algorithms, we sometimes want to implement random-
ized multithreaded algorithms. This problem explores how to adapt the variousperformance measures in order to handle the expected behavior of such algorithms.It also asks you to design and analyze a multithreaded algorithm for randomizedquicksort.
a.Explain how to modify the work law (27.2), span law (27.3), and greedy sched-
uler bound (27.4) to work with expectations when T
P,T1,a n d T1are all ran-
dom variables.
b.Consider a randomized multithreaded algorithm for which 1% of the time we
have T1D104andT10;000D1,b u tf o r 99%o ft h et i m ew eh a v e T1D
T10;000D109. Argue that the speedup of a randomized multithreaded algo-
rithm should be deﬁned as E ŒT1/c141=EŒTP/c141, rather than E ŒT1=TP/c141.
c.Argue that the parallelism of a randomized multithreaded algorithm should be
deﬁned as the ratio E ŒT1/c141=EŒT1/c141.
d.Multithread the R ANDOMIZED -QUICKSORT algorithm on page 179 by using
nested parallelism. (Do not parallelize R ANDOMIZED -PARTITION .) Give the
pseudocode for your P-R ANDOMIZED -QUICKSORT algorithm.
e.Analyze your multithreaded algorithm for randomized quicksort. ( Hint: Re-
view the analysis of R ANDOMIZED -SELECT on page 216.)
Chapter notes
Parallel computers, models for parallel computers, and algorithmic models for par-
allel programming have been around in various forms for years. Prior editions ofthis book included material on sorting networks and the PRAM (Parallel Random-Access Machine) model. The data-parallel model [48, 168] is another popular al-gorithmic programming model, which features operations on vectors and matricesas primitives.812 Chapter 27 Multithreaded Algorithms
Graham [149] and Brent [55] showed that there exist schedulers achieving the
bound of Theorem 27.1. Eager, Zahorjan, and Lazowska [98] showed that anygreedy scheduler achieves this bound and proposed the methodology of using workand span (although not by those names) to analyze parallel algorithms. Blelloch[47] developed an algorithmic programming model based on work and span (whichhe called the “depth” of the computation) for data-parallel programming. Blumofeand Leiserson [52] gave a distributed scheduling algorithm for dynamic multi-threading based on randomized “work-stealing” and showed that it achieves the
bound E ŒT
P/c141/DC4T1=PCO.T 1/. Arora, Blumofe, and Plaxton [19] and Blelloch,
Gibbons, and Matias [49] also provided provably good algorithms for schedulingdynamic multithreaded computations.
The multithreaded pseudocode and programming model were heavily inﬂuenced
by the Cilk [51, 118] project at MIT and the Cilk++ [71] extensions to C++ dis-tributed by Cilk Arts, Inc. Many of the multithreaded algorithms in this chapterappeared in unpublished lecture notes by C. E. Leiserson and H. Prokop and havebeen implemented in Cilk or Cilk++. The multithreaded merge-sorting algorithmwas inspired by an algorithm of Akl [12].
The notion of sequential consistency is due to Lamport [223].28 Matrix Operations
Because operations on matrices lie at the heart of scientiﬁc computing, efﬁcient al-
gorithms for working with matrices have many practical applications. This chapterfocuses on how to multiply matrices and solve sets of simultaneous linear equa-tions. Appendix D reviews the basics of matrices.
Section 28.1 shows how to solve a set of linear equations using LUP decomposi-
tions. Then, Section 28.2 explores the close relationship between multiplying andinverting matrices. Finally, Section 28.3 discusses the important class of symmetricpositive-deﬁnite matrices and shows how we can use them to ﬁnd a least-squaressolution to an overdetermined set of linear equations.
One important issue that arises in practice is numerical stability . Due to the
limited precision of ﬂoating-point representations in actual computers, round-off
errors in numerical computations may become ampliﬁed over the course of a com-putation, leading to incorrect results; we call such computations numerically un-
stable . Although we shall brieﬂy consider numerical stability on occasion, we do
not focus on it in this chapter. We refer you to the excellent book by Golub and
Van Loan [144] for a thorough discussion of stability issues.
28.1 Solving systems of linear equations
Numerous applications need to solve sets of simultaneous linear equations. Wecan formulate a linear system as a matrix equation in which each matrix or vectorelement belongs to a ﬁeld, typically the real numbers R. This section discusses how
to solve a system of linear equations using a method called LUP decomposition.
We start with a set of linear equations in nunknowns x
1;x2;:::;x n:814 Chapter 28 Matrix Operations
a11x1Ca12x2C/SOH/SOH/SOHC a1nxnDb1;
a21x1Ca22x2C/SOH/SOH/SOHC a2nxnDb2;
:::
an1x1Can2x2C/SOH/SOH/SOHC annxnDbn:(28.1)
Asolution to the equations (28.1) is a set of values for x1;x2;:::;x nthat satisfy
all of the equations simultaneously. In this section, we treat only the case in whichthere are exactly nequations in nunknowns.
We can conveniently rewrite equations (28.1) as the matrix-vector equation
˙
a11a12/SOH/SOH/SOHa1n
a21a22/SOH/SOH/SOHa2n
::::::::::::
a
n1an2/SOH/SOH/SOHann/BEL˙
x1
x2
:::
xn/BEL
D˙
b1
b2
:::
bn/BEL
or, equivalently, letting AD.aij/,xD.xi/,a n d bD.bi/,a s
AxDb: (28.2)
IfAis nonsingular, it possesses an inverse A/NUL1,a n d
xDA/NUL1b (28.3)
is the solution vector. We can prove that xis the unique solution to equation (28.2)
as follows. If there are two solutions, xandx0,t h e n AxDAx0Dband, letting I
denote an identity matrix,
xDIx
D.A/NUL1A/x
DA/NUL1.Ax/
DA/NUL1.Ax0/
D.A/NUL1A/x0
Dx0:
In this section, we shall be concerned predominantly with the case in which A
is nonsingular or, equivalently (by Theorem D.1), the rank of Ais equal to the
number nof unknowns. There are other possibilities, however, which merit a brief
discussion. If the number of equations is less than the number nof unknowns—or,
more generally, if the rank of Ais less than n—then the system is underdeter-
mined . An underdetermined system typically has inﬁnitely many solutions, al-
though it may have no solutions at all if the equations are inconsistent. If thenumber of equations exceeds the number nof unknowns, the system is overdeter-
mined , and there may not exist any solutions. Section 28.3 addresses the important28.1 Solving systems of linear equations 815
problem of ﬁnding good approximate solutions to overdetermined systems of linear
equations.
Let us return to our problem of solving the system AxDbofnequations in n
unknowns. We could compute A/NUL1and then, using equation (28.3), multiply b
byA/NUL1, yielding xDA/NUL1b. This approach suffers in practice from numerical
instability. Fortunately, another approach—LUP decomposition—is numericallystable and has the further advantage of being faster in practice.
Overview of LUP decomposition
The idea behind LUP decomposition is to ﬁnd three n/STXnmatrices L,U,a n d P
such that
PADLU ; (28.4)
where
/SILis a unit lower-triangular matrix,
/SIUis an upper-triangular matrix, and
/SIPis a permutation matrix.
We call matrices L,U,a n d Psatisfying equation (28.4) an LUP decomposition
of the matrix A. We shall show that every nonsingular matrix Apossesses such a
decomposition.
Computing an LUP decomposition for the matrix Ahas the advantage that we
can more easily solve linear systems when they are triangular, as is the case for
both matrices LandU. Once we have found an LUP decomposition for A,w e
can solve equation (28.2), AxDb, by solving only triangular linear systems, as
follows. Multiplying both sides of AxDbbyPyields the equivalent equation
PAxDPb, which, by Exercise D.1-4, amounts to permuting the equations (28.1).
Using our decomposition (28.4), we obtain
LUxDPb :
We can now solve this equation by solving two triangular linear systems. Let us
deﬁne yDUx,w h e r e xis the desired solution vector. First, we solve the lower-
triangular system
LyDPb (28.5)
for the unknown vector yby a method called “forward substitution.” Having solved
fory, we then solve the upper-triangular system
UxDy (28.6)816 Chapter 28 Matrix Operations
for the unknown xby a method called “back substitution.” Because the permu-
tation matrix Pis invertible (Exercise D.2-3), multiplying both sides of equa-
tion (28.4) by P/NUL1gives P/NUL1PADP/NUL1LU,s ot h a t
ADP/NUL1LU : (28.7)
Hence, the vector xis our solution to AxDb:
AxDP/NUL1LUx (by equation (28.7))
DP/NUL1Ly (by equation (28.6))
DP/NUL1Pb (by equation (28.5))
Db:
Our next step is to show how forward and back substitution work and then attack
the problem of computing the LUP decomposition itself.
Forward and back substitution
Forward substitution can solve the lower-triangular system (28.5) in ‚.n2/time,
given L,P,a n d b. For convenience, we represent the permutation Pcompactly
by an array /EMŒ 1::n /c141 .F o r iD1 ;2;:::;n , the entry /EMŒi/c141 indicates that Pi;/EMŒi/c141D1
andPijD0forj¤/EMŒi/c141. Thus, PAhasa/EMŒi/c141;j in row iand column j,a n d Pb
hasb/EMŒi/c141as its ith element. Since Lis unit lower-triangular, we can rewrite equa-
tion (28.5) as
y1 Db/EMŒ1/c141;
l21y1C y2 Db/EMŒ2/c141;
l31y1Cl32y2C y3Db/EMŒ3/c141;
:::
ln1y1Cln2y2Cln3y3C/SOH/SOH/SOHC ynDb/EMŒn/c141:
The ﬁrst equation tells us that y1Db/EMŒ1/c141. Knowing the value of y1, we can
substitute it into the second equation, yielding
y2Db/EMŒ2/c141/NULl21y1:
Now, we can substitute both y1andy2into the third equation, obtaining
y3Db/EMŒ3/c141/NUL.l31y1Cl32y2/:
In general, we substitute y1;y2;:::;y i/NUL1“forward” into the ith equation to solve
foryi:28.1 Solving systems of linear equations 817
yiDb/EMŒi/c141/NULi/NUL1X
jD1lijyj:
Having solved for y,w es o l v ef o r xin equation (28.6) using back substitution ,
which is similar to forward substitution. Here, we solve the nth equation ﬁrst and
work backward to the ﬁrst equation. Like forward substitution, this process runsin‚.n
2/time. Since Uis upper-triangular, we can rewrite the system (28.6) as
u11x1Cu12x2C/SOH/SOH/SOHC u1;n/NUL2xn/NUL2Cu1;n/NUL1xn/NUL1C u1nxnDy1;
u22x2C/SOH/SOH/SOHC u2;n/NUL2xn/NUL2Cu2;n/NUL1xn/NUL1C u2nxnDy2;
:::
un/NUL2;n/NUL2xn/NUL2Cun/NUL2;n/NUL1xn/NUL1Cun/NUL2;nxnDyn/NUL2;
un/NUL1;n/NUL1xn/NUL1Cun/NUL1;nxnDyn/NUL1;
un;nxnDyn:
Thus, we can solve for xn;xn/NUL1;:::;x 1successively as follows:
xnDyn=un;n;
xn/NUL1D.yn/NUL1/NULun/NUL1;nxn/=u n/NUL1;n/NUL1;
xn/NUL2D.yn/NUL2/NUL.un/NUL2;n/NUL1xn/NUL1Cun/NUL2;nxn//=u n/NUL2;n/NUL2;
:::
or, in general,
xiD 
yi/NULnX
jDiC1uijxj!
=uii:
Given P,L,U,a n d b, the procedure LUP-S OLVE solves for xby combining
forward and back substitution. The pseudocode assumes that the dimension nap-
pears in the attribute L:rows and that the permutation matrix Pis represented by
the array /EM.
LUP-S OLVE . L;U;/EM ;b/
1nDL:rows
2l e t xbe a new vector of length n
3foriD1ton
4 yiDb/EMŒi/c141/NULPi/NUL1
jD1lijyj
5foriDndownto 1
6 xiD/NUL
yi/NULPn
jDiC1uijxj/SOH
=uii
7return x818 Chapter 28 Matrix Operations
Procedure LUP-S OLVE solves for yusing forward substitution in lines 3–4, and
then it solves for xusing backward substitution in lines 5–6. Since the summation
within each of the forloops includes an implicit loop, the running time is ‚.n2/.
As an example of these methods, consider the system of linear equations deﬁned
by/NUL
120344563/SOH
xD/NUL
3
78/SOH
;
where
AD/NUL
120
344563/SOH
;
bD/NUL
3
7
8/SOH
;
and we wish to solve for the unknown x. The LUP decomposition is
LD/NUL
10 0
0:2 1 00:6 0:5 1/SOH
;
UD/NUL
56 3
00 : 8/NUL0:6
002 : 5/SOH
;
PD/NUL
001
100010/SOH
:
(You might want to verify that PADLU.) Using forward substitution, we solve
LyDPbfory:/NUL
10 0
0:2 1 00:6 0:5 1/SOH/NUL
y1
y2
y3/SOH
D/NUL
837/SOH
;
obtaining
yD/NUL
8
1:4
1:5/SOH
by computing ﬁrst y1,t h e n y2, and ﬁnally y3. Using back substitution, we solve
UxDyforx:28.1 Solving systems of linear equations 819
/NUL
56 3
00 : 8/NUL0:6
002 : 5/SOH/NUL
x1
x2
x3/SOH
D/NUL
8
1:41:5/SOH
;
thereby obtaining the desired answer
xD/NUL
/NUL1:4
2:2
0:6/SOH
by computing ﬁrst x3,t h e n x2, and ﬁnally x1.
Computing an LU decomposition
We have now shown that if we can create an LUP decomposition for a nonsingular
matrix A, then forward and back substitution can solve the system AxDbof
linear equations. Now we show how to efﬁciently compute an LUP decompositionforA. We start with the case in which Ais an n/STXnnonsingular matrix and Pis
absent (or, equivalently, PDI
n). In this case, we factor ADLU. We call the
two matrices LandUanLU decomposition ofA.
We use a process known as Gaussian elimination to create an LU decomposi-
tion. We start by subtracting multiples of the ﬁrst equation from the other equationsin order to remove the ﬁrst variable from those equations. Then, we subtract mul-tiples of the second equation from the third and subsequent equations so that nowthe ﬁrst and second variables are removed from them. We continue this processuntil the system that remains has an upper-triangular form—in fact, it is the ma-
trixU. The matrix Lis made up of the row multipliers that cause variables to be
eliminated.
Our algorithm to implement this strategy is recursive. We wish to construct an
LU decomposition for an n/STXnnonsingular matrix A.I fnD1, then we are done,
since we can choose LDI
1andUDA.F o r n>1 , we break Ainto four parts:
AD˙
a11
a12/SOH/SOH/SOHa1n
a21
a22/SOH/SOH/SOHa2n
:::
:::::::::
a
n1
an2/SOH/SOH/SOHann/BEL
D/DC2a11wT
/ETBA0/DC3
;
where /ETBis a column .n/NUL1/-vector, wTi sar o w .n/NUL1/-vector, and A0is an
.n/NUL1//STX.n/NUL1/matrix. Then, using matrix algebra (verify the equations by820 Chapter 28 Matrix Operations
simply multiplying through), we can factor Aas
AD/DC2a11wT
/ETBA0/DC3
D/DC210
/ETB=a 11In/NUL1/DC3/DC2a11 wT
0A0/NUL/ETBwT=a11/DC3
: (28.8)
The0s in the ﬁrst and second matrices of equation (28.8) are row and col-
umn .n/NUL1/-vectors, respectively. The term /ETBwT=a11,f o r m e db yt a k i n gt h e
outer product of /ETBandwand dividing each element of the result by a11,i sa n
.n/NUL1//STX.n/NUL1/matrix, which conforms in size to the matrix A0from which it is
subtracted. The resulting .n/NUL1//STX.n/NUL1/matrix
A0/NUL/ETBwT=a11 (28.9)
is called the Schur complement ofAwith respect to a11.
We claim that if Ais nonsingular, then the Schur complement is nonsingular,
too. Why? Suppose that the Schur complement, which is .n/NUL1//STX.n/NUL1/,i s
singular. Then by Theorem D.1, it has row rank strictly less than n/NUL1. Because
the bottom n/NUL1entries in the ﬁrst column of the matrix/DC2a11 wT
0A0/NUL/ETBwT=a11/DC3
are all 0, the bottom n/NUL1rows of this matrix must have row rank strictly less
thann/NUL1. The row rank of the entire matrix, therefore, is strictly less than n.
Applying Exercise D.2-8 to equation (28.8), Ahas rank strictly less than n,a n d
from Theorem D.1 we derive the contradiction that Ais singular.
Because the Schur complement is nonsingular, we can now recursively ﬁnd an
LU decomposition for it. Let us say that
A0/NUL/ETBwT=a11DL0U0;
where L0is unit lower-triangular and U0is upper-triangular. Then, using matrix
algebra, we have
AD/DC210
/ETB=a 11In/NUL1/DC3/DC2a11 wT
0A0/NUL/ETBwT=a11/DC3
D/DC210
/ETB=a 11In/NUL1/DC3/DC2a11 wT
0L0U0/DC3
D/DC210
/ETB=a 11L0/DC3/DC2a11wT
0U0/DC3
DLU ;
thereby providing our LU decomposition. (Note that because L0is unit lower-
triangular, so is L, and because U0is upper-triangular, so is U.)28.1 Solving systems of linear equations 821
Of course, if a11D0, this method doesn’t work, because it divides by 0.I ta l s o
doesn’t work if the upper leftmost entry of the Schur complement A0/NUL/ETBwT=a11
is0, since we divide by it in the next step of the recursion. The elements by
which we divide during LU decomposition are called pivots , and they occupy the
diagonal elements of the matrix U. The reason we include a permutation matrix P
during LUP decomposition is that it allows us to avoid dividing by 0.W h e nw eu s e
permutations to avoid division by 0(or by small numbers, which would contribute
to numerical instability), we are pivoting .
An important class of matrices for which LU decomposition always works cor-
rectly is the class of symmetric positive-deﬁnite matrices. Such matrices requireno pivoting, and thus we can employ the recursive strategy outlined above with-out fear of dividing by 0. We shall prove this result, as well as several others, in
Section 28.3.
Our code for LU decomposition of a matrix Afollows the recursive strategy, ex-
cept that an iteration loop replaces the recursion. (This transformation is a standardoptimization for a “tail-recursive” procedure—one whose last operation is a recur-sive call to itself. See Problem 7-4.) It assumes that the attribute A:rows gives
the dimension of A. We initialize the matrix Uwith 0s below the diagonal and
matrix Lwith1s on its diagonal and 0s above the diagonal.
LU-D
ECOMPOSITION .A/
1nDA:rows
2l e t LandUbe new n/STXnmatrices
3 initialize Uwith0s below the diagonal
4 initialize Lwith1s on the diagonal and 0s above the diagonal
5forkD1ton
6 ukkDakk
7 foriDkC1ton
8 likDaik=ukk //likholds /ETBi
9 ukiDaki //ukiholds wT
i
10 foriDkC1ton
11 forjDkC1ton
12 aijDaij/NULlikukj
13return LandU
The outer forloop beginning in line 5 iterates once for each recursive step. Within
this loop, line 6 determines the pivot to be ukkDakk.T h e forloop in lines 7–9
(which does not execute when kDn), uses the /ETBandwTvectors to update L
andU. Line 8 determines the elements of the /ETBvector, storing /ETBiinlik, and line 9
computes the elements of the wTvector, storing wT
iinuki. Finally, lines 10–12
compute the elements of the Schur complement and store them back into the ma-822 Chapter 28 Matrix Operations
2315
61 351 92 1 91 02 34 1 01 13 1
(a)315
342411 691 8249 2 1
(b)231532 41412217 1 7
(c)2315342414 22173
(d)
(e)2
4
1
/EOT
2 315
61 3 5 1 9
21 91 02 3
41 01 13 1˘
D/EOT
1000
3100
1410
2171˘/EOT
2315
0424
0012
0003˘
AL U
Figure 28.1 The operation of LU-D ECOMPOSITION .(a)The matrix A.(b)The element a11D2
in the black circle is the pivot, the shaded column is /ETB=a11, and the shaded row is wT. The elements
ofUcomputed thus far are above the horizontal line, and the elements of Lare to the left of the
vertical line. The Schur complement matrix A0/NUL/ETBwT=a11occupies the lower right. (c)We now
operate on the Schur complement matrix produced from part (b). The element a22D4in the black
circle is the pivot, and the shaded column and row are /ETB=a22andwT(in the partitioning of the Schur
complement), respectively. Lines divide the matrix into the elements of Ucomputed so far (above),
the elements of Lcomputed so far (left), and the new Schur complement (lower right). (d)After the
next step, the matrix Ais factored. (The element 3in the new Schur complement becomes part of U
when the recursion terminates.) (e)The factorization ADLU.
trixA. (We don’t need to divide by akkin line 12 because we already did so when
we computed likin line 8.) Because line 12 is triply nested, LU-D ECOMPOSITION
runs in time ‚.n3/.
Figure 28.1 illustrates the operation of LU-D ECOMPOSITION . It shows a stan-
dard optimization of the procedure in which we store the signiﬁcant elements of L
andUin place in the matrix A. That is, we can set up a correspondence between
each element aijand either lij(ifi>j )o ruij(ifi/DC4j) and update the ma-
trixAso that it holds both LandUwhen the procedure terminates. To obtain
the pseudocode for this optimization from the above pseudocode, just replace each
reference to lorubya; you can easily verify that this transformation preserves
correctness.
Computing an LUP decomposition
Generally, in solving a system of linear equations AxDb, we must pivot on off-
diagonal elements of Ato avoid dividing by 0. Dividing by 0would, of course,
be disastrous. But we also want to avoid dividing by a small value—even if Ais28.1 Solving systems of linear equations 823
nonsingular—because numerical instabilities can result. We therefore try to pivot
on a large value.
The mathematics behind LUP decomposition is similar to that of LU decom-
position. Recall that we are given an n/STXnnonsingular matrix A, and we wish
to ﬁnd a permutation matrix P, a unit lower-triangular matrix L, and an upper-
triangular matrix Usuch that PADLU. Before we partition the matrix A,a sw e
did for LU decomposition, we move a nonzero element, say ak1, from somewhere
in the ﬁrst column to the .1; 1/ position of the matrix. For numerical stability, we
choose ak1as the element in the ﬁrst column with the greatest absolute value. (The
ﬁrst column cannot contain only 0s, for then Awould be singular, because its de-
terminant would be 0, by Theorems D.4 and D.5.) In order to preserve the set of
equations, we exchange row 1with row k, which is equivalent to multiplying Aby
a permutation matrix Qon the left (Exercise D.1-4). Thus, we can write QAas
QAD/DC2ak1wT
/ETBA0/DC3
;
where /ETBD.a21;a31;:::;a n1/T, except that a11replaces ak1;wTD.ak2;ak3;
:::;a kn/;a n d A0is an.n/NUL1//STX.n/NUL1/matrix. Since ak1¤0, we can now perform
much the same linear algebra as for LU decomposition, but now guaranteeing thatwe do not divide by 0:
QAD/DC2a
k1wT
/ETBA0/DC3
D/DC210
/ETB=a k1In/NUL1/DC3/DC2ak1 wT
0A0/NUL/ETBwT=ak1/DC3
:
As we saw for LU decomposition, if Ais nonsingular, then the Schur comple-
ment A0/NUL/ETBwT=ak1is nonsingular, too. Therefore, we can recursively ﬁnd an
LUP decomposition for it, with unit lower-triangular matrix L0, upper-triangular
matrix U0, and permutation matrix P0, such that
P0.A0/NUL/ETBwT=ak1/DL0U0:
Deﬁne
PD/DC210
0P0/DC3
Q;
which is a permutation matrix, since it is the product of two permutation matrices
(Exercise D.1-4). We now have824 Chapter 28 Matrix Operations
PAD/DC210
0P0/DC3
QA
D/DC210
0P0/DC3/DC210
/ETB=a k1In/NUL1/DC3/DC2ak1 wT
0A0/NUL/ETBwT=ak1/DC3
D/DC210
P0/ETB=a k1P0/DC3/DC2ak1 wT
0A0/NUL/ETBwT=ak1/DC3
D/DC210
P0/ETB=a k1In/NUL1/DC3/DC2ak1 wT
0P0.A0/NUL/ETBwT=ak1//DC3
D/DC210
P0/ETB=a k1In/NUL1/DC3/DC2ak1 wT
0L0U0/DC3
D/DC210
P0/ETB=a k1L0/DC3/DC2ak1wT
0U0/DC3
DLU ;
yielding the LUP decomposition. Because L0is unit lower-triangular, so is L,a n d
because U0is upper-triangular, so is U.
Notice that in this derivation, unlike the one for LU decomposition, we must
multiply both the column vector /ETB=a k1and the Schur complement A0/NUL/ETBwT=ak1
by the permutation matrix P0. Here is the pseudocode for LUP decomposition:
LUP-D ECOMPOSITION .A/
1nDA:rows
2l e t /EMŒ 1::n /c141 be a new array
3foriD1ton
4 /EMŒi/c141Di
5forkD1ton
6 pD0
7 foriDkton
8 ifjaikj>p
9 pDjaikj
10 k0Di
11 ifp==0
12 error “singular matrix”
13 exchange /EMŒk/c141 with/EMŒk0/c141
14 foriD1ton
15 exchange akiwithak0i
16 foriDkC1ton
17 aikDaik=akk
18 forjDkC1ton
19 aijDaij/NULaikakj28.1 Solving systems of linear equations 825
Like LU-D ECOMPOSITION , our LUP-D ECOMPOSITION procedure replaces
the recursion with an iteration loop. As an improvement over a direct implemen-tation of the recursion, we dynamically maintain the permutation matrix Pas an
array /EM,w h e r e /EMŒi/c141Djmeans that the ith row of Pcontains a 1in column j.
We also implement the code to compute LandU“in place” in the matrix A. Thus,
when the procedure terminates,
a
ijD(
lijifi>j;
uijifi/DC4j:
Figure 28.2 illustrates how LUP-D ECOMPOSITION factors a matrix. Lines 3–4
initialize the array /EMto represent the identity permutation. The outer forloop
beginning in line 5 implements the recursion. Each time through the outer loop,
lines 6–10 determine the element ak0kwith largest absolute value of those in the
current ﬁrst column (column k)o ft h e .n/NULkC1//STX.n/NULkC1/matrix whose
LUP decomposition we are ﬁnding. If all elements in the current ﬁrst column arezero, lines 11–12 report that the matrix is singular. To pivot, we exchange /EMŒk
0/c141
with /EMŒk/c141 in line 13 and exchange the kth and k0th rows of Ain lines 14–15,
thereby making the pivot element akk. (The entire rows are swapped because in
the derivation of the method above, not only is A0/NUL/ETBwT=ak1multiplied by P0,b u t
so is /ETB=a k1.) Finally, the Schur complement is computed by lines 16–19 in much
the same way as it is computed by lines 7–12 of LU-D ECOMPOSITION , except that
here the operation is written to work in place.
Because of its triply nested loop structure, LUP-D ECOMPOSITION has a run-
ning time of ‚.n3/, which is the same as that of LU-D ECOMPOSITION . Thus,
pivoting costs us at most a constant factor in time.
Exercises
28.1-1
Solve the equation/NUL
100410
/NUL651/SOH/NUL
x1
x2
x3/SOH
D/NUL
3
14
/NUL7/SOH
by using forward substitution.
28.1-2
Find an LU decomposition of the matrix/NUL
4/NUL56
8/NUL67
12/NUL71 2/SOH
:826 Chapter 28 Matrix Operations
2 0 2 0.6
334 – 2
5 542
–1 –2 3.4 –1
(a)1
234
2 0 2 0.6334 – 25 542
–1 –2 3.4 –1
(b)3
214
0.4 –2 0.4 –.20.6 0 1.6 –3.25 542
–0.2 –1 4.2 –0.6
(c)3
214
0.4 –2 0.4 –0.20.6 0 1.6 –3.25542
–0.2 –1 4.2 –0.6
(d)3
2140.4 –2 0.4 –0.2
0.6 0 1.6 –3.25542
–0.2 –1 4.2 –0.6
(e)3
21
40.4 –2 0.4 –0.2
0.6 0 1.6 –3.25542
–0.2 0.5 4 –0.5
(f)3
21
4
0.4 –2 0.4 –0.2
0.6 0 1.6 –3.25542
–0.2 0.5 4–0.5
(g)3
21
40.4 –2 0.4 –0.20.6 0 1.6 –3.25542
–0.2 0.5
4–0.5
(h)3
21
40.4 –2 0.4 –0.2
0.6 0 0.4 –35542
–0.2 0.5 4–0.5
(i)3
21
4
(j)/EOT
0010
10000001
0100˘/EOT
202 0 : 6
334/NUL2
5542
/NUL1/NUL23 : 4/NUL1˘
D/EOT
10 0 0
0:4 1 0 0
/NUL0:2 0:5 1 0
0:6 0 0:4 1˘/EOT
55 4 2
0/NUL20 : 4/NUL0:2
00 4/NUL0:5
00 0/NUL3˘
PA L U
Figure 28.2 The operation of LUP-D ECOMPOSITION .(a)The input matrix Awith the identity
permutation of the rows on the left. The ﬁrst step of the algorithm determines that the element 5
in the black circle in the third row is the pivot for the ﬁrst column. (b)Rows 1and3are swapped
and the permutation is updated. The shaded column and row represent /ETBandwT.(c)The vector /ETB
is replaced by /ETB=5, and the lower right of the matrix is updated with the Schur complement. Lines
divide the matrix into three regions: elements of U(above), elements of L(left), and elements of the
Schur complement (lower right). (d)–(f) The second step. (g)–(i) The third step. No further changes
occur on the fourth (ﬁnal) step. (j)The LUP decomposition PADLU.28.2 Inverting matrices 827
28.1-3
Solve the equation/NUL
154203
582/SOH/NUL
x1
x2
x3/SOH
D/NUL
12
9
5/SOH
by using an LUP decomposition.
28.1-4
Describe the LUP decomposition of a diagonal matrix.
28.1-5
Describe the LUP decomposition of a permutation matrix A, and prove that it is
unique.
28.1-6
Show that for all n/NAK1, there exists a singular n/STXnmatrix that has an LU decom-
position.
28.1-7
In LU-D ECOMPOSITION , is it necessary to perform the outermost forloop itera-
tion when kDn? How about in LUP-D ECOMPOSITION ?
28.2 Inverting matrices
Although in practice we do not generally use matrix inverses to solve systems of
linear equations, preferring instead to use more numerically stable techniques suchas LUP decomposition, sometimes we need to compute a matrix inverse. In thissection, we show how to use LUP decomposition to compute a matrix inverse.We also prove that matrix multiplication and computing the inverse of a matrix
are equivalently hard problems, in that (subject to technical conditions) we can
use an algorithm for one to solve the other in the same asymptotic running time.Thus, we can use Strassen’s algorithm (see Section 4.2) for matrix multiplicationto invert a matrix. Indeed, Strassen’s original paper was motivated by the problemof showing that a set of a linear equations could be solved more quickly than bythe usual method.828 Chapter 28 Matrix Operations
Computing a matrix inverse from an LUP decomposition
Suppose that we have an LUP decomposition of a matrix Ain the form of three
matrices L,U,a n d Psuch that PADLU. Using LUP-S OLVE , we can solve
an equation of the form AxDbin time ‚.n2/. Since the LUP decomposition
depends on Abut not b, we can run LUP-S OLVE on a second set of equations of
the form AxDb0in additional time ‚.n2/. In general, once we have the LUP
decomposition of A, we can solve, in time ‚.kn2/,kversions of the equation
AxDbthat differ only in b.
We can think of the equation
AXDIn; (28.10)
which deﬁnes the matrix X, the inverse of A, as a set of ndistinct equations of the
form AxDb. To be precise, let Xidenote the ith column of X, and recall that the
unit vector eiis the ith column of In. We can then solve equation (28.10) for Xby
using the LUP decomposition for Ato solve each equation
AX iDei
separately for Xi. Once we have the LUP decomposition, we can compute each of
thencolumns Xiin time ‚.n2/, and so we can compute Xfrom the LUP decom-
position of Ain time ‚.n3/. Since we can determine the LUP decomposition of A
in time ‚.n3/, we can compute the inverse A/NUL1of a matrix Ain time ‚.n3/.
Matrix multiplication and matrix inversion
We now show that the theoretical speedups obtained for matrix multiplication
translate to speedups for matrix inversion. In fact, we prove something stronger:matrix inversion is equivalent to matrix multiplication, in the following sense.IfM.n/ denotes the time to multiply two n/STXnmatrices, then we can invert a
nonsingular n/STXnmatrix in time O.M.n// . Moreover, if I.n/ denotes the time
to invert a nonsingular n/STXnmatrix, then we can multiply two n/STXnmatrices in
timeO.I.n// . We prove these results as two separate theorems.
Theorem 28.1 (Multiplication is no harder than inversion)
If we can invert an n/STXnmatrix in time I.n/,w h e r e I.n/D/DEL.n
2/andI.n/
satisﬁes the regularity condition I.3n/DO.I.n// , then we can multiply two n/STXn
matrices in time O.I.n// .
Proof LetAandBben/STXnmatrices whose matrix product Cwe wish to com-
pute. We deﬁne the 3n/STX3nmatrix Dby28.2 Inverting matrices 829
DD/NUL
InA0
0I nB
00 I n/SOH
:
The inverse of Dis
D/NUL1D/NUL
In/NULAA B
0I n/NULB
00I n/SOH
;
and thus we can compute the product ABby taking the upper right n/STXnsubmatrix
ofD/NUL1.
We can construct matrix Din‚.n2/time, which is O.I.n// because we assume
thatI.n/D/DEL.n2/, and we can invert DinO.I.3n//DO.I.n// time, by the
regularity condition on I.n/. We thus have M.n/DO.I.n// .
Note that I.n/ satisﬁes the regularity condition whenever I.n/D‚.nclgdn/
for any constants c>0 andd/NAK0.
The proof that matrix inversion is no harder than matrix multiplication relies
on some properties of symmetric positive-deﬁnite matrices that we will prove inSection 28.3.
Theorem 28.2 (Inversion is no harder than multiplication)
Suppose we can multiply two n/STXnreal matrices in time M.n/ ,w h e r e M.n/D
/DEL.n
2/andM.n/ satisﬁes the two regularity conditions M.nCk/DO.M.n// for
anykin the range 0/DC4k/DC4nandM.n=2//DC4cM.n/ for some constant c<1 = 2 .
Then we can compute the inverse of any real nonsingular n/STXnmatrix in time
O.M.n// .
Proof We prove the theorem here for real matrices. Exercise 28.2-6 asks you to
generalize the proof for matrices whose entries are complex numbers.
We can assume that nis an exact power of 2,s i n c ew eh a v e
/DC2A0
0I k/DC3/NUL1
D/DC2A/NUL10
0I k/DC3
for any k>0 . Thus, by choosing ksuch that nCkis a power of 2, we enlarge
the matrix to a size that is the next power of 2 and obtain the desired answer A/NUL1
from the answer to the enlarged problem. The ﬁrst regularity condition on M.n/
ensures that this enlargement does not cause the running time to increase by more
than a constant factor.
For the moment, let us assume that the n/STXnmatrix Ais symmetric and positive-
deﬁnite. We partition each of Aand its inverse A/NUL1into four n=2/STXn=2submatri-
ces:830 Chapter 28 Matrix Operations
AD/DC2BCT
CD/DC3
and A/NUL1D/DC2RT
UV/DC3
: (28.11)
Then, if we let
SDD/NULCB/NUL1CT(28.12)
be the Schur complement of Awith respect to B(we shall see more about this form
of Schur complement in Section 28.3), we have
A/NUL1D/DC2RT
UV/DC3
D/DC2B/NUL1CB/NUL1CTS/NUL1CB/NUL1/NULB/NUL1CTS/NUL1
/NULS/NUL1CB/NUL1S/NUL1/DC3
; (28.13)
since AA/NUL1DIn, as you can verify by performing the matrix multiplication. Be-
cause Ais symmetric and positive-deﬁnite, Lemmas 28.4 and 28.5 in Section 28.3
imply that BandSare both symmetric and positive-deﬁnite. By Lemma 28.3 in
Section 28.3, therefore, the inverses B/NUL1andS/NUL1exist, and by Exercise D.2-6,
B/NUL1andS/NUL1are symmetric, so that .B/NUL1/TDB/NUL1and.S/NUL1/TDS/NUL1.T h e r e -
fore, we can compute the submatrices R,T,U,a n d VofA/NUL1as follows, where
all matrices mentioned are n=2/STXn=2:
1. Form the submatrices B,C,CT,a n d DofA.
2. Recursively compute the inverse B/NUL1ofB.
3. Compute the matrix product WDCB/NUL1, and then compute its transpose WT,
which equals B/NUL1CT(by Exercise D.1-2 and .B/NUL1/TDB/NUL1).
4. Compute the matrix product XDWCT, which equals CB/NUL1CT,a n dt h e n
compute the matrix SDD/NULXDD/NULCB/NUL1CT.
5. Recursively compute the inverse S/NUL1ofS, and set VtoS/NUL1.
6. Compute the matrix product YDS/NUL1W, which equals S/NUL1CB/NUL1,a n d
then compute its transpose YT, which equals B/NUL1CTS/NUL1(by Exercise D.1-2,
.B/NUL1/TDB/NUL1,a n d .S/NUL1/TDS/NUL1). Set Tto/NULYTandUto/NULY.
7. Compute the matrix product ZDWTY, which equals B/NUL1CTS/NUL1CB/NUL1,a n d
setRtoB/NUL1CZ.
Thus, we can invert an n/STXnsymmetric positive-deﬁnite matrix by inverting two
n=2/STXn=2matrices in steps 2 and 5; performing four multiplications of n=2/STXn=2
matrices in steps 3, 4, 6, and 7; plus an additional cost of O.n2/for extracting
submatrices from A, inserting submatrices into A/NUL1, and performing a constant
number of additions, subtractions, and transposes on n=2/STXn=2matrices. We get
the recurrence
I.n//DC42I.n=2/C4M.n=2/CO.n2/
D2I.n=2/C‚.M.n//
DO.M.n// :28.2 Inverting matrices 831
The second line holds because the second regularity condition in the statement
of the theorem implies that 4M.n=2/ < 2M.n/ and because we assume that
M.n/D/DEL.n2/. The third line follows because the second regularity condition
allows us to apply case 3 of the master theorem (Theorem 4.1).
It remains to prove that we can obtain the same asymptotic running time for ma-
trix multiplication as for matrix inversion when Ais invertible but not symmetric
and positive-deﬁnite. The basic idea is that for any nonsingular matrix A,t h em a -
trixATAis symmetric (by Exercise D.1-2) and positive-deﬁnite (by Theorem D.6).
The trick, then, is to reduce the problem of inverting Ato the problem of invert-
ingATA.
The reduction is based on the observation that when Ais an n/STXnnonsingular
matrix, we have
A/NUL1D.ATA//NUL1AT;
since ..ATA//NUL1AT/AD.ATA//NUL1.ATA/DInand a matrix inverse is unique.
Therefore, we can compute A/NUL1by ﬁrst multiplying ATbyAto obtain ATA,t h e n
inverting the symmetric positive-deﬁnite matrix ATAusing the above divide-and-
conquer algorithm, and ﬁnally multiplying the result by AT. Each of these three
steps takes O.M.n// time, and thus we can invert any nonsingular matrix with real
entries in O.M.n// time.
The proof of Theorem 28.2 suggests a means of solving the equation AxDb
by using LU decomposition without pivoting, so long as Ais nonsingular. We
multiply both sides of the equation by AT, yielding .ATA/xDATb. This trans-
formation doesn’t affect the solution x,s i n c e ATis invertible, and so we can fac-
tor the symmetric positive-deﬁnite matrix ATAby computing an LU decomposi-
tion. We then use forward and back substitution to solve for xwith the right-hand
sideATb. Although this method is theoretically correct, in practice the procedure
LUP-D ECOMPOSITION works much better. LUP decomposition requires fewer
arithmetic operations by a constant factor, and it has somewhat better numerical
properties.
Exercises
28.2-1
LetM.n/ be the time to multiply two n/STXnmatrices, and let S.n/ denote the time
required to square an n/STXnmatrix. Show that multiplying and squaring matri-
ces have essentially the same difﬁculty: an M.n/ -time matrix-multiplication al-
gorithm implies an O.M.n// -time squaring algorithm, and an S.n/ -time squaring
algorithm implies an O.S.n// -time matrix-multiplication algorithm.832 Chapter 28 Matrix Operations
28.2-2
LetM.n/ be the time to multiply two n/STXnmatrices, and let L.n/ be the time to
compute the LUP decomposition of an n/STXnmatrix. Show that multiplying matri-
ces and computing LUP decompositions of matrices have essentially the same dif-ﬁculty: an M.n/ -time matrix-multiplication algorithm implies an O.M.n// -time
LUP-decomposition algorithm, and an L.n/ -time LUP-decomposition algorithm
implies an O.L.n// -time matrix-multiplication algorithm.
28.2-3
LetM.n/ be the time to multiply two n/STXnmatrices, and let D.n/ denote the
time required to ﬁnd the determinant of an n/STXnmatrix. Show that multiply-
ing matrices and computing the determinant have essentially the same difﬁculty:anM.n/ -time matrix-multiplication algorithm implies an O.M.n// -time determi-
nant algorithm, and a D.n/ -time determinant algorithm implies an O.D.n// -time
matrix-multiplication algorithm.
28.2-4
LetM.n/ be the time to multiply two n/STXnboolean matrices, and let T .n/ be the
time to ﬁnd the transitive closure of an n/STXnboolean matrix. (See Section 25.2.)
Show that an M.n/ -time boolean matrix-multiplication algorithm implies an
O.M.n/ lgn/-time transitive-closure algorithm, and a T .n/ -time transitive-closure
algorithm implies an O.T .n// -time boolean matrix-multiplication algorithm.
28.2-5
Does the matrix-inversion algorithm based on Theorem 28.2 work when matrixelements are drawn from the ﬁeld of integers modulo 2? Explain.
28.2-6 ?
Generalize the matrix-inversion algorithm of Theorem 28.2 to handle matrices ofcomplex numbers, and prove that your generalization works correctly. ( Hint: In-
stead of the transpose of A,u s et h e conjugate transpose A
/ETX, which you obtain from
the transpose of Aby replacing every entry with its complex conjugate. Instead of
symmetric matrices, consider Hermitian matrices, which are matrices Asuch that
ADA/ETX.)
28.3 Symmetric positive-deﬁnite matrices and least-squares approximation
Symmetric positive-deﬁnite matrices have many interesting and desirable proper-
ties. For example, they are nonsingular, and we can perform LU decompositionon them without having to worry about dividing by 0. In this section, we shall28.3 Symmetric positive-deﬁnite matrices and least-squares approximation 833
prove several other important properties of symmetric positive-deﬁnite matrices
and show an interesting application to curve ﬁtting by a least-squares approxima-tion.
The ﬁrst property we prove is perhaps the most basic.
Lemma 28.3
Any positive-deﬁnite matrix is nonsingular.
Proof Suppose that a matrix Ais singular. Then by Corollary D.3, there exists a
nonzero vector xsuch that AxD0. Hence, x
TAxD0,a n d Acannot be positive-
deﬁnite.
The proof that we can perform LU decomposition on a symmetric positive-
deﬁnite matrix Awithout dividing by 0is more involved. We begin by proving
properties about certain submatrices of A.D e ﬁ n et h e kthleading submatrix ofA
to be the matrix Akconsisting of the intersection of the ﬁrst krows and ﬁrst k
columns of A.
Lemma 28.4
IfAis a symmetric positive-deﬁnite matrix, then every leading submatrix of Ais
symmetric and positive-deﬁnite.
Proof That each leading submatrix Akis symmetric is obvious. To prove that Ak
is positive-deﬁnite, we assume that it is not and derive a contradiction. If Akis not
positive-deﬁnite, then there exists a k-vector xk¤0such that xT
kAkxk/DC40.L e t A
ben/STXn,a n d
AD/DC2AkBT
BC/DC3
(28.14)
for submatrices B(which is .n/NULk//STXk)a n d C(which is .n/NULk//STX.n/NULk/). Deﬁne
then-vector xD.xT
k0/T,w h e r e n/NULk0s follow xk.T h e nw eh a v e
xTAxD.xT
k0//DC2AkBT
BC/DC3/DC2xk
0/DC3
D.xT
k0//DC2Akxk
Bxk/DC3
DxT
kAkxk
/DC40;
which contradicts Abeing positive-deﬁnite.
834 Chapter 28 Matrix Operations
We now turn to some essential properties of the Schur complement. Let Abe
a symmetric positive-deﬁnite matrix, and let Akb eal e a d i n g k/STXksubmatrix
ofA. Partition Aonce again according to equation (28.14). We generalize equa-
tion (28.9) to deﬁne the Schur complement SofAwith respect to Akas
SDC/NULBA/NUL1
kBT: (28.15)
(By Lemma 28.4, Akis symmetric and positive-deﬁnite; therefore, A/NUL1
kexists by
Lemma 28.3, and Sis well deﬁned.) Note that our earlier deﬁnition (28.9) of the
Schur complement is consistent with equation (28.15), by letting kD1.
The next lemma shows that the Schur-complement matrices of symmetric posi-
tive-deﬁnite matrices are themselves symmetric and positive-deﬁnite. We used thisresult in Theorem 28.2, and we need its corollary to prove the correctness of LUdecomposition for symmetric positive-deﬁnite matrices.
Lemma 28.5 (Schur complement lemma)
IfAis a symmetric positive-deﬁnite matrix and A
kis a leading k/STXksubmatrix
ofA, then the Schur complement SofAwith respect to Akis symmetric and
positive-deﬁnite.
Proof Because Ais symmetric, so is the submatrix C. By Exercise D.2-6, the
product BA/NUL1
kBTis symmetric, and by Exercise D.1-1, Sis symmetric.
It remains to show that Sis positive-deﬁnite. Consider the partition of Agiven in
equation (28.14). For any nonzero vector x,w eh a v e xTAx > 0 by the assumption
thatAis positive-deﬁnite. Let us break xinto two subvectors yand´compatible
withAkandC, respectively. Because A/NUL1
kexists, we have
xTAxD.yT´T//DC2AkBT
BC/DC3/DC2y
´/DC3
D.yT´T//DC2AkyCBT´
ByCC´/DC3
DyTAkyCyTBT´C´TByC´TC´
D.yCA/NUL1
kBT´/TAk.yCA/NUL1
kBT´/C´T.C/NULBA/NUL1
kBT/´ ; (28.16)
by matrix magic. (Verify by multiplying through.) This last equation amounts to
“completing the square” of the quadratic form. (See Exercise 28.3-2.)
Since xTAx > 0 holds for any nonzero x, let us pick any nonzero ´and then
choose yD/NULA/NUL1
kBT´, which causes the ﬁrst term in equation (28.16) to vanish,
leaving
´T.C/NULBA/NUL1
kBT/´D´TS´
as the value of the expression. For any ´¤0, we therefore have ´TS´D
xTAx > 0 , and thus Sis positive-deﬁnite.
28.3 Symmetric positive-deﬁnite matrices and least-squares approximation 835
Corollary 28.6
LU decomposition of a symmetric positive-deﬁnite matrix never causes a divisionby0.
Proof LetAbe a symmetric positive-deﬁnite matrix. We shall prove something
stronger than the statement of the corollary: every pivot is strictly positive. The ﬁrstpivot is a
11.L e te1be the ﬁrst unit vector, from which we obtain a11DeT
1Ae1>0.
Since the ﬁrst step of LU decomposition produces the Schur complement of A
with respect to A1D.a11/, Lemma 28.5 implies by induction that all pivots are
positive.
Least-squares approximation
One important application of symmetric positive-deﬁnite matrices arises in ﬁtting
curves to given sets of data points. Suppose that we are given a set of mdata points
.x1;y1/; .x 2;y2/ ;:::;. x m;ym/;
where we know that the yiare subject to measurement errors. We would like to
determine a function F.x/ such that the approximation errors
/DC1iDF.x i//NULyi (28.17)
are small for iD1 ;2;:::;m . The form of the function Fdepends on the problem
at hand. Here, we assume that it has the form of a linearly weighted sum,
F.x/DnX
jD1cjfj.x/ ;
where the number of summands nand the speciﬁc basis functions fjare chosen
based on knowledge of the problem at hand. A common choice is fj.x/Dxj/NUL1,
which means that
F.x/Dc1Cc2xCc3x2C/SOH/SOH/SOHC cnxn/NUL1
is a polynomial of degree n/NUL1inx. Thus, given mdata points .x1;y1/; .x 2;y2/;
:::;. x m;ym/, we wish to calculate ncoefﬁcients c1;c2;:::;c nthat minimize the
approximation errors /DC11;/DC12;:::;/DC1 m.
By choosing nDm, we can calculate each yiexactly in equation (28.17). Such
a high-degree F“ﬁts the noise” as well as the data, however, and generally gives
poor results when used to predict yfor previously unseen values of x.I t i s u s u -
ally better to choose nsigniﬁcantly smaller than mand hope that by choosing the
coefﬁcients cjwell, we can obtain a function Fthat ﬁnds the signiﬁcant patterns
in the data points without paying undue attention to the noise. Some theoretical836 Chapter 28 Matrix Operations
principles exist for choosing n, but they are beyond the scope of this text. In any
case, once we choose a value of nthat is less than m, we end up with an overde-
termined set of equations whose solution we wish to approximate. We now showhow to do so.
Let
AD˙
f1.x1/f 2.x1/ ::: f n.x1/
f1.x2/f 2.x2/ ::: f n.x2/
::::::::::::
f
1.xm/f 2.xm/ ::: f n.xm//BEL
denote the matrix of values of the basis functions at the given points; that is,a
ijDfj.xi/.L e t cD.ck/denote the desired n-vector of coefﬁcients. Then,
AcD˙
f1.x1/f 2.x1/ ::: f n.x1/
f1.x2/f 2.x2/ ::: f n.x2/
::::::::::::
f
1.xm/f 2.xm/ ::: f n.xm//BEL˙
c1
c2
:::
cn/BEL
D˙
F.x 1/
F.x 2/
:::
F.xm//BEL
is the m-vector of “predicted values” for y. Thus,
/DC1DAc/NULy
is the m-vector of approximation errors .
To minimize approximation errors, we choose to minimize the norm of the error
vector /DC1, which gives us a least-squares solution ,s i n c e
k/DC1kD mX
iD1/DC12
i!1=2
:
Because
k/DC1k2DkAc/NULyk2DmX
iD1 nX
jD1aijcj/NULyi!2
;
we can minimizek/DC1kby differentiatingk/DC1k2with respect to each ckand then
setting the result to 0:28.3 Symmetric positive-deﬁnite matrices and least-squares approximation 837
dk/DC1k2
dckDmX
iD12 nX
jD1aijcj/NULyi!
aikD0: (28.18)
Thenequations (28.18) for kD1 ;2;:::;n are equivalent to the single matrix
equation
.Ac/NULy/TAD0
or, equivalently (using Exercise D.1-2), to
AT.Ac/NULy/D0;
which implies
ATAcDATy: (28.19)
In statistics, this is called the normal equation . The matrix ATAis symmetric
by Exercise D.1-2, and if Ahas full column rank, then by Theorem D.6, ATA
is positive-deﬁnite as well. Hence, .ATA//NUL1exists, and the solution to equa-
tion (28.19) is
cD/NUL
.ATA//NUL1AT/SOH
y
DACy; (28.20)
where the matrix ACD..ATA//NUL1AT/is the pseudoinverse of the matrix A.T h e
pseudoinverse naturally generalizes the notion of a matrix inverse to the case inwhich Ais not square. (Compare equation (28.20) as the approximate solution to
AcDywith the solution A
/NUL1bas the exact solution to AxDb.)
As an example of producing a least-squares ﬁt, suppose that we have ﬁve data
points
.x1;y1/D./NUL1; 2/ ;
.x2;y2/D.1; 1/ ;
.x3;y3/D.2; 1/ ;
.x4;y4/D.3; 0/ ;
.x5;y5/D.5; 3/ ;
shown as black dots in Figure 28.3. We wish to ﬁt these points with a quadratic
polynomial
F.x/Dc1Cc2xCc3x2:
We start with the matrix of basis-function values838 Chapter 28 Matrix Operations
0.51.01.52.02.53.0
0.0
12345 0 –1 –2xy
F(x) = 1.2 – 0.757 x + 0.214 x2
Figure 28.3 The least-squares ﬁt of a quadratic polynomial to the set of ﬁve data points
f./NUL1; 2/; .1; 1/; .2; 1/; .3; 0/; .5; 3/ g. The black dots are the data points, and the white dots are their
estimated values predicted by the polynomial F.x/D1:2/NUL0:757xC0:214x2, the quadratic poly-
nomial that minimizes the sum of the squared errors. Each shaded line shows the error for one data
point.
AD/NUL
1x 1x2
1
1x 2x2
2
1x 3x2
3
1x 4x2
4
1x 5x2
5/SOH
D/NUL
1/NUL11
11 1
12 413 9
15 2 5/SOH
;
whose pseudoinverse is
ACD/NUL
0:500 0:300 0:200 0:100 /NUL0:100
/NUL0:388 0:093 0:190 0:193 /NUL0:088
0:060/NUL0:036/NUL0:048/NUL0:036 0:060/SOH
:
Multiplying ybyAC, we obtain the coefﬁcient vector
cD/NUL
1:200
/NUL0:757
0:214/SOH
;
which corresponds to the quadratic polynomial28.3 Symmetric positive-deﬁnite matrices and least-squares approximation 839
F.x/D1:200/NUL0:757xC0:214x2
as the closest-ﬁtting quadratic to the given data, in a least-squares sense.
As a practical matter, we solve the normal equation (28.19) by multiplying y
byATand then ﬁnding an LU decomposition of ATA.I fAhas full rank, the
matrix ATAis guaranteed to be nonsingular, because it is symmetric and positive-
deﬁnite. (See Exercise D.1-2 and Theorem D.6.)
Exercises
28.3-1
Prove that every diagonal element of a symmetric positive-deﬁnite matrix is posi-tive.
28.3-2
LetAD/DC2ab
bc/DC3
be a2/STX2symmetric positive-deﬁnite matrix. Prove that its
determinant ac/NULb
2is positive by “completing the square” in a manner similar to
that used in the proof of Lemma 28.5.
28.3-3
Prove that the maximum element in a symmetric positive-deﬁnite matrix lies onthe diagonal.
28.3-4
Prove that the determinant of each leading submatrix of a symmetric positive-
deﬁnite matrix is positive.
28.3-5
LetA
kdenote the kth leading submatrix of a symmetric positive-deﬁnite matrix A.
Prove that det .Ak/=det.Ak/NUL1/is the kth pivot during LU decomposition, where,
by convention, det .A0/D1.
28.3-6
Find the function of the form
F.x/Dc1Cc2xlgxCc3ex
that is the best least-squares ﬁt to the data points
.1; 1/; .2; 1/; .3; 3/; .4; 8/ :840 Chapter 28 Matrix Operations
28.3-7
Show that the pseudoinverse ACsatisﬁes the following four equations:
AACADA;
ACAACDAC;
.AAC/TDAAC;
.ACA/TDACA:
Problems
28-1 Tridiagonal systems of linear equations
Consider the tridiagonal matrix
ADˇ
1/NUL1000
/NUL12/NUL100
0/NUL12/NUL10
00/NUL12/NUL1
000/NUL12/CR
:
a.Find an LU decomposition of A.
b.Solve the equation AxD/NUL11111/SOHTby using forward and back sub-
stitution.
c.Find the inverse of A.
d.Show how, for any n/STXnsymmetric positive-deﬁnite, tridiagonal matrix Aand
anyn-vector b, to solve the equation AxDbinO.n/ time by performing an
LU decomposition. Argue that any method based on forming A/NUL1is asymptot-
ically more expensive in the worst case.
e.Show how, for any n/STXnnonsingular, tridiagonal matrix Aand any n-vector b,t o
solve the equation AxDbinO.n/ time by performing an LUP decomposition.
28-2 Splines
A practical method for interpolating a set of points with a curve is to use cu-
bic splines . We are given a set f.xi;yi/WiD0; 1; : : : ; ngofnC1point-value
pairs, where x0<x 1</SOH/SOH/SOH<x n. We wish to ﬁt a piecewise-cubic curve
(spline) f. x/ to the points. That is, the curve f. x/ is made up of ncubic polyno-
mials fi.x/DaiCbixCcix2Cdix3foriD0; 1; : : : ; n/NUL1, where if xfalls inProblems for Chapter 28 841
the range xi/DC4x/DC4xiC1, then the value of the curve is given by f. x/Dfi.x/NULxi/.
The points xiat which the cubic polynomials are “pasted” together are called knots .
For simplicity, we shall assume that xiDiforiD0; 1; : : : ; n .
To ensure continuity of f. x/ , we require that
f. x i/Dfi.0/Dyi;
f. x iC1/Dfi.1/DyiC1
foriD0; 1; : : : ; n/NUL1. To ensure that f. x/ is sufﬁciently smooth, we also insist
that the ﬁrst derivative be continuous at each knot:
f0.xiC1/Df0
i.1/Df0
iC1.0/
foriD0; 1; : : : ; n/NUL2.
a.Suppose that for iD0; 1; : : : ; n , we are given not only the point-value pairs
f.xi;yi/gbut also the ﬁrst derivatives DiDf0.xi/at each knot. Express each
coefﬁcient ai,bi,ci,a n d diin terms of the values yi,yiC1,Di,a n d DiC1.
(Remember that xiDi.) How quickly can we compute the 4ncoefﬁcients
from the point-value pairs and ﬁrst derivatives?
The question remains of how to choose the ﬁrst derivatives of f. x/ at the knots.
One method is to require the second derivatives to be continuous at the knots:
f00.xiC1/Df00
i.1/Df00
iC1.0/
foriD0; 1; : : : ; n/NUL2. At the ﬁrst and last knots, we assume that f00.x0/D
f00
0.0/D0andf00.xn/Df00
n/NUL1.1/D0; these assumptions make f. x/ anatural
cubic spline.
b.Use the continuity constraints on the second derivative to show that for iD
1 ;2;:::;n/NUL1,
Di/NUL1C4DiCDiC1D3.y iC1/NULyi/NUL1/: (28.21)
c.Show that
2D0CD1D3.y 1/NULy0/; (28.22)
Dn/NUL1C2DnD3.y n/NULyn/NUL1/: (28.23)
d.Rewrite equations (28.21)–(28.23) as a matrix equation involving the vector
DDhD0;D1;:::;D niof unknowns. What attributes does the matrix in your
equation have?
e.Argue that a natural cubic spline can interpolate a set of nC1point-value pairs
inO.n/ time (see Problem 28-1).842 Chapter 28 Matrix Operations
f.Show how to determine a natural cubic spline that interpolates a set of nC1
points .xi;yi/satisfying x0<x 1</SOH/SOH/SOH<x n,e v e nw h e n xiis not necessarily
equal to i. What matrix equation must your method solve, and how quickly
does your algorithm run?
Chapter notes
Many excellent texts describe numerical and scientiﬁc computation in much greaterdetail than we have room for here. The following are especially readable: Georgeand Liu [132], Golub and Van Loan [144], Press, Teukolsky, Vetterling, and Flan-nery [283, 284], and Strang [323, 324].
Golub and Van Loan [144] discuss numerical stability. They show why det .A/
is not necessarily a good indicator of the stability of a matrix A, proposing instead
to usekAk
1kA/NUL1k1,w h e r ekAk1Dmax 1/DC4i/DC4nPn
jD1jaijj. They also address
the question of how to compute this value without actually computing A/NUL1.
Gaussian elimination, upon which the LU and LUP decompositions are based,
was the ﬁrst systematic method for solving linear systems of equations. It was alsoone of the earliest numerical algorithms. Although it was known earlier, its dis-covery is commonly attributed to C. F. Gauss (1777–1855). In his famous paper
[325], Strassen showed that an n/STXnmatrix can be inverted in O.n
lg7/time. Wino-
grad [358] originally proved that matrix multiplication is no harder than matrixinversion, and the converse is due to Aho, Hopcroft, and Ullman [5].
Another important matrix decomposition is the singular value decomposition ,
orSVD . The SVD factors an m/STXnmatrix AintoADQ
1†QT
2,w h e r e †is an
m/STXnmatrix with nonzero values only on the diagonal, Q1ism/STXmwith mutually
orthonormal columns, and Q2isn/STXn, also with mutually orthonormal columns.
Two vectors are orthonormal if their inner product is 0and each vector has a norm
of1. The books by Strang [323, 324] and Golub and Van Loan [144] contain good
treatments of the SVD.
Strang [324] has an excellent presentation of symmetric positive-deﬁnite matri-
ces and of linear algebra in general.29 Linear Programming
Many problems take the form of maximizing or minimizing an objective, given
limited resources and competing constraints. If we can specify the objective asa linear function of certain variables, and if we can specify the constraints onresources as equalities or inequalities on those variables, then we have a linear-
programming problem . Linear programs arise in a variety of practical applica-
tions. We begin by studying an application in electoral politics.
A political problem
Suppose that you are a politician trying to win an election. Your district has three
different types of areas—urban, suburban, and rural. These areas have, respec-tively, 100,000, 200,000, and 50,000 registered voters. Although not all the reg-istered voters actually go to the polls, you decide that to govern effectively, youwould like at least half the registered voters in each of the three regions to vote foryou. You are honorable and would never consider supporting policies in which youdo not believe. You realize, however, that certain issues may be more effective inwinning votes in certain places. Your primary issues are building more roads, gun
control, farm subsidies, and a gasoline tax dedicated to improved public transit.According to your campaign staff’s research, you can estimate how many votes
you win or lose from each population segment by spending $1,000 on advertising
on each issue. This information appears in the table of Figure 29.1. In this table,each entry indicates the number of thousands of either urban, suburban, or ruralvoters who would be won over by spending $1,000 on advertising in support of aparticular issue. Negative entries denote votes that would be lost. Your task is toﬁgure out the minimum amount of money that you need to spend in order to win50,000 urban votes, 100,000 suburban votes, and 25,000 rural votes.
You could, by trial and error, devise a strategy that wins the required number
of votes, but the strategy you come up with might not be the least expensive one.For example, you could devote $20,000 of advertising to building roads, $0 to guncontrol, $4,000 to farm subsidies, and $9,000 to a gasoline tax. In this case, you844 Chapter 29 Linear Programming
policy
 urban suburban rural
build roads
 /NUL2 53
gun control
 82/NUL5
farm subsidies
 00 1 0
gasoline tax
 10 0 /NUL2
Figure 29.1 The effects of policies on voters. Each entry describes the number of thousands of
urban, suburban, or rural voters who could be won over by spending $1,000 on advertising support
of a policy on a particular issue. Negative entries denote votes that would be lost.
would win 20./NUL2/C0.8/C4.0/C9.10/D50thousand urban votes, 20.5/C0.2/C
4.0/C9.0/D100thousand suburban votes, and 20.3/C0./NUL5/C4.10/C9./NUL2/D
82thousand rural votes. You would win the exact number of votes desired in the
urban and suburban areas and more than enough votes in the rural area. (In fact,in the rural area, you would receive more votes than there are voters.) In order togarner these votes, you would have paid for 20C0C4C9D33thousand dollars
of advertising.
Naturally, you may wonder whether this strategy is the best possible. That is,
could you achieve your goals while spending less on advertising? Additional trial
and error might help you to answer this question, but wouldn’t you rather have a
systematic method for answering such questions? In order to develop one, we shall
formulate this question mathematically. We introduce 4variables:
/SIx1is the number of thousands of dollars spent on advertising on building roads,
/SIx2is the number of thousands of dollars spent on advertising on gun control,
/SIx3is the number of thousands of dollars spent on advertising on farm subsidies,
and
/SIx4is the number of thousands of dollars spent on advertising on a gasoline tax.
We can write the requirement that we win at least 50,000 urban votes as
/NUL2x1C8x2C0x3C10x 4/NAK50 : (29.1)
Similarly, we can write the requirements that we win at least 100,000 suburban
votes and 25,000 rural votes as
5x1C2x2C0x3C0x4/NAK100 (29.2)
and3x
1/NUL5x2C10x 3/NUL2x4/NAK25 : (29.3)
Any setting of the variables x1;x2;x3;x4that satisﬁes inequalities (29.1)–(29.3)
yields a strategy that wins a sufﬁcient number of each type of vote. In order toChapter 29 Linear Programming 845
keep costs as small as possible, you would like to minimize the amount spent on
advertising. That is, you want to minimize the expression
x1Cx2Cx3Cx4: (29.4)
Although negative advertising often occurs in political campaigns, there is no such
thing as negative-cost advertising. Consequently, we require that
x1/NAK0; x 2/NAK0; x 3/NAK0;andx4/NAK0: (29.5)
Combining inequalities (29.1)–(29.3) and (29.5) with the objective of minimiz-
ing (29.4), we obtain what is known as a “linear program.” We format this problemas
minimize x
1C x2C x3C x4 (29.6)
subject to
/NUL2x1C8x2C 0x3C10x 4/NAK 50 (29.7)
5x1C2x2C 0x3C 0x4/NAK100 (29.8)
3x1/NUL5x2C10x 3/NUL 2x4/NAK 25 (29.9)
x1;x2;x3;x4 /NAK 0: (29.10)
The solution of this linear program yields your optimal strategy.
General linear programs
In the general linear-programming problem, we wish to optimize a linear function
subject to a set of linear inequalities. Given a set of real numbers a1;a2;:::;a nand
a set of variables x1;x2;:::;x n,w ed e ﬁ n ea linear function fon those variables
by
f. x 1;x2;:::;x n/Da1x1Ca2x2C/SOH/SOH/SOHC anxnDnX
jD1ajxj:
Ifbis a real number and fis a linear function, then the equation
f. x 1;x2;:::;x n/Db
is alinear equality and the inequalities
f. x 1;x2;:::;x n//DC4b
and
f. x 1;x2;:::;x n//NAKb846 Chapter 29 Linear Programming
arelinear inequalities . We use the general term linear constraints to denote either
linear equalities or linear inequalities. In linear programming, we do not allowstrict inequalities. Formally, a linear-programming problem is the problem of
either minimizing or maximizing a linear function subject to a ﬁnite set of linearconstraints. If we are to minimize, then we call the linear program a minimization
linear program , and if we are to maximize, then we call the linear program a
maximization linear program .
The remainder of this chapter covers how to formulate and solve linear pro-
grams. Although several polynomial-time algorithms for linear programming have
been developed, we will not study them in this chapter. Instead, we shall study thesimplex algorithm, which is the oldest linear-programming algorithm. The simplexalgorithm does not run in polynomial time in the worst case, but it is fairly efﬁcientand widely used in practice.
An overview of linear programming
In order to describe properties of and algorithms for linear programs, we ﬁnd it
convenient to express them in canonical forms. We shall use two forms, standard
andslack , in this chapter. We will deﬁne them precisely in Section 29.1. Infor-
mally, a linear program in standard form is the maximization of a linear function
subject to linear inequalities , whereas a linear program in slack form is the max-
imization of a linear function subject to linear equalities . We shall typically use
standard form for expressing linear programs, but we ﬁnd it more convenient touse slack form when we describe the details of the simplex algorithm. For now, werestrict our attention to maximizing a linear function on nvariables subject to a set
ofmlinear inequalities.
Let us ﬁrst consider the following linear program with two variables:
maximize x
1C x2 (29.11)
subject to
4x1/NUL x2/DC4 8 (29.12)
2x1C x2/DC410 (29.13)
5x1/NUL2x2/NAK/NUL 2 (29.14)
x1;x2/NAK 0: (29.15)
We call any setting of the variables x1andx2that satisﬁes all the constraints
(29.12)–(29.15) a feasible solution to the linear program. If we graph the con-
straints in the .x1;x2/-Cartesian coordinate system, as in Figure 29.2(a), we seeChapter 29 Linear Programming 847
4x1 – x2≤ 8
2x1 + 
x2≤ 10x2
x1x2≥ 0x1≥ 05x1 – 2x2≥ –2
(a)x2
x1
(b)x1 + x2 = 0x1 + x2 = 4x1 + x2 = 8
Figure 29.2 (a) The linear program given in (29.12)–(29.15). Each constraint is represented by
a line and a direction. The intersection of the constraints, which is the feasible region, is shaded.
(b)The dotted lines show, respectively, the points for which the objective value is 0,4,a n d 8.T h e
optimal solution to the linear program is x1D2andx2D6with objective value 8.
that the set of feasible solutions (shaded in the ﬁgure) forms a convex region1in
the two-dimensional space. We call this convex region the feasible region and the
function we wish to maximize the objective function . Conceptually, we could eval-
uate the objective function x1Cx2at each point in the feasible region; we call the
value of the objective function at a particular point the objective value . We could
then identify a point that has the maximum objective value as an optimal solution.For this example (and for most linear programs), the feasible region contains aninﬁnite number of points, and so we need to determine an efﬁcient way to ﬁnd apoint that achieves the maximum objective value without explicitly evaluating theobjective function at every point in the feasible region.
In two dimensions, we can optimize via a graphical procedure. The set of points
for which x
1Cx2D´,f o ra n y ´, is a line with a slope of /NUL1.I fw ep l o t x1Cx2D0,
we obtain the line with slope /NUL1through the origin, as in Figure 29.2(b). The
intersection of this line and the feasible region is the set of feasible solutions thathave an objective value of 0. In this case, that intersection of the line with the
feasible region is the single point .0; 0/ . More generally, for any ´, the intersection
1An intuitive deﬁnition of a convex region is that it fulﬁlls the requirement that for any two points in
the region, all points on a line segment between them are also in the region.848 Chapter 29 Linear Programming
of the line x1Cx2D´and the feasible region is the set of feasible solutions that
have objective value ´. Figure 29.2(b) shows the lines x1Cx2D0,x1Cx2D4,
andx1Cx2D8. Because the feasible region in Figure 29.2 is bounded, there
must be some maximum value ´for which the intersection of the line x1Cx2D´
and the feasible region is nonempty. Any point at which this occurs is an optimalsolution to the linear program, which in this case is the point x
1D2andx2D6
with objective value 8.
It is no accident that an optimal solution to the linear program occurs at a vertex
of the feasible region. The maximum value of ´for which the line x1Cx2D´
intersects the feasible region must be on the boundary of the feasible region, andthus the intersection of this line with the boundary of the feasible region is either asingle vertex or a line segment. If the intersection is a single vertex, then there isjust one optimal solution, and it is that vertex. If the intersection is a line segment,every point on that line segment must have the same objective value; in particular,both endpoints of the line segment are optimal solutions. Since each endpoint of aline segment is a vertex, there is an optimal solution at a vertex in this case as well.
Although we cannot easily graph linear programs with more than two variables,
the same intuition holds. If we have three variables, then each constraint corre-
sponds to a half-space in three-dimensional space. The intersection of these half-spaces forms the feasible region. The set of points for which the objective functionobtains a given value ´is now a plane (assuming no degenerate conditions). If all
coefﬁcients of the objective function are nonnegative, and if the origin is a feasible
solution to the linear program, then as we move this plane away from the origin, in
a direction normal to the objective function, we ﬁnd points of increasing objectivevalue. (If the origin is not feasible or if some coefﬁcients in the objective functionare negative, the intuitive picture becomes slightly more complicated.) As in twodimensions, because the feasible region is convex, the set of points that achievethe optimal objective value must include a vertex of the feasible region. Simi-larly, if we have nvariables, each constraint deﬁnes a half-space in n-dimensional
space. We call the feasible region formed by the intersection of these half-spaces a
simplex . The objective function is now a hyperplane and, because of convexity, an
optimal solution still occurs at a vertex of the simplex.
Thesimplex algorithm takes as input a linear program and returns an optimal
solution. It starts at some vertex of the simplex and performs a sequence of itera-tions. In each iteration, it moves along an edge of the simplex from a current vertexto a neighboring vertex whose objective value is no smaller than that of the current
vertex (and usually is larger.) The simplex algorithm terminates when it reaches
a local maximum, which is a vertex from which all neighboring vertices have asmaller objective value. Because the feasible region is convex and the objectivefunction is linear, this local optimum is actually a global optimum. In Section 29.4,Chapter 29 Linear Programming 849
we shall use a concept called “duality” to show that the solution returned by the
simplex algorithm is indeed optimal.
Although the geometric view gives a good intuitive view of the operations of the
simplex algorithm, we shall not refer to it explicitly when developing the detailsof the simplex algorithm in Section 29.3. Instead, we take an algebraic view. Weﬁrst write the given linear program in slack form, which is a set of linear equalities.These linear equalities express some of the variables, called “basic variables,” interms of other variables, called “nonbasic variables.” We move from one vertex
to another by making a basic variable become nonbasic and making a nonbasic
variable become basic. We call this operation a “pivot” and, viewed algebraically,it is nothing more than rewriting the linear program in an equivalent slack form.
The two-variable example described above was particularly simple. We shall
need to address several more details in this chapter. These issues include iden-tifying linear programs that have no solutions, linear programs that have no ﬁniteoptimal solution, and linear programs for which the origin is not a feasible solution.
Applications of linear programming
Linear programming has a large number of applications. Any textbook on opera-
tions research is ﬁlled with examples of linear programming, and linear program-
ming has become a standard tool taught to students in most business schools. The
election scenario is one typical example. Two more examples of linear program-
ming are the following:
/SIAn airline wishes to schedule its ﬂight crews. The Federal Aviation Adminis-tration imposes many constraints, such as limiting the number of consecutivehours that each crew member can work and insisting that a particular crew workonly on one model of aircraft during each month. The airline wants to schedulecrews on all of its ﬂights using as few crew members as possible.
/SIAn oil company wants to decide where to drill for oil. Siting a drill at a particu-lar location has an associated cost and, based on geological surveys, an expectedpayoff of some number of barrels of oil. The company has a limited budget forlocating new drills and wants to maximize the amount of oil it expects to ﬁnd,given this budget.
With linear programs, we also model and solve graph and combinatorial prob-
lems, such as those appearing in this textbook. We have already seen a specialcase of linear programming used to solve systems of difference constraints in Sec-tion 24.4. In Section 29.2, we shall study how to formulate several graph andnetwork-ﬂow problems as linear programs. In Section 35.4, we shall use linearprogramming as a tool to ﬁnd an approximate solution to another graph problem.850 Chapter 29 Linear Programming
Algorithms for linear programming
This chapter studies the simplex algorithm. This algorithm, when implemented
carefully, often solves general linear programs quickly in practice. With somecarefully contrived inputs, however, the simplex algorithm can require exponentialtime. The ﬁrst polynomial-time algorithm for linear programming was the ellipsoid
algorithm , which runs slowly in practice. A second class of polynomial-time algo-
rithms are known as interior-point methods . In contrast to the simplex algorithm,
which moves along the exterior of the feasible region and maintains a feasible solu-tion that is a vertex of the simplex at each iteration, these algorithms move throughthe interior of the feasible region. The intermediate solutions, while feasible, arenot necessarily vertices of the simplex, but the ﬁnal solution is a vertex. For largeinputs, interior-point algorithms can run as fast as, and sometimes faster than, thesimplex algorithm. The chapter notes point you to more information about thesealgorithms.
If we add to a linear program the additional requirement that all variables take
on integer values, we have an integer linear program . Exercise 34.5-3 asks you
to show that just ﬁnding a feasible solution to this problem is NP-hard; sinceno polynomial-time algorithms are known for any NP-hard problems, there is noknown polynomial-time algorithm for integer linear programming. In contrast, wecan solve a general linear-programming problem in polynomial time.
In this chapter, if we have a linear program with variables xD.x
1;x2;:::;x n/
and wish to refer to a particular setting of the variables, we shall use the notation
NxD.Nx1;Nx2;:::;Nxn/.
29.1 Standard and slack forms
This section describes two formats, standard form and slack form, that are use-
ful when we specify and work with linear programs. In standard form, all theconstraints are inequalities, whereas in slack form, all constraints are equalities(except for those that require the variables to be nonnegative).
Standard form
Instandard form ,w ea r eg i v e n nreal numbers c
1;c2;:::;c n;mreal numbers
b1;b2;:::;b m;a n d mnreal numbers aijforiD1 ;2;:::;m andjD1 ;2;:::;n .
We wish to ﬁnd nreal numbers x1;x2;:::;x nthat29.1 Standard and slack forms 851
maximizenX
jD1cjxj (29.16)
subject to
nX
jD1aijxj/DC4biforiD1 ;2;:::;m (29.17)
xj/NAK0forjD1 ;2;:::;n: (29.18)
Generalizing the terminology we introduced for the two-variable linear program,
we call expression (29.16) the objective function and the nCminequalities in
lines (29.17) and (29.18) the constraints .T h e nconstraints in line (29.18) are the
nonnegativity constraints . An arbitrary linear program need not have nonnegativ-
ity constraints, but standard form requires them. Sometimes we ﬁnd it convenient
to express a linear program in a more compact form. If we create an m/STXnmatrix
AD.aij/,a nm-vector bD.bi/,a nn-vector cD.cj/,a n da n n-vector xD.xj/,
then we can rewrite the linear program deﬁned in (29.16)–(29.18) as
maximize cTx (29.19)
subject to
Ax/DC4b (29.20)
x/NAK0: (29.21)
In line (29.19), cTxis the inner product of two vectors. In inequality (29.20), Ax
is a matrix-vector product, and in inequality (29.21), x/NAK0means that each entry
of the vector xmust be nonnegative. We see that we can specify a linear program
in standard form by a tuple . A ;b;c/ , and we shall adopt the convention that A,b,
andcalways have the dimensions given above.
We now introduce terminology to describe solutions to linear programs. We used
some of this terminology in the earlier example of a two-variable linear program.
We call a setting of the variables Nxthat satisﬁes all the constraints a feasible solu-
tion, whereas a setting of the variables Nxthat fails to satisfy at least one constraint
is aninfeasible solution . We say that a solution Nxhasobjective value cTNx. A fea-
sible solutionNxwhose objective value is maximum over all feasible solutions is an
optimal solution , and we call its objective value cTNxtheoptimal objective value .
If a linear program has no feasible solutions, we say that the linear program is in-
feasible ; otherwise it is feasible . If a linear program has some feasible solutions
but does not have a ﬁnite optimal objective value, we say that the linear program
isunbounded . Exercise 29.1-9 asks you to show that a linear program can have a
ﬁnite optimal objective value even if the feasible region is not bounded.852 Chapter 29 Linear Programming
Converting linear programs into standard form
It is always possible to convert a linear program, given as minimizing or maxi-
mizing a linear function subject to linear constraints, into standard form. A linearprogram might not be in standard form for any of four possible reasons:
1. The objective function might be a minimization rather than a maximization.
2. There might be variables without nonnegativity constraints.3. There might be equality constraints , which have an equal sign rather than a
less-than-or-equal-to sign.
4. There might be inequality constraints , but instead of having a less-than-or-
equal-to sign, they have a greater-than-or-equal-to sign.
When converting one linear program Linto another linear program L
0, we would
like the property that an optimal solution to L0yields an optimal solution to L.T o
capture this idea, we say that two maximization linear programs LandL0are
equivalent if for each feasible solution NxtoLwith objective value ´, there is
a corresponding feasible solution Nx0toL0with objective value ´, and for each
feasible solutionNx0toL0with objective value ´, there is a corresponding feasible
solutionNxtoLwith objective value ´. (This deﬁnition does not imply a one-to-
one correspondence between feasible solutions.) A minimization linear program L
and a maximization linear program L0are equivalent if for each feasible solution Nx
toLwith objective value ´, there is a corresponding feasible solution Nx0toL0with
objective value/NUL´, and for each feasible solution Nx0toL0with objective value ´,
there is a corresponding feasible solution NxtoLwith objective value /NUL´.
We now show how to remove, one by one, each of the possible problems in the
list above. After removing each one, we shall argue that the new linear program isequivalent to the old one.
To convert a minimization linear program Linto an equivalent maximization lin-
ear program L
0, we simply negate the coefﬁcients in the objective function. Since
LandL0have identical sets of feasible solutions and, for any feasible solution, the
objective value in Lis the negative of the objective value in L0, these two linear
programs are equivalent. For example, if we have the linear program
minimize/NUL2x1C3x2
subject to
x1C x2D7
x1/NUL2x2/DC44
x1/NAK0;
and we negate the coefﬁcients of the objective function, we obtain29.1 Standard and slack forms 853
maximize 2x1/NUL3x2
subject to
x1C x2D7
x1/NUL2x2/DC44
x1/NAK0:
Next, we show how to convert a linear program in which some of the variables
do not have nonnegativity constraints into one in which each variable has a non-negativity constraint. Suppose that some variable x
jdoes not have a nonnegativity
constraint. Then, we replace each occurrence of xjbyx0
j/NULx00
j, and add the non-
negativity constraints x0
j/NAK0andx00
j/NAK0. Thus, if the objective function has a
term cjxj, we replace it by cjx0
j/NULcjx00
j, and if constraint ih a sat e r m aijxj,w e
replace it by aijx0
j/NULaijx00
j. Any feasible solution yxto the new linear program cor-
responds to a feasible solution Nxto the original linear program with NxjDyx0
j/NULyx00
j
and with the same objective value. Also, any feasible solution Nxto the original
linear program corresponds to a feasible solution yxto the new linear program with
yx0
jDNxjandyx00
jD0ifNxj/NAK0, or withyx00
jDNxjandyx0
jD0ifNxj<0.T h e t w o
linear programs have the same objective value regardless of the sign of Nxj. Thus,
the two linear programs are equivalent. We apply this conversion scheme to eachvariable that does not have a nonnegativity constraint to yield an equivalent linearprogram in which all variables have nonnegativity constraints.
Continuing the example, we want to ensure that each variable has a correspond-
ing nonnegativity constraint. Variable x
1has such a constraint, but variable x2does
not. Therefore, we replace x2by two variables x0
2andx00
2, and we modify the linear
program to obtain
maximize 2x1/NUL3x0
2C3x00
2
subject to
x1C x0
2/NUL x00
2D7 (29.22)
x1/NUL2x0
2C2x00
2/DC44
x1;x0
2;x00
2/NAK0:
Next, we convert equality constraints into inequality constraints. Suppose that a
linear program has an equality constraint f. x 1;x2;:::;x n/Db.S i n c e xDyif
and only if both x/NAKyandx/DC4y, we can replace this equality constraint by the
pair of inequality constraints f. x 1;x2;:::;x n//DC4bandf. x 1;x2;:::;x n//NAKb.
Repeating this conversion for each equality constraint yields a linear program inwhich all constraints are inequalities.
Finally, we can convert the greater-than-or-equal-to constraints to less-than-or-
equal-to constraints by multiplying these constraints through by /NUL1. That is, any
inequality of the form854 Chapter 29 Linear Programming
nX
jD1aijxj/NAKbi
is equivalent to
nX
jD1/NULaijxj/DC4/NULbi:
Thus, by replacing each coefﬁcient aijby/NULaijand each value biby/NULbi, we obtain
an equivalent less-than-or-equal-to constraint.
Finishing our example, we replace the equality in constraint (29.22) by two in-
equalities, obtaining
maximize 2x1/NUL3x0
2C3x00
2
subject to
x1C x0
2/NUL x00
2/DC47
x1C x0
2/NUL x00
2/NAK7 (29.23)
x1/NUL2x0
2C2x00
2/DC44
x1;x0
2;x00
2/NAK0:
Finally, we negate constraint (29.23). For consistency in variable names, we re-
name x0
2tox2andx00
2tox3, obtaining the standard form
maximize 2x1/NUL3x2C3x3 (29.24)
subject to
x1C x2/NUL x3/DC4 7 (29.25)
/NULx1/NUL x2C x3/DC4/NUL 7 (29.26)
x1/NUL2x2C2x3/DC4 4 (29.27)
x1;x2;x3/NAK 0: (29.28)
Converting linear programs into slack form
To efﬁciently solve a linear program with the simplex algorithm, we prefer to ex-
press it in a form in which some of the constraints are equality constraints. More
precisely, we shall convert it into a form in which the nonnegativity constraints arethe only inequality constraints, and the remaining constraints are equalities. Let
nX
jD1aijxj/DC4bi (29.29)29.1 Standard and slack forms 855
be an inequality constraint. We introduce a new variable sand rewrite inequal-
ity (29.29) as the two constraints
sDbi/NULnX
jD1aijxj; (29.30)
s/NAK0: (29.31)
We call saslack variable because it measures the slack , or difference, between
the left-hand and right-hand sides of equation (29.29). (We shall soon see why weﬁnd it convenient to write the constraint with only the slack variable on the left-hand side.) Because inequality (29.29) is true if and only if both equation (29.30)and inequality (29.31) are true, we can convert each inequality constraint of a lin-ear program in this way to obtain an equivalent linear program in which the onlyinequality constraints are the nonnegativity constraints. When converting fromstandard to slack form, we shall use x
nCi(instead of s) to denote the slack variable
associated with the ith inequality. The ith constraint is therefore
xnCiDbi/NULnX
jD1aijxj; (29.32)
along with the nonnegativity constraint xnCi/NAK0.
By converting each constraint of a linear program in standard form, we obtain a
linear program in a different form. For example, for the linear program described
in (29.24)–(29.28), we introduce slack variables x4,x5,a n d x6, obtaining
maximize 2x1/NUL3x2C3x3 (29.33)
subject to
x4D 7/NUL x1/NUL x2C x3 (29.34)
x5D/NUL 7C x1C x2/NUL x3 (29.35)
x6D 4/NUL x1C2x2/NUL2x3 (29.36)
x1;x2;x3;x4;x5;x6/NAK 0: (29.37)
In this linear program, all the constraints except for the nonnegativity constraints
are equalities, and each variable is subject to a nonnegativity constraint. We writeeach equality constraint with one of the variables on the left-hand side of the equal-ity and all others on the right-hand side. Furthermore, each equation has the sameset of variables on the right-hand side, and these variables are also the only onesthat appear in the objective function. We call the variables on the left-hand side ofthe equalities basic variables and those on the right-hand side nonbasic variables .
For linear programs that satisfy these conditions, we shall sometimes omit the
words “maximize” and “subject to,” as well as the explicit nonnegativity con-straints. We shall also use the variable ´to denote the value of the objective func-856 Chapter 29 Linear Programming
tion. We call the resulting format slack form . If we write the linear program given
in (29.33)–(29.37) in slack form, we obtain
´D 2x1/NUL3x2C3x3 (29.38)
x4D 7/NUL x1/NUL x2C x3 (29.39)
x5D/NUL 7C x1C x2/NUL x3 (29.40)
x6D 4/NUL x1C2x2/NUL2x3: (29.41)
As with standard form, we ﬁnd it convenient to have a more concise notation
for describing a slack form. As we shall see in Section 29.3, the sets of basic andnonbasic variables will change as the simplex algorithm runs. We use Nto denote
the set of indices of the nonbasic variables and Bto denote the set of indices of
the basic variables. We always have that jNjDn,jBjDm,a n d N[BD
f1 ;2;:::;nCmg. The equations are indexed by the entries of B, and the variables
on the right-hand sides are indexed by the entries of N. As in standard form, we use
b
i,cj,a n d aijto denote constant terms and coefﬁcients. We also use /ETBto denote
an optional constant term in the objective function. (We shall see a little later that
including the constant term in the objective function makes it easy to determine the
value of the objective function.) Thus we can concisely deﬁne a slack form by a
tuple . N;B;A ;b;c;/ETB/ , denoting the slack form
´D/ETBCX
j2Ncjxj (29.42)
xiDbi/NULX
j2Naijxjfori2B; (29.43)
in which all variables xare constrained to be nonnegative. Because we subtract
the sumP
j2Naijxjin (29.43), the values aijare actually the negatives of the
coefﬁcients as they “appear” in the slack form.
For example, in the slack form
´D28/NULx3
6/NULx5
6/NUL2x6
3
x1D 8Cx3
6Cx5
6/NULx6
3
x2D 4/NUL8x3
3/NUL2x5
3Cx6
3
x4D18/NULx3
2Cx5
2;
we have BDf1; 2; 4g,NDf3; 5; 6g,29.1 Standard and slack forms 857
AD/NUL
a13a15a16
a23a25a26
a43a45a46/SOH
D/NUL
/NUL1=6/NUL1=6 1=3
8=3 2=3/NUL1=3
1=2/NUL1=2 0/SOH
;
bD/NUL
b1
b2
b4/SOH
D/NUL
8
4
18/SOH
;
cD/NULc3c5c6/SOHTD/NUL/NUL1=6/NUL1=6/NUL2=3/SOHT,a n d /ETBD28. Note that the
indices into A,b,a n d care not necessarily sets of contiguous integers; they depend
on the index sets BandN. As an example of the entries of Abeing the negatives
of the coefﬁcients as they appear in the slack form, observe that the equation for x1
includes the term x3=6, yet the coefﬁcient a13is actually/NUL1=6rather thanC1=6.
Exercises
29.1-1
If we express the linear program in (29.24)–(29.28) in the compact notation of(29.19)–(29.21), what are n,m,A,b,a n d c?
29.1-2
Give three feasible solutions to the linear program in (29.24)–(29.28). What is theobjective value of each one?
29.1-3
For the slack form in (29.38)–(29.41), what are N,B,A,b,c,a n d /ETB?
29.1-4
Convert the following linear program into standard form:
minimize 2x
1C7x2Cx3
subject to
x1/NULx3D 7
3x1C x2/NAK24
x2 /NAK 0
x3/DC4 0:858 Chapter 29 Linear Programming
29.1-5
Convert the following linear program into slack form:
maximize 2x1/NUL6x3
subject to
x1C x2/NUL x3/DC47
3x1/NUL x2/NAK8
/NULx1C2x2C2x3/NAK0
x1;x2;x3/NAK0:
What are the basic and nonbasic variables?
29.1-6
Show that the following linear program is infeasible:
maximize 3x1/NUL2x2
subject to
x1C x2/DC4 2
/NUL2x1/NUL2x2/DC4/NUL 10
x1;x2/NAK 0:
29.1-7
Show that the following linear program is unbounded:
maximize x1/NUL x2
subject to
/NUL2x1C x2/DC4/NUL 1
/NULx1/NUL2x2/DC4/NUL 2
x1;x2/NAK 0:
29.1-8
Suppose that we have a general linear program with nvariables and mconstraints,
and suppose that we convert it into standard form. Give an upper bound on thenumber of variables and constraints in the resulting linear program.
29.1-9
Give an example of a linear program for which the feasible region is not bounded,
but the optimal objective value is ﬁnite.29.2 Formulating problems as linear programs 859
29.2 Formulating problems as linear programs
Although we shall focus on the simplex algorithm in this chapter, it is also impor-
tant to be able to recognize when we can formulate a problem as a linear program.Once we cast a problem as a polynomial-sized linear program, we can solve itin polynomial time by the ellipsoid algorithm or interior-point methods. Several
linear-programming software packages can solve problems efﬁciently, so that once
the problem is in the form of a linear program, such a package can solve it.
We shall look at several concrete examples of linear-programming problems. We
start with two problems that we have already studied: the single-source shortest-paths problem (see Chapter 24) and the maximum-ﬂow problem (see Chapter 26).We then describe the minimum-cost-ﬂow problem. Although the minimum-cost-ﬂow problem has a polynomial-time algorithm that is not based on linear program-ming, we won’t describe the algorithm. Finally, we describe the multicommodity-ﬂow problem, for which the only known polynomial-time algorithm is based onlinear programming.
When we solved graph problems in Part VI, we used attribute notation, such
as/ETB:dand.u; /ETB/: f. Linear programs typically use subscripted variables rather
than objects with attached attributes, however. Therefore, when we express vari-
ables in linear programs, we shall indicate vertices and edges through subscripts.
For example, we denote the shortest-path weight for vertex /ETBnot by /ETB:dbut by d
/ETB.
Similarly, we denote the ﬂow from vertex uto vertex /ETBnot by .u; /ETB/: fbut by fu/ETB.
For quantities that are given as inputs to problems, such as edge weights or capac-ities, we shall continue to use notations such as w.u;/ETB/ andc.u:/ETB/ .
Shortest paths
We can formulate the single-source shortest-paths problem as a linear program.
In this section, we shall focus on how to formulate the single-pair shortest-pathproblem, leaving the extension to the more general single-source shortest-pathsproblem as Exercise 29.2-3.
In the single-pair shortest-path problem, we are given a weighted, directed graph
GD.V; E/ , with weight function wWE! Rmapping edges to real-valued
weights, a source vertex s, and destination vertex t. We wish to compute the
value d
t, which is the weight of a shortest path from stot. To express this prob-
lem as a linear program, we need to determine a set of variables and constraints thatdeﬁne when we have a shortest path from stot. Fortunately, the Bellman-Ford al-
gorithm does exactly this. When the Bellman-Ford algorithm terminates, it hascomputed, for each vertex /ETB,av a l u e d
/ETB(using subscript notation here rather than
attribute notation) such that for each edge .u; /ETB/2E,w eh a v e d/ETB/DC4duCw.u;/ETB/ .860 Chapter 29 Linear Programming
The source vertex initially receives a value dsD0, which never changes. Thus
we obtain the following linear program to compute the shortest-path weight from s
tot:
maximize dt (29.44)
subject to
d/ETB/DC4duCw.u;/ETB/ for each edge .u; /ETB/2E; (29.45)
dsD0: (29.46)
You might be surprised that this linear program maximizes an objective function
when it is supposed to compute shortest paths. We do not want to minimize theobjective function, since then setting Nd
/ETBD0for all /ETB2Vwould yield an optimal
solution to the linear program without solving the shortest-paths problem. We
maximize because an optimal solution to the shortest-paths problem sets each Nd/ETB
to min uW.u;/ETB/ 2E˚NduCw.u;/ETB//TAB
,s ot h a tNd/ETBis the largest value that is less than or
equal to all of the values in the set˚NduCw.u;/ETB//TAB
. We want to maximize d/ETB
for all vertices /ETBon a shortest path from stotsubject to these constraints on all
vertices /ETB, and maximizing dtachieves this goal.
This linear program has jVjvariables d/ETB, one for each vertex /ETB2V.I t a l s o
hasjEjC1constraints: one for each edge, plus the additional constraint that the
source vertex’s shortest-path weight always has the value 0.
Maximum ﬂow
Next, we express the maximum-ﬂow problem as a linear program. Recall that we
are given a directed graph GD.V; E/ in which each edge .u; /ETB/2Ehas a
nonnegative capacity c.u;/ETB//NAK0, and two distinguished vertices: a source sand
as i n k t. As deﬁned in Section 26.1, a ﬂow is a nonnegative real-valued function
fWV/STXV!Rthat satisﬁes the capacity constraint and ﬂow conservation. A
maximum ﬂow is a ﬂow that satisﬁes these constraints and maximizes the ﬂowvalue, which is the total ﬂow coming out of the source minus the total ﬂow into thesource. A ﬂow, therefore, satisﬁes linear constraints, and the value of a ﬂow is alinear function. Recalling also that we assume that c.u;/ETB/D0if.u; /ETB/62Eand
that there are no antiparallel edges, we can express the maximum-ﬂow problem asa linear program:
maximizeX
/ETB2Vfs/ETB/NULX
/ETB2Vf/ETBs (29.47)
subject to
fu/ETB/DC4c.u;/ETB/ for each u; /ETB2V; (29.48)X
/ETB2Vf/ETBuDX
/ETB2Vfu/ETBfor each u2V/NULfs; tg; (29.49)
fu/ETB/NAK0 for each u; /ETB2V: (29.50)29.2 Formulating problems as linear programs 861
This linear program has jVj2variables, corresponding to the ﬂow between each
pair of vertices, and it has 2jVj2CjVj/NUL2constraints.
It is usually more efﬁcient to solve a smaller-sized linear program. The linear
program in (29.47)–(29.50) has, for ease of notation, a ﬂow and capacity of 0for
each pair of vertices u; /ETBwith.u; /ETB/62E. It would be more efﬁcient to rewrite the
linear program so that it has O.VCE/constraints. Exercise 29.2-5 asks you to
do so.
Minimum-cost ﬂow
In this section, we have used linear programming to solve problems for which we
already knew efﬁcient algorithms. In fact, an efﬁcient algorithm designed specif-ically for a problem, such as Dijkstra’s algorithm for the single-source shortest-
paths problem, or the push-relabel method for maximum ﬂow, will often be more
efﬁcient than linear programming, both in theory and in practice.
The real power of linear programming comes from the ability to solve new prob-
lems. Recall the problem faced by the politician in the beginning of this chapter.The problem of obtaining a sufﬁcient number of votes, while not spending toomuch money, is not solved by any of the algorithms that we have studied in thisbook, yet we can solve it by linear programming. Books abound with such real-world problems that linear programming can solve. Linear programming is alsoparticularly useful for solving variants of problems for which we may not alreadyknow of an efﬁcient algorithm.
Consider, for example, the following generalization of the maximum-ﬂow prob-
lem. Suppose that, in addition to a capacity c.u;/ETB/ for each edge .u; /ETB/ ,w ea r e
given a real-valued cost a.u;/ETB/ . As in the maximum-ﬂow problem, we assume that
c.u;/ETB/D0if.u; /ETB/62E, and that there are no antiparallel edges. If we send f
u/ETB
units of ﬂow over edge .u; /ETB/ , we incur a cost of a.u;/ETB/f u/ETB. We are also given a
ﬂow demand d.W ew i s ht os e n d dunits of ﬂow from stotwhile minimizing the
total costP
.u;/ETB/ 2Ea.u;/ETB/f u/ETBincurred by the ﬂow. This problem is known as the
minimum-cost-ﬂow problem .
Figure 29.3(a) shows an example of the minimum-cost-ﬂow problem. We wish
to send 4units of ﬂow from stotwhile incurring the minimum total cost. Any
particular legal ﬂow, that is, a function fsatisfying constraints (29.48)–(29.49),
incurs a total cost ofP
.u;/ETB/ 2Ea.u;/ETB/f u/ETB. We wish to ﬁnd the particular 4-unit
ﬂow that minimizes this cost. Figure 29.3(b) shows an optimal solution, with totalcostP
.u;/ETB/ 2Ea.u;/ETB/f u/ETBD.2/SOH2/C.5/SOH2/C.3/SOH1/C.7/SOH1/C.1/SOH3/D27:
There are polynomial-time algorithms speciﬁcally designed for the minimum-
cost-ﬂow problem, but they are beyond the scope of this book. We can, however,express the minimum-cost-ﬂow problem as a linear program. The linear programlooks similar to the one for the maximum-ﬂow problem with the additional con-862 Chapter 29 Linear Programming
sx
t
y
(a)c = 1
a = 3c = 5
a = 2
c = 4
a = 1c = 2a = 7
c = 2a = 5sx
t
y
(b)1/1
a = 32/5
a = 2
3/4
a = 11/2
a = 7
2/2
a = 5
Figure 29.3 (a) An example of a minimum-cost-ﬂow problem. We denote the capacities by cand
the costs by a.V e r t e x sis the source and vertex tis the sink, and we wish to send 4units of ﬂow
from stot.(b)A solution to the minimum-cost ﬂow problem in which 4units of ﬂow are sent from s
tot. For each edge, the ﬂow and capacity are written as ﬂow/capacity.
straint that the value of the ﬂow be exactly dunits, and with the new objective
function of minimizing the cost:
minimizeX
.u;/ETB/ 2Ea.u;/ETB/f u/ETB (29.51)
subject to
fu/ETB/DC4c.u;/ETB/ for each u; /ETB2V;X
/ETB2Vf/ETBu/NULX
/ETB2Vfu/ETBD0 for each u2V/NULfs; tg;
X
/ETB2Vfs/ETB/NULX
/ETB2Vf/ETBsDd;
fu/ETB/NAK0 for each u; /ETB2V: (29.52)
Multicommodity ﬂow
As a ﬁnal example, we consider another ﬂow problem. Suppose that the Lucky
Puck company from Section 26.1 decides to diversify its product line and shipnot only hockey pucks, but also hockey sticks and hockey helmets. Each piece ofequipment is manufactured in its own factory, has its own warehouse, and mustbe shipped, each day, from factory to warehouse. The sticks are manufactured inVancouver and must be shipped to Saskatoon, and the helmets are manufactured in
Edmonton and must be shipped to Regina. The capacity of the shipping network
does not change, however, and the different items, or commodities , must share the
same network.
This example is an instance of a multicommodity-ﬂow problem . In this problem,
we are again given a directed graph GD.V; E/ in which each edge .u; /ETB/2E
has a nonnegative capacity c.u;/ETB//NAK0. As in the maximum-ﬂow problem, we im-
plicitly assume that c.u;/ETB/D0for.u; /ETB/62E, and that the graph has no antipar-29.2 Formulating problems as linear programs 863
allel edges. In addition, we are given kdifferent commodities, K1;K2;:::;K k,
where we specify commodity iby the triple KiD.si;ti;di/. Here, vertex siis
the source of commodity i, vertex tiis the sink of commodity i,a n d diis the de-
mand for commodity i, which is the desired ﬂow value for the commodity from si
toti. We deﬁne a ﬂow for commodity i, denoted by fi, (so that fiu/ETBis the ﬂow of
commodity ifrom vertex uto vertex /ETB) to be a real-valued function that satisﬁes
the ﬂow-conservation and capacity constraints. We now deﬁne fu/ETB,t h eaggregate
ﬂow, to be the sum of the various commodity ﬂows, so that fu/ETBDPk
iD1fiu/ETB.T h e
aggregate ﬂow on edge .u; /ETB/ must be no more than the capacity of edge .u; /ETB/ .
We are not trying to minimize any objective function in this problem; we needonly determine whether such a ﬂow exists. Thus, we write a linear program with a“null” objective function:
minimize 0
subject to
kX
iD1fiu/ETB/DC4c.u;/ETB/ for each u; /ETB2V;
X
/ETB2Vfiu/ETB/NULX
/ETB2Vfi/ETBuD0 for each iD1 ;2;:::;k and
for each u2V/NULfsi;tig;
X
/ETB2Vfi;si;/ETB/NULX
/ETB2Vfi;/ETB;s iDdi for each iD1 ;2;:::;k;
fiu/ETB/NAK0 for each u; /ETB2Vand
for each iD1 ;2;:::;k:
The only known polynomial-time algorithm for this problem expresses it as a linear
program and then solves it with a polynomial-time linear-programming algorithm.
Exercises
29.2-1
Put the single-pair shortest-path linear program from (29.44)–(29.46) into standard
form.
29.2-2
Write out explicitly the linear program corresponding to ﬁnding the shortest pathfrom node sto node yin Figure 24.2(a).
29.2-3
In the single-source shortest-paths problem, we want to ﬁnd the shortest-pathweights from a source vertex sto all vertices /ETB2V.G i v e n a g r a p h G, write a864 Chapter 29 Linear Programming
linear program for which the solution has the property that d/ETBis the shortest-path
weight from sto/ETBfor each vertex /ETB2V.
29.2-4
Write out explicitly the linear program corresponding to ﬁnding the maximum ﬂowin Figure 26.1(a).
29.2-5
Rewrite the linear program for maximum ﬂow (29.47)–(29.50) so that it uses onlyO.VCE/constraints.
29.2-6
Write a linear program that, given a bipartite graph GD.V; E/ , solves the maxi-
mum-bipartite-matching problem.
29.2-7
In the minimum-cost multicommodity-ﬂow problem , we are given directed graph
GD.V; E/ in which each edge .u; /ETB/2Ehas a nonnegative capacity c.u;/ETB//NAK0
and a cost a.u;/ETB/ . As in the multicommodity-ﬂow problem, we are given kdif-
ferent commodities, K
1;K2;:::;K k, where we specify commodity iby the triple
KiD.si;ti;di/.W ed e ﬁ n et h eﬂ o w fifor commodity iand the aggregate ﬂow fu/ETB
on edge .u; /ETB/ as in the multicommodity-ﬂow problem. A feasible ﬂow is one
in which the aggregate ﬂow on each edge .u; /ETB/ is no more than the capacity of
edge .u; /ETB/ . The cost of a ﬂow isP
u;/ETB2Va.u;/ETB/f u/ETB, and the goal is to ﬁnd the
feasible ﬂow of minimum cost. Express this problem as a linear program.
29.3 The simplex algorithm
The simplex algorithm is the classical method for solving linear programs. In con-trast to most of the other algorithms in this book, its running time is not polynomialin the worst case. It does yield insight into linear programs, however, and is oftenremarkably fast in practice.
In addition to having a geometric interpretation, described earlier in this chapter,
the simplex algorithm bears some similarity to Gaussian elimination, discussed inSection 28.1. Gaussian elimination begins with a system of linear equalities whose
solution is unknown. In each iteration, we rewrite this system in an equivalent
form that has some additional structure. After some number of iterations, we haverewritten the system so that the solution is simple to obtain. The simplex algo-rithm proceeds in a similar manner, and we can view it as Gaussian elimination forinequalities.29.3 The simplex algorithm 865
We now describe the main idea behind an iteration of the simplex algorithm.
Associated with each iteration will be a “basic solution” that we can easily obtainfrom the slack form of the linear program: set each nonbasic variable to 0and
compute the values of the basic variables from the equality constraints. An iterationconverts one slack form into an equivalent slack form. The objective value of theassociated basic feasible solution will be no less than that at the previous iteration,and usually greater. To achieve this increase in the objective value, we choose anonbasic variable such that if we were to increase that variable’s value from 0,t h e n
the objective value would increase, too. The amount by which we can increase
the variable is limited by the other constraints. In particular, we raise it until somebasic variable becomes 0. We then rewrite the slack form, exchanging the roles
of that basic variable and the chosen nonbasic variable. Although we have used aparticular setting of the variables to guide the algorithm, and we shall use it in ourproofs, the algorithm does not explicitly maintain this solution. It simply rewritesthe linear program until an optimal solution becomes “obvious.”
An example of the simplex algorithm
We begin with an extended example. Consider the following linear program in
standard form:
maximize 3x
1C x2C2x3 (29.53)
subject to
x1C x2C3x3/DC430 (29.54)
2x1C2x2C5x3/DC424 (29.55)
4x1C x2C2x3/DC436 (29.56)
x1;x2;x3/NAK 0: (29.57)
In order to use the simplex algorithm, we must convert the linear program into
slack form; we saw how to do so in Section 29.1. In addition to being an algebraicmanipulation, slack is a useful algorithmic concept. Recalling from Section 29.1that each variable has a corresponding nonnegativity constraint, we say that anequality constraint is tight for a particular setting of its nonbasic variables if they
cause the constraint’s basic variable to become 0. Similarly, a setting of the non-
basic variables that would make a basic variable become negative violates that
constraint. Thus, the slack variables explicitly maintain how far each constraint is
from being tight, and so they help to determine how much we can increase values
of nonbasic variables without violating any constraints.
Associating the slack variables x
4,x5,a n d x6with inequalities (29.54)–(29.56),
respectively, and putting the linear program into slack form, we obtain866 Chapter 29 Linear Programming
´D 3x1C x2C2x3 (29.58)
x4D30/NUL x1/NUL x2/NUL3x3 (29.59)
x5D24/NUL2x1/NUL2x2/NUL5x3 (29.60)
x6D36/NUL4x1/NUL x2/NUL2x3: (29.61)
The system of constraints (29.59)–(29.61) has 3 equations and 6 variables. Any
setting of the variables x1,x2,a n d x3deﬁnes values for x4,x5,a n d x6; therefore,
we have an inﬁnite number of solutions to this system of equations. A solution isfeasible if all of x
1;x2;:::;x 6are nonnegative, and there can be an inﬁnite num-
ber of feasible solutions as well. The inﬁnite number of possible solutions to asystem such as this one will be useful in later proofs. We focus on the basic solu-
tion: set all the (nonbasic) variables on the right-hand side to 0and then compute
the values of the (basic) variables on the left-hand side. In this example, the ba-sic solution is .Nx
1;Nx2;:::;Nx6/D. 0 ;0 ;0 ;3 0 ;2 4;3 6 / and it has objective value
´D.3/SOH0/C.1/SOH0/C.2/SOH0/D0. Observe that this basic solution sets NxiDbi
for each i2B. An iteration of the simplex algorithm rewrites the set of equations
and the objective function so as to put a different set of variables on the right-
hand side. Thus, a different basic solution is associated with the rewritten problem.
We emphasize that the rewrite does not in any way change the underlying linear-
programming problem; the problem at one iteration has the identical set of feasible
solutions as the problem at the previous iteration. The problem does, however,have a different basic solution than that of the previous iteration.
If a basic solution is also feasible, we call it a basic feasible solution .A sw er u n
the simplex algorithm, the basic solution is almost always a basic feasible solution.We shall see in Section 29.5, however, that for the ﬁrst few iterations of the simplexalgorithm, the basic solution might not be feasible.
Our goal, in each iteration, is to reformulate the linear program so that the basic
solution has a greater objective value. We select a nonbasic variable x
ewhose
coefﬁcient in the objective function is positive, and we increase the value of xeas
much as possible without violating any of the constraints. The variable xebecomes
basic, and some other variable xlbecomes nonbasic. The values of other basic
variables and of the objective function may also change.
To continue the example, let’s think about increasing the value of x1.A s w e
increase x1, the values of x4,x5,a n d x6all decrease. Because we have a nonnega-
tivity constraint for each variable, we cannot allow any of them to become negative.Ifx
1increases above 30,t h e n x4becomes negative, and x5andx6become nega-
tive when x1increases above 12and9, respectively. The third constraint (29.61) is
the tightest constraint, and it limits how much we can increase x1. Therefore, we
switch the roles of x1andx6. We solve equation (29.61) for x1and obtain
x1D9/NULx2
4/NULx3
2/NULx6
4: (29.62)29.3 The simplex algorithm 867
To rewrite the other equations with x6on the right-hand side, we substitute for x1
using equation (29.62). Doing so for equation (29.59), we obtain
x4D30/NULx1/NULx2/NUL3x3
D30/NUL/DLE
9/NULx2
4/NULx3
2/NULx6
4/DC1
/NULx2/NUL3x3
D21/NUL3x2
4/NUL5x3
2Cx6
4: (29.63)
Similarly, we combine equation (29.62) with constraint (29.60) and with objective
function (29.58) to rewrite our linear program in the following form:
´D27Cx2
4Cx3
2/NUL3x6
4(29.64)
x1D 9/NULx2
4/NULx3
2/NULx6
4(29.65)
x4D21/NUL3x2
4/NUL5x3
2Cx6
4(29.66)
x5D 6/NUL3x2
2/NUL4x3Cx6
2: (29.67)
We call this operation a pivot . As demonstrated above, a pivot chooses a nonbasic
variable xe, called the entering variable , and a basic variable xl, called the leaving
variable , and exchanges their roles.
The linear program described in equations (29.64)–(29.67) is equivalent to the
linear program described in equations (29.58)–(29.61). We perform two operations
in the simplex algorithm: rewrite equations so that variables move between the left-
hand side and the right-hand side, and substitute one equation into another. The ﬁrstoperation trivially creates an equivalent problem, and the second, by elementarylinear algebra, also creates an equivalent problem. (See Exercise 29.3-3.)
To demonstrate this equivalence, observe that our original basic solution .0; 0;
0; 30; 24; 36/ satisﬁes the new equations (29.65)–(29.67) and has objective value
27C.1=4//SOH0C.1=2//SOH0/NUL.3=4//SOH36D0. The basic solution associated with the
new linear program sets the nonbasic values to 0and is .9; 0; 0; 21; 6; 0/ , with ob-
jective value ´D27. Simple arithmetic veriﬁes that this solution also satisﬁes
equations (29.59)–(29.61) and, when plugged into objective function (29.58), hasobjective value .3/SOH9/C.1/SOH0/C.2/SOH0/D27.
Continuing the example, we wish to ﬁnd a new variable whose value we wish to
increase. We do not want to increase x
6, since as its value increases, the objective
value decreases. We can attempt to increase either x2orx3; let us choose x3.H o w
far can we increase x3without violating any of the constraints? Constraint (29.65)
limits it to 18, constraint (29.66) limits it to 42=5 , and constraint (29.67) limits
it to3=2. The third constraint is again the tightest one, and therefore we rewrite
the third constraint so that x3is on the left-hand side and x5is on the right-hand868 Chapter 29 Linear Programming
side. We then substitute this new equation, x3D3=2/NUL3x2=8/NULx5=4Cx6=8,i n t o
equations (29.64)–(29.66) and obtain the new, but equivalent, system
´D111
4Cx2
16/NULx5
8/NUL11x 6
16(29.68)
x1D33
4/NULx2
16Cx5
8/NUL5x6
16(29.69)
x3D3
2/NUL3x2
8/NULx5
4Cx6
8(29.70)
x4D69
4C3x2
16C5x5
8/NULx6
16: (29.71)
This system has the associated basic solution .33=4; 0; 3=2; 69=4; 0; 0/ , with ob-
jective value 111=4 . Now the only way to increase the objective value is to in-
crease x2. The three constraints give upper bounds of 132,4,a n d1, respectively.
(We get an upper bound of 1from constraint (29.71) because, as we increase x2,
the value of the basic variable x4increases also. This constraint, therefore, places
no restriction on how much we can increase x2.) We increase x2to4, and it be-
comes nonbasic. Then we solve equation (29.70) for x2and substitute in the other
equations to obtain
´D28/NULx3
6/NULx5
6/NUL2x6
3(29.72)
x1D 8Cx3
6Cx5
6/NULx6
3(29.73)
x2D 4/NUL8x3
3/NUL2x5
3Cx6
3(29.74)
x4D18/NULx3
2Cx5
2: (29.75)
At this point, all coefﬁcients in the objective function are negative. As we shall see
later in this chapter, this situation occurs only when we have rewritten the linearprogram so that the basic solution is an optimal solution. Thus, for this problem,the solution .8; 4; 0; 18; 0; 0/ , with objective value 28, is optimal. We can now
return to our original linear program given in (29.53)–(29.57). The only variablesin the original linear program are x
1,x2,a n d x3, and so our solution is x1D8,
x2D4,a n d x3D0, with objective value .3/SOH8/C.1/SOH4/C.2/SOH0/D28.N o t e
that the values of the slack variables in the ﬁnal solution measure how much slackremains in each inequality. Slack variable x
4is18, and in inequality (29.54), the
left-hand side, with value 8C4C0D12,i s18less than the right-hand side of 30.
Slack variables x5andx6are0and indeed, in inequalities (29.55) and (29.56),
the left-hand and right-hand sides are equal. Observe also that even though the
coefﬁcients in the original slack form are integral, the coefﬁcients in the other
linear programs are not necessarily integral, and the intermediate solutions are not29.3 The simplex algorithm 869
necessarily integral. Furthermore, the ﬁnal solution to a linear program need not
be integral; it is purely coincidental that this example has an integral solution.
Pivoting
We now formalize the procedure for pivoting. The procedure P IVOT t a k e sa si n -
put a slack form, given by the tuple . N;B;A ;b;c;/ETB/ , the index lof the leav-
ing variable xl, and the index eof the entering variable xe. It returns the tuple
.yN;yB;yA;yb;yc;y/ETB/describing the new slack form. (Recall again that the entries of
them/STXnmatrices AandyAare actually the negatives of the coefﬁcients that appear
in the slack form.)
PIVOT. N;B;A ;b;c;/ETB;l;e/
1//Compute the coefﬁcients of the equation for new basic variable xe.
2l e tyAb ean e w m/STXnmatrix
3ybeDbl=ale
4foreachj2N/NULfeg
5yaejDalj=ale
6yaelD1=a le
7//Compute the coefﬁcients of the remaining constraints.
8foreachi2B/NULflg
9ybiDbi/NULaieybe
10 foreachj2N/NULfeg
11yaijDaij/NULaieyaej
12yailD/NULaieyael
13//Compute the objective function.
14y/ETBD/ETBCceybe
15foreachj2N/NULfeg
16ycjDcj/NULceyaej
17yclD/NULceyael
18//Compute new sets of basic and nonbasic variables.
19yNDN/NULfeg[flg
20yBDB/NULflg[feg
21return .yN;yB;yA;yb;yc;y/ETB/
PIVOT works as follows. Lines 3–6 compute the coefﬁcients in the new equation
forxeby rewriting the equation that has xlon the left-hand side to instead have xe
on the left-hand side. Lines 8–12 update the remaining equations by substituting
the right-hand side of this new equation for each occurrence of xe. Lines 14–17
do the same substitution for the objective function, and lines 19 and 20 update the870 Chapter 29 Linear Programming
sets of nonbasic and basic variables. Line 21 returns the new slack form. As given,
ifaleD0,PIVOT would cause an error by dividing by 0, but as we shall see in the
proofs of Lemmas 29.2 and 29.12, we call P IVOT only when ale¤0.
We now summarize the effect that P IVOT has on the values of the variables in
the basic solution.
Lemma 29.1
Consider a call to P IVOT. N;B;A ;b;c;/ETB;l;e/ in which ale¤0. Let the values
returned from the call be .yN;yB;yA;yb;yc;y/ETB/,a n dl e tNxdenote the basic solution after
the call. Then
1.NxjD0for each j2yN.
2.NxeDbl=ale.
3.NxiDbi/NULaieybefor each i2yB/NULfeg.
Proof The ﬁrst statement is true because the basic solution always sets all non-
basic variables to 0. When we set each nonbasic variable to 0in a constraint
xiDybi/NULX
j2yNyaijxj;
we have thatNxiDybifor each i2yB.S i n c e e2yB, line 3 of P IVOT gives
NxeDybeDbl=ale;
which proves the second statement. Similarly, using line 9 for each i2yB/NULfeg,
we have
NxiDybiDbi/NULaieybe;
which proves the third statement.
The formal simplex algorithm
We are now ready to formalize the simplex algorithm, which we demonstrated by
example. That example was a particularly nice one, and we could have had severalother issues to address:
/SIHow do we determine whether a linear program is feasible?
/SIWhat do we do if the linear program is feasible, but the initial basic solution isnot feasible?
/SIHow do we determine whether a linear program is unbounded?
/SIHow do we choose the entering and leaving variables?29.3 The simplex algorithm 871
In Section 29.5, we shall show how to determine whether a problem is feasible,
and if so, how to ﬁnd a slack form in which the initial basic solution is feasible.Therefore, let us assume that we have a procedure I
NITIALIZE -SIMPLEX . A ;b;c/
that takes as input a linear program in standard form, that is, an m/STXnmatrix
AD.aij/,a nm-vector bD.bi/,a n da n n-vector cD.cj/. If the problem is
infeasible, the procedure returns a message that the program is infeasible and thenterminates. Otherwise, the procedure returns a slack form for which the initialbasic solution is feasible.
The procedure S
IMPLEX takes as input a linear program in standard form, as just
described. It returns an n-vectorNxD.Nxj/that is an optimal solution to the linear
program described in (29.19)–(29.21).
SIMPLEX . A ;b;c/
1. N;B;A ;b;c;/ETB/DINITIALIZE -SIMPLEX . A ;b;c/
2l e t /c129be a new vector of length n
3while some index j2Nhascj>0
4 choose an index e2Nfor which ce>0
5 foreach index i2B
6 ifaie>0
7 /c129iDbi=aie
8 else/c129iD1
9 choose an index l2Bthat minimizes /c129i
10 if/c129l==1
11 return “unbounded”
12 else. N;B;A ;b;c;/ETB/DPIVOT. N;B;A ;b;c;/ETB;l;e/
13foriD1ton
14 ifi2B
15NxiDbi
16 elseNxiD0
17return .Nx1;Nx2;:::;Nxn/
The S IMPLEX procedure works as follows. In line 1, it calls the procedure
INITIALIZE -SIMPLEX . A ;b;c/ , described above, which either determines that the
linear program is infeasible or returns a slack form for which the basic solution isfeasible. The while loop of lines 3–12 forms the main part of the algorithm. If all
coefﬁcients in the objective function are negative, then the while loop terminates.
Otherwise, line 4 selects a variable x
e, whose coefﬁcient in the objective function
is positive, as the entering variable. Although we may choose any such variable as
the entering variable, we assume that we use some prespeciﬁed deterministic rule.
Next, lines 5–9 check each constraint and pick the one that most severely limits
the amount by which we can increase xewithout violating any of the nonnegativ-872 Chapter 29 Linear Programming
ity constraints; the basic variable associated with this constraint is xl.A g a i n , w e
are free to choose one of several variables as the leaving variable, but we assumethat we use some prespeciﬁed deterministic rule. If none of the constraints lim-its the amount by which the entering variable can increase, the algorithm returns“unbounded” in line 11. Otherwise, line 12 exchanges the roles of the enteringand leaving variables by calling P
IVOT. N;B;A ;b;c;/ETB;l;e/ , as described above.
Lines 13–16 compute a solution Nx1;Nx2;:::;Nxnfor the original linear-programming
variables by setting all the nonbasic variables to 0and each basic variable Nxitobi,
and line 17 returns these values.
To show that S IMPLEX is correct, we ﬁrst show that if S IMPLEX has an initial
feasible solution and eventually terminates, then it either returns a feasible solutionor determines that the linear program is unbounded. Then, we show that S
IMPLEX
terminates. Finally, in Section 29.4 (Theorem 29.10) we show that the solutionreturned is optimal.
Lemma 29.2
Given a linear program . A ;b;c/ , suppose that the call to I
NITIALIZE -SIMPLEX in
line 1 of S IMPLEX returns a slack form for which the basic solution is feasible.
Then if S IMPLEX returns a solution in line 17, that solution is a feasible solution to
the linear program. If S IMPLEX returns “unbounded” in line 11, the linear program
is unbounded.
Proof We use the following three-part loop invariant:
At the start of each iteration of the while loop of lines 3–12,
1. the slack form is equivalent to the slack form returned by the call of
INITIALIZE -SIMPLEX ,
2. for each i2B,w eh a v e bi/NAK0,a n d
3. the basic solution associated with the slack form is feasible.
Initialization: The equivalence of the slack forms is trivial for the ﬁrst itera-
tion. We assume, in the statement of the lemma, that the call to I NITIALIZE -
SIMPLEX in line 1 of S IMPLEX returns a slack form for which the basic solution
is feasible. Thus, the third part of the invariant is true. Because the basic so-lution is feasible, each basic variable x
iis nonnegative. Furthermore, since the
basic solution sets each basic variable xitobi,w eh a v et h a t bi/NAK0for all
i2B. Thus, the second part of the invariant holds.
Maintenance: We shall show that each iteration of the while loop maintains the
loop invariant, assuming that the return statement in line 11 does not execute.
We shall handle the case in which line 11 executes when we discuss termination.29.3 The simplex algorithm 873
An iteration of the while loop exchanges the role of a basic and a nonbasic
variable by calling the P IVOT procedure. By Exercise 29.3-3, the slack form is
equivalent to the one from the previous iteration which, by the loop invariant,is equivalent to the initial slack form.
We now demonstrate the second part of the loop invariant. We assume that at
the start of each iteration of the while loop, b
i/NAK0for each i2B, and we shall
show that these inequalities remain true after the call to P IVOT in line 12. Since
the only changes to the variables biand the set Bof basic variables occur in this
assignment, it sufﬁces to show that line 12 maintains this part of the invariant.
We let bi,aij,a n d Brefer to values before the call of P IVOT ,a n dybirefer to
values returned from P IVOT .
First, we observe that ybe/NAK0because bl/NAK0by the loop invariant, ale>0by
l i n e s6a n d9o fS IMPLEX ,a n dybeDbl=aleby line 3 of P IVOT .
For the remaining indices i2B/NULflg,w eh a v et h a t
ybiDbi/NULaieybe (by line 9 of P IVOT )
Dbi/NULaie.bl=ale/(by line 3 of P IVOT ) . (29.76)
We have two cases to consider, depending on whether aie>0 oraie/DC40.
Ifaie>0, then since we chose lsuch that
bl=ale/DC4bi=aiefor all i2B; (29.77)
we have
ybiDbi/NULaie.bl=ale/(by equation (29.76))
/NAKbi/NULaie.bi=aie/(by inequality (29.77))
Dbi/NULbi
D0;
and thusybi/NAK0.I faie/DC40, then because ale,bi,a n d blare all nonnegative,
equation (29.76) implies that ybimust be nonnegative, too.
We now argue that the basic solution is feasible, i.e., that all variables have non-
negative values. The nonbasic variables are set to 0and thus are nonnegative.
Each basic variable xiis deﬁned by the equation
xiDbi/NULX
j2Naijxj:
The basic solution sets NxiDbi. Using the second part of the loop invariant, we
conclude that each basic variable Nxiis nonnegative.874 Chapter 29 Linear Programming
Termination: Thewhile loop can terminate in one of two ways. If it terminates
because of the condition in line 3, then the current basic solution is feasible andline 17 returns this solution. The other way it terminates is by returning “un-bounded” in line 11. In this case, for each iteration of the forloop in lines 5–8,
when line 6 is executed, we ﬁnd that a
ie/DC40. Consider the solution Nxdeﬁned as
NxiD/c128
1 ifiDe;
0 ifi2N/NULfeg;
bi/NULP
j2NaijNxjifi2B:
We now show that this solution is feasible, i.e., that all variables are nonneg-
ative. The nonbasic variables other than Nxeare0,a n dNxeD1 >0; thus all
nonbasic variables are nonnegative. For each basic variable Nxi,w eh a v e
NxiDbi/NULX
j2NaijNxj
Dbi/NULaieNxe:
The loop invariant implies that bi/NAK0, and we have aie/DC40andNxeD1 >0.
Thus,Nxi/NAK0.
Now we show that the objective value for the solution Nxis unbounded. From
equation (29.42), the objective value is
´D/ETBCX
j2NcjNxj
D/ETBCceNxe:
Since ce>0 (by line 4 of S IMPLEX )a n dNxeD1 , the objective value is 1,
and thus the linear program is unbounded.
It remains to show that S IMPLEX terminates, and when it does terminate, the
solution it returns is optimal. Section 29.4 will address optimality. We now discusstermination.
Termination
In the example given in the beginning of this section, each iteration of the simplex
algorithm increased the objective value associated with the basic solution. As Ex-ercise 29.3-2 asks you to show, no iteration of S
IMPLEX can decrease the objective
value associated with the basic solution. Unfortunately, it is possible that an itera-tion leaves the objective value unchanged. This phenomenon is called degeneracy ,
and we shall now study it in greater detail.29.3 The simplex algorithm 875
The assignment in line 14 of P IVOT ,y/ETBD/ETBCceybe, changes the objective value.
Since S IMPLEX calls P IVOT only when ce>0, the only way for the objective
value to remain unchanged (i.e., y/ETBD/ETB)i sf o rybeto be 0. This value is assigned
asybeDbl=alein line 3 of P IVOT . Since we always call P IVOT withale¤0,w e
see that forybeto equal 0, and hence the objective value to be unchanged, we must
have blD0.
Indeed, this situation can occur. Consider the linear program
´D x1Cx2Cx3
x4D8/NULx1/NULx2
x5D x2/NULx3:
Suppose that we choose x1as the entering variable and x4as the leaving variable.
After pivoting, we obtain
´D8Cx3/NULx4
x1D8/NULx2/NULx4
x5D x2/NULx3:
At this point, our only choice is to pivot with x3entering and x5leaving. Since
b5D0, the objective value of 8remains unchanged after pivoting:
´D8Cx2/NULx4/NULx5
x1D8/NULx2/NULx4
x3D x2/NULx5:
The objective value has not changed, but our slack form has. Fortunately, if we
pivot again, with x2entering and x1leaving, the objective value increases (to 16),
and the simplex algorithm can continue.
Degeneracy can prevent the simplex algorithm from terminating, because it can
lead to a phenomenon known as cycling : the slack forms at two different itera-
tions of S IMPLEX are identical. Because of degeneracy, S IMPLEX could choose a
sequence of pivot operations that leave the objective value unchanged but repeat
a slack form within the sequence. Since S IMPLEX is a deterministic algorithm, if
it cycles, then it will cycle through the same series of slack forms forever, never
terminating.
Cycling is the only reason that S IMPLEX might not terminate. To show this fact,
we must ﬁrst develop some additional machinery.
At each iteration, S IMPLEX maintains A,b,c,a n d /ETBin addition to the sets
NandB. Although we need to explicitly maintain A,b,c,a n d /ETBin order to
implement the simplex algorithm efﬁciently, we can get by without maintaining
them. In other words, the sets of basic and nonbasic variables sufﬁce to uniquely
determine the slack form. Before proving this fact, we prove a useful algebraiclemma.876 Chapter 29 Linear Programming
Lemma 29.3
LetIbe a set of indices. For each j2I,l e t˛jandˇjbe real numbers, and let xj
be a real-valued variable. Let /CRbe any real number. Suppose that for any settings
of the xj,w eh a v e
X
j2I˛jxjD/CRCX
j2Iˇjxj: (29.78)
Then ˛jDˇjfor each j2I,a n d /CRD0.
Proof Since equation (29.78) holds for any values of the xj, we can use particular
values to draw conclusions about ˛,ˇ,a n d /CR.I f w e l e t xjD0for each j2I,
we conclude that /CRD0. Now pick an arbitrary index j2I, and set xjD1and
xkD0for all k¤j. Then we must have ˛jDˇj. Since we picked jas any
index in I, we conclude that ˛jDˇjfor each j2I.
A particular linear program has many different slack forms; recall that each slack
form has the same set of feasible and optimal solutions as the original linear pro-gram. We now show that the slack form of a linear program is uniquely determinedby the set of basic variables. That is, given the set of basic variables, a unique slackform (unique set of coefﬁcients and right-hand sides) is associated with those basicvariables.
Lemma 29.4
Let. A ;b;c/ be a linear program in standard form. Given a set Bof basic variables,
the associated slack form is uniquely determined.
Proof Assume for the purpose of contradiction that there are two different slack
forms with the same set Bof basic variables. The slack forms must also have
identical sets NDf1 ;2;:::;nCmg/NULBof nonbasic variables. We write the ﬁrst
slack form as
´D/ETBCX
j2Ncjxj (29.79)
xiDbi/NULX
j2Naijxjfori2B; (29.80)
and the second as
´D/ETB0CX
j2Nc0
jxj (29.81)
xiDb0
i/NULX
j2Na0
ijxjfori2B: (29.82)29.3 The simplex algorithm 877
Consider the system of equations formed by subtracting each equation in
line (29.82) from the corresponding equation in line (29.80). The resulting sys-tem is
0D.b
i/NULb0
i//NULX
j2N.aij/NULa0
ij/xjfori2B
or, equivalently,
X
j2NaijxjD.bi/NULb0
i/CX
j2Na0
ijxjfori2B:
Now, for each i2B, apply Lemma 29.3 with ˛jDaij,ˇjDa0
ij,/CRDbi/NULb0
i,a n d
IDN.S i n c e ˛iDˇi,w eh a v et h a t aijDa0
ijfor each j2N, and since /CRD0,
we have that biDb0
i. Thus, for the two slack forms, Aandbare identical to A0
andb0. Using a similar argument, Exercise 29.3-1 shows that it must also be the
case that cDc0and/ETBD/ETB0, and hence that the slack forms must be identical.
We can now show that cycling is the only possible reason that S IMPLEX might
not terminate.
Lemma 29.5
If S IMPLEX fails to terminate in at most/NULnCm
m/SOH
iterations, then it cycles.
Proof By Lemma 29.4, the set Bof basic variables uniquely determines a slack
form. There are nCmvariables andjBjDm, and therefore, there are at most/NULnCm
m/SOH
ways to choose B. Thus, there are only at most/NULnCm
m/SOH
unique slack forms.
Therefore, if S IMPLEX runs for more than/NULnCm
m/SOH
iterations, it must cycle.
Cycling is theoretically possible, but extremely rare. We can prevent it by choos-
ing the entering and leaving variables somewhat more carefully. One option is toperturb the input slightly so that it is impossible to have two solutions with thesame objective value. Another option is to break ties by always choosing the vari-able with the smallest index, a strategy known as Bland’s rule . We omit the proof
that these strategies avoid cycling.
Lemma 29.6
If lines 4 and 9 of S
IMPLEX always break ties by choosing the variable with the
smallest index, then S IMPLEX must terminate.
We conclude this section with the following lemma.878 Chapter 29 Linear Programming
Lemma 29.7
Assuming that I NITIALIZE -SIMPLEX returns a slack form for which the basic so-
lution is feasible, S IMPLEX either reports that a linear program is unbounded, or it
terminates with a feasible solution in at most/NULnCm
m/SOH
iterations.
Proof Lemmas 29.2 and 29.6 show that if I NITIALIZE -SIMPLEX returns a slack
form for which the basic solution is feasible, S IMPLEX either reports that a linear
program is unbounded, or it terminates with a feasible solution. By the contra-positive of Lemma 29.5, if S
IMPLEX terminates with a feasible solution, then it
terminates in at most/NULnCm
m/SOH
iterations.
Exercises
29.3-1
Complete the proof of Lemma 29.4 by showing that it must be the case that cDc0
and/ETBD/ETB0.
29.3-2
Show that the call to P IVOT in line 12 of S IMPLEX never decreases the value of /ETB.
29.3-3
Prove that the slack form given to the P IVOT procedure and the slack form that the
procedure returns are equivalent.
29.3-4
Suppose we convert a linear program . A ;b;c/ in standard form to slack form.
Show that the basic solution is feasible if and only if bi/NAK0foriD1 ;2;:::;m .
29.3-5
Solve the following linear program using S IMPLEX :
maximize 18x 1C12:5x 2
subject to
x1C x2/DC420
x1/DC412
x2/DC416
x1;x2/NAK 0:29.4 Duality 879
29.3-6
Solve the following linear program using S IMPLEX :
maximize 5x1/NUL3x2
subject to
x1/NUL x2/DC41
2x1C x2/DC42
x1;x2/NAK0:
29.3-7
Solve the following linear program using S IMPLEX :
minimize x1C x2C x3
subject to
2x1C7:5x 2C 3x3/NAK10000
20x 1C 5x2C10x 3/NAK30000
x1;x2;x3/NAK 0:
29.3-8
In the proof of Lemma 29.5, we argued that there are at most/NULmCn
n/SOH
ways to choose
a setBof basic variables. Give an example of a linear program in which there are
strictly fewer than/NULmCn
n/SOH
ways to choose the set B.
29.4 Duality
We have proven that, under certain assumptions, S IMPLEX terminates. We have not
yet shown that it actually ﬁnds an optimal solution to a linear program, however.In order to do so, we introduce a powerful concept called linear-programming
duality .
Duality enables us to prove that a solution is indeed optimal. We saw an exam-
ple of duality in Chapter 26 with Theorem 26.6, the max-ﬂow min-cut theorem.Suppose that, given an instance of a maximum-ﬂow problem, we ﬁnd a ﬂow f
with valuejfj. How do we know whether fis a maximum ﬂow? By the max-ﬂow
min-cut theorem, if we can ﬁnd a cut whose value is also jfj,t h e nw eh a v ev e r -
iﬁed that fis indeed a maximum ﬂow. This relationship provides an example of
duality: given a maximization problem, we deﬁne a related minimization problemsuch that the two problems have the same optimal objective values.
Given a linear program in which the objective is to maximize, we shall describe
how to formulate a dual linear program in which the objective is to minimize and880 Chapter 29 Linear Programming
whose optimal value is identical to that of the original linear program. When refer-
ring to dual linear programs, we call the original linear program the primal .
Given a primal linear program in standard form, as in (29.16)–(29.18), we deﬁne
the dual linear program as
minimizemX
iD1biyi (29.83)
subject to
mX
iD1aijyi/NAKcjforjD1 ;2;:::;n; (29.84)
yi/NAK0foriD1 ;2;:::;m: (29.85)
To form the dual, we change the maximization to a minimization, exchange the
roles of coefﬁcients on the right-hand sides and the objective function, and replaceeach less-than-or-equal-to by a greater-than-or-equal-to. Each of the mconstraints
in the primal has an associated variable y
iin the dual, and each of the nconstraints
in the dual has an associated variable xjin the primal. For example, consider the
linear program given in (29.53)–(29.57). The dual of this linear program is
minimize 30y 1C24y 2C36y 3 (29.86)
subject to
y1C 2y2C 4y3/NAK3 (29.87)
y1C 2y2C y3/NAK1 (29.88)
3y1C 5y2C 2y3/NAK2 (29.89)
y1;y2;y3/NAK0: (29.90)
We shall show in Theorem 29.10 that the optimal value of the dual linear pro-
gram is always equal to the optimal value of the primal linear program. Further-
more, the simplex algorithm actually implicitly solves both the primal and the dual
linear programs simultaneously, thereby providing a proof of optimality.
We begin by demonstrating weak duality , which states that any feasible solu-
tion to the primal linear program has a value no greater than that of any feasible
solution to the dual linear program.
Lemma 29.8 (Weak linear-programming duality)
LetNxbe any feasible solution to the primal linear program in (29.16)–(29.18) and
letNybe any feasible solution to the dual linear program in (29.83)–(29.85). Then,
we have
nX
jD1cjNxj/DC4mX
iD1biNyi:29.4 Duality 881
Proof We have
nX
jD1cjNxj/DC4nX
jD1 mX
iD1aijNyi!
Nxj(by inequalities (29.84))
DmX
iD1 nX
jD1aijNxj!
Nyi
/DC4mX
iD1biNyi (by inequalities (29.17)) .
Corollary 29.9
LetNxbe a feasible solution to a primal linear program . A ;b;c/ ,a n dl e tNybe a
feasible solution to the corresponding dual linear program. If
nX
jD1cjNxjDmX
iD1biNyi;
thenNxandNyare optimal solutions to the primal and dual linear programs, respec-
tively.
Proof By Lemma 29.8, the objective value of a feasible solution to the primal
cannot exceed that of a feasible solution to the dual. The primal linear program isa maximization problem and the dual is a minimization problem. Thus, if feasiblesolutionsNxandNyhave the same objective value, neither can be improved.
Before proving that there always is a dual solution whose value is equal to that
of an optimal primal solution, we describe how to ﬁnd such a solution. Whenwe ran the simplex algorithm on the linear program in (29.53)–(29.57), the ﬁnaliteration yielded the slack form (29.72)–(29.75) with objective ´D28/NULx
3=6/NUL
x5=6/NUL2x6=3,BDf1; 2; 4g,a n d NDf3; 5; 6g. As we shall show below, the basic
solution associated with the ﬁnal slack form is indeed an optimal solution to thelinear program; an optimal solution to linear program (29.53)–(29.57) is therefore.Nx
1;Nx2;Nx3/D. 8 ;4;0 / , with objective value .3/SOH8/C.1/SOH4/C.2/SOH0/D28.A s
we also show below, we can read off an optimal dual solution: the negatives of thecoefﬁcients of the primal objective function are the values of the dual variables.
More precisely, suppose that the last slack form of the primal is
´D/ETB
0CX
j2Nc0
jxj
xiDb0
i/NULX
j2Na0
ijxjfori2B:882 Chapter 29 Linear Programming
Then, to produce an optimal dual solution, we set
NyiD(
/NULc0
nCiif.nCi/2N;
0 otherwise :(29.91)
Thus, an optimal solution to the dual linear program deﬁned in (29.86)–(29.90)
isNy1D0(since nC1D42B),Ny2D/NULc0
5D1=6,a n dNy3D/NULc0
6D2=3.
Evaluating the dual objective function (29.86), we obtain an objective value of.30/SOH0/C.24/SOH.1=6//C.36/SOH.2=3//D28, which conﬁrms that the objective value
of the primal is indeed equal to the objective value of the dual. Combining thesecalculations with Lemma 29.8 yields a proof that the optimal objective value of the
primal linear program is 28. We now show that this approach applies in general:
we can ﬁnd an optimal solution to the dual and simultaneously prove that a solutionto the primal is optimal.
Theorem 29.10 (Linear-programming duality)
Suppose that S
IMPLEX returns valuesNxD.Nx1;Nx2;:::;Nxn/for the primal lin-
ear program . A ;b;c/ .L e t NandBdenote the nonbasic and basic variables for
the ﬁnal slack form, let c0denote the coefﬁcients in the ﬁnal slack form, and let
NyD.Ny1;Ny2;:::;Nym/be deﬁned by equation (29.91). Then Nxis an optimal so-
lution to the primal linear program, Nyis an optimal solution to the dual linear
program, and
nX
jD1cjNxjDmX
iD1biNyi: (29.92)
Proof By Corollary 29.9, if we can ﬁnd feasible solutions NxandNythat satisfy
equation (29.92), then NxandNymust be optimal primal and dual solutions. We
shall now show that the solutions NxandNydescribed in the statement of the theorem
satisfy equation (29.92).
Suppose that we run S IMPLEX on a primal linear program, as given in lines
(29.16)–(29.18). The algorithm proceeds through a series of slack forms until itterminates with a ﬁnal slack form with objective function
´D/ETB
0CX
j2Nc0
jxj: (29.93)
Since S IMPLEX terminated with a solution, by the condition in line 3 we know that
c0
j/DC40for all j2N: (29.94)29.4 Duality 883
If we deﬁne
c0
jD0for all j2B; (29.95)
we can rewrite equation (29.93) as
´D/ETB0CX
j2Nc0
jxj
D/ETB0CX
j2Nc0
jxjCX
j2Bc0
jxj(because c0
jD0ifj2B)
D/ETB0CnCmX
jD1c0
jxj (because N[BDf1 ;2;:::;nCmg) . (29.96)
For the basic solution Nxassociated with this ﬁnal slack form, NxjD0for all j2N,
and´D/ETB0. Since all slack forms are equivalent, if we evaluate the original objec-
tive function onNx, we must obtain the same objective value:
nX
jD1cjNxjD/ETB0CnCmX
jD1c0
jNxj (29.97)
D/ETB0CX
j2Nc0
jNxjCX
j2Bc0
jNxj
D/ETB0CX
j2N.c0
j/SOH0/CX
j2B.0/SOHNxj/ (29.98)
D/ETB0:
We shall now show that Ny, deﬁned by equation (29.91), is feasible for the dual
linear program and that its objective valuePm
iD1biNyiequalsPn
jD1cjNxj. Equa-
tion (29.97) says that the ﬁrst and last slack forms, evaluated at Nx, are equal. More
generally, the equivalence of all slack forms implies that for anyset of values
xD.x1;x2;:::;x n/,w eh a v e
nX
jD1cjxjD/ETB0CnCmX
jD1c0
jxj:
Therefore, for any particular set of values NxD.Nx1;Nx2;:::;Nxn/,w eh a v e884 Chapter 29 Linear Programming
nX
jD1cjNxj
D/ETB0CnCmX
jD1c0
jNxj
D/ETB0CnX
jD1c0
jNxjCnCmX
jDnC1c0
jNxj
D/ETB0CnX
jD1c0
jNxjCmX
iD1c0
nCiNxnCi
D/ETB0CnX
jD1c0
jNxjCmX
iD1./NULNyi/NxnCi (by equations (29.91) and (29.95))
D/ETB0CnX
jD1c0
jNxjCmX
iD1./NULNyi/ 
bi/NULnX
jD1aijNxj!
(by equation (29.32))
D/ETB0CnX
jD1c0
jNxj/NULmX
iD1biNyiCmX
iD1nX
jD1.aijNxj/Nyi
D/ETB0CnX
jD1c0
jNxj/NULmX
iD1biNyiCnX
jD1mX
iD1.aijNyi/Nxj
D 
/ETB0/NULmX
iD1biNyi!
CnX
jD1 
c0
jCmX
iD1aijNyi!
Nxj;
so that
nX
jD1cjNxjD 
/ETB0/NULmX
iD1biNyi!
CnX
jD1 
c0
jCmX
iD1aijNyi!
Nxj: (29.99)
Applying Lemma 29.3 to equation (29.99), we obtain
/ETB0/NULmX
iD1biNyiD0; (29.100)
c0
jCmX
iD1aijNyiDcjforjD1 ;2;:::;n: (29.101)
By equation (29.100), we have thatPm
iD1biNyiD/ETB0, and hence the objective value
of the dual/DLEPm
iD1biNyi/DC1
is equal to that of the primal ( /ETB0). It remains to show29.4 Duality 885
that the solutionNyis feasible for the dual problem. From inequalities (29.94) and
equations (29.95), we have that c0
j/DC40for all jD1 ;2;:::;nCm. Hence, for any
jD1 ;2;:::;n , equations (29.101) imply that
cjDc0
jCmX
iD1aijNyi
/DC4mX
iD1aijNyi;
which satisﬁes the constraints (29.84) of the dual. Finally, since c0
j/DC40for each
j2N[B, when we setNyaccording to equation (29.91), we have that each Nyi/NAK0,
and so the nonnegativity constraints are satisﬁed as well.
We have shown that, given a feasible linear program, if I NITIALIZE -SIMPLEX
returns a feasible solution, and if S IMPLEX terminates without returning “un-
bounded,” then the solution returned is indeed an optimal solution. We have also
shown how to construct an optimal solution to the dual linear program.
Exercises
29.4-1
Formulate the dual of the linear program given in Exercise 29.3-5.
29.4-2
Suppose that we have a linear program that is not in standard form. We couldproduce the dual by ﬁrst converting it to standard form, and then taking the dual.
It would be more convenient, however, to be able to produce the dual directly.
Explain how we can directly take the dual of an arbitrary linear program.
29.4-3
Write down the dual of the maximum-ﬂow linear program, as given in lines(29.47)–(29.50) on page 860. Explain how to interpret this formulation as aminimum-cut problem.
29.4-4
Write down the dual of the minimum-cost-ﬂow linear program, as given in lines(29.51)–(29.52) on page 862. Explain how to interpret this problem in terms ofgraphs and ﬂows.
29.4-5
Show that the dual of the dual of a linear program is the primal linear program.886 Chapter 29 Linear Programming
29.4-6
Which result from Chapter 26 can be interpreted as weak duality for the maximum-ﬂow problem?
29.5 The initial basic feasible solution
In this section, we ﬁrst describe how to test whether a linear program is feasible,and if it is, how to produce a slack form for which the basic solution is feasible.We conclude by proving the fundamental theorem of linear programming, whichsays that the S
IMPLEX procedure always produces the correct result.
Finding an initial solution
In Section 29.3, we assumed that we had a procedure I NITIALIZE -SIMPLEX that
determines whether a linear program has any feasible solutions, and if it does, givesa slack form for which the basic solution is feasible. We describe this procedurehere.
A linear program can be feasible, yet the initial basic solution might not be
feasible. Consider, for example, the following linear program:
maximize 2x
1/NUL x2 (29.102)
subject to
2x1/NUL x2/DC4 2 (29.103)
x1/NUL5x2/DC4/NUL 4 (29.104)
x1;x2/NAK 0: (29.105)
If we were to convert this linear program to slack form, the basic solution would
setx1D0andx2D0. This solution violates constraint (29.104), and so it is not a
feasible solution. Thus, I NITIALIZE -SIMPLEX cannot just return the obvious slack
form. In order to determine whether a linear program has any feasible solutions,
we will formulate an auxiliary linear program . For this auxiliary linear program,
we can ﬁnd (with a little work) a slack form for which the basic solution is feasible.
Furthermore, the solution of this auxiliary linear program determines whether the
initial linear program is feasible and if so, it provides a feasible solution with whichwe can initialize S
IMPLEX .
Lemma 29.11
LetLbe a linear program in standard form, given as in (29.16)–(29.18). Let x0be
a new variable, and let Lauxbe the following linear program with nC1variables:29.5 The initial basic feasible solution 887
maximize /NULx0 (29.106)
subject to
nX
jD1aijxj/NULx0/DC4biforiD1 ;2;:::;m; (29.107)
xj/NAK0forjD0; 1; : : : ; n : (29.108)
Then Lis feasible if and only if the optimal objective value of Lauxis0.
Proof Suppose that Lhas a feasible solution NxD.Nx1;Nx2;:::;Nxn/. Then the
solutionNx0D0combined withNxis a feasible solution to Lauxwith objective
value 0.S i n c e x0/NAK0is a constraint of Lauxand the objective function is to
maximize/NULx0, this solution must be optimal for Laux.
Conversely, suppose that the optimal objective value of Lauxis0.T h e nNx0D0,
and the remaining solution values of Nxsatisfy the constraints of L.
We now describe our strategy to ﬁnd an initial basic feasible solution for a linear
program Lin standard form:
INITIALIZE -SIMPLEX . A ;b;c/
1l e t kbe the index of the minimum bi
2ifbk/NAK0 //is the initial basic solution feasible?
3 return .f1 ;2;:::;ng;fnC1; nC2;:::;nCmg;A ;b;c;0 /
4f o r m Lauxby adding/NULx0to the left-hand side of each constraint
and setting the objective function to /NULx0
5l e t . N;B;A ;b;c;/ETB/ be the resulting slack form for Laux
6lDnCk
7//LauxhasnC1nonbasic variables and mbasic variables.
8. N;B;A ;b;c;/ETB/DPIVOT. N;B;A ;b;c;/ETB;l;0 /
9//The basic solution is now feasible for Laux.
10 iterate the while loop of lines 3–12 of S IMPLEX until an optimal solution
toLauxis found
11ifthe optimal solution to LauxsetsNx0to0
12 ifNx0is basic
13 perform one (degenerate) pivot to make it nonbasic14 from the ﬁnal slack form of L
aux, remove x0from the constraints and
restore the original objective function of L, but replace each basic
variable in this objective function by the right-hand side of its
associated constraint
15 return the modiﬁed ﬁnal slack form
16else return “infeasible”888 Chapter 29 Linear Programming
INITIALIZE -SIMPLEX works as follows. In lines 1–3, we implicitly test the
basic solution to the initial slack form for Lgiven by NDf1 ;2;:::;ng,BD
fnC1; nC2;:::;nCmg,NxiDbifor all i2B,a n dNxjD0for all j2N.
(Creating the slack form requires no explicit effort, as the values of A,b,a n d care
the same in both slack and standard forms.) If line 2 ﬁnds this basic solution to befeasible—that is,Nx
i/NAK0for all i2N[B—then line 3 returns the slack form.
Otherwise, in line 4, we form the auxiliary linear program Lauxas in Lemma 29.11.
Since the initial basic solution to Lis not feasible, the initial basic solution to the
slack form for Lauxcannot be feasible either. To ﬁnd a basic feasible solution, we
perform a single pivot operation. Line 6 selects lDnCkas the index of the
basic variable that will be the leaving variable in the upcoming pivot operation.Since the basic variables are x
nC1;xnC2;:::;x nCm, the leaving variable xlwill be
the one with the most negative value. Line 8 performs that call of P IVOT , with
x0entering and xlleaving. We shall see shortly that the basic solution resulting
from this call of P IVOT will be feasible. Now that we have a slack form for which
the basic solution is feasible, we can, in line 10, repeatedly call P IVOT to fully
solve the auxiliary linear program. As the test in line 11 demonstrates, if we ﬁndan optimal solution to L
auxwith objective value 0, then in lines 12–14, we create
a slack form for Lfor which the basic solution is feasible. To do so, we ﬁrst,
in lines 12–13, handle the degenerate case in which x0may still be basic with
valueNx0D0. In this case, we perform a pivot step to remove x0from the basis,
using any e2Nsuch that a0e¤0as the entering variable. The new basic
solution remains feasible; the degenerate pivot does not change the value of any
variable. Next we delete all x0terms from the constraints and restore the original
objective function for L. The original objective function may contain both basic
and nonbasic variables. Therefore, in the objective function we replace each basicvariable by the right-hand side of its associated constraint. Line 15 then returnsthis modiﬁed slack form. If, on the other hand, line 11 discovers that the originallinear program Lis infeasible, then line 16 returns this information.
We now demonstrate the operation of I
NITIALIZE -SIMPLEX on the linear pro-
gram (29.102)–(29.105). This linear program is feasible if we can ﬁnd nonneg-ative values for x
1andx2that satisfy inequalities (29.103) and (29.104). Using
Lemma 29.11, we formulate the auxiliary linear program
maximize /NULx0 (29.109)
subject to
2x1/NUL x2/NUL x0/DC4 2 (29.110)
x1/NUL5x2/NUL x0/DC4/NUL 4 (29.111)
x1;x2;x0/NAK 0:
By Lemma 29.11, if the optimal objective value of this auxiliary linear program
is0, then the original linear program has a feasible solution. If the optimal objective29.5 The initial basic feasible solution 889
value of this auxiliary linear program is negative, then the original linear program
does not have a feasible solution.
We write this linear program in slack form, obtaining
´D/NUL x0
x3D 2/NUL2x1C x2Cx0
x4D/NUL 4/NUL x1C5x2Cx0:
We are not out of the woods yet, because the basic solution, which would set
x4D/NUL4, is not feasible for this auxiliary linear program. We can, however, with
one call to P IVOT , convert this slack form into one in which the basic solution is
feasible. As line 8 indicates, we choose x0to be the entering variable. In line 6, we
choose as the leaving variable x4, which is the basic variable whose value in the
basic solution is most negative. After pivoting, we have the slack form
´D/NUL 4/NULx1C5x2/NULx4
x0D 4Cx1/NUL5x2Cx4
x3D 6/NULx1/NUL4x2Cx4:
The associated basic solution is .Nx0;Nx1;Nx2;Nx3;Nx4/D. 4;0 ;0 ;6 ;0 / , which is feasi-
ble. We now repeatedly call P IVOT until we obtain an optimal solution to Laux.I n
this case, one call to P IVOT withx2entering and x0leaving yields
´D/NUL x0
x2D4
5/NULx0
5Cx1
5Cx4
5
x3D14
5C4x0
5/NUL9x1
5Cx4
5:
This slack form is the ﬁnal solution to the auxiliary problem. Since this solution
hasx0D0, we know that our initial problem was feasible. Furthermore, since
x0D0, we can just remove it from the set of constraints. We then restore the
original objective function, with appropriate substitutions made to include onlynonbasic variables. In our example, we get the objective function
2x
1/NULx2D2x1/NUL/DC24
5/NULx0
5Cx1
5Cx4
5/DC3
:
Setting x0D0and simplifying, we get the objective function
/NUL4
5C9x1
5/NULx4
5;
and the slack form890 Chapter 29 Linear Programming
´D/NUL4
5C9x1
5/NULx4
5
x2D4
5Cx1
5Cx4
5
x3D14
5/NUL9x1
5Cx4
5:
This slack form has a feasible basic solution, and we can return it to procedure
SIMPLEX .
We now formally show the correctness of I NITIALIZE -SIMPLEX .
Lemma 29.12
If a linear program Lhas no feasible solution, then I NITIALIZE -SIMPLEX returns
“infeasible.” Otherwise, it returns a valid slack form for which the basic solutionis feasible.
Proof First suppose that the linear program Lhas no feasible solution. Then by
Lemma 29.11, the optimal objective value of L
aux, deﬁned in (29.106)–(29.108),
is nonzero, and by the nonnegativity constraint on x0, the optimal objective value
must be negative. Furthermore, this objective value must be ﬁnite, since settingx
iD0,f o riD1 ;2;:::;n ,a n d x0Djminm
iD1fbigjis feasible, and this solution
has objective value /NULjminm
iD1fbigj. Therefore, line 10 of I NITIALIZE -SIMPLEX
ﬁnds a solution with a nonpositive objective value. Let Nxbe the basic solution
associated with the ﬁnal slack form. We cannot have Nx0D0, because then Laux
would have objective value 0, which contradicts that the objective value is negative.
Thus the test in line 11 results in line 16 returning “infeasible.”
Suppose now that the linear program Ldoes have a feasible solution. From
Exercise 29.3-4, we know that if bi/NAK0foriD1 ;2;:::;m , then the basic solution
associated with the initial slack form is feasible. In this case, lines 2–3 return theslack form associated with the input. (Converting the standard form to slack formis easy, since A,b,a n d care the same in both.)
In the remainder of the proof, we handle the case in which the linear program is
feasible but we do not return in line 3. We argue that in this case, lines 4–10 ﬁnd afeasible solution to L
auxwith objective value 0. First, by lines 1–2, we must have
bk<0;
and
bk/DC4bifor each i2B: (29.112)
In line 8, we perform one pivot operation in which the leaving variable xl(recall
thatlDnCk,s ot h a t bl<0) is the left-hand side of the equation with mini-
mum bi, and the entering variable is x0, the extra added variable. We now show29.5 The initial basic feasible solution 891
that after this pivot, all entries of bare nonnegative, and hence the basic solution
toLauxis feasible. Letting Nxbe the basic solution after the call to P IVOT ,a n d
lettingybandyBbe values returned by P IVOT , Lemma 29.1 implies that
NxiD(
bi/NULaieybeifi2yB/NULfeg;
bl=ale ifiDe:(29.113)
The call to P IVOT in line 8 has eD0. If we rewrite inequalities (29.107), to
include coefﬁcients ai0,
nX
jD0aijxj/DC4biforiD1 ;2;:::;m; (29.114)
then
ai0DaieD/NUL1for each i2B: (29.115)
(Note that ai0is the coefﬁcient of x0as it appears in inequalities (29.114), not
the negation of the coefﬁcient, because Lauxis in standard rather than slack form.)
Since l2B,w ea l s oh a v et h a t aleD/NUL1. Thus, bl=ale>0,a n ds oNxe>0.F o r
the remaining basic variables, we have
NxiDbi/NULaieybe (by equation (29.113))
Dbi/NULaie.bl=ale/(by line 3 of P IVOT )
Dbi/NULbl (by equation (29.115) and aleD/NUL1)
/NAK0 (by inequality (29.112)) ,
which implies that each basic variable is now nonnegative. Hence the basic solu-
tion after the call to P IVOT in line 8 is feasible. We next execute line 10, which
solves Laux. Since we have assumed that Lhas a feasible solution, Lemma 29.11
implies that Lauxhas an optimal solution with objective value 0. Since all the slack
forms are equivalent, the ﬁnal basic solution to Lauxmust haveNx0D0, and after
removing x0from the linear program, we obtain a slack form that is feasible for L.
Line 15 then returns this slack form.
Fundamental theorem of linear programming
We conclude this chapter by showing that the S IMPLEX procedure works. In par-
ticular, any linear program either is infeasible, is unbounded, or has an optimalsolution with a ﬁnite objective value. In each case, S
IMPLEX acts appropriately.892 Chapter 29 Linear Programming
Theorem 29.13 (Fundamental theorem of linear programming)
Any linear program L, given in standard form, either
1. has an optimal solution with a ﬁnite objective value,
2. is infeasible, or
3. is unbounded.
IfLis infeasible, S IMPLEX returns “infeasible.” If Lis unbounded, S IMPLEX
returns “unbounded.” Otherwise, S IMPLEX returns an optimal solution with a ﬁnite
objective value.
Proof By Lemma 29.12, if linear program Lis infeasible, then S IMPLEX returns
“infeasible.” Now suppose that the linear program Lis feasible. By Lemma 29.12,
INITIALIZE -SIMPLEX returns a slack form for which the basic solution is feasible.
By Lemma 29.7, therefore, S IMPLEX either returns “unbounded” or terminates
with a feasible solution. If it terminates with a ﬁnite solution, then Theorem 29.10tells us that this solution is optimal. On the other hand, if S
IMPLEX returns “un-
bounded,” Lemma 29.2 tells us the linear program Lis indeed unbounded. Since
SIMPLEX always terminates in one of these ways, the proof is complete.
Exercises
29.5-1
Give detailed pseudocode to implement lines 5 and 14 of I NITIALIZE -SIMPLEX .
29.5-2
Show that when the main loop of S IMPLEX is run by I NITIALIZE -SIMPLEX , it can
never return “unbounded.”
29.5-3
Suppose that we are given a linear program Lin standard form, and suppose that
for both Land the dual of L, the basic solutions associated with the initial slack
forms are feasible. Show that the optimal objective value of Lis0.
29.5-4
Suppose that we allow strict inequalities in a linear program. Show that in this
case, the fundamental theorem of linear programming does not hold.29.5 The initial basic feasible solution 893
29.5-5
Solve the following linear program using S IMPLEX :
maximize x1C3x2
subject to
x1/NUL x2/DC4 8
/NULx1/NUL x2/DC4/NUL 3
/NULx1C4x2/DC4 2
x1;x2/NAK 0:
29.5-6
Solve the following linear program using S IMPLEX :
maximize x1/NUL2x2
subject to
x1C2x2/DC4 4
/NUL2x1/NUL6x2/DC4/NUL 12
x2/DC4 1
x1;x2/NAK 0:
29.5-7
Solve the following linear program using S IMPLEX :
maximize x1C3x2
subject to
/NULx1C x2/DC4/NUL 1
/NULx1/NUL x2/DC4/NUL 3
/NULx1C4x2/DC4 2
x1;x2/NAK 0:
29.5-8
Solve the linear program given in (29.6)–(29.10).
29.5-9
Consider the following 1-variable linear program, which we call P:
maximize tx
subject to
rx/DC4s
x/NAK0;
where r,s,a n d tare arbitrary real numbers. Let Dbe the dual of P.894 Chapter 29 Linear Programming
State for which values of r,s,a n d tyou can assert that
1. Both PandDhave optimal solutions with ﬁnite objective values.
2.Pis feasible, but Dis infeasible.
3.Dis feasible, but Pis infeasible.
4. Neither PnorDis feasible.
Problems
29-1 Linear-inequality feasibility
Given a set of mlinear inequalities on nvariables x1;x2;:::;x n,t h elinear-
inequality feasibility problem asks whether there is a setting of the variables that
simultaneously satisﬁes each of the inequalities.
a.Show that if we have an algorithm for linear programming, we can use it to
solve a linear-inequality feasibility problem. The number of variables and con-straints that you use in the linear-programming problem should be polynomialinnandm.
b.Show that if we have an algorithm for the linear-inequality feasibility problem,
we can use it to solve a linear-programming problem. The number of variables
and linear inequalities that you use in the linear-inequality feasibility problemshould be polynomial in nandm, the number of variables and constraints in
the linear program.
29-2 Complementary slackness
Complementary slackness describes a relationship between the values of primal
variables and dual constraints and between the values of dual variables and pri-mal constraints. Let Nxbe a feasible solution to the primal linear program given
in (29.16)–(29.18), and let Nybe a feasible solution to the dual linear program given
in (29.83)–(29.85). Complementary slackness states that the following conditionsare necessary and sufﬁcient for NxandNyto be optimal:
mX
iD1aijNyiDcjorNxjD0forjD1 ;2;:::;n
and
nX
jD1aijNxjDbiorNyiD0foriD1 ;2;:::;m:Problems for Chapter 29 895
a.Verify that complementary slackness holds for the linear program in lines
(29.53)–(29.57).
b.Prove that complementary slackness holds for any primal linear program and
its corresponding dual.
c.Prove that a feasible solution Nxto a primal linear program given in lines
(29.16)–(29.18) is optimal if and only if there exist values NyD.Ny1;Ny2;:::;Nym/
such that
1.Nyis a feasible solution to the dual linear program given in (29.83)–(29.85),
2.Pm
iD1aijNyiDcjfor all jsuch thatNxj>0,a n d
3.NyiD0for all isuch thatPn
jD1aijNxj<b i.
29-3 Integer linear programming
Aninteger linear-programming problem is a linear-programming problem with
the additional constraint that the variables xmust take on integral values. Exer-
cise 34.5-3 shows that just determining whether an integer linear program has afeasible solution is NP-hard, which means that there is no known polynomial-timealgorithm for this problem.
a.Show that weak duality (Lemma 29.8) holds for an integer linear program.
b.Show that duality (Theorem 29.10) does not always hold for an integer linear
program.
c.Given a primal linear program in standard form, let us deﬁne Pto be the opti-
mal objective value for the primal linear program, Dto be the optimal objective
value for its dual, IPto be the optimal objective value for the integer version of
the primal (that is, the primal with the added constraint that the variables takeon integer values), and IDto be the optimal objective value for the integer ver-
sion of the dual. Assuming that both the primal integer program and the dualinteger program are feasible and bounded, show that
IP/DC4PDD/DC4ID:
29-4 Farkas’s lemma
LetAbe an m/STXnmatrix and cbe an n-vector. Then Farkas’s lemma states that
exactly one of the systems896 Chapter 29 Linear Programming
Ax/DC40;
cTx>0
and
ATyDc;
y/NAK0
is solvable, where xis an n-vector and yis an m-vector. Prove Farkas’s lemma.
29-5 Minimum-cost circulation
In this problem, we consider a variant of the minimum-cost-ﬂow problem from
Section 29.2 in which we are not given a demand, a source, or a sink. Instead,
we are given, as before, a ﬂow network and edge costs a.u;/ETB/ . A ﬂow is feasible
if it satisﬁes the capacity constraint on every edge and ﬂow conservation at every
vertex. The goal is to ﬁnd, among all feasible ﬂows, the one of minimum cost. Wecall this problem the minimum-cost-circulation problem.
a.Formulate the minimum-cost-circulation problem as a linear program.
b.Suppose that for all edges .u; /ETB/2E,w eh a v e a.u;/ETB/ > 0 . Characterize an
optimal solution to the minimum-cost-circulation problem.
c.Formulate the maximum-ﬂow problem as a minimum-cost-circulation problem
linear program. That is given a maximum-ﬂow problem instance GD.V; E/
with source s,s i n k tand edge capacities c, create a minimum-cost-circulation
problem by giving a (possibly different) network G
0D.V0;E0/with edge
capacities c0and edge costs a0such that you can discern a solution to the
maximum-ﬂow problem from a solution to the minimum-cost-circulation prob-lem.
d.Formulate the single-source shortest-path problem as a minimum-cost-circu-
lation problem linear program.
Chapter notes
This chapter only begins to study the wide ﬁeld of linear programming. A num-ber of books are devoted exclusively to linear programming, including those byChv´atal [69], Gass [130], Karloff [197], Schrijver [303], and Vanderbei [344].
Many other books give a good coverage of linear programming, including thoseby Papadimitriou and Steiglitz [271] and Ahuja, Magnanti, and Orlin [7]. Thecoverage in this chapter draws on the approach taken by Chv´ atal.Notes for Chapter 29 897
The simplex algorithm for linear programming was invented by G. Dantzig
in 1947. Shortly after, researchers discovered how to formulate a number of prob-lems in a variety of ﬁelds as linear programs and solve them with the simplexalgorithm. As a result, applications of linear programming ﬂourished, along withseveral algorithms. Variants of the simplex algorithm remain the most popularmethods for solving linear-programming problems. This history appears in a num-ber of places, including the notes in [69] and [197].
The ellipsoid algorithm was the ﬁrst polynomial-time algorithm for linear pro-
gramming and is due to L. G. Khachian in 1979; it was based on earlier work by
N. Z. Shor, D. B. Judin, and A. S. Nemirovskii. Gr¨ otschel, Lov´ asz, and Schrijver
[154] describe how to use the ellipsoid algorithm to solve a variety of problems incombinatorial optimization. To date, the ellipsoid algorithm does not appear to becompetitive with the simplex algorithm in practice.
Karmarkar’s paper [198] includes a description of the ﬁrst interior-point algo-
rithm. Many subsequent researchers designed interior-point algorithms. Good sur-
veys appear in the article of Goldfarb and Todd [141] and the book by Ye [361].
Analysis of the simplex algorithm remains an active area of research. V. Klee
and G. J. Minty constructed an example on which the simplex algorithm runs
through 2
n/NUL1iterations. The simplex algorithm usually performs very well in
practice and many researchers have tried to give theoretical justiﬁcation for thisempirical observation. A line of research begun by K. H. Borgwardt, and carriedon by many others, shows that under certain probabilistic assumptions on the in-
put, the simplex algorithm converges in expected polynomial time. Spielman and
Teng [322] made progress in this area, introducing the “smoothed analysis of algo-rithms” and applying it to the simplex algorithm.
The simplex algorithm is known to run efﬁciently in certain special cases. Par-
ticularly noteworthy is the network-simplex algorithm, which is the simplex al-gorithm, specialized to network-ﬂow problems. For certain network problems,including the shortest-paths, maximum-ﬂow, and minimum-cost-ﬂow problems,variants of the network-simplex algorithm run in polynomial time. See, for exam-ple, the article by Orlin [268] and the citations therein.30 Polynomials and the FFT
The straightforward method of adding two polynomials of degree ntakes ‚.n/
time, but the straightforward method of multiplying them takes ‚.n2/time. In this
chapter, we shall show how the fast Fourier transform, or FFT, can reduce the timeto multiply polynomials to ‚.n lgn/.
The most common use for Fourier transforms, and hence the FFT, is in signal
processing. A signal is given in the time domain : as a function mapping time to
amplitude. Fourier analysis allows us to express the signal as a weighted sum ofphase-shifted sinusoids of varying frequencies. The weights and phases associatedwith the frequencies characterize the signal in the frequency domain . Among the
many everyday applications of FFT’s are compression techniques used to encode
digital video and audio information, including MP3 ﬁles. Several ﬁne books delve
into the rich area of signal processing; the chapter notes reference a few of them.
Polynomials
Apolynomial in the variable xover an algebraic ﬁeld Frepresents a function A.x/
as a formal sum:
A.x/D
n/NUL1X
jD0ajxj:
We call the values a0;a1;:::;a n/NUL1thecoefﬁcients of the polynomial. The co-
efﬁcients are drawn from a ﬁeld F, typically the set Cof complex numbers. A
polynomial A.x/ hasdegree kif its highest nonzero coefﬁcient is ak; we write
that degree .A/Dk. Any integer strictly greater than the degree of a polynomial
is adegree-bound of that polynomial. Therefore, the degree of a polynomial of
degree-bound nmay be any integer between 0andn/NUL1,i n c l u s i v e .
We can deﬁne a variety of operations on polynomials. For polynomial addi-
tion,i fA.x/ andB.x/ are polynomials of degree-bound n,t h e i r sum is a polyno-Chapter 30 Polynomials and the FFT 899
mialC.x/ , also of degree-bound n, such that C.x/DA.x/CB.x/ for all xin the
underlying ﬁeld. That is, if
A.x/Dn/NUL1X
jD0ajxj
and
B.x/Dn/NUL1X
jD0bjxj;
then
C.x/Dn/NUL1X
jD0cjxj;
where cjDajCbjforjD0; 1; : : : ; n/NUL1. For example, if we have the
polynomials A.x/D6x3C7x2/NUL10xC9andB.x/D/NUL2x3C4x/NUL5,t h e n
C.x/D4x3C7x2/NUL6xC4.
Forpolynomial multiplication ,i fA.x/ andB.x/ are polynomials of degree-
bound n,t h e i r product C.x/ is a polynomial of degree-bound 2n/NUL1such that
C.x/DA.x/B.x/ for all xin the underlying ﬁeld. You probably have multi-
plied polynomials before, by multiplying each term in A.x/ by each term in B.x/
and then combining terms with equal powers. For example, we can multiplyA.x/D6x
3C7x2/NUL10xC9andB.x/D/NUL2x3C4x/NUL5as follows:
6x3C7x2/NUL10xC9
/NUL2x3C4x/NUL5
/NUL30x3/NUL35x2C50x/NUL45
24x4C28x3/NUL40x2C36x
/NUL12x6/NUL14x5C20x4/NUL18x3
/NUL12x6/NUL14x5C44x4/NUL20x3/NUL75x2C86x/NUL45
Another way to express the product C.x/ is
C.x/D2n/NUL2X
jD0cjxj; (30.1)
where
cjDjX
kD0akbj/NULk: (30.2)900 Chapter 30 Polynomials and the FFT
Note that degree .C /Ddegree .A/Cdegree .B/, implying that if Ais a polyno-
mial of degree-bound naandBis a polynomial of degree-bound nb,t h e n Cis a
polynomial of degree-bound naCnb/NUL1. Since a polynomial of degree-bound k
is also a polynomial of degree-bound kC1, we will normally say that the product
polynomial Cis a polynomial of degree-bound naCnb.
Chapter outline
Section 30.1 presents two ways to represent polynomials: the coefﬁcient represen-
tation and the point-value representation. The straightforward methods for multi-plying polynomials—equations (30.1) and (30.2)—take ‚.n
2/t i m ew h e nw er e p -
resent polynomials in coefﬁcient form, but only ‚.n/ time when we represent them
in point-value form. We can, however, multiply polynomials using the coefﬁcient
representation in only ‚.n lgn/time by converting between the two representa-
tions. To see why this approach works, we must ﬁrst study complex roots of unity,which we do in Section 30.2. Then, we use the FFT and its inverse, also describedin Section 30.2, to perform the conversions. Section 30.3 shows how to implementthe FFT quickly in both serial and parallel models.
This chapter uses complex numbers extensively, and within this chapter we use
the symbol iexclusively to denotep
/NUL1.
30.1 Representing polynomials
The coefﬁcient and point-value representations of polynomials are in a sense equiv-
alent; that is, a polynomial in point-value form has a unique counterpart in co-
efﬁcient form. In this section, we introduce the two representations and showhow to combine them so that we can multiply two degree-bound npolynomials
in‚.n lgn/time.
Coefﬁcient representation
Acoefﬁcient representation of a polynomial A.x/DP
n/NUL1
jD0ajxjof degree-
bound nis a vector of coefﬁcients aD.a0;a1;:::;a n/NUL1/. In matrix equations
in this chapter, we shall generally treat vectors as column vectors.
The coefﬁcient representation is convenient for certain operations on polyno-
mials. For example, the operation of evaluating the polynomial A.x/ at a given
point x0consists of computing the value of A.x 0/. We can evaluate a polynomial
in‚.n/ time using Horner’s rule :
A.x 0/Da0Cx0.a1Cx0.a2C/SOH/SOH/SOHC x0.an/NUL2Cx0.an/NUL1///SOH/SOH/SOH// :30.1 Representing polynomials 901
Similarly, adding two polynomials represented by the coefﬁcient vectors aD
.a0;a1;:::;a n/NUL1/andbD.b0;b1;:::;b n/NUL1/takes ‚.n/ time: we just produce
the coefﬁcient vector cD.c0;c1;:::;c n/NUL1/,w h e r e cjDajCbjforjD
0; 1; : : : ; n/NUL1.
Now, consider multiplying two degree-bound npolynomials A.x/ andB.x/ rep-
resented in coefﬁcient form. If we use the method described by equations (30.1)and (30.2), multiplying polynomials takes time ‚.n
2/, since we must multiply
each coefﬁcient in the vector aby each coefﬁcient in the vector b. The operation
of multiplying polynomials in coefﬁcient form seems to be considerably more difﬁ-
cult than that of evaluating a polynomial or adding two polynomials. The resultingcoefﬁcient vector c, given by equation (30.2), is also called the convolution of the
input vectors aandb, denoted cDa˝b. Since multiplying polynomials and
computing convolutions are fundamental computational problems of considerablepractical importance, this chapter concentrates on efﬁcient algorithms for them.
Point-value representation
Apoint-value representation of a polynomial A.x/ of degree-bound nis a set of
npoint-value pairs
f.x
0;y0/; .x 1;y1/ ;:::;. x n/NUL1;yn/NUL1/g
such that all of the xkare distinct and
ykDA.x k/ (30.3)
forkD0; 1; : : : ; n/NUL1. A polynomial has many different point-value representa-
tions, since we can use any set of ndistinct points x0;x1;:::;x n/NUL1as a basis for
the representation.
Computing a point-value representation for a polynomial given in coefﬁcient
form is in principle straightforward, since all we have to do is select ndistinct
points x0;x1;:::;x n/NUL1a n dt h e ne v a l u a t e A.x k/forkD0; 1; : : : ; n/NUL1. With
Horner’s method, evaluating a polynomial at npoints takes time ‚.n2/.W es h a l l
see later that if we choose the points xkcleverly, we can accelerate this computation
to run in time ‚.n lgn/.
The inverse of evaluation—determining the coefﬁcient form of a polynomial
from a point-value representation—is interpolation . The following theorem shows
that interpolation is well deﬁned when the desired interpolating polynomial musthave a degree-bound equal to the given number of point-value pairs.
Theorem 30.1 (Uniqueness of an interpolating polynomial)
For any setf.x
0;y0/; .x 1;y1/ ;:::;. x n/NUL1;yn/NUL1/gofnpoint-value pairs such that
all the xkvalues are distinct, there is a unique polynomial A.x/ of degree-bound n
such that ykDA.x k/forkD0; 1; : : : ; n/NUL1.902 Chapter 30 Polynomials and the FFT
Proof The proof relies on the existence of the inverse of a certain matrix. Equa-
tion (30.3) is equivalent to the matrix equation˙1x 0 x2
0/SOH/SOH/SOHxn/NUL1
0
1x 1 x2
1/SOH/SOH/SOHxn/NUL1
1:::::::::::::::
1x
n/NUL1x2
n/NUL1/SOH/SOH/SOHxn/NUL1
n/NUL1/BEL˙a0
a1
:::
an/NUL1/BEL
D˙y0
y1
:::
yn/NUL1/BEL
: (30.4)
The matrix on the left is denoted V.x 0;x1;:::;x n/NUL1/and is known as a Vander-
monde matrix. By Problem D-1, this matrix has determinant
Y
0/DC4j< k /DC4n/NUL1.xk/NULxj/;
and therefore, by Theorem D.5, it is invertible (that is, nonsingular) if the xkare
distinct. Thus, we can solve for the coefﬁcients ajuniquely given the point-value
representation:
aDV.x 0;x1;:::;x n/NUL1//NUL1y:
The proof of Theorem 30.1 describes an algorithm for interpolation based on
solving the set (30.4) of linear equations. Using the LU decomposition algorithmsof Chapter 28, we can solve these equations in time O.n
3/.
A faster algorithm for n-point interpolation is based on Lagrange’s formula :
A.x/Dn/NUL1X
kD0ykY
j¤k.x/NULxj/
Y
j¤k.xk/NULxj/: (30.5)
You may wish to verify that the right-hand side of equation (30.5) is a polynomial
of degree-bound nthat satisﬁes A.x k/Dykfor all k. Exercise 30.1-5 asks you
how to compute the coefﬁcients of Ausing Lagrange’s formula in time ‚.n2/.
Thus, n-point evaluation and interpolation are well-deﬁned inverse operations
that transform between the coefﬁcient representation of a polynomial and a point-
value representation.1The algorithms described above for these problems take
time‚.n2/.
The point-value representation is quite convenient for many operations on poly-
nomials. For addition, if C.x/DA.x/CB.x/ ,t h e n C.x k/DA.x k/CB.x k/for
any point xk. More precisely, if we have a point-value representation for A,
1Interpolation is a notoriously tricky problem from the point of view of numerical stability. Although
the approaches described here are mathematically co rrect, small differences in the inputs or round-off
errors during computation can cause large differences in the result.30.1 Representing polynomials 903
f.x0;y0/; .x 1;y1/ ;:::;. x n/NUL1;yn/NUL1/g;
and for B,
f.x0;y0
0/; .x 1;y0
1/ ;:::;. x n/NUL1;y0
n/NUL1/g
(note that AandBare evaluated at the same npoints), then a point-value repre-
sentation for Cis
f.x0;y0Cy0
0/; .x 1;y1Cy0
1/ ;:::;. x n/NUL1;yn/NUL1Cy0
n/NUL1/g:
Thus, the time to add two polynomials of degree-bound nin point-value form
is‚.n/ .
Similarly, the point-value representation is convenient for multiplying polyno-
mials. If C.x/DA.x/B.x/ ,t h e n C.x k/DA.x k/B.x k/for any point xk,a n d
we can pointwise multiply a point-value representation for Aby a point-value rep-
resentation for Bto obtain a point-value representation for C. We must face the
problem, however, that degree .C /Ddegree .A/Cdegree .B/;i fAandBare of
degree-bound n,t h e n Cis of degree-bound 2n. A standard point-value represen-
tation for AandBconsists of npoint-value pairs for each polynomial. When we
multiply these together, we get npoint-value pairs, but we need 2npairs to interpo-
late a unique polynomial Cof degree-bound 2n. (See Exercise 30.1-4.) We must
therefore begin with “extended” point-value representations for Aand for Bcon-
sisting of 2npoint-value pairs each. Given an extended point-value representation
forA,
f.x0;y0/; .x 1;y1/ ;:::;. x 2n/NUL1;y2n/NUL1/g;
and a corresponding extended point-value representation for B,
f.x0;y0
0/; .x 1;y0
1/ ;:::;. x 2n/NUL1;y0
2n/NUL1/g;
then a point-value representation for Cis
f.x0;y0y0
0/; .x 1;y1y0
1/ ;:::;. x 2n/NUL1;y2n/NUL1y0
2n/NUL1/g:
Given two input polynomials in extended point-value form, we see that the time to
multiply them to obtain the point-value form of the result is ‚.n/ , much less than
the time required to multiply polynomials in coefﬁcient form.
Finally, we consider how to evaluate a polynomial given in point-value form at a
new point. For this problem, we know of no simpler approach than converting the
polynomial to coefﬁcient form ﬁrst, and then evaluating it at the new point.
Fast multiplication of polynomials in coefﬁcient form
Can we use the linear-time multiplication method for polynomials in point-value
form to expedite polynomial multiplication in coefﬁcient form? The answer hinges904 Chapter 30 Polynomials and the FFT
a0;a1;:::;a n/NUL1
b0;b1;:::;b n/NUL1c0;c1;:::;c 2n/NUL2Ordinary multiplication
Time ‚.n2/
Evaluation
Time ‚.n lgn/ Time ‚.n lgn/Interpolation
Pointwise multiplication
Time ‚.n/A.!0
2n/; B.!0
2n/
A.!1
2n/; B.!1
2n/
A.!2n/NUL1
2n/; B.!2n/NUL1
2n/::::::C.!0
2n/
C.!1
2n/
C.!2n/NUL1
2n/Coefﬁcient
Point-value
representationsrepresentations
Figure 30.1 A graphical outline of an efﬁcient polynomial-multiplication process. Representations
on the top are in coefﬁcient form, while those on the bottom are in point-value form. The arrows
from left to right correspond to the multiplication operation. The !2nterms are complex .2n/th roots
of unity.
on whether we can convert a polynomial quickly from coefﬁcient form to point-
value form (evaluate) and vice versa (interpolate).
We can use any points we want as evaluation points, but by choosing the eval-
uation points carefully, we can convert between representations in only ‚.n lgn/
time. As we shall see in Section 30.2, if we choose “complex roots of unity” as
the evaluation points, we can produce a point-value representation by taking the
discrete Fourier transform (or DFT) of a coefﬁcient vector. We can perform the
inverse operation, interpolation, by taking the “inverse DFT” of point-value pairs,yielding a coefﬁcient vector. Section 30.2 will show how the FFT accomplishesthe DFT and inverse DFT operations in ‚.n lgn/time.
Figure 30.1 shows this strategy graphically. One minor detail concerns degree-
bounds. The product of two polynomials of degree-bound nis a polynomial of
degree-bound 2n. Before evaluating the input polynomials AandB, therefore,
we ﬁrst double their degree-bounds to 2nby adding nhigh-order coefﬁcients of 0.
Because the vectors have 2nelements, we use “complex .2n/th roots of unity,”
which are denoted by the !
2nterms in Figure 30.1.
Given the FFT, we have the following ‚.n lgn/-time procedure for multiplying
two polynomials A.x/ andB.x/ of degree-bound n, where the input and output
representations are in coefﬁcient form. We assume that ni sap o w e ro f 2; we can
always meet this requirement by adding high-order zero coefﬁcients.
1.Double degree-bound: Create coefﬁcient representations of A.x/ andB.x/ as
degree-bound 2npolynomials by adding nhigh-order zero coefﬁcients to each.30.1 Representing polynomials 905
2.Evaluate: Compute point-value representations of A.x/ andB.x/ of length 2n
by applying the FFT of order 2non each polynomial. These representations
contain the values of the two polynomials at the .2n/th roots of unity.
3.Pointwise multiply: Compute a point-value representation for the polynomial
C.x/DA.x/B.x/ by multiplying these values together pointwise. This repre-
sentation contains the value of C.x/ at each .2n/th root of unity.
4.Interpolate: Create the coefﬁcient representation of the polynomial C.x/ by
applying the FFT on 2npoint-value pairs to compute the inverse DFT.
Steps (1) and (3) take time ‚.n/ , and steps (2) and (4) take time ‚.n lgn/. Thus,
once we show how to use the FFT, we will have proven the following.
Theorem 30.2
We can multiply two polynomials of degree-bound nin time ‚.n lgn/, with both
the input and output representations in coefﬁcient form.
Exercises
30.1-1
Multiply the polynomials A.x/D7x3/NULx2Cx/NUL10andB.x/D8x3/NUL6xC3
using equations (30.1) and (30.2).
30.1-2
Another way to evaluate a polynomial A.x/ of degree-bound nat a given point x0
is to divide A.x/ by the polynomial .x/NULx0/, obtaining a quotient polynomial q.x/
of degree-bound n/NUL1and a remainder r, such that
A.x/Dq.x/.x/NULx0/Cr:
Clearly, A.x 0/Dr. Show how to compute the remainder rand the coefﬁcients
ofq.x/ in time ‚.n/ from x0and the coefﬁcients of A.
30.1-3
Derive a point-value representation for Arev.x/DPn/NUL1
jD0an/NUL1/NULjxjfrom a point-
value representation for A.x/DPn/NUL1
jD0ajxj, assuming that none of the points is 0.
30.1-4
Prove that ndistinct point-value pairs are necessary to uniquely specify a polyno-
mial of degree-bound n, that is, if fewer than ndistinct point-value pairs are given,
they fail to specify a unique polynomial of degree-bound n.(Hint: Using Theo-
rem 30.1, what can you say about a set of n/NUL1point-value pairs to which you add
one more arbitraril ychosen point-value pair?)906 Chapter 30 Polynomials and the FFT
30.1-5
Show how to use equation (30.5) to interpolate in time ‚.n2/.(Hint: First compute
the coefﬁcient representation of the polynomialQ
j.x/NULxj/a n dt h e nd i v i d eb y
.x/NULxk/as necessary for the numerator of each term; see Exercise 30.1-2. You can
compute each of the ndenominators in time O.n/ .)
30.1-6
Explain what is wrong with the “obvious” approach to polynomial division usinga point-value representation, i.e., dividing the corresponding yvalues. Discuss
separately the case in which the division comes out exactly and the case in which
it doesn’t.
30.1-7
Consider two sets AandB, each having nintegers in the range from 0to10n.W e
wish to compute the Cartesian sum ofAandB,d e ﬁ n e db y
CDfxCyWx2Aandy2Bg:
Note that the integers in Care in the range from 0to20n.W e w a n t t o ﬁ n d t h e
elements of Cand the number of times each element of Cis realized as a sum of
elements in AandB. Show how to solve the problem in O.n lgn/time. ( Hint:
Represent AandBas polynomials of degree at most 10n.)
30.2 The DFT and FFT
In Section 30.1, we claimed that if we use complex roots of unity, we can evaluate
and interpolate polynomials in ‚.n lgn/time. In this section, we deﬁne complex
roots of unity and study their properties, deﬁne the DFT, and then show how theFFT computes the DFT and its inverse in ‚.n lgn/time.
Complex roots of unity
Acomplex nth root of unity is a complex number !such that
!
nD1:
There are exactly ncomplex nth roots of unity: e2/EMik=nforkD0; 1; : : : ; n/NUL1.
To interpret this formula, we use the deﬁnition of the exponential of a complexnumber:
e
iuDcos.u/Cisin.u/ :
Figure 30.2 shows that the ncomplex roots of unity are equally spaced around the
circle of unit radius centered at the origin of the complex plane. The value30.2 The DFT and FFT 907
1 /NUL1i
/NULi!0
8D!8
8!1
8!2
8
!3
8
!4
8
!5
8
!6
8!7
8
Figure 30.2 The values of !0
8;!1
8;:::;!7
8in the complex plane, where !8De2/EMi=8is the prin-
cipal 8th root of unity.
!nDe2/EMi=n(30.6)
is theprincipal nth root of unity ;2all other complex nth roots of unity are powers
of!n.
Thencomplex nth roots of unity,
!0
n;!1
n;:::;!n/NUL1
n;
form a group under multiplication (see Section 31.3). This group has the same
structure as the additive group .Zn;C/modulo n,s i n c e !n
nD!0
nD1implies that
!j
n!k
nD!jCk
nD!.jCk/modn
n . Similarly, !/NUL1
nD!n/NUL1
n. The following lemmas
furnish some essential properties of the complex nth roots of unity.
Lemma 30.3 (Cancellation lemma)
For any integers n/NAK0,k/NAK0,a n d d>0 ,
!dk
dnD!k
n: (30.7)
Proof The lemma follows directly from equation (30.6), since
!dk
dnD/NUL
e2/EMi=dn/SOHdk
D/NUL
e2/EMi=n/SOHk
D!k
n:
2Many other authors deﬁne !ndifferently: !nDe/NUL2/EMi=n. This alternative deﬁnition tends to be
used for signal-processing applications. The underlying mathematics is substantially the same with
either deﬁnition of !n.908 Chapter 30 Polynomials and the FFT
Corollary 30.4
For any even integer n>0 ,
!n=2
nD!2D/NUL1:
Proof The proof is left as Exercise 30.2-1.
Lemma 30.5 (Halving lemma)
Ifn>0 is even, then the squares of the ncomplex nth roots of unity are the n=2
complex .n=2/ th roots of unity.
Proof By the cancellation lemma, we have .!k
n/2D!k
n=2, for any nonnegative
integer k. Note that if we square all of the complex nth roots of unity, then we
obtain each .n=2/ th root of unity exactly twice, since
.!kCn=2
n /2D!2kCn
n
D!2k
n!n
n
D!2k
n
D.!k
n/2:
Thus, !k
nand!kCn=2
n have the same square. We could also have used Corol-
lary 30.4 to prove this property, since !n=2
nD/NUL1implies !kCn=2
nD/NUL!k
n,a n d
thus.!kCn=2
n /2D.!k
n/2.
As we shall see, the halving lemma is essential to our divide-and-conquer ap-
proach for converting between coefﬁcient and point-value representations of poly-nomials, since it guarantees that the recursive subproblems are only half as large.
Lemma 30.6 (Summation lemma)
For any integer n/NAK1and nonzero integer knot divisible by n,
n/NUL1X
jD0/NUL
!k
n/SOHjD0:
Proof Equation (A.5) applies to complex values as well as to reals, and so we
have30.2 The DFT and FFT 909
n/NUL1X
jD0/NUL
!k
n/SOHjD.!k
n/n/NUL1
!k
n/NUL1
D.!n
n/k/NUL1
!k
n/NUL1
D.1/k/NUL1
!k
n/NUL1
D0:
Because we require that kis not divisible by n, and because !k
nD1only when k
is divisible by n, we ensure that the denominator is not 0.
The DFT
Recall that we wish to evaluate a polynomial
A.x/Dn/NUL1X
jD0ajxj
of degree-bound nat!0
n;!1
n;!2
n;:::;!n/NUL1
n(that is, at the ncomplex nth roots of
unity).3We assume that Ais given in coefﬁcient form: aD.a0;a1;:::;a n/NUL1/.L e t
us deﬁne the results yk,f o rkD0; 1; : : : ; n/NUL1,b y
ykDA.!k
n/
Dn/NUL1X
jD0aj!kj
n: (30.8)
The vector yD.y0;y1;:::;y n/NUL1/is thediscrete Fourier transform (DFT) of the
coefﬁcient vector aD.a0;a1;:::;a n/NUL1/. We also write yDDFT n.a/.
The FFT
By using a method known as the fast Fourier transform (FFT) , which takes ad-
vantage of the special properties of the complex roots of unity, we can computeDFT
n.a/in time ‚.n lgn/, as opposed to the ‚.n2/time of the straightforward
method. We assume throughout that nis an exact power of 2. Although strategies
3The length nis actually what we referred to as 2nin Section 30.1, since we double the degree-bound
of the given polynomials prior to evaluation. In the context of polynomial multiplication, therefore,we are actually working with complex .2n/th roots of unity.910 Chapter 30 Polynomials and the FFT
for dealing with non-power-of- 2sizes are known, they are beyond the scope of this
book.
The FFT method employs a divide-and-conquer strategy, using the even-indexed
and odd-indexed coefﬁcients of A.x/ separately to deﬁne the two new polynomials
AŒ0/c141.x/andAŒ1/c141.x/of degree-bound n=2:
AŒ0/c141.x/Da0Ca2xCa4x2C/SOH/SOH/SOHC an/NUL2xn=2/NUL1;
AŒ1/c141.x/Da1Ca3xCa5x2C/SOH/SOH/SOHC an/NUL1xn=2/NUL1:
Note that AŒ0/c141contains all the even-indexed coefﬁcients of A(the binary represen-
tation of the index ends in 0)a n d AŒ1/c141contains all the odd-indexed coefﬁcients (the
binary representation of the index ends in 1). It follows that
A.x/DAŒ0/c141.x2/CxAŒ1/c141.x2/; (30.9)
so that the problem of evaluating A.x/ at!0
n;!1
n;:::;!n/NUL1
nreduces to
1. evaluating the degree-bound n=2polynomials AŒ0/c141.x/andAŒ1/c141.x/at the points
.!0
n/2;. !1
n/2;:::;. !n/NUL1
n/2; (30.10)
and then
2. combining the results according to equation (30.9).
By the halving lemma, the list of values (30.10) consists not of ndistinct val-
ues but only of the n=2 complex .n=2/ th roots of unity, with each root occurring
exactly twice. Therefore, we recursively evaluate the polynomials AŒ0/c141andAŒ1/c141
of degree-bound n=2 at the n=2 complex .n=2/ th roots of unity. These subprob-
lems have exactly the same form as the original problem, but are half the size.
We have now successfully divided an n-element DFT ncomputation into two n=2-
element DFT n=2computations. This decomposition is the basis for the follow-
ing recursive FFT algorithm, which computes the DFT of an n-element vector
aD.a0;a1;:::;a n/NUL1/,w h e r e nis a power of 2.30.2 The DFT and FFT 911
RECURSIVE -FFT .a/
1nDa:length //nis a power of 2
2ifn==1
3 return a
4!nDe2/EMi=n
5!D1
6aŒ0/c141D.a0;a2;:::;a n/NUL2/
7aŒ1/c141D.a1;a3;:::;a n/NUL1/
8yŒ0/c141DRECURSIVE -FFT .aŒ0/c141/
9yŒ1/c141DRECURSIVE -FFT .aŒ1/c141/
10forkD0ton=2/NUL1
11 ykDyŒ0/c141
kC!yŒ1/c141
k
12 ykC.n=2/DyŒ0/c141
k/NUL!yŒ1/c141
k
13 !D!! n
14return y //yis assumed to be a column vector
The R ECURSIVE -FFT procedure works as follows. Lines 2–3 represent the basis
of the recursion; the DFT of one element is the element itself, since in this case
y0Da0!0
1
Da0/SOH1
Da0:
Lines 6–7 deﬁne the coefﬁcient vectors for the polynomials AŒ0/c141andAŒ1/c141.L i n e s
4, 5, and 13 guarantee that !is updated properly so that whenever lines 11–12
are executed, we have !D!k
n. (Keeping a running value of !from iteration
to iteration saves time over computing !k
nfrom scratch each time through the for
loop.) Lines 8–9 perform the recursive DFT n=2computations, setting, for kD
0; 1; : : : ; n=2/NUL1,
yŒ0/c141
kDAŒ0/c141.!k
n=2/;
yŒ1/c141
kDAŒ1/c141.!k
n=2/;
or, since !k
n=2D!2k
nby the cancellation lemma,
yŒ0/c141
kDAŒ0/c141.!2k
n/;
yŒ1/c141
kDAŒ1/c141.!2k
n/:912 Chapter 30 Polynomials and the FFT
Lines 11–12 combine the results of the recursive DFT n=2calculations. For y0;y1;
:::;y n=2/NUL1, line 11 yields
ykDyŒ0/c141
kC!k
nyŒ1/c141
k
DAŒ0/c141.!2k
n/C!k
nAŒ1/c141.!2k
n/
DA.!k
n/ (by equation (30.9)) .
Foryn=2;yn=2C1;:::;y n/NUL1, letting kD0; 1; : : : ; n=2/NUL1, line 12 yields
ykC.n=2/DyŒ0/c141
k/NUL!k
nyŒ1/c141
k
DyŒ0/c141
kC!kC.n=2/
n yŒ1/c141
k(since !kC.n=2/
nD/NUL!k
n)
DAŒ0/c141.!2k
n/C!kC.n=2/
n AŒ1/c141.!2k
n/
DAŒ0/c141.!2kCn
n/C!kC.n=2/
n AŒ1/c141.!2kCn
n/(since !2kCn
nD!2k
n)
DA.!kC.n=2/
n / (by equation (30.9)) .
Thus, the vector yreturned by R ECURSIVE -FFT is indeed the DFT of the input
vector a.
Lines 11 and 12 multiply each value yŒ1/c141
kby!k
n,f o r kD0; 1; : : : ; n=2/NUL1.
Line 11 adds this product to yŒ0/c141
k, and line 12 subtracts it. Because we use each
factor !k
nin both its positive and negative forms, we call the factors !k
ntwiddle
factors .
To determine the running time of procedure R ECURSIVE -FFT, we note that
exclusive of the recursive calls, each invocation takes time ‚.n/ ,w h e r e nis the
length of the input vector. The recurrence for the running time is therefore
T .n/D2T .n=2/C‚.n/
D‚.n lgn/ :
Thus, we can evaluate a polynomial of degree-bound nat the complex nth roots of
unity in time ‚.n lgn/using the fast Fourier transform.
Interpolation at the complex roots of unity
We now complete the polynomial multiplication scheme by showing how to in-
terpolate the complex roots of unity by a polynomial, which enables us to convertfrom point-value form back to coefﬁcient form. We interpolate by writing the DFTas a matrix equation and then looking at the form of the matrix inverse.
From equation (30.4), we can write the DFT as the matrix product yDV
na,
where Vnis a Vandermonde matrix containing the appropriate powers of !n:30.2 The DFT and FFT 913/STX
y0
y1
y2
y3
:::
yn/NUL1/ETX
D/STX
11 1 1 /SOH/SOH/SOH 1
1! n !2
n !3
n/SOH/SOH/SOH !n/NUL1
n
1!2
n !4
n !6
n/SOH/SOH/SOH !2.n/NUL1/
n
1!3
n !6
n !9
n/SOH/SOH/SOH !3.n/NUL1/
n::::::::::::::::::
1!
n/NUL1
n!2.n/NUL1/
n !3.n/NUL1/
n/SOH/SOH/SOH!.n/NUL1/.n/NUL1/
n/ETX/STX
a0
a1
a2
a3
:::
an/NUL1/ETX
:
The.k; j / entry of Vnis!kj
n,f o r j;kD0; 1; : : : ; n/NUL1. The exponents of the
entries of Vnform a multiplication table.
For the inverse operation, which we write as aDDFT/NUL1
n.y/, we proceed by
multiplying yby the matrix V/NUL1
n, the inverse of Vn.
Theorem 30.7
Forj;kD0; 1; : : : ; n/NUL1,t h e.j; k/ entry of V/NUL1
nis!/NULkj
n=n.
Proof We show that V/NUL1
nVnDIn,t h en/STXnidentity matrix. Consider the .j; j0/
entry of V/NUL1
nVn:
ŒV/NUL1
nVn/c141jj0Dn/NUL1X
kD0.!/NULkj
n=n/.!kj0
n/
Dn/NUL1X
kD0!k.j0/NULj/
n =n :
This summation equals 1ifj0Dj, and it is 0otherwise by the summation lemma
(Lemma 30.6). Note that we rely on /NUL.n/NUL1//DC4j0/NULj/DC4n/NUL1,s ot h a t j0/NULjis
not divisible by n, in order for the summation lemma to apply.
Given the inverse matrix V/NUL1
n,w eh a v et h a tD F T/NUL1
n.y/is given by
ajD1
nn/NUL1X
kD0yk!/NULkj
n (30.11)
forjD0; 1; : : : ; n/NUL1. By comparing equations (30.8) and (30.11), we see that
by modifying the FFT algorithm to switch the roles of aandy, replace !nby!/NUL1
n,
and divide each element of the result by n, we compute the inverse DFT (see Ex-
ercise 30.2-4). Thus, we can compute DFT/NUL1
nin‚.n lgn/time as well.
We see that, by using the FFT and the inverse FFT, we can transform a poly-
nomial of degree-bound nback and forth between its coefﬁcient representation
and a point-value representation in time ‚.n lgn/. In the context of polynomial
multiplication, we have shown the following.914 Chapter 30 Polynomials and the FFT
Theorem 30.8 (Convolution theorem)
For any two vectors aandbof length n,w h e r e ni sap o w e ro f 2,
a˝bDDFT/NUL1
2n.DFT 2n.a//SOHDFT 2n.b// ;
where the vectors aandbare padded with 0s to length 2nand/SOHdenotes the com-
ponentwise product of two 2n-element vectors.
Exercises
30.2-1
Prove Corollary 30.4.
30.2-2
Compute the DFT of the vector . 0 ;1 ;2;3 / .
30.2-3
Do Exercise 30.1-1 by using the ‚.n lgn/-time scheme.
30.2-4
Write pseudocode to compute DFT/NUL1
nin‚.n lgn/time.
30.2-5
Describe the generalization of the FFT procedure to the case in which ni sap o w e r
of3. Give a recurrence for the running time, and solve the recurrence.
30.2-6 ?
Suppose that instead of performing an n-element FFT over the ﬁeld of complex
numbers (where nis even), we use the ring Zmof integers modulo m,w h e r e
mD2tn=2C1andtis an arbitrary positive integer. Use !D2tinstead of !n
as a principal nth root of unity, modulo m. Prove that the DFT and the inverse DFT
are well deﬁned in this system.
30.2-7
G i v e nal i s to fv a l u e s ´0;´1;:::;´ n/NUL1(possibly with repetitions), show how to ﬁnd
the coefﬁcients of a polynomial P.x/ of degree-bound nC1that has zeros only
at´0;´1;:::;´ n/NUL1(possibly with repetitions). Your procedure should run in time
O.n lg2n/.(Hint: The polynomial P.x/ has a zero at ´jif and only if P.x/ is a
multiple of .x/NUL´j/.)
30.2-8 ?
Thechirp transform of a vector aD.a0;a1;:::;a n/NUL1/is the vector yD
.y0;y1;:::;y n/NUL1/,w h e r e ykDPn/NUL1
jD0aj´kjand´is any complex number. The30.3 Efﬁcient FFT implementations 915
DFT is therefore a special case of the chirp transform, obtained by taking ´D!n.
Show how to evaluate the chirp transform in time O.n lgn/for any complex num-
ber´.(Hint: Use the equation
ykD´k2=2n/NUL1X
jD0/DLE
aj´j2=2/DC1/DLE
´/NUL.k/NULj/2=2/DC1
to view the chirp transform as a convolution.)
30.3 Efﬁcient FFT implementations
Since the practical applications of the DFT, such as signal processing, demand the
utmost speed, this section examines two efﬁcient FFT implementations. First, we
shall examine an iterative version of the FFT algorithm that runs in ‚.n lgn/time
but can have a lower constant hidden in the ‚-notation than the recursive version
in Section 30.2. (Depending on the exact implementation, the recursive versionmay use the hardware cache more efﬁciently.) Then, we shall use the insights thatled us to the iterative implementation to design an efﬁcient parallel FFT circuit.
An iterative FFT implementation
We ﬁrst note that the forloop of lines 10–13 of R
ECURSIVE -FFT involves com-
puting the value !k
nyŒ1/c141
ktwice. In compiler terminology, we call such a value a
common subexpression . We can change the loop to compute it only once, storing
it in a temporary variable t.
forkD0ton=2/NUL1
tD!yŒ1/c141
k
ykDyŒ0/c141
kCt
ykC.n=2/DyŒ0/c141
k/NULt
!D!! n
The operation in this loop, multiplying the twiddle factor !D!k
nbyyŒ1/c141
k, storing
the product into t, and adding and subtracting tfrom yŒ0/c141
k, is known as a butterﬂy
operation and is shown schematically in Figure 30.3.
We now show how to make the FFT algorithm iterative rather than recursive
in structure. In Figure 30.4, we have arranged the input vectors to the recursivecalls in an invocation of R
ECURSIVE -FFT in a tree structure, where the initial
call is for nD8. The tree has one node for each call of the procedure, labeled916 Chapter 30 Polynomials and the FFT
+
– •
(a) (b)yŒ0/c141
kyŒ0/c141
k
yŒ1/c141
kyŒ1/c141
k!k
n !k
nyŒ0/c141
kC!k
nyŒ1/c141
kyŒ0/c141
kC!k
nyŒ1/c141
k
yŒ0/c141
k/NUL!k
nyŒ1/c141
kyŒ0/c141
k/NUL!k
nyŒ1/c141
k
Figure 30.3 A butterﬂy operation. (a)The two input values enter from the left, the twiddle fac-
tor!k
nis multiplied by yŒ1/c141
k, and the sum and difference are output on the right. (b)As i m p l i ﬁ e d
drawing of a butterﬂy operation. We will use this representation in a parallel FFT circuit.
(a0,a1,a2,a3,a4,a5,a6,a7)
(a0,a2,a4,a6)
(a0,a4)( a2,a6)
(a0)( a4)( a2)( a6)(a1,a3,a5,a7)
(a1,a5)
(a1)( a5)(a3,a7)
(a3)( a7)
Figure 30.4 The tree of input vectors to the recursive calls of the R ECURSIVE -FFT procedure. The
initial invocation is for nD8.
by the corresponding input vector. Each R ECURSIVE -FFT invocation makes two
recursive calls, unless it has received a 1-element vector. The ﬁrst call appears in
the left child, and the second call appears in the right child.
Looking at the tree, we observe that if we could arrange the elements of the
initial vector ainto the order in which they appear in the leaves, we could trace
the execution of the R ECURSIVE -FFT procedure, but bottom up instead of top
down. First, we take the elements in pairs, compute the DFT of each pair usingone butterﬂy operation, and replace the pair with its DFT. The vector then holdsn=2 2 -element DFTs. Next, we take these n=2 DFTs in pairs and compute the
DFT of the four vector elements they come from by executing two butterﬂy oper-ations, replacing two 2-element DFTs with one 4-element DFT. The vector then
holds n=4 4 -element DFTs. We continue in this manner until the vector holds two
.n=2/ -element DFTs, which we combine using n=2 butterﬂy operations into the
ﬁnaln-element DFT.
To turn this bottom-up approach into code, we use an array AŒ0 : : n/NUL1/c141that
initially holds the elements of the input vector ain the order in which they appear30.3 Efﬁcient FFT implementations 917
in the leaves of the tree of Figure 30.4. (We shall show later how to determine this
order, which is known as a bit-reversal permutation.) Because we have to combineDFTs on each level of the tree, we introduce a variable sto count the levels, ranging
from 1(at the bottom, when we are combining pairs to form 2-element DFTs)
to lgn(at the top, when we are combining two .n=2/ -element DFTs to produce the
ﬁnal result). The algorithm therefore has the following structure:
1forsD1tolgn
2 forkD0ton/NUL1by2
s
3 combine the two 2s/NUL1-element DFTs in
AŒk : : kC2s/NUL1/NUL1/c141andAŒkC2s/NUL1::kC2s/NUL1/c141
into one 2s-element DFT in AŒk : : kC2s/NUL1/c141
We can express the body of the loop (line 3) as more precise pseudocode. We
copy the forloop from the R ECURSIVE -FFT procedure, identifying yŒ0/c141with
AŒk : : kC2s/NUL1/NUL1/c141andyŒ1/c141with AŒkC2s/NUL1::kC2s/NUL1/c141. The twiddle fac-
tor used in each butterﬂy operation depends on the value of s;i ti sap o w e ro f !m,
where mD2s. (We introduce the variable msolely for the sake of readability.)
We introduce another temporary variable uthat allows us to perform the butterﬂy
operation in place. When we replace line 3 of the overall structure by the loop
body, we get the following pseudocode, which forms the basis of the parallel im-
plementation we shall present later. The code ﬁrst calls the auxiliary procedure
BIT-REVERSE -COPY.a; A/ to copy vector ainto array Ain the initial order in
which we need the values.
ITERATIVE -FFT .a/
1B IT-REVERSE -COPY.a; A/
2nDa:length //nis a power of 2
3forsD1tolgn
4 mD2s
5 !mDe2/EMi=m
6 forkD0ton/NUL1bym
7 !D1
8 forjD0tom=2/NUL1
9 tD! AŒkCjCm=2/c141
10 uDAŒkCj/c141
11 AŒkCj/c141DuCt
12 AŒkCjCm=2/c141Du/NULt
13 !D!! m
14return A
How does B IT-REVERSE -COPY get the elements of the input vector ainto the
desired order in the array A? The order in which the leaves appear in Figure 30.4918 Chapter 30 Polynomials and the FFT
is abit-reversal permutation . That is, if we let rev .k/be the lg n-bit integer
formed by reversing the bits of the binary representation of k,t h e nw ew a n tt o
place vector element akin array position AŒrev.k//c141. In Figure 30.4, for exam-
ple, the leaves appear in the order 0; 4; 2; 6; 1; 5; 3; 7 ; this sequence in binary is
000; 100; 010; 110; 001; 101; 011; 111 , and when we reverse the bits of each value
we get the sequence 000; 001; 010; 011; 100; 101; 110; 111 . To see that we want a
bit-reversal permutation in general, we note that at the top level of the tree, indiceswhose low-order bit is 0go into the left subtree and indices whose low-order bit
is1go into the right subtree. Stripping off the low-order bit at each level, we con-
tinue this process down the tree, until we get the order given by the bit-reversalpermutation at the leaves.
Since we can easily compute the function rev .k/,t h eB
IT-REVERSE -COPY pro-
cedure is simple:
BIT-REVERSE -COPY.a; A/
1nDa:length
2forkD0ton/NUL1
3 AŒrev.k//c141Dak
The iterative FFT implementation runs in time ‚.n lgn/. The call to B IT-
REVERSE -COPY.a; A/ certainly runs in O.n lgn/time, since we iterate ntimes
and can reverse an integer between 0 and n/NUL1, with lg nbits, in O.lgn/time.
(In practice, because we usually know the initial value of nin advance, we would
probably code a table mapping kto rev .k/,m a k i n gB IT-REVERSE -COPY run in
‚.n/ time with a low hidden constant. Alternatively, we could use the clever amor-
tized reverse binary counter scheme described in Problem 17-1.) To complete the
proof that I TERATIVE -FFT runs in time ‚.n lgn/, we show that L.n/ , the number
of times the body of the innermost loop (lines 8–13) executes, is ‚.n lgn/.T h e
forloop of lines 6–13 iterates n=mDn=2stimes for each value of s,a n dt h e
innermost loop of lines 8–13 iterates m=2D2s/NUL1times. Thus,
L.n/DlgnX
sD1n
2s/SOH2s/NUL1
DlgnX
sD1n
2
D‚.n lgn/ :30.3 Efﬁcient FFT implementations 919
a0
a1
a2
a3
a4
a5
a6
a7y0
y1
y2
y3
y4
y5
y6
y7
stage sD1 stage sD2 stage sD3!0
2!0
2!0
2!0
2
!0
4!0
4
!1
4!1
4
!0
8
!1
8
!2
8
!3
8
Figure 30.5 A circuit that computes the FFT in parallel, here shown on nD8inputs. Each
butterﬂy operation takes as input the values on two wires, along with a twiddle factor, and it produces
as outputs the values on two wires. The stages of butterﬂies are labeled to correspond to iterations
of the outermost loop of the I TERA TIVE -FFT procedure. Only the top and bottom wires passing
through a butterﬂy interact with it; wires that pass through the middle of a butterﬂy do not affect
that butterﬂy, nor are their values changed by that butterﬂy. For example, the top butterﬂy in stage 2
has nothing to do with wire 1(the wire whose output is labeled y1); its inputs and outputs are only
on wires 0and2(labeled y0andy2, respectively). This circuit has depth ‚.lgn/and performs
‚.n lgn/butterﬂy operations altogether.
A parallel FFT circuit
We can exploit many of the properties that allowed us to implement an efﬁcient
iterative FFT algorithm to produce an efﬁcient parallel algorithm for the FFT. We
will express the parallel FFT algorithm as a circuit. Figure 30.5 shows a parallelFFT circuit, which computes the FFT on ninputs, for nD8. The circuit begins
with a bit-reverse permutation of the inputs, followed by lg nstages, each stage
consisting of n=2 butterﬂies executed in parallel. The depth of the circuit—the
maximum number of computational elements between any output and any input
that can reach it—is therefore ‚.lgn/.
The leftmost part of the parallel FFT circuit performs the bit-reverse permuta-
tion, and the remainder mimics the iterative I
TERATIVE -FFT procedure. Because
each iteration of the outermost forloop performs n=2independent butterﬂy opera-
tions, the circuit performs them in parallel. The value of sin each iteration within920 Chapter 30 Polynomials and the FFT
ITERATIVE -FFT corresponds to a stage of butterﬂies shown in Figure 30.5. For
sD1 ;2;:::; lgn,s t a g e sconsists of n=2sgroups of butterﬂies (corresponding to
each value of kin ITERATIVE -FFT), with 2s/NUL1butterﬂies per group (corresponding
to each value of jin ITERATIVE -FFT). The butterﬂies shown in Figure 30.5 corre-
spond to the butterﬂy operations of the innermost loop (lines 9–12 of I TERATIVE -
FFT). Note also that the twiddle factors used in the butterﬂies correspond to those
used in I TERATIVE -FFT: in stage s,w eu s e !0
m;!1
m;:::;!m=2/NUL1
m ,w h e r e mD2s.
Exercises
30.3-1
Show how I TERATIVE -FFT computes the DFT of the input vector . 0 ;2;3 ;/NUL1; 4;
5; 7; 9/ .
30.3-2
Show how to implement an FFT algorithm with the bit-reversal permutation occur-ring at the end, rather than at the beginning, of the computation. ( Hint: Consider
the inverse DFT.)
30.3-3
How many times does I
TERATIVE -FFT compute twiddle factors in each stage?
Rewrite I TERATIVE -FFT to compute twiddle factors only 2s/NUL1times in stage s.
30.3-4 ?
Suppose that the adders within the butterﬂy operations of the FFT circuit some-times fail in such a manner that they always produce a zero output, independentof their inputs. Suppose that exactly one adder has failed, but that you don’t knowwhich one. Describe how you can identify the failed adder by supplying inputs tothe overall FFT circuit and observing the outputs. How efﬁcient is your method?
Problems
30-1 Divide-and-conquer multiplicationa.Show how to multiply two linear polynomials axCbandcxCdusing only
three multiplications. ( Hint: One of the multiplications is .aCb//SOH.cCd/.)
b.Give two divide-and-conquer algorithms for multiplying two polynomials of
degree-bound nin‚.n
lg3/time. The ﬁrst algorithm should divide the input
polynomial coefﬁcients into a high half and a low half, and the second algorithmshould divide them according to whether their index is odd or even.Problems for Chapter 30 921
c.Show how to multiply two n-bit integers in O.nlg3/steps, where each step
operates on at most a constant number of 1-bit values.
30-2 Toeplitz matrices
AToeplitz matrix is an n/STXnmatrix AD.aij/such that aijDai/NUL1;j/NUL1for
iD2;3 ;:::;n andjD2;3 ;:::;n .
a.Is the sum of two Toeplitz matrices necessarily Toeplitz? What about the prod-
uct?
b.Describe how to represent a Toeplitz matrix so that you can add two n/STXn
Toeplitz matrices in O.n/ time.
c.Give an O.n lgn/-time algorithm for multiplying an n/STXnToeplitz matrix by a
vector of length n. Use your representation from part (b).
d.Give an efﬁcient algorithm for multiplying two n/STXnToeplitz matrices. Analyze
its running time.
30-3 Multidimensional fast Fourier transform
We can generalize the 1-dimensional discrete Fourier transform deﬁned by equa-
tion (30.8) to ddimensions. The input is a d-dimensional array AD.aj1;j2;:::;j d/
whose dimensions are n1;n2;:::;n d,w h e r e n1n2/SOH/SOH/SOHndDn.W e d e ﬁ n e t h e
d-dimensional discrete Fourier transform by the equation
yk1;k2;:::;k dDn1/NUL1X
j1D0n2/NUL1X
j2D0/SOH/SOH/SOHnd/NUL1X
jdD0aj1;j2;:::;j d!j1k1
n1!j2k2
n2/SOH/SOH/SOH!jdkd
nd
for0/DC4k1<n 1,0/DC4k2<n 2,..., 0/DC4kd<n d.
a.Show that we can compute a d-dimensional DFT by computing 1-dimensional
DFTs on each dimension in turn. That is, we ﬁrst compute n=n 1separate
1-dimensional DFTs along dimension 1. Then, using the result of the DFTs
along dimension 1as the input, we compute n=n 2separate 1-dimensional DFTs
along dimension 2. Using this result as the input, we compute n=n 3separate
1-dimensional DFTs along dimension 3, and so on, through dimension d.
b.Show that the ordering of dimensions does not matter, so that we can compute
ad-dimensional DFT by computing the 1-dimensional DFTs in any order of
theddimensions.922 Chapter 30 Polynomials and the FFT
c.Show that if we compute each 1-dimensional DFT by computing the fast Four-
ier transform, the total time to compute a d-dimensional DFT is O.n lgn/,
independent of d.
30-4 Evaluating all derivatives of a polynomial at a point
Given a polynomial A.x/ of degree-bound n, we deﬁne its tth derivative by
A.t/.x/D„
A.x/ iftD0;
d
dxA.t/NUL1/.x/ if1/DC4t/DC4n/NUL1;
0 ift/NAKn:
From the coefﬁcient representation .a0;a1;:::;a n/NUL1/ofA.x/ and a given point x0,
we wish to determine A.t/.x0/fortD0; 1; : : : ; n/NUL1.
a.Given coefﬁcients b0;b1;:::;b n/NUL1such that
A.x/Dn/NUL1X
jD0bj.x/NULx0/j;
show how to compute A.t/.x0/,f o rtD0; 1; : : : ; n/NUL1,i nO.n/ time.
b.Explain how to ﬁnd b0;b1;:::;b n/NUL1inO.n lgn/time, given A.x 0C!k
n/for
kD0; 1; : : : ; n/NUL1.
c.Prove that
A.x 0C!k
n/Dn/NUL1X
rD0 
!kr
n
rŠn/NUL1X
jD0f. j/ g. r/NULj/!
;
where f. j/Daj/SOHjŠand
g.l/D(
x/NULl
0=./NULl/Š if/NUL.n/NUL1//DC4l/DC40;
0 if1/DC4l/DC4n/NUL1:
d.Explain how to evaluate A.x 0C!k
n/forkD0; 1; : : : ; n/NUL1inO.n lgn/
time. Conclude that we can evaluate all nontrivial derivatives of A.x/ atx0in
O.n lgn/time.Problems for Chapter 30 923
30-5 Polynomial evaluation at multiple points
We have seen how to evaluate a polynomial of degree-bound nat a single point in
O.n/ time using Horner’s rule. We have also discovered how to evaluate such a
polynomial at all ncomplex roots of unity in O.n lgn/time using the FFT. We
shall now show how to evaluate a polynomial of degree-bound natnarbitrary
points in O.n lg2n/time.
To do so, we shall assume that we can compute the polynomial remainder when
one such polynomial is divided by another in O.n lgn/time, a result that we state
without proof. For example, the remainder of 3x3Cx2/NUL3xC1when divided by
x2CxC2is
.3x3Cx2/NUL3xC1/mod.x2CxC2/D/NUL7xC5:
Given the coefﬁcient representation of a polynomial A.x/DPn/NUL1
kD0akxkand
npoints x0;x1;:::;x n/NUL1, we wish to compute the nvalues A.x 0/; A.x 1/ ;:::;
A.x n/NUL1/.F o r 0/DC4i/DC4j/DC4n/NUL1, deﬁne the polynomials Pij.x/DQj
kDi.x/NULxk/
andQij.x/DA.x/ modPij.x/. Note that Qij.x/has degree at most j/NULi.
a.Prove that A.x/ mod.x/NUL´/DA.´/ for any point ´.
b.Prove that Qkk.x/DA.x k/and that Q0;n/NUL1.x/DA.x/ .
c.Prove that for i/DC4k/DC4j,w eh a v e Qik.x/DQij.x/modPik.x/and
Qkj.x/DQij.x/modPkj.x/.
d.Give an O.n lg2n/-time algorithm to evaluate A.x 0/; A.x 1/ ;:::;A . x n/NUL1/.
30-6 FFT using modular arithmetic
As deﬁned, the discrete Fourier transform requires us to compute with complexnumbers, which can result in a loss of precision due to round-off errors. For someproblems, the answer is known to contain only integers, and by using a variant ofthe FFT based on modular arithmetic, we can guarantee that the answer is calcu-lated exactly. An example of such a problem is that of multiplying two polynomialswith integer coefﬁcients. Exercise 30.2-6 gives one approach, using a modulus oflength /DEL.n/ bits to handle a DFT on npoints. This problem gives another ap-
proach, which uses a modulus of the more reasonable length O.lgn/; it requires
that you understand the material of Chapter 31. Let nbe a power of 2.
a.Suppose that we search for the smallest ksuch that pDknC1is prime. Give
a simple heuristic argument why we might expect kto be approximately ln n.
(The value of kmight be much larger or smaller, but we can reasonably expect
to examine O.lgn/candidate values of kon average.) How does the expected
length of pcompare to the length of n?924 Chapter 30 Polynomials and the FFT
Letgbe a generator of Z/ETX
p,a n dl e t wDgkmodp.
b.Argue that the DFT and the inverse DFT are well-deﬁned inverse operations
modulo p,w h e r e wis used as a principal nth root of unity.
c.Show how to make the FFT and its inverse work modulo pin time O.n lgn/,
where operations on words of O.lgn/bits take unit time. Assume that the
algorithm is given pandw.
d.Compute the DFT modulo pD17of the vector . 0 ;5 ;3 ;7 ;7 ;2;1 ;6 / . Note that
gD3is a generator of Z/ETX
17.
Chapter notes
Van Loan’s book [343] provides an outstanding treatment of the fast Fourier trans-
form. Press, Teukolsky, Vetterling, and Flannery [283, 284] have a good descrip-tion of the fast Fourier transform and its applications. For an excellent introduction
to signal processing, a popular FFT application area, see the texts by Oppenheim
and Schafer [266] and Oppenheim and Willsky [267]. The Oppenheim and Schaferbook also shows how to handle cases in which nis not an integer power of 2.
Fourier analysis is not limited to 1-dimensional data. It is widely used in image
processing to analyze data in 2or more dimensions. The books by Gonzalez and
Woods [146] and Pratt [281] discuss multidimensional Fourier transforms and their
use in image processing, and books by Tolimieri, An, and Lu [338] and Van Loan[343] discuss the mathematics of multidimensional fast Fourier transforms.
Cooley and Tukey [76] are widely credited with devising the FFT in the 1960s.
The FFT had in fact been discovered many times previously, but its importance wasnot fully realized before the advent of modern digital computers. Although Press,Teukolsky, Vetterling, and Flannery attribute the origins of the method to Rungeand K¨ onig in 1924, an article by Heideman, Johnson, and Burrus [163] traces the
history of the FFT as far back as C. F. Gauss in 1805.
Frigo and Johnson [117] developed a fast and ﬂexible implementation of the
FFT, called FFTW (“fastest Fourier transform in the West”). FFTW is designed forsituations requiring multiple DFT computations on the same problem size. Beforeactually computing the DFTs, FFTW executes a “planner,” which, by a series oftrial runs, determines how best to decompose the FFT computation for the givenproblem size on the host machine. FFTW adapts to use the hardware cache ef-ﬁciently, and once subproblems are small enough, FFTW solves them with opti-mized, straight-line code. Furthermore, FFTW has the unusual advantage of taking‚.n lgn/time for any problem size n,e v e nw h e n nis a large prime.Notes for Chapter 30 925
Although the standard Fourier transform assumes that the input represents points
that are uniformly spaced in the time domain, other techniques can approximate theFFT on “nonequispaced” data. The article by Ware [348] provides an overview.31 Number-Theoretic Algorithms
Number theory was once viewed as a beautiful but largely useless subject in pure
mathematics. Today number-theoretic algorithms are used widely, due in large partto the invention of cryptographic schemes based on large prime numbers. Theseschemes are feasible because we can ﬁnd large primes easily, and they are securebecause we do not know how to factor the product of large primes (or solve relatedproblems, such as computing discrete logarithms) efﬁciently. This chapter presentssome of the number theory and related algorithms that underlie such applications.
Section 31.1 introduces basic concepts of number theory, such as divisibility,
modular equivalence, and unique factorization. Section 31.2 studies one of theworld’s oldest algorithms: Euclid’s algorithm for computing the greatest commondivisor of two integers. Section 31.3 reviews concepts of modular arithmetic. Sec-tion 31.4 then studies the set of multiples of a given number a, modulo n,a n ds h o w s
how to ﬁnd all solutions to the equation ax/DC1b.mod n/by using Euclid’s algo-
rithm. The Chinese remainder theorem is presented in Section 31.5. Section 31.6
considers powers of a given number a, modulo n, and presents a repeated-squaring
algorithm for efﬁciently computing a
bmodn,g i v e n a,b,a n d n. This operation is
at the heart of efﬁcient primality testing and of much modern cryptography. Sec-tion 31.7 then describes the RSA public-key cryptosystem. Section 31.8 examinesa randomized primality test. We can use this test to ﬁnd large primes efﬁciently,which we need to do in order to create keys for the RSA cryptosystem. Finally,Section 31.9 reviews a simple but effective heuristic for factoring small integers. Itis a curious fact that factoring is one problem people may wish to be intractable,since the security of RSA depends on the difﬁculty of factoring large integers.
Size of inputs and cost of arithmetic computations
Because we shall be working with large integers, we need to adjust how we think
about the size of an input and about the cost of elementary arithmetic operations.
In this chapter, a “large input” typically means an input containing “large in-
tegers” rather than an input containing “many integers” (as for sorting). Thus,31.1 Elementary number-theoretic notions 927
we shall measure the size of an input in terms of the number of bits required to
represent that input, not just the number of integers in the input. An algorithmwith integer inputs a
1;a2;:::;a kis apolynomial-time algorithm if it runs in time
polynomial in lg a1;lga2;:::; lgak, that is, polynomial in the lengths of its binary-
encoded inputs.
In most of this book, we have found it convenient to think of the elemen-
tary arithmetic operations (multiplications, divisions, or computing remainders)as primitive operations that take one unit of time. By counting the number of such
arithmetic operations that an algorithm performs, we have a basis for making a
reasonable estimate of the algorithm’s actual running time on a computer. Elemen-tary operations can be time-consuming, however, when their inputs are large. Itthus becomes convenient to measure how many bit operations a number-theoretic
algorithm requires. In this model, multiplying two ˇ-bit integers by the ordinary
method uses ‚.ˇ
2/bit operations. Similarly, we can divide a ˇ-bit integer by a
shorter integer or take the remainder of a ˇ-bit integer when divided by a shorter in-
teger in time ‚.ˇ2/by simple algorithms. (See Exercise 31.1-12.) Faster methods
are known. For example, a simple divide-and-conquer method for multiplying two
ˇ-bit integers has a running time of ‚.ˇlg3/, and the fastest known method has
a running time of ‚.ˇ lgˇlg lgˇ/. For practical purposes, however, the ‚.ˇ2/
algorithm is often best, and we shall use this bound as a basis for our analyses.
We shall generally analyze algorithms in this chapter in terms of both the number
of arithmetic operations and the number of bit operations they require.
31.1 Elementary number-theoretic notions
This section provides a brief review of notions from elementary number theoryconcerning the set ZDf:::;/NUL2;/NUL1; 0; 1; 2; : : :gof integers and the set ND
f0; 1; 2; : : :gof natural numbers.
Divisibility and divisors
The notion of one integer being divisible by another is key to the theory of numbers.
The notation dja(read “ ddivides a”) means that aDkdfor some integer k.
Every integer divides 0.I fa>0 anddja,t h e njdj/DC4jaj.I fdja,t h e nw ea l s o
say that ais amultiple ofd.I fddoes not divide a, we write d−a.
Ifdjaandd/NAK0, we say that dis adivisor ofa. Note that djaif and only
if/NULdja, so that no generality is lost by deﬁning the divisors to be nonnegative,
with the understanding that the negative of any divisor of aalso divides a.A928 Chapter 31 Number-Theoretic Algorithms
divisor of a nonzero integer ais at least 1but not greater than jaj. For example, the
divisors of 24are1,2,3,4,6,8,12,a n d 24.
Every positive integer ais divisible by the trivial divisors 1anda. The nontrivial
divisors of aare the factors ofa. For example, the factors of 20are2,4,5,a n d 10.
Prime and composite numbers
An integer a>1 whose only divisors are the trivial divisors 1andais aprime
number or, more simply, a prime . Primes have many special properties and play a
critical role in number theory. The ﬁrst 20 primes, in order, are
2; 3; 5; 7; 11; 13; 17; 19; 23; 29; 31; 37; 41; 43; 47; 53; 59; 61; 67; 71 :Exercise 31.1-2 asks you to prove that there are inﬁnitely many primes. An integer
a>1 that is not prime is a composite number or, more simply, a composite .F o r
example, 39is composite because 3j39. We call the integer 1aunit, and it is
neither prime nor composite. Similarly, the integer 0and all negative integers are
neither prime nor composite.
The division theorem, remainders, and modular equivalence
G i v e na ni n t e g e r n, we can partition the integers into those that are multiples of n
and those that are not multiples of n. Much number theory is based upon reﬁning
this partition by classifying the nonmultiples of naccording to their remainders
when divided by
n. The following theorem provides the basis for this reﬁnement.
We omit the proof (but see, for example, Niven and Zuckerman [265]).
Theorem 31.1 (Division theorem)
For any integer aand any positive integer n, there exist unique integers qandr
such that 0/DC4r<n andaDqnCr.
The value qDba=ncis the quotient of the division. The value rDamodn
is the remainder (orresidue ) of the division. We have that njaif and only if
amodnD0.
We can partition the integers into nequivalence classes according to their re-
mainders modulo n.T h eequivalence class modulo ncontaining an integer ais
Œa/c141nDfaCknWk2Zg:
For example, Œ3/c1417Df:::;/NUL11;/NUL4; 3; 10; 17; : : :g; we can also denote this set by
Œ/NUL4/c1417andŒ10/c141 7. Using the notation deﬁned on page 54, we can say that writing
a2Œb/c141nis the same as writing a/DC1b.mod n/. The set of all such equivalence
classes is31.1 Elementary number-theoretic notions 929
ZnDfŒa/c141nW0/DC4a/DC4n/NUL1g: (31.1)
When you see the deﬁnition
ZnDf0; 1; : : : ; n/NUL1g; (31.2)
you should read it as equivalent to equation (31.1) with the understanding that 0
represents Œ0/c141n,1represents Œ1/c141n, and so on; each class is represented by its smallest
nonnegative element. You should keep the underlying equivalence classes in mind,however. For example, if we refer to /NUL1a sam e m b e ro f Z
n, we are really referring
toŒn/NUL1/c141n,s i n c e/NUL1/DC1n/NUL1.mod n/.
Common divisors and greatest common divisors
Ifdis a divisor of aanddis also a divisor of b,t h e n dis acommon divisor ofa
andb. For example, the divisors of 30are1,2,3,5,6,10,15,a n d 30,a n ds ot h e
common divisors of 24and30are1,2,3,a n d 6. Note that 1is a common divisor
of any two integers.
An important property of common divisors is that
djaanddjbimplies dj.aCb/anddj.a/NULb/ : (31.3)
More generally, we have that
djaanddjbimplies dj.axCby/ (31.4)
for any integers xandy. Also, if ajb, then eitherjaj/DC4jbjorbD0,w h i c h
implies that
ajbandbjaimplies aD˙b: (31.5)
Thegreatest common divisor of two integers aandb, not both zero, is the
largest of the common divisors of aandb;w ed e n o t ei tb yg c d .a; b/ . For example,
gcd.24; 30/D6,g c d.5; 7/D1, and gcd .0; 9/D9.I faandbare both nonzero,
then gcd .a; b/ is an integer between 1and min .jaj;jbj/. We deﬁne gcd .0; 0/ to
be0; this deﬁnition is necessary to make standard properties of the gcd function
(such as equation (31.9) below) universally valid.
The following are elementary properties of the gcd function:
gcd.a; b/Dgcd.b; a/ ; (31.6)
gcd.a; b/Dgcd./NULa;b/ ; (31.7)
gcd.a; b/Dgcd.jaj;jbj/; (31.8)
gcd.a; 0/Djaj; (31.9)
gcd.a; ka/Djaj for any k2Z: (31.10)
The following theorem provides an alternative and useful characterization of
gcd.a; b/ .930 Chapter 31 Number-Theoretic Algorithms
Theorem 31.2
Ifaandbare any integers, not both zero, then gcd .a; b/ is the smallest positive
element of the setfaxCbyWx;y2Zgof linear combinations of aandb.
Proof Letsbe the smallest positive such linear combination of aandb,a n dl e t
sDaxCbyfor some x;y2Z.L e t qDba=sc. Equation (3.8) then implies
amodsDa/NULqs
Da/NULq.axCby/
Da. 1/NULqx/Cb./NULqy/ ;
and so amodsis a linear combination of aandbas well. But, since 0/DC4
amods<s ,w eh a v et h a t amodsD0, because sis the smallest positive such lin-
ear combination. Therefore, we have that sjaand, by analogous reasoning, sjb.
Thus, sis a common divisor of aandb, and so gcd .a; b//NAKs. Equation (31.4)
implies that gcd .a; b/js,s i n c eg c d .a; b/ divides both aandbandsis a linear
combination of aandb. But gcd .a; b/jsands>0 imply that gcd .a; b//DC4s.
Combining gcd .a; b//NAKsand gcd .a; b//DC4syields gcd .a; b/Ds. We conclude
thatsis the greatest common divisor of aandb.
Corollary 31.3
For any integers aandb,i fdjaanddjb,t h e n djgcd.a; b/ .
Proof This corollary follows from equation (31.4), because gcd .a; b/ is a linear
combination of aandbby Theorem 31.2.
Corollary 31.4
For all integers aandband any nonnegative integer n,
gcd.an; bn/Dngcd.a; b/ :
Proof IfnD0, the corollary is trivial. If n>0 ,t h e ng c d .an; bn/ is the smallest
positive element of the set fanxCbn yWx;y2Zg,w h i c hi s ntimes the smallest
positive element of the set faxCbyWx;y2Zg.
Corollary 31.5
For all positive integers n,a,a n d b,i fnjaband gcd .a; n/D1,t h e n njb.
Proof We leave the proof as Exercise 31.1-5.
31.1 Elementary number-theoretic notions 931
Relatively prime integers
Two integers aandbarerelatively prime if their only common divisor is 1,t h a t
is, if gcd .a; b/D1. For example, 8and15are relatively prime, since the divisors
of8are1,2,4,a n d 8, and the divisors of 15are1,3,5,a n d 15. The following
theorem states that if two integers are each relatively prime to an integer p,t h e n
their product is relatively prime to p.
Theorem 31.6
For any integers a,b,a n d p, if both gcd .a; p/D1and gcd .b; p/D1,t h e n
gcd.ab;p/D1.
Proof It follows from Theorem 31.2 that there exist integers x,y,x0,a n d y0such
that
axCpyD1;
bx0Cpy0D1:
Multiplying these equations and rearranging, we have
ab.xx0/Cp.ybx0Cy0axCpyy0/D1:
Since 1is thus a positive linear combination of abandp, an appeal to Theo-
rem 31.2 completes the proof.
Integers n1,n2, ..., nkarepairwise relatively prime if, whenever i¤j,w e
have gcd .ni;nj/D1.
Unique factorization
An elementary but important fact about divisibility by primes is the following.
Theorem 31.7
For all primes pand all integers aandb,i fpjab,t h e n pjaorpjb(or both).
Proof Assume for the purpose of contradiction that pjab,b u tt h a t p−aand
p−b. Thus, gcd .a; p/D1and gcd .b; p/D1, since the only divisors of pare1
andp, and we assume that pdivides neither anorb. Theorem 31.6 then implies
that gcd .ab;p/D1, contradicting our assumption that pjab,s i n c e pjab
implies gcd .ab;p/Dp. This contradiction completes the proof.
A consequence of Theorem 31.7 is that we can uniquely factor any composite
integer into a product of primes.932 Chapter 31 Number-Theoretic Algorithms
Theorem 31.8 (Unique factorization)
There is exactly one way to write any composite integer aas a product of the form
aDpe1
1pe2
2/SOH/SOH/SOHper
r;
where the piare prime, p1<p 2</SOH/SOH/SOH<p r,a n dt h e eiare positive integers.
Proof We leave the proof as Exercise 31.1-11.
As an example, the number 6000 is uniquely factored into primes as 24/SOH3/SOH53.
Exercises
31.1-1
Prove that if a>b>0 andcDaCb,t h e n cmodaDb.
31.1-2
Prove that there are inﬁnitely many primes. ( Hint: Show that none of the primes
p1;p2;:::;p kdivide .p1p2/SOH/SOH/SOHpk/C1.)
31.1-3
Prove that if ajbandbjc,t h e n ajc.
31.1-4
Prove that if pis prime and 0<k<p ,t h e ng c d .k; p/D1.
31.1-5
Prove Corollary 31.5.
31.1-6
Prove that if pis prime and 0<k<p ,t h e n pj/NULp
k/SOH
. Conclude that for all integers
aandband all primes p,
.aCb/p/DC1apCbp.mod p/ :
31.1-7
Prove that if aandbare any positive integers such that ajb,t h e n
.xmodb/modaDxmoda
for any x. Prove, under the same assumptions, that
x/DC1y.mod b/implies x/DC1y.mod a/
for any integers xandy.31.2 Greatest common divisor 933
31.1-8
For any integer k>0 ,a ni n t e g e r nis akth power if there exists an integer asuch
thatakDn. Furthermore, n>1 is anontrivial power if it is a kth power for
some integer k>1 . Show how to determine whether a given ˇ-bit integer nis a
nontrivial power in time polynomial in ˇ.
31.1-9
Prove equations (31.6)–(31.10).
31.1-10
Show that the gcd operator is associative. That is, prove that for all integers a,b,
andc,
gcd.a;gcd.b; c//Dgcd.gcd.a; b/; c/ :
31.1-11 ?
Prove Theorem 31.8.
31.1-12
Give efﬁcient algorithms for the operations of dividing a ˇ-bit integer by a shorter
integer and of taking the remainder of a ˇ-bit integer when divided by a shorter
integer. Your algorithms should run in time ‚.ˇ2/.
31.1-13
Give an efﬁcient algorithm to convert a given ˇ-bit (binary) integer to a decimal
representation. Argue that if multiplication or division of integers whose lengthis at most ˇtakes time M.ˇ/ , then we can convert binary to decimal in time
‚.M.ˇ/ lgˇ/.(Hint: Use a divide-and-conquer approach, obtaining the top and
bottom halves of the result with separate recursions.)
31.2 Greatest common divisor
In this section, we describe Euclid’s algorithm for efﬁciently computing the great-est common divisor of two integers. When we analyze the running time, we shallsee a surprising connection with the Fibonacci numbers, which yield a worst-case
input for Euclid’s algorithm.
We restrict ourselves in this section to nonnegative integers. This restriction is
justiﬁed by equation (31.8), which states that gcd .a; b/Dgcd.jaj;jbj/.934 Chapter 31 Number-Theoretic Algorithms
In principle, we can compute gcd .a; b/ for positive integers aandbfrom the
prime factorizations of aandb. Indeed, if
aDpe1
1pe2
2/SOH/SOH/SOHper
r; (31.11)
bDpf1
1pf2
2/SOH/SOH/SOHpfr
r; (31.12)
with zero exponents being used to make the set of primes p1;p2;:::;p rthe same
for both aandb, then, as Exercise 31.2-1 asks you to show,
gcd.a; b/Dpmin.e1;f1/
1 pmin.e2;f2/
2/SOH/SOH/SOHpmin.er;fr/
r : (31.13)
As we shall show in Section 31.9, however, the best algorithms to date for factoring
do not run in polynomial time. Thus, this approach to computing greatest common
divisors seems unlikely to yield an efﬁcient algorithm.
Euclid’s algorithm for computing greatest common divisors relies on the follow-
ing theorem.
Theorem 31.9 (GCD recursion theorem)
For any nonnegative integer aand any positive integer b,
gcd.a; b/Dgcd.b; a modb/ :
Proof We shall show that gcd .a; b/ and gcd .b; a modb/divide each other, so
that by equation (31.5) they must be equal (since they are both nonnegative).
We ﬁrst show that gcd .a; b/jgcd.b; a modb/.I f w e l e t dDgcd.a; b/ ,t h e n
djaanddjb. By equation (3.8), amodbDa/NULqb,w h e r e qDba=bc.
Since amodbis thus a linear combination of aandb, equation (31.4) implies that
dj.amodb/. Therefore, since djbanddj.amodb/, Corollary 31.3 implies
thatdjgcd.b; a modb/or, equivalently, that
gcd.a; b/jgcd.b; a modb/: (31.14)
Showing that gcd .b; a modb/jgcd.a; b/ is almost the same. If we now let
dDgcd.b; a modb/,t h e n djbanddj.amodb/.S i n c e aDqbC.amodb/,
where qDba=bc,w eh a v et h a t ais a linear combination of band.amodb/.B y
equation (31.4), we conclude that dja.S i n c e djbanddja,w eh a v et h a t
djgcd.a; b/ by Corollary 31.3 or, equivalently, that
gcd.b; a modb/jgcd.a; b/: (31.15)
Using equation (31.5) to combine equations (31.14) and (31.15) completes the
proof.
31.2 Greatest common divisor 935
Euclid’s algorithm
The Elements of Euclid (circa 300 B.C.) describes the following gcd algorithm,
although it may be of even earlier origin. We express Euclid’s algorithm as arecursive program based directly on Theorem 31.9. The inputs aandbare arbitrary
nonnegative integers.
E
UCLID .a; b/
1ifb==0
2 return a
3else return EUCLID .b; a modb/
As an example of the running of E UCLID , consider the computation of gcd .30; 21/ :
EUCLID .30; 21/DEUCLID .21; 9/
DEUCLID .9; 3/
DEUCLID .3; 0/
D3:
This computation calls E UCLID recursively three times.
The correctness of E UCLID follows from Theorem 31.9 and the property that if
the algorithm returns ain line 2, then bD0, so that equation (31.9) implies that
gcd.a; b/Dgcd.a; 0/Da. The algorithm cannot recurse indeﬁnitely, since the
second argument strictly decreases in each recursive call and is always nonnegative.Therefore, E
UCLID always terminates with the correct answer.
The running time of Euclid’s algorithm
We analyze the worst-case running time of E UCLID as a function of the size of
aandb. We assume with no loss of generality that a>b/NAK0. To justify this
assumption, observe that if b>a/NAK0,t h e nE UCLID .a; b/ immediately makes the
recursive call E UCLID .b; a/ . That is, if the ﬁrst argument is less than the second
argument, E UCLID spends one recursive call swapping its arguments and then pro-
ceeds. Similarly, if bDa>0 , the procedure terminates after one recursive call,
since amodbD0.
The overall running time of E UCLID is proportional to the number of recursive
calls it makes. Our analysis makes use of the Fibonacci numbers Fk,d e ﬁ n e db y
the recurrence (3.22).
Lemma 31.10
Ifa>b/NAK1and the call E UCLID .a; b/ performs k/NAK1recursive calls, then
a/NAKFkC2andb/NAKFkC1.936 Chapter 31 Number-Theoretic Algorithms
Proof The proof proceeds by induction on k. For the basis of the induction, let
kD1. Then, b/NAK1DF2, and since a>b ,w em u s th a v e a/NAK2DF3.S i n c e
b>. a modb/, in each recursive call the ﬁrst argument is strictly larger than the
second; the assumption that a>b therefore holds for each recursive call.
Assume inductively that the lemma holds if k/NUL1recursive calls are made; we
shall then prove that the lemma holds for krecursive calls. Since k>0 ,w eh a v e
b>0 ,a n dE UCLID .a; b/ calls E UCLID .b; a modb/recursively, which in turn
makes k/NUL1recursive calls. The inductive hypothesis then implies that b/NAKFkC1
(thus proving part of the lemma), and amodb/NAKFk.W eh a v e
bC.amodb/DbC.a/NULbba=bc/
/DC4a;
since a>b>0 impliesba=bc/NAK1. Thus,
a/NAKbC.amodb/
/NAKFkC1CFk
DFkC2:
The following theorem is an immediate corollary of this lemma.
Theorem 31.11 (Lam ´e’s theorem)
For any integer k/NAK1,i fa>b/NAK1andb<F kC1, then the call E UCLID .a; b/
makes fewer than krecursive calls.
We can show that the upper bound of Theorem 31.11 is the best possible by
showing that the call E UCLID .FkC1;Fk/makes exactly k/NUL1recursive calls
when k/NAK2. We use induction on k. For the base case, kD2, and the call
EUCLID .F3;F2/makes exactly one recursive call, to E UCLID .1; 0/ .( W e h a v e t o
start at kD2, because when kD1we do not have F2>F 1.) For the induc-
tive step, assume that E UCLID .Fk;Fk/NUL1/makes exactly k/NUL2recursive calls. For
k>2 ,w eh a v e Fk>F k/NUL1>0andFkC1DFkCFk/NUL1, and so by Exercise 31.1-1,
we have FkC1modFkDFk/NUL1. Thus, we have
gcd.FkC1;Fk/Dgcd.Fk;FkC1modFk/
Dgcd.Fk;Fk/NUL1/:
Therefore, the call E UCLID .FkC1;Fk/recurses one time more than the call
EUCLID .Fk;Fk/NUL1/, or exactly k/NUL1times, meeting the upper bound of Theo-
rem 31.11.
Since Fkis approximately /RSk=p
5,w h e r e /RSis the golden ratio .1Cp
5/=2 de-
ﬁned by equation (3.24), the number of recursive calls in E UCLID isO.lgb/.( S e e31.2 Greatest common divisor 937
abba=bcdx y
99 78 1 3 /NUL11 14
78 21 3 3 3 /NUL11
21 15 1 3 /NUL23
15 6 2 3 1 /NUL2
63 2 3 0 1
30 — 3 1 0
Figure 31.1 How E XTENDED -EUCLID computes gcd .99; 78/ . Each line shows one level of the
recursion: the values of the inputs aandb, the computed value ba=bc, and the values d,x,a n d y
returned. The triple .d; x; y/ returned becomes the triple .d0;x0;y0/used at the next higher level
of recursion. The call E XTENDED -EUCLID .99; 78/ returns .3;/NUL11; 14/ ,s ot h a tg c d .99; 78/D3D
99/SOH./NUL11/C78/SOH14.
Exercise 31.2-5 for a tighter bound.) Therefore, if we call E UCLID on two ˇ-bit
numbers, then it performs O.ˇ/ arithmetic operations and O.ˇ3/bit operations
(assuming that multiplication and division of ˇ-bit numbers take O.ˇ2/bit oper-
ations). Problem 31-2 asks you to show an O.ˇ2/bound on the number of bit
operations.
The extended form of Euclid’s algorithm
We now rewrite Euclid’s algorithm to compute additional useful information.
Speciﬁcally, we extend the algorithm to compute the integer coefﬁcients xandy
such that
dDgcd.a; b/DaxCby : (31.16)
Note that xandymay be zero or negative. We shall ﬁnd these coefﬁcients useful
later for computing modular multiplicative inverses. The procedure E XTENDED -
EUCLID takes as input a pair of nonnegative integers and returns a triple of the
form . d;x;y/ that satisﬁes equation (31.16).
EXTENDED -EUCLID .a; b/
1ifb==0
2 return . a;1 ;0 /
3else.d0;x0;y0/DEXTENDED -EUCLID .b; a modb/
4 . d;x;y/D.d0;y0;x0/NULba=bcy0/
5 return . d;x;y/
Figure 31.1 illustrates how E XTENDED -EUCLID computes gcd .99; 78/ .
The E XTENDED -EUCLID procedure is a variation of the E UCLID procedure.
Line 1 is equivalent to the test “ b==0” in line 1 of E UCLID .I fbD0,t h e n938 Chapter 31 Number-Theoretic Algorithms
EXTENDED -EUCLID returns not only dDain line 2, but also the coefﬁcients
xD1andyD0,s ot h a t aDaxCby.I fb¤0,EXTENDED -EUCLID ﬁrst
computes .d0;x0;y0/such that d0Dgcd.b; a modb/and
d0Dbx0C.amodb/y0: (31.17)
As for E UCLID , we have in this case dDgcd.a; b/Dd0Dgcd.b; a modb/.
To obtain xandysuch that dDaxCby, we start by rewriting equation (31.17)
using the equation dDd0and equation (3.8):
dDbx0C.a/NULbba=bc/y0
Day0Cb.x0/NULba=bcy0/:
Thus, choosing xDy0andyDx0/NULba=bcy0satisﬁes the equation dDaxCby,
proving the correctness of E XTENDED -EUCLID .
Since the number of recursive calls made in E UCLID is equal to the number
of recursive calls made in E XTENDED -EUCLID , the running times of E UCLID
and E XTENDED -EUCLID are the same, to within a constant factor. That is, for
a>b>0 , the number of recursive calls is O.lgb/.
Exercises
31.2-1
Prove that equations (31.11) and (31.12) imply equation (31.13).
31.2-2
Compute the values . d;x;y/ that the call E XTENDED -EUCLID .899; 493/ returns.
31.2-3
Prove that for all integers a,k,a n d n,
gcd.a; n/Dgcd.aCkn;n/ :
31.2-4
Rewrite E UCLID in an iterative form that uses only a constant amount of memory
(that is, stores only a constant number of integer values).
31.2-5
Ifa>b/NAK0, show that the call E UCLID .a; b/ makes at most 1Clog/RSbrecursive
calls. Improve this bound to 1Clog/RS.b=gcd.a; b// .
31.2-6
What does E XTENDED -EUCLID .FkC1;Fk/return? Prove your answer correct.31.3 Modular arithmetic 939
31.2-7
Deﬁne the gcd function for more than two arguments by the recursive equationgcd.a
0;a1;:::;a n/Dgcd.a0;gcd.a1;a2;:::;a n//. Show that the gcd function
returns the same answer independent of the order in which its arguments are speci-ﬁed. Also show how to ﬁnd integers x
0;x1;:::;x nsuch that gcd .a0;a1;:::;a n/D
a0x0Ca1x1C/SOH/SOH/SOHC anxn. Show that the number of divisions performed by your
algorithm is O.nClg.maxfa0;a1;:::;a ng//.
31.2-8
Deﬁne lcm .a1;a2;:::;a n/to be the least common multiple of the nintegers
a1;a2;:::;a n, that is, the smallest nonnegative integer that is a multiple of each ai.
Show how to compute lcm .a1;a2;:::;a n/efﬁciently using the (two-argument) gcd
operation as a subroutine.
31.2-9
Prove that n1,n2,n3,a n d n4are pairwise relatively prime if and only if
gcd.n1n2;n3n4/Dgcd.n1n3;n2n4/D1:
More generally, show that n1;n2;:::;n kare pairwise relatively prime if and only
if a set ofdlgkepairs of numbers derived from the niare relatively prime.
31.3 Modular arithmetic
Informally, we can think of modular arithmetic as arithmetic as usual over the
integers, except that if we are working modulo n, then every result xis replaced
by the element off0; 1; : : : ; n/NUL1gthat is equivalent to x, modulo n(that is, xis
replaced by xmodn). This informal model sufﬁces if we stick to the operations
of addition, subtraction, and multiplication. A more formal model for modulararithmetic, which we now give, is best described within the framework of grouptheory.
Finite groups
Agroup .S;˚/is a set Stogether with a binary operation ˚deﬁned on Sfor
which the following properties hold:
1.Closure: For all a,b2S,w eh a v e a˚b2S.
2.Identity: There exists an element e2S, called the identity of the group, such
thate˚aDa˚eDafor all a2S.
3.Associativity: For all a,b,c2S,w eh a v e .a˚b/˚cDa˚.b˚c/.940 Chapter 31 Number-Theoretic Algorithms
4.Inverses: For each a2S, there exists a unique element b2S, called the
inverse ofa, such that a˚bDb˚aDe.
As an example, consider the familiar group .Z;C/of the integers Zunder the
operation of addition: 0is the identity, and the inverse of ais/NULa. If a group .S;˚/
satisﬁes the commutative law a˚bDb˚afor all a;b2S,t h e ni ti sa n abelian
group . If a group .S;˚/satisﬁesjSj<1,t h e ni ti sa ﬁnite group .
The groups deﬁned by modular addition and multiplication
We can form two ﬁnite abelian groups by using addition and multiplication mod-
ulon,w h e r e nis a positive integer. These groups are based on the equivalence
classes of the integers modulo n, deﬁned in Section 31.1.
To deﬁne a group on Zn, we need to have suitable binary operations, which
we obtain by redeﬁning the ordinary operations of addition and multiplication.We can easily deﬁne addition and multiplication operations for Z
n, because the
equivalence class of two integers uniquely determines the equivalence class of their
sum or product. That is, if a/DC1a0.mod n/andb/DC1b0.mod n/,t h e n
aCb/DC1a0Cb0.mod n/ ;
ab/DC1a0b0.mod n/ :
Thus, we deﬁne addition and multiplication modulo n, denotedCnand/SOHn,b y
Œa/c141nCnŒb/c141nDŒaCb/c141n; (31.18)
Œa/c141n/SOHnŒb/c141nDŒab/c141 n:
(We can deﬁne subtraction similarly on ZnbyŒa/c141n/NULnŒb/c141nDŒa/NULb/c141n, but divi-
sion is more complicated, as we shall see.) These facts justify the common andconvenient practice of using the smallest nonnegative element of each equivalenceclass as its representative when performing computations in Z
n. We add, subtract,
and multiply as usual on the representatives, but we replace each result xby the
representative of its class, that is, by xmodn.
Using this deﬁnition of addition modulo n,w ed e ﬁ n et h e additive group
modulo nas.Zn;Cn/. The size of the additive group modulo nisjZnjDn.
Figure 31.2(a) gives the operation table for the group .Z6;C6/.
Theorem 31.12
The system .Zn;Cn/is a ﬁnite abelian group.
Proof Equation (31.18) shows that .Zn;Cn/is closed. Associativity and com-
mutativity ofCnfollow from the associativity and commutativity of C:31.3 Modular arithmetic 941
012345
0
123450 12345
01 2345012345
012 345
0123 45
012345
(a)1 2 4 7 8 11 13 14
1
2478
1113141 2 4 7 8 11 13 14
2 4 8 14 1 7 11 13481 1 3 2 1 4 7 1 171 4 1 341 12 1 88 1 21 141 3 1 47
1 171 421 31 8 413 11 7 1 14 8 4 21 4 1 3 1 1 87421
(b)+6 ·15
Figure 31.2 Two ﬁnite groups. Equivalence classes are denoted by their representative elements.
(a)The group .Z6;C6/.(b)The group .Z/ETX
15;/SOH15/.
.Œa/c141 nCnŒb/c141n/CnŒc/c141nDŒaCb/c141nCnŒc/c141n
DŒ.aCb/Cc/c141n
DŒaC.bCc//c141n
DŒa/c141nCnŒbCc/c141n
DŒa/c141nCn.Œb/c141 nCnŒc/c141n/;
Œa/c141nCnŒb/c141nDŒaCb/c141n
DŒbCa/c141n
DŒb/c141nCnŒa/c141n:
The identity element of .Zn;Cn/is0(that is, Œ0/c141n). The (additive) inverse of
an element a(that is, of Œa/c141n) is the element/NULa(that is, Œ/NULa/c141norŒn/NULa/c141n), since
Œa/c141nCnŒ/NULa/c141nDŒa/NULa/c141nDŒ0/c141n.
Using the deﬁnition of multiplication modulo n,w ed e ﬁ n et h e multiplicative
group modulo nas.Z/ETX
n;/SOHn/. The elements of this group are the set Z/ETX
nof elements
inZnthat are relatively prime to n, so that each one has a unique inverse, modulo n:
Z/ETX
nDfŒa/c141n2ZnWgcd.a; n/D1g:
To see that Z/ETX
nis well deﬁned, note that for 0/DC4a<n ,w eh a v e a/DC1.aCkn/
.mod n/for all integers k. By Exercise 31.2-3, therefore, gcd .a; n/D1implies
gcd.aCkn;n/D1for all integers k.S i n c e Œa/c141nDfaCknWk2Zg, the set Z/ETX
n
is well deﬁned. An example of such a group is
Z/ETX
15Df1; 2; 4; 7; 8; 11; 13; 14 g;942 Chapter 31 Number-Theoretic Algorithms
where the group operation is multiplication modulo 15. (Here we denote an el-
ement Œa/c14115asa; for example, we denote Œ7/c14115as7.) Figure 31.2(b) shows the
group .Z/ETX
15;/SOH15/. For example, 8/SOH11/DC113 . mod 15/, working in Z/ETX
15.T h ei d e n -
tity for this group is 1.
Theorem 31.13
The system .Z/ETX
n;/SOHn/is a ﬁnite abelian group.
Proof Theorem 31.6 implies that .Z/ETX
n;/SOHn/is closed. Associativity and commu-
tativity can be proved for /SOHnas they were forCnin the proof of Theorem 31.12.
The identity element is Œ1/c141n. To show the existence of inverses, let abe an element
ofZ/ETX
nand let . d;x;y/ be returned by E XTENDED -EUCLID .a; n/ . Then, dD1,
since a2Z/ETX
n,a n d
axCnyD1 (31.19)
or, equivalently,ax/DC11.mod n/ :
Thus, Œx/c141
nis a multiplicative inverse of Œa/c141n, modulo n. Furthermore, we claim
thatŒx/c141n2Z/ETX
n. To see why, equation (31.19) demonstrates that the smallest pos-
itive linear combination of xandnmust be 1. Therefore, Theorem 31.2 implies
that gcd .x; n/D1. We defer the proof that inverses are uniquely deﬁned until
Corollary 31.26.
As an example of computing multiplicative inverses, suppose that aD5and
nD11.T h e n E XTENDED -EUCLID .a; n/ returns . d;x;y/D.1;/NUL2;1/,s ot h a t
1D5/SOH./NUL2/C11/SOH1. Thus, Œ/NUL2/c14111(i.e.,Œ9/c14111) is the multiplicative inverse of Œ5/c14111.
When working with the groups .Zn;Cn/and.Z/ETX
n;/SOHn/in the remainder of this
chapter, we follow the convenient practice of denoting equivalence classes by theirrepresentative elements and denoting the operations C
nand/SOHnby the usual arith-
metic notationsCand/SOH(or juxtaposition, so that abDa/SOHb) respectively. Also,
equivalences modulo nmay also be interpreted as equations in Zn. For example,
the following two statements are equivalent:
ax/DC1b.mod n/ ;
Œa/c141n/SOHnŒx/c141nDŒb/c141n:
As a further convenience, we sometimes refer to a group .S;˚/merely as S
when the operation ˚is understood from context. We may thus refer to the groups
.Zn;Cn/and.Z/ETX
n;/SOHn/asZnand Z/ETX
n, respectively.
We denote the (multiplicative) inverse of an element aby.a/NUL1modn/. Division
inZ/ETX
nis deﬁned by the equation a=b/DC1ab/NUL1.mod n/. For example, in Z/ETX
1531.3 Modular arithmetic 943
we have that 7/NUL1/DC113 . mod 15/,s i n c e 7/SOH13D91/DC11.mod 15/,s ot h a t
4=7/DC14/SOH13/DC17.mod 15/.
The size of Z/ETX
nis denoted /RS.n/ . This function, known as Euler’s phi function ,
satisﬁes the equation
/RS.n/DnY
pWpis prime and pjn/DC2
1/NUL1
p/DC3
; (31.20)
so that pruns over all the primes dividing n(including nitself, if nis prime).
We shall not prove this formula here. Intuitively, we begin with a list of the n
remaindersf0; 1; : : : ; n/NUL1gand then, for each prime pthat divides n, cross out
every multiple of pin the list. For example, since the prime divisors of 45 are 3
and 5,
/RS.45/D45/DC2
1/NUL1
3/DC3/DC2
1/NUL1
5/DC3
D45/DC22
3/DC3/DC24
5/DC3
D24 :
Ifpis prime, then Z/ETX
pDf1 ;2;:::;p/NUL1g,a n d
/RS.p/Dp/DC2
1/NUL1
p/DC3
Dp/NUL1: (31.21)
Ifnis composite, then /RS.n/ < n/NUL1, although it can be shown that
/RS.n/ >n
e/CRln lnnC3
ln lnn(31.22)
forn/NAK3,w h e r e /CRD0:5772156649 : : : isEuler’s constant . A somewhat simpler
(but looser) lower bound for n>5 is
/RS.n/ >n
6ln lnn: (31.23)
The lower bound (31.22) is essentially the best possible, since
lim inf
n!1/RS.n/
n=ln lnnDe/NUL/CR: (31.24)
Subgroups
If.S;˚/is a group, S0/DC2S,a n d .S0;˚/is also a group, then .S0;˚/is asubgroup
of.S;˚/. For example, the even integers form a subgroup of the integers under the
operation of addition. The following theorem provides a useful tool for recognizingsubgroups.944 Chapter 31 Number-Theoretic Algorithms
Theorem 31.14 (A nonempty closed subset of a ﬁnite group is a subgroup)
If.S;˚/is a ﬁnite group and S0is any nonempty subset of Ssuch that a˚b2S0
for all a;b2S0,t h e n .S0;˚/is a subgroup of .S;˚/.
Proof We leave the proof as Exercise 31.3-3.
For example, the set f0;2;4;6gforms a subgroup of Z8, since it is nonempty
and closed under the operation C(that is, it is closed under C8).
The following theorem provides an extremely useful constraint on the size of a
subgroup; we omit the proof.
Theorem 31.15 (Lagrange’s theorem)
If.S;˚/is a ﬁnite group and .S0;˚/is a subgroup of .S;˚/,t h e njS0jis a divisor
ofjSj.
A subgroup S0of a group Sis aproper subgroup if S0¤S. We shall use the
following corollary in our analysis in Section 31.8 of the Miller-Rabin primalitytest procedure.
Corollary 31.16
IfS
0is a proper subgroup of a ﬁnite group S,t h e njS0j/DC4jSj=2.
Subgroups generated by an element
Theorem 31.14 gives us an easy way to produce a subgroup of a ﬁnite group .S;˚/:
choose an element aand take all elements that can be generated from ausing the
group operation. Speciﬁcally, deﬁne a.k/fork/NAK1by
a.k/DkM
iD1aDa˚a˚/SOH/SOH/SOH˚ a œ
k:
F o re x a m p l e ,i fw et a k e aD2in the group Z6, the sequence a.1/;a.2/;a.3/;:::is
2;4;0;2;4;0;2;4;0;::: :
In the group Zn,w eh a v e a.k/Dkamodn, and in the group Z/ETX
n,w eh a v e a.k/D
akmodn.W ed e ﬁ n et h e subgroup generated by a, denotedhaior.hai;˚/,b y
haiDf a.k/Wk/NAK1g:
We say that agenerates the subgrouphaior that ais agenerator ofhai.S i n c e Sis
ﬁnite,haiis a ﬁnite subset of S, possibly including all of S. Since the associativity
of˚implies31.3 Modular arithmetic 945
a.i/˚a.j /Da.iCj/;
haiis closed and therefore, by Theorem 31.14, haiis a subgroup of S. For example,
inZ6,w eh a v e
h0iDf0g;
h1iDf0; 1; 2; 3; 4; 5g;
h2iDf0; 2; 4g:
Similarly, in Z/ETX
7,w eh a v e
h1iDf1g;
h2iDf1; 2; 4g;
h3iDf1; 2; 3; 4; 5; 6g:
Theorder ofa(in the group S), denoted ord .a/, is deﬁned as the smallest posi-
tive integer tsuch that a.t/De.
Theorem 31.17
For any ﬁnite group .S;˚/and any a2S, the order of ais equal to the size of the
subgroup it generates, or ord .a/Djhaij.
Proof LettDord.a/.S i n c e a.t/Deanda.tCk/Da.t/˚a.k/Da.k/for
k/NAK1,i fi>t ,t h e n a.i/Da.j /for some j< i . Thus, as we generate ele-
ments by a, we see no new elements after a.t/. Thus,haiDf a.1/;a.2/;:::;a.t/g,
and sojhaij/DC4t. To show thatjhaij/NAKt, we show that each element of the se-
quence a.1/;a.2/;:::;a.t/is distinct. Suppose for the purpose of contradiction that
a.i/Da.j /for some iandjsatisfying 1/DC4i<j/DC4t. Then, a.iCk/Da.jCk/
fork/NAK0. But this equality implies that a.iC.t/NULj/ /Da.jC.t/NULj/ /De, a contradic-
tion, since iC.t/NULj/<t buttis the least positive value such that a.t/De.T h e r e -
fore, each element of the sequence a.1/;a.2/;:::;a.t/is distinct, andjhaij/NAKt.W e
conclude that ord .a/Djhaij.
Corollary 31.18
The sequence a.1/;a.2/;:::is periodic with period tDord.a/;t h a ti s , a.i/Da.j /
if and only if i/DC1j.mod t/.
Consistent with the above corollary, we deﬁne a.0/aseanda.i/asa.imodt/,
where tDord.a/, for all integers i.
Corollary 31.19
If.S;˚/is a ﬁnite group with identity e, then for all a2S,
a.jSj/De:946 Chapter 31 Number-Theoretic Algorithms
Proof Lagrange’s theorem (Theorem 31.15) implies that ord .a/jjSj,a n ds o
jSj/DC10.mod t/,w h e r e tDord.a/. Therefore, a.jSj/Da.0/De.
Exercises
31.3-1
Draw the group operation tables for the groups .Z4;C4/and.Z/ETX
5;/SOH5/. Show that
these groups are isomorphic by exhibiting a one-to-one correspondence ˛between
their elements such that aCb/DC1c.mod 4/if and only if ˛.a//SOH˛.b//DC1˛.c/
.mod 5/.
31.3-2
List all subgroups of Z9and of Z/ETX
13.
31.3-3
Prove Theorem 31.14.
31.3-4
Show that if pis prime and eis a positive integer, then
/RS.pe/Dpe/NUL1.p/NUL1/ :
31.3-5
Show that for any integer n>1 and for any a2Z/ETX
n, the function faWZ/ETX
n! Z/ETX
n
deﬁned by fa.x/Daxmodnis a permutation of Z/ETX
n.
31.4 Solving modular linear equations
We now consider the problem of ﬁnding solutions to the equation
ax/DC1b.mod n/ ; (31.25)
where a>0 andn>0 . This problem has several applications; for example,
we shall use it as part of the procedure for ﬁnding keys in the RSA public-key
cryptosystem in Section 31.7. We assume that a,b,a n d nare given, and we wish
to ﬁnd all values of x, modulo n, that satisfy equation (31.25). The equation may
have zero, one, or more than one such solution.
Lethaidenote the subgroup of Zngenerated by a.S i n c ehaiDf a.x/Wx>0gD
faxmodnWx>0g, equation (31.25) has a solution if and only if Œb/c1412hai.L a -
grange’s theorem (Theorem 31.15) tells us that jhaijmust be a divisor of n.T h e
following theorem gives us a precise characterization of hai.31.4 Solving modular linear equations 947
Theorem 31.20
For any positive integers aandn,i fdDgcd.a; n/ ,t h e n
haiDh diDf0; d; 2d; : : : ; ..n=d/ /NUL1/dg (31.26)
inZn, and thus
jhaijDn=d :
Proof We begin by showing that d2hai. Recall that E XTENDED -EUCLID .a; n/
produces integers x0andy0such that ax0Cny0Dd. Thus, ax0/DC1d.mod n/,s o
thatd2hai. In other words, dis a multiple of ainZn.
Since d2hai, it follows that every multiple of dbelongs tohai, because any
multiple of a multiple of ais itself a multiple of a. Thus,haicontains every element
inf0; d; 2d; : : : ; ..n=d/ /NUL1/dg.T h a ti s ,hdi/DC2hai.
We now show that hai/DC2h di.I fm2hai,t h e n mDaxmodnfor some
integer x,a n ds o mDaxCnyfor some integer y.H o w e v e r , djaanddjn,a n d
sodjmby equation (31.4). Therefore, m2hdi.
Combining these results, we have that haiDh di. To see thatjhaijDn=d,
observe that there are exactly n=d multiples of dbetween 0andn/NUL1,i n c l u s i v e .
Corollary 31.21
The equation ax/DC1b.mod n/is solvable for the unknown xif and only if djb,
where dDgcd.a; n/ .
Proof The equation ax/DC1b.mod n/is solvable if and only if Œb/c1412hai,w h i c h
is the same as saying
.bmodn/2f0; d; 2d; : : : ; ..n=d/ /NUL1/dg;
by Theorem 31.20. If 0/DC4b<n ,t h e n b2haiif and only if djb, since the
members ofhaiare precisely the multiples of d.I fb<0 orb/NAKn, the corollary
then follows from the observation that djbif and only if dj.bmodn/,s i n c e b
andbmodndiffer by a multiple of n, which is itself a multiple of d.
Corollary 31.22
The equation ax/DC1b.mod n/either has ddistinct solutions modulo n,w h e r e
dDgcd.a; n/ , or it has no solutions.
Proof Ifax/DC1b.mod n/has a solution, then b2hai. By Theorem 31.17,
ord.a/Djhaij, and so Corollary 31.18 and Theorem 31.20 imply that the sequence
aimodn,f o riD0; 1; : : : , is periodic with period jhaijDn=d.I fb2hai,t h e n b
appears exactly dtimes in the sequence aimodn,f o riD0; 1; : : : ; n/NUL1,s i n c e948 Chapter 31 Number-Theoretic Algorithms
the length- .n=d/ block of valueshairepeats exactly dtimes as iincreases from 0
ton/NUL1. The indices xof the dpositions for which axmodnDbare the solutions
of the equation ax/DC1b.mod n/.
Theorem 31.23
LetdDgcd.a; n/ , and suppose that dDax0Cny0for some integers x0andy0
(for example, as computed by E XTENDED -EUCLID ). If djb, then the equation
ax/DC1b.mod n/has as one of its solutions the value x0,w h e r e
x0Dx0.b=d/ modn:
Proof We have
ax0/DC1ax0.b=d/ . mod n/
/DC1d.b=d/ . mod n/ (because ax0/DC1d.mod n/)
/DC1b. mod n/ ;
and thus x0is a solution to ax/DC1b.mod n/.
Theorem 31.24
Suppose that the equation ax/DC1b.mod n/is solvable (that is, djb,w h e r e
dDgcd.a; n/ )a n dt h a t x0is any solution to this equation. Then, this equa-
tion has exactly ddistinct solutions, modulo n,g i v e nb y xiDx0Ci.n=d/ for
iD0; 1; : : : ; d/NUL1.
Proof Because n=d > 0 and0/DC4i.n=d/ < n foriD0; 1; : : : ; d/NUL1,t h e
values x0;x1;:::;x d/NUL1are all distinct, modulo n.S i n c e x0is a solution of ax/DC1b
.mod n/,w eh a v e ax0modn/DC1b.mod n/. Thus, for iD0; 1; : : : ; d/NUL1,w e
have
aximodnDa.x 0Cin=d/ modn
D.ax 0Cain=d/ modn
Dax0modn(because djaimplies that ain=d is a multiple of n)
/DC1b.mod n/ ;
and hence axi/DC1b.mod n/,m a k i n g xia solution, too. By Corollary 31.22, the
equation ax/DC1b.mod n/has exactly dsolutions, so that x0;x1;:::;x d/NUL1must
be all of them.
We have now developed the mathematics needed to solve the equation ax/DC1b
.mod n/; the following algorithm prints all solutions to this equation. The inputs
aandnare arbitrary positive integers, and bis an arbitrary integer.31.4 Solving modular linear equations 949
MODULAR -LINEAR -EQUATION -SOLVER . a;b;n /
1.d; x0;y0/DEXTENDED -EUCLID .a; n/
2ifdjb
3 x0Dx0.b=d/ modn
4 foriD0tod/NUL1
5 print .x0Ci.n=d// modn
6elseprint “no solutions”
As an example of the operation of this procedure, consider the equation 14x/DC1
30 . mod 100/ (here, aD14,bD30,a n d nD100). Calling E XTENDED -
EUCLID in line 1, we obtain .d; x0;y0/D.2;/NUL7; 1/.S i n c e 2j30, lines 3–5
execute. Line 3 computes x0D./NUL7/.15/ mod100D95. The loop on lines 4–5
prints the two solutions 95 and 45.
The procedure M ODULAR -LINEAR -EQUATION -SOLVER works as follows.
Line 1 computes dDgcd.a; n/ , along with two values x0andy0such that dD
ax0Cny0, demonstrating that x0is a solution to the equation ax0/DC1d.mod n/.
Ifddoes not divide b, then the equation ax/DC1b.mod n/has no solution, by
Corollary 31.21. Line 2 checks to see whether djb; if not, line 6 reports that there
are no solutions. Otherwise, line 3 computes a solution x0toax/DC1b.mod n/,
in accordance with Theorem 31.23. Given one solution, Theorem 31.24 states thatadding multiples of .n=d/ , modulo n, yields the other d/NUL1solutions. The for
loop of lines 4–5 prints out all dsolutions, beginning with x
0and spaced n=d
apart, modulo n.
MODULAR -LINEAR -EQUATION -SOLVER performs O.lgnCgcd.a; n// arith-
metic operations, since E XTENDED -EUCLID performs O.lgn/arithmetic opera-
tions, and each iteration of the forloop of lines 4–5 performs a constant number of
arithmetic operations.
The following corollaries of Theorem 31.24 give specializations of particular
interest.
Corollary 31.25
For any n>1 ,i fg c d .a; n/D1, then the equation ax/DC1b.mod n/has a unique
solution, modulo n.
IfbD1, a common case of considerable interest, the xwe are looking for is a
multiplicative inverse ofa, modulo n.
Corollary 31.26
For any n>1 ,i fg c d .a; n/D1, then the equation ax/DC11.mod n/has a unique
solution, modulo n. Otherwise, it has no solution.
950 Chapter 31 Number-Theoretic Algorithms
Thanks to Corollary 31.26, we can use the notation a/NUL1modnto refer to the
multiplicative inverse of a, modulo n,w h e n aandnare relatively prime. If
gcd.a; n/D1, then the unique solution to the equation ax/DC11.mod n/is the
integer xreturned by E XTENDED -EUCLID , since the equation
gcd.a; n/D1DaxCny
implies ax/DC11.mod n/. Thus, we can compute a/NUL1modnefﬁciently using
EXTENDED -EUCLID .
Exercises
31.4-1
Find all solutions to the equation 35x/DC110 . mod 50/.
31.4-2
Prove that the equation ax/DC1ay . mod n/implies x/DC1y.mod n/whenever
gcd.a; n/D1. Show that the condition gcd .a; n/D1is necessary by supplying a
counterexample with gcd .a; n/ > 1 .
31.4-3
Consider the following change to line 3 of the procedure M ODULAR -LINEAR -
EQUATION -SOLVER :
3 x0Dx0.b=d/ mod.n=d/
Will this work? Explain why or why not.
31.4-4 ?
Letpbe prime and f. x//DC1f0Cf1xC/SOH/SOH/SOHC ftxt.mod p/be a polyno-
mial of degree t, with coefﬁcients fidrawn from Zp. We say that a2Zp
is azero offiff. a //DC10.mod p/. Prove that if ais a zero of f,t h e n
f. x//DC1.x/NULa/g.x/ . mod p/for some polynomial g.x/ of degree t/NUL1. Prove
by induction on tthat if pis prime, then a polynomial f. x/ of degree tcan have
at most tdistinct zeros modulo p.
31.5 The Chinese remainder theorem
Around A.D. 100, the Chinese mathematician Sun-Ts˘ u solved the problem of ﬁnd-
ing those integers xthat leave remainders 2, 3, and 2 when divided by 3, 5, and 7
respectively. One such solution is xD23; all solutions are of the form 23C105k31.5 The Chinese remainder theorem 951
for arbitrary integers k. The “Chinese remainder theorem” provides a correspon-
dence between a system of equations modulo a set of pairwise relatively primemoduli (for example, 3, 5, and 7) and an equation modulo their product (for exam-ple, 105).
The Chinese remainder theorem has two major applications. Let the inte-
gernbe factored as nDn
1n2/SOH/SOH/SOHnk, where the factors niare pairwise relatively
prime. First, the Chinese remainder theorem is a descriptive “structure theorem”that describes the structure of Z
nas identical to that of the Cartesian product
Zn1/STXZn2/STX/SOH/SOH/SOH/STX Znkwith componentwise addition and multiplication modulo ni
in the ith component. Second, this description helps us to design efﬁcient algo-
rithms, since working in each of the systems Znican be more efﬁcient (in terms of
bit operations) than working modulo n.
Theorem 31.27 (Chinese remainder theorem)
LetnDn1n2/SOH/SOH/SOHnk, where the niare pairwise relatively prime. Consider the
correspondence
a$.a1;a2;:::;a k/; (31.27)
where a2Zn,ai2Zni,a n d
aiDamodni
foriD1 ;2;:::;k . Then, mapping (31.27) is a one-to-one correspondence (bijec-
tion) between Znand the Cartesian product Zn1/STXZn2/STX/SOH/SOH/SOH/STX Znk. Operations per-
formed on the elements of Zncan be equivalently performed on the corresponding
k-tuples by performing the operations independently in each coordinate position in
the appropriate system. That is, if
a$.a1;a2;:::;a k/;
b$.b1;b2;:::;b k/;
then.aCb/modn$..a
1Cb1/modn1;:::;. a kCbk/modnk/; (31.28)
.a/NULb/modn$..a1/NULb1/modn1;:::;. a k/NULbk/modnk/; (31.29)
.ab/ modn$.a1b1modn1;:::;a kbkmodnk/: (31.30)
Proof Transforming between the two representations is fairly straightforward.
Going from ato.a1;a2;:::;a k/is quite easy and requires only k“mod” opera-
tions.
Computing afrom inputs .a1;a2;:::;a k/is a bit more complicated. We begin
by deﬁning miDn=n iforiD1 ;2;:::;k ; thus miis the product of all of the nj’s
other than ni:miDn1n2/SOH/SOH/SOHni/NUL1niC1/SOH/SOH/SOHnk.W en e x td e ﬁ n e952 Chapter 31 Number-Theoretic Algorithms
ciDmi.m/NUL1
imodni/ (31.31)
foriD1 ;2;:::;k . Equation (31.31) is always well deﬁned: since miandniare
relatively prime (by Theorem 31.6), Corollary 31.26 guarantees that m/NUL1
imodni
exists. Finally, we can compute aa saf u n c t i o no f a1,a2,..., akas follows:
a/DC1.a1c1Ca2c2C/SOH/SOH/SOHC akck/.mod n/ : (31.32)
We now show that equation (31.32) ensures that a/DC1ai.mod ni/foriD
1 ;2;:::;k . Note that if j¤i,t h e n mj/DC10.mod ni/, which implies that cj/DC1
mj/DC10.mod ni/. Note also that ci/DC11.mod ni/, from equation (31.31). We
thus have the appealing and useful correspondence
ci$. 0 ;0 ;:::;0 ;1 ;0 ;:::;0 /;
a vector that has 0s everywhere except in the ith coordinate, where it has a 1;t h eci
thus form a “basis” for the representation, in a certain sense. For each i, therefore,
we have
a/DC1aici .mod ni/
/DC1aimi.m/NUL1
imodni/.mod ni/
/DC1ai .mod ni/;
which is what we wished to show: our method of computing afrom the ai’s pro-
duces a result athat satisﬁes the constraints a/DC1ai.mod ni/foriD1 ;2;:::;k .
The correspondence is one-to-one, since we can transform in both directions.Finally, equations (31.28)–(31.30) follow directly from Exercise 31.1-7, sincexmodn
iD.xmodn/modnifor any xandiD1 ;2;:::;k .
We shall use the following corollaries later in this chapter.
Corollary 31.28
Ifn1;n2;:::;n kare pairwise relatively prime and nDn1n2/SOH/SOH/SOHnk, then for any
integers a1;a2;:::;a k, the set of simultaneous equations
x/DC1ai.mod ni/;
foriD1 ;2;:::;k , has a unique solution modulo nfor the unknown x.
Corollary 31.29
Ifn1;n2;:::;n kare pairwise relatively prime and nDn1n2/SOH/SOH/SOHnk, then for all
integers xanda,
x/DC1a.mod ni/
foriD1 ;2;:::;k if and only if
x/DC1a.mod n/ :
31.5 The Chinese remainder theorem 953
0123456789 1 0 1 1 1 2
0
 0 4 01 55 53 0 5 4 52 06 03 51 05 02 5
1
2 6 1 4 11 65 63 1 6 4 62 16 13 61 15 1
2
52 27 2 42 17 57 32 7 47 22 62 37 12
3
1 35 32 8 3 4 31 85 83 3 8 4 82 36 33 8
4
39 14 54 29 4 44 19 59 34 9 49 24 64
Figure 31.3 An illustration of the Chinese remainder theorem for n1D5andn2D13. For this
example, c1D26andc2D40.I n r o w i, column jis shown the value of a, modulo 65, such
thatamod5Diandamod13Dj. Note that row 0, column 0 contains a 0. Similarly, row 4,
column 12 contains a 64 (equivalent to /NUL1). Since c1D26, moving down a row increases aby26.
Similarly, c2D40means that moving right by a column increases aby40. Increasing aby1
corresponds to moving diagonally downward and to the right, wrapping around from the bottom to
the top and from the right to the left.
As an example of the application of the Chinese remainder theorem, suppose we
are given the two equations
a/DC12.mod 5/ ;
a/DC13.mod 13/ ;
so that a1D2,n1Dm2D5,a2D3,a n d n2Dm1D13, and we wish
to compute amod65,s i n c e nDn1n2D65. Because 13/NUL1/DC12.mod 5/and
5/NUL1/DC18.mod 13/,w eh a v e
c1D13.2 mod5/D26 ;
c2D5.8mod13/D40 ;
and
a/DC12/SOH26C3/SOH40 . mod 65/
/DC152C120 . mod 65/
/DC142 . mod 65/ :
See Figure 31.3 for an illustration of the Chinese remainder theorem, modulo 65.
Thus, we can work modulo nby working modulo ndirectly or by working in the
transformed representation using separate modulo nicomputations, as convenient.
The computations are entirely equivalent.
Exercises
31.5-1
Find all solutions to the e quations x/DC14.mod 5/andx/DC15.mod 11/.954 Chapter 31 Number-Theoretic Algorithms
31.5-2
Find all integers xthat leave remainders 1, 2, 3 when divided by 9, 8, 7 respectively.
31.5-3
Argue that, under the deﬁnitions of Theorem 31.27, if gcd .a; n/D1,t h e n
.a/NUL1modn/$..a/NUL1
1modn1/; .a/NUL1
2modn2/ ;:::;. a/NUL1
kmodnk// :
31.5-4
Under the deﬁnitions of Theorem 31.27, prove that for any polynomial f, the num-
ber of roots of the equation f. x//DC10.mod n/equals the product of the number
of roots of each of the equations f. x//DC10.mod n1/,f. x//DC10.mod n2/, ...,
f. x//DC10.mod nk/.
31.6 Powers of an element
Just as we often consider the multiples of a given element a, modulo n, we consider
the sequence of powers of a, modulo n,w h e r e a2Z/ETX
n:
a0;a1;a2;a3;:::; (31.33)
modulo n. Indexing from 0,t h e0th value in this sequence is a0modnD1,a n d
theith value is aimodn. For example, the powers of 3modulo 7are
i 01234567891 01 1 /SOH/SOH/SOH
3imod71326451326 4 5 /SOH/SOH/SOH
whereas the powers of 2modulo 7are
i 01234567891 01 1 /SOH/SOH/SOH
2imod71241241241 2 4 /SOH/SOH/SOH
In this section, lethaidenote the subgroup of Z/ETX
ngenerated by aby repeated
multiplication, and let ord n.a/(the “order of a, modulo n”) denote the order of a
inZ/ETX
n. For example,h2iDf1; 2; 4ginZ/ETX
7, and ord 7.2/D3. Using the deﬁnition of
the Euler phi function /RS.n/ as the size of Z/ETX
n(see Section 31.3), we now translate
Corollary 31.19 into the notation of Z/ETX
nto obtain Euler’s theorem and specialize it
toZ/ETX
p,w h e r e pis prime, to obtain Fermat’s theorem.
Theorem 31.30 (Euler’s theorem)
For any integer n>1 ,
a/RS.n//DC11.mod n/for all a2Z/ETX
n:
31.6 Powers of an element 955
Theorem 31.31 (Fermat’s theorem)
Ifpis prime, then
ap/NUL1/DC11.mod p/for all a2Z/ETX
p:
Proof By equation (31.21), /RS.p/Dp/NUL1ifpis prime.
Fermat’s theorem applies to every element in Zpexcept 0,s i n c e 062Z/ETX
p.F o ra l l
a2Zp, however, we have ap/DC1a.mod p/ifpis prime.
If ord n.g/DjZ/ETX
nj, then every element in Z/ETX
ni sap o w e ro f g, modulo n,a n d
gis aprimitive root or agenerator ofZ/ETX
n. For example, 3is a primitive root,
modulo 7,b u t 2is not a primitive root, modulo 7.I f Z/ETX
npossesses a primitive
root, the group Z/ETX
niscyclic . We omit the proof of the following theorem, which is
proven by Niven and Zuckerman [265].
Theorem 31.32
The values of n>1 for which Z/ETX
nis cyclic are 2,4,pe,a n d 2pe, for all primes
p>2 and all positive integers e.
Ifgis a primitive root of Z/ETX
nandais any element of Z/ETX
n, then there exists a ´such
thatg´/DC1a.mod n/.T h i s ´is adiscrete logarithm or anindex ofa, modulo n,
to the base g; we denote this value as ind n;g.a/.
Theorem 31.33 (Discrete logarithm theorem)
Ifgis a primitive root of Z/ETX
n, then the equation gx/DC1gy.mod n/holds if and
only if the equation x/DC1y.mod /RS.n// holds.
Proof Suppose ﬁrst that x/DC1y.mod /RS.n// . Then, xDyCk/RS.n/ for some
integer k. Therefore,
gx/DC1gyCk/RS.n/.mod n/
/DC1gy/SOH.g/RS.n//k.mod n/
/DC1gy/SOH1k.mod n/ (by Euler’s theorem)
/DC1gy.mod n/ :
Conversely, suppose that gx/DC1gy.mod n/. Because the sequence of powers of g
generates every element of hgiandjhgijD/RS.n/ , Corollary 31.18 implies that
the sequence of powers of gis periodic with period /RS.n/ . Therefore, if gx/DC1gy
.mod n/, then we must have x/DC1y.mod /RS.n// .
We now turn our attention to the square roots of 1, modulo a prime power. The
following theorem will be useful in our development of a primality-testing algo-
rithm in Section 31.8.956 Chapter 31 Number-Theoretic Algorithms
Theorem 31.34
Ifpis an odd prime and e/NAK1, then the equation
x2/DC11.mod pe/ (31.34)
has only two solutions, namely xD1andxD/NUL1.
Proof Equation (31.34) is equivalent to
pej.x/NUL1/.xC1/ :
Since p>2 , we can have pj.x/NUL1/orpj.xC1/, but not both. (Otherwise,
by property (31.3), pwould also divide their difference .xC1//NUL.x/NUL1/D2.)
Ifp−.x/NUL1/,t h e ng c d .pe;x/NUL1/D1, and by Corollary 31.5, we would have
pej.xC1/.T h a t i s , x/DC1/NUL 1.mod pe/. Symmetrically, if p −.xC1/,
then gcd .pe;xC1/D1, and Corollary 31.5 implies that pej.x/NUL1/,s ot h a t
x/DC11.mod pe/. Therefore, either x/DC1/NUL1.mod pe/orx/DC11.mod pe/.
A number xis anontrivial square root of 1, modulo n, if it satisﬁes the equation
x2/DC11.mod n/butxis equivalent to neither of the two “trivial” square roots:
1or/NUL1, modulo n. For example, 6is a nontrivial square root of 1, modulo 35.
We shall use the following corollary to Theorem 31.34 in the correctness proof inSection 31.8 for the Miller-Rabin primality-testing procedure.
Corollary 31.35
If there exists a nontrivial square root of 1, modulo n,t h e n nis composite.
Proof By the contrapositive of Theorem 31.34, if there exists a nontrivial square
root of 1, modulo n,t h e n ncannot be an odd prime or a power of an odd prime.
Ifx
2/DC11.mod 2/,t h e n x/DC11.mod 2/, and so all square roots of 1, modulo 2,
are trivial. Thus, ncannot be prime. Finally, we must have n>1 for a nontrivial
square root of 1to exist. Therefore, nmust be composite.
Raising to powers with repeated squaring
A frequently occurring operation in number-theoretic computations is raising one
number to a power modulo another number, also known as modular exponentia-
tion. More precisely, we would like an efﬁcient way to compute abmodn,w h e r e
aandbare nonnegative integers and nis a positive integer. Modular exponenti-
ation is an essential operation in many primality-testing routines and in the RSApublic-key cryptosystem. The method of repeated squaring solves this problem
efﬁciently using the binary representation of b.
Lethb
k;bk/NUL1;:::;b 1;b0ibe the binary representation of b. (That is, the binary
representation is kC1bits long, bkis the most signiﬁcant bit, and b0is the least31.6 Powers of an element 957
i
 9 8 76543210
bi
1 0 00110000
c
 1 2 4 8 17 35 70 140 280 560
d
 7 49 157 526 160 241 298 166 67 1
Figure 31.4 The results of M ODULAR -EXPONENTIATION when computing ab.mod n/,w h e r e
aD7,bD560Dh1000110000i,a n d nD561. The values are shown after each execution of the
forloop. The ﬁnal result is 1.
signiﬁcant bit.) The following procedure computes acmodnascis increased by
doublings and incrementations from 0tob.
MODULAR -EXPONENTIATION . a;b;n /
1cD0
2dD1
3l e thbk;bk/NUL1;:::;b 0ibe the binary representation of b
4foriDkdownto 0
5 cD2c
6 dD.d/SOHd/modn
7 ifbi==1
8 cDcC1
9 dD.d/SOHa/modn
10return d
The essential use of squaring in line 6 of each iteration explains the name “repeated
squaring.” As an example, for aD7,bD560,a n d nD561, the algorithm
computes the sequence of values modulo 561 shown in Figure 31.4; the sequenceof exponents used appears in the row of the table labeled by c.
The variable cis not really needed by the algorithm but is included for the fol-
lowing two-part loop invariant:
Just prior to each iteration of the forloop of lines 4–9,
1. The value of cis the same as the preﬁx hb
k;bk/NUL1;:::;b iC1iof the binary
representation of b,a n d
2.dDacmodn.
We use this loop invariant as follows:
Initialization: Initially, iDk, so that the preﬁx hbk;bk/NUL1;:::;b iC1iis empty,
which corresponds to cD0. Moreover, dD1Da0modn.958 Chapter 31 Number-Theoretic Algorithms
Maintenance: Letc0andd0denote the values of canddat the end of an iteration
of the forloop, and thus the values prior to the next iteration. Each iteration
updates c0D2c(ifbiD0)o rc0D2cC1(ifbiD1), so that cwill be correct
prior to the next iteration. If biD0,t h e n d0Dd2modnD.ac/2modnD
a2cmodnDac0modn.I fbiD1,t h e n d0Dd2amodnD.ac/2amodnD
a2cC1modnDac0modn. In either case, dDacmodnprior to the next
iteration.
Termination: At termination, iD/NUL1. Thus, cDb,s i n c e chas the value of the
preﬁxhbk;bk/NUL1;:::;b 0iofb’s binary representation. Hence dDacmodnD
abmodn.
If the inputs a,b,a n d nareˇ-bit numbers, then the total number of arith-
metic operations required is O.ˇ/ and the total number of bit operations required
isO.ˇ3/.
Exercises
31.6-1
Draw a table showing the order of every element in Z/ETX
11. Pick the smallest primitive
rootgand compute a table giving ind 11;g.x/for all x2Z/ETX
11.
31.6-2
Give a modular exponentiation algorithm that examines the bits of bfrom right to
left instead of left to right.
31.6-3
Assuming that you know /RS.n/ , explain how to compute a/NUL1modnfor any a2Z/ETX
n
using the procedure M ODULAR -EXPONENTIATION .
31.7 The RSA public-key cryptosystem
With a public-key cryptosystem, we can encrypt messages sent between two com-
municating parties so that an eavesdropper who overhears the encrypted messageswill not be able to decode them. A public-key cryptosystem also enables a partyto append an unforgeable “digital signature” to the end of an electronic message.Such a signature is the electronic version of a handwritten signature on a paper doc-ument. It can be easily checked by anyone, forged by no one, yet loses its validityif any bit of the message is altered. It therefore provides authentication of both theidentity of the signer and the contents of the signed message. It is the perfect tool31.7 The RSA public-key cryptosystem 959
for electronically signed business contracts, electronic checks, electronic purchase
orders, and other electronic communications that parties wish to authenticate.
The RSA public-key cryptosystem relies on the dramatic difference between the
ease of ﬁnding large prime numbers and the difﬁculty of factoring the product oftwo large prime numbers. Section 31.8 describes an efﬁcient procedure for ﬁndinglarge prime numbers, and Section 31.9 discusses the problem of factoring largeintegers.
Public-key cryptosystems
In a public-key cryptosystem, each participant has both a public key and a secret
key. Each key is a piece of information. For example, in the RSA cryptosystem,
each key consists of a pair of integers. The participants “Alice” and “Bob” are
traditionally used in cryptography examples; we denote their public and secret
keys as P
A,SAfor Alice and PB,SBfor Bob.
Each participant creates his or her own public and secret keys. Secret keys are
kept secret, but public keys can be revealed to anyone or even published. In fact,it is often convenient to assume that everyone’s public key is available in a pub-lic directory, so that any participant can easily obtain the public key of any otherparticipant.
The public and secret keys specify functions that can be applied to any message.
LetDdenote the set of permissible messages. For example, Dmight be the set of
all ﬁnite-length bit sequences. In the simplest, and original, formulation of public-key cryptography, we require that the public and secret keys specify one-to-onefunctions from Dto itself. We denote the function corresponding to Alice’s public
keyP
AbyPA./and the function corresponding to her secret key SAbySA./.T h e
functions PA./andSA./are thus permutations of D. We assume that the functions
PA./andSA./are efﬁciently computable given the corresponding key PAorSA.
The public and secret keys for any participant are a “matched pair” in that they
specify functions that are inverses of each other. That is,
MDSA.PA.M // ; (31.35)
MDPA.SA.M // (31.36)
for any message M2D. Transforming Mwith the two keys PAandSAsucces-
sively, in either order, yields the message Mback.
In a public-key cryptosystem, we require that no one but Alice be able to com-
pute the function SA./in any practical amount of time. This assumption is crucial
to keeping encrypted mail sent to Alice private and to knowing that Alice’s digi-tal signatures are authentic. Alice must keep S
Asecret; if she does not, she loses
her uniqueness and the cryptosystem cannot provide her with unique capabilities.The assumption that only Alice can compute S
A./must hold even though everyone960 Chapter 31 Number-Theoretic Algorithms
decryptcommunication channel
encryptBob Alice
eavesdropperM M PA SA
CCDPA.M /
Figure 31.5 Encryption in a public key system. Bob encrypts the message Musing Alice’s public
keyPAand transmits the resulting ciphertext CDPA.M/ over a communication channel to Al-
ice. An eavesdropper who captures the transmitted ciphertext gains no information about M. Alice
receives Cand decrypts it using her secret key to obtain the original message MDSA.C /.
knows PAand can compute PA./, the inverse function to SA./, efﬁciently. In order
to design a workable public-key cryptosystem, we must ﬁgure out how to create
a system in which we can reveal a transformation PA./without thereby revealing
how to compute the corresponding inverse transformation SA./. This task appears
formidable, but we shall see how to accomplish it.
In a public-key cryptosystem, encryption works as shown in Figure 31.5. Sup-
pose Bob wishes to send Alice a message Mencrypted so that it will look like
unintelligible gibberish to an eavesdropper. The scenario for sending the messagegoes as follows.
/SIBob obtains Alice’s public key PA(from a public directory or directly from
Alice).
/SIBob computes the ciphertext CDPA.M / corresponding to the message M
and sends Cto Alice.
/SIWhen Alice receives the ciphertext C, she applies her secret key SAto retrieve
the original message: SA.C /DSA.PA.M //DM.
Because SA./andPA./are inverse functions, Alice can compute Mfrom C.B e -
cause only Alice is able to compute SA./, Alice is the only one who can compute M
from C. Because Bob encrypts Musing PA./, only Alice can understand the trans-
mitted message.
We can just as easily implement digital signatures within our formulation of a
public-key cryptosystem. (There are other ways of approaching the problem of
constructing digital signatures, but we shall not go into them here.) Suppose now
that Alice wishes to send Bob a digitally signed response M0. Figure 31.6 shows
how the digital-signature scenario proceeds.
/SIAlice computes her digital signature /ESCfor the message M0using her secret
keySAand the equation /ESCDSA.M0/.31.7 The RSA public-key cryptosystem 961
sign
communication channelverify
=? acceptBob Alice
M0
M0PA SA/ESC
.M0;/ESC//ESCDSA.M0/
Figure 31.6 Digital signatures in a public-key system. Alice signs the message M0by appending
her digital signature /ESCDSA.M0/to it. She transmits the message/signature pair .M0;/ESC/to Bob,
who veriﬁes it by checking the equation M0DPA./ESC/. If the equation holds, he accepts .M0;/ESC/as
a message that Alice has signed.
/SIAlice sends the message/signature pair .M0;/ESC/to Bob.
/SIWhen Bob receives .M0;/ESC/, he can verify that it originated from Alice by us-
ing Alice’s public key to verify the equation M0DPA./ESC/. (Presumably, M0
contains Alice’s name, so Bob knows whose public key to use.) If the equation
holds, then Bob concludes that the message M0was actually signed by Alice.
If the equation fails to hold, Bob concludes either that the message M0or the
digital signature /ESCwas corrupted by transmission errors or that the pair .M0;/ESC/
is an attempted forgery.
Because a digital signature provides both authentication of the signer’s identity and
authentication of the contents of the signed message, it is analogous to a handwrit-
ten signature at the end of a written document.
A digital signature must be veriﬁable by anyone who has access to the signer’s
public key. A signed message can be veriﬁed by one party and then passed on toother parties who can also verify the signature. For example, the message mightbe an electronic check from Alice to Bob. After Bob veriﬁes Alice’s signature onthe check, he can give the check to his bank, who can then also verify the signatureand effect the appropriate funds transfer.
A signed message is not necessarily encrypted; the message can be “in the clear”
and not protected from disclosure. By composing the above protocols for encryp-
tion and for signatures, we can create messages that are both signed and encrypted.
The signer ﬁrst appends his or her digital signature to the message and then en-
crypts the resulting message/signature pair with the public key of the intended re-
cipient. The recipient decrypts the received message with his or her secret key toobtain both the original message and its digital signature. The recipient can then
verify the signature using the public key of the signer. The corresponding com-
bined process using paper-based systems would be to sign the paper document and962 Chapter 31 Number-Theoretic Algorithms
then seal the document inside a paper envelope that is opened only by the intended
recipient.
The RSA cryptosystem
In the RSA public-key cryptosystem , a participant creates his or her public and
secret keys with the following procedure:
1. Select at random two large prime numbers pandqsuch that p¤q. The primes
pandqmight be, say, 1024 bits each.
2. Compute nDpq.
3. Select a small odd integer ethat is relatively prime to /RS.n/ , which, by equa-
tion (31.20), equals .p/NUL1/.q/NUL1/.
4. Compute das the multiplicative inverse of e, modulo /RS.n/ . (Corollary 31.26
guarantees that dexists and is uniquely deﬁned. We can use the technique of
Section 31.4 to compute d,g i v e n eand/RS.n/ .)
5. Publish the pair PD.e; n/ as the participant’s RSA public key .
6. Keep secret the pair SD.d; n/ as the participant’s RSA secret key .
For this scheme, the domain Dis the set Zn. To transform a message Masso-
ciated with a public key PD.e; n/ , compute
P.M/DMemodn: (31.37)
To transform a ciphertext Cassociated with a secret key SD.d; n/ , compute
S.C/DCdmodn: (31.38)
These equations apply to both encryption and signatures. To create a signature, the
signer applies his or her secret key to the message to be signed, rather than to aciphertext. To verify a signature, the public key of the signer is applied to it, ratherthan to a message to be encrypted.
We can implement the public-key and secret-key operations using the procedure
M
ODULAR -EXPONENTIATION described in Section 31.6. To analyze the running
time of these operations, assume that the public key .e; n/ and secret key .d; n/
satisfy lg eDO.1/ ,l gd/DC4ˇ,a n dl g n/DC4ˇ. Then, applying a public key requires
O.1/ modular multiplications and uses O.ˇ2/bit operations. Applying a secret
key requires O.ˇ/ modular multiplications, using O.ˇ3/bit operations.
Theorem 31.36 (Correctness of RSA)
The RSA equations (31.37) and (31.38) deﬁne inverse transformations of Znsatis-
fying equations (31.35) and (31.36).31.7 The RSA public-key cryptosystem 963
Proof From equations (31.37) and (31.38), we have that for any M2Zn,
P.S.M//DS.P.M//DMed.mod n/ :
Since eanddare multiplicative inverses modulo /RS.n/D.p/NUL1/.q/NUL1/,
edD1Ck.p/NUL1/.q/NUL1/
for some integer k.B u tt h e n ,i f M6/DC10.mod p/,w eh a v e
Med/DC1M.Mp/NUL1/k.q/NUL1/.mod p/
/DC1M..M modp/p/NUL1/k.q/NUL1/.mod p/
/DC1M.1/k.q/NUL1/.mod p/ (by Theorem 31.31)
/DC1M. mod p/ :
Also, Med/DC1M. mod p/ifM/DC10.mod p/. Thus,
Med/DC1M. mod p/
for all M. Similarly,
Med/DC1M. mod q/
for all M. Thus, by Corollary 31.29 to the Chinese remainder theorem,
Med/DC1M. mod n/
for all M.
The security of the RSA cryptosystem rests in large part on the difﬁculty of fac-
toring large integers. If an adversary can factor the modulus nin a public key, then
the adversary can derive the secret key from the public key, using the knowledge
of the factors pandqin the same way that the creator of the public key used them.
Therefore, if factoring large integers is easy, then breaking the RSA cryptosystemis easy. The converse statement, that if factoring large integers is hard, then break-ing RSA is hard, is unproven. After two decades of research, however, no easiermethod has been found to break the RSA public-key cryptosystem than to factorthe modulus n. And as we shall see in Section 31.9, factoring large integers is sur-
prisingly difﬁcult. By randomly selecting and multiplying together two 1024 -bit
primes, we can create a public key that cannot be “broken” in any feasible amountof time with current technology. In the absence of a fundamental breakthrough inthe design of number-theoretic algorithms, and when implemented with care fol-lowing recommended standards, the RSA cryptosystem is capable of providing ahigh degree of security in applications.
In order to achieve security with the RSA cryptosystem, however, we should
use integers that are quite long—hundreds or even more than one thousand bits964 Chapter 31 Number-Theoretic Algorithms
long—to resist possible advances in the art of factoring. At the time of this
writing (2009), RSA moduli were commonly in the range of 768 to 2048 bits.To create moduli of such sizes, we must be able to ﬁnd large primes efﬁciently.Section 31.8 addresses this problem.
For efﬁciency, RSA is often used in a “hybrid” or “key-management” mode
with fast non-public-key cryptosystems. With such a system, the encryption anddecryption keys are identical. If Alice wishes to send a long message Mto Bob
privately, she selects a random key Kfor the fast non-public-key cryptosystem and
encrypts Musing K, obtaining ciphertext C. Here, Cis as long as M,b u t K
is quite short. Then, she encrypts Kusing Bob’s public RSA key. Since Kis
short, computing P
B.K/ is fast (much faster than computing PB.M /). She then
transmits .C; P B.K// to Bob, who decrypts PB.K/ to obtain Kand then uses K
to decrypt C, obtaining M.
We can use a similar hybrid approach to make digital signatures efﬁciently.
This approach combines RSA with a public collision-resistant hash function h—a
function that is easy to compute but for which it is computationally infeasible to
ﬁnd two messages MandM0such that h.M /Dh.M0/.T h e v a l u e h.M / is
a short (say, 256-bit) “ﬁngerprint” of the message M. If Alice wishes to sign a
message M, she ﬁrst applies htoMto obtain the ﬁngerprint h.M / , which she
then encrypts with her secret key. She sends .M; S A.h.M /// to Bob as her signed
version of M. Bob can verify the signature by computing h.M / and verifying
thatPAapplied to SA.h.M // as received equals h.M / . Because no one can create
two messages with the same ﬁngerprint, it is computationally infeasible to alter a
signed message and preserve the validity of the signature.
Finally, we note that the use of certiﬁcates makes distributing public keys much
easier. For example, assume there is a “trusted authority” Twhose public key
is known by everyone. Alice can obtain from Ta signed message (her certiﬁcate)
stating that “Alice’s public key is PA.” This certiﬁcate is “self-authenticating” since
everyone knows PT. Alice can include her certiﬁcate with her signed messages,
so that the recipient has Alice’s public key immediately available in order to verifyher signature. Because her key was signed by T, the recipient knows that Alice’s
key is really Alice’s.
Exercises
31.7-1
Consider an RSA key set with pD11,qD29,nD319,a n d eD3.W h a t
value of dshould be used in the secret key? What is the encryption of the message
MD100?31.8 Primality testing 965
31.7-2
Prove that if Alice’s public exponent eis3and an adversary obtains Alice’s secret
exponent d,w h e r e 0 < d < /RS.n/ , then the adversary can factor Alice’s modulus n
in time polynomial in the number of bits in n. (Although you are not asked to prove
it, you may be interested to know that this result remains true even if the conditioneD3is removed. See Miller [255].)
31.7-3 ?
Prove that RSA is multiplicative in the sense that
P
A.M1/PA.M2//DC1PA.M1M2/.mod n/ :
Use this fact to prove that if an adversary had a procedure that could efﬁciently
decrypt 1 percent of messages from Znencrypted with PA, then he could employ
a probabilistic algorithm to decrypt every message encrypted with PAwith high
probability.
?31.8 Primality testing
In this section, we consider the problem of ﬁnding large primes. We begin with adiscussion of the density of primes, proceed to examine a plausible, but incomplete,approach to primality testing, and then present an effective randomized primality
test due to Miller and Rabin.
The density of prime numbers
For many applications, such as cryptography, we need to ﬁnd large “random”
primes. Fortunately, large primes are not too rare, so that it is feasible to test
random integers of the appropriate size until we ﬁnd a prime. The prime distribu-
tion function /EM.n/ speciﬁes the number of primes that are less than or equal to n.
For example, /EM.10/D4, since there are 4 prime numbers less than or equal to 10,
namely, 2, 3, 5, and 7. The prime number theorem gives a useful approximationto/EM.n/ .
Theorem 31.37 (Prime number theorem)
lim
n!1/EM.n/
n=lnnD1:
The approximation n=lnngives reasonably accurate estimates of /EM.n/ even
for small n. For example, it is off by less than 6%a t nD109,w h e r e /EM.n/D966 Chapter 31 Number-Theoretic Algorithms
50,847,534 and n=lnn/EM48,254,942. (To a number theorist, 109i sas m a l ln u m -
ber.)
We can view the process of randomly selecting an integer nand determining
whether it is prime as a Bernoulli trial (see Section C.4). By the prime numbertheorem, the probability of a success—that is, the probability that nis prime—is
approximately 1=lnn. The geometric distribution tells us how many trials we need
to obtain a success, and by equation (C.32), the expected number of trials is ap-proximately ln n. Thus, we would expect to examine approximately ln nintegers
chosen randomly near nin order to ﬁnd a prime that is of the same length as n.
For example, we expect that ﬁnding a 1024 -bit prime would require testing ap-
proximately ln 2
1024/EM710randomly chosen 1024 -bit numbers for primality. (Of
course, we can cut this ﬁgure in half by choosing only odd integers.)
In the remainder of this section, we consider the problem of determining whether
or not a large odd integer nis prime. For notational convenience, we assume that n
has the prime factorization
nDpe1
1pe2
2/SOH/SOH/SOHper
r; (31.39)
where r/NAK1,p1;p2;:::;p rare the prime factors of n,a n d e1;e2;:::;e rare posi-
tive integers. The integer nis prime if and only if rD1ande1D1.
One simple approach to the problem of testing for primality is trial division .W e
try dividing nby each integer 2;3 ;:::;bp
nc. (Again, we may skip even integers
greater than 2.) It is easy to see that nis prime if and only if none of the trial divi-
sors divides n. Assuming that each trial division takes constant time, the worst-case
running time is ‚.p
n/, which is exponential in the length of n. (Recall that if n
is encoded in binary using ˇbits, then ˇDdlg.nC1/e,a n ds op
nD‚.2ˇ=2/.)
Thus, trial division works well only if nis very small or happens to have a small
prime factor. When it works, trial division has the advantage that it not only de-termines whether nis prime or composite, but also determines one of n’s prime
factors if nis composite.
In this section, we are interested only in ﬁnding out whether a given number n
is prime; if nis composite, we are not concerned with ﬁnding its prime factor-
ization. As we shall see in Section 31.9, computing the prime factorization of a
number is computationally expensive. It is perhaps surprising that it is much easier
to tell whether or not a given number is prime than it is to determine the prime
factorization of the number if it is not prime.
Pseudoprimality testing
We now consider a method for primality testing that “almost works” and in fact
is good enough for many practical applications. Later on, we shall present a re-31.8 Primality testing 967
ﬁnement of this method that removes the small defect. Let ZC
ndenote the nonzero
elements of Zn:
ZC
nDf1 ;2;:::;n/NUL1g:
Ifnis prime, then ZC
nDZ/ETX
n.
We say that nis abase- apseudoprime ifnis composite and
an/NUL1/DC11.mod n/ : (31.40)
Fermat’s theorem (Theorem 31.31) implies that if nis prime, then nsatisﬁes equa-
tion (31.40) for every ainZC
n. Thus, if we can ﬁnd any a2ZC
nsuch that ndoes
notsatisfy equation (31.40), then nis certainly composite. Surprisingly, the con-
verse almost holds, so that this criterion forms an almost perfect test for primality.
We test to see whether nsatisﬁes equation (31.40) for aD2. If not, we declare n
to be composite by returning COMPOSITE . Otherwise, we return PRIME , guessing
thatnis prime (when, in fact, all we know is that nis either prime or a base- 2
pseudoprime).
The following procedure pretends in this manner to be checking the primality
ofn. It uses the procedure M ODULAR -EXPONENTIATION from Section 31.6. We
assume that the input nis an odd integer greater than 2.
PSEUDOPRIME .n/
1ifMODULAR -EXPONENTIATION .2; n/NUL1; n/6/DC11.mod n/
2 return COMPOSITE //deﬁnitely
3else return PRIME //we hope!
This procedure can make errors, but only of one type. That is, if it says that n
is composite, then it is always correct. If it says that nis prime, however, then it
makes an error only if nis a base- 2pseudoprime.
How often does this procedure err? Surprisingly rarely. There are only 22 values
ofnless than 10,000 for which it errs; the ﬁrst four such values are 341, 561,
645, and 1105. We won’t prove it, but the probability that this program makes anerror on a randomly chosen ˇ-bit number goes to zero as ˇ!1 .U s i n g m o r e
precise estimates due to Pomerance [279] of the number of base- 2pseudoprimes of
a given size, we may estimate that a randomly chosen 512-bit number that is calledprime by the above procedure has less than one chance in 10
20of being a base- 2
pseudoprime, and a randomly chosen 1024-bit number that is called prime has lessthan one chance in 10
41of being a base- 2pseudoprime. So if you are merely
trying to ﬁnd a large prime for some application, for all practical purposes youalmost never go wrong by choosing large numbers at random until one of themcauses P
SEUDOPRIME to return PRIME . But when the numbers being tested for
primality are not randomly chosen, we need a better approach for testing primality.968 Chapter 31 Number-Theoretic Algorithms
As we shall see, a little more cleverness, and some randomization, will yield a
primality-testing routine that works well on all inputs.
Unfortunately, we cannot entirely eliminate all the errors by simply checking
equation (31.40) for a second base number, say aD3, because there exist com-
posite integers n, known as Carmichael numbers , that satisfy equation (31.40) for
alla2Z/ETX
n. (We note that equation (31.40) does fail when gcd .a; n/ > 1 —that
is, when a62Z/ETX
n—but hoping to demonstrate that nis composite by ﬁnding such
anacan be difﬁcult if nhas only large prime factors.) The ﬁrst three Carmichael
numbers are 561, 1105, and 1729. Carmichael numbers are extremely rare; there
are, for example, only 255 of them less than 100,000,000. Exercise 31.8-2 helpsexplain why they are so rare.
We next show how to improve our primality test so that it won’t be fooled by
Carmichael numbers.
The Miller-Rabin randomized primality test
The Miller-Rabin primality test overcomes the problems of the simple test P
SEU-
DOPRIME with two modiﬁcations:
/SIIt tries several randomly chosen base values ainstead of just one base value.
/SIWhile computing each modular exponentiation, it looks for a nontrivial square
root of 1, modulo n, during the ﬁnal set of squarings. If it ﬁnds one, it stops
and returns COMPOSITE . Corollary 31.35 from Section 31.6 justiﬁes detecting
composites in this manner.
The pseudocode for the Miller-Rabin primality test follows. The input n>2 is
the odd number to be tested for primality, and sis the number of randomly cho-
sen base values from ZC
nto be tried. The code uses the random-number generator
RANDOM described on page 117: R ANDOM .1; n/NUL1/returns a randomly chosen
integer asatisfying 1/DC4a/DC4n/NUL1. The code uses an auxiliary procedure W ITNESS
such that W ITNESS .a; n/ isTRUE if and only if ais a “witness” to the composite-
ness of n—that is, if it is possible using ato prove (in a manner that we shall see)
thatnis composite. The test W ITNESS .a; n/ is an extension of, but more effective
than, the test
an/NUL16/DC11.mod n/
that formed the basis (using aD2)f o rP SEUDOPRIME . We ﬁrst present and
justify the construction of W ITNESS , and then we shall show how we use it in the
Miller-Rabin primality test. Let n/NUL1D2tuwhere t/NAK1anduis odd; i.e.,
the binary representation of n/NUL1is the binary representation of the odd integer u
followed by exactly tzeros. Therefore, an/NUL1/DC1.au/2t.mod n/, so that we can31.8 Primality testing 969
compute an/NUL1modnby ﬁrst computing aumodnand then squaring the result t
times successively.
WITNESS .a; n/
1l e t tandube such that t/NAK1,uis odd, and n/NUL1D2tu
2x0DMODULAR -EXPONENTIATION . a;u ;n /
3foriD1tot
4 xiDx2
i/NUL1modn
5 ifxi==1andxi/NUL1¤1andxi/NUL1¤n/NUL1
6 return TRUE
7ifxt¤1
8 return TRUE
9return FALSE
This pseudocode for W ITNESS computes an/NUL1modnby ﬁrst computing the
value x0Daumodnin line 2 and then squaring the result ttimes in a row in the
forloop of lines 3–6. By induction on i, the sequence x0,x1, ..., xtof values
computed satisﬁes the equation xi/DC1a2iu.mod n/foriD0; 1; : : : ; t ,s ot h a ti n
particular xt/DC1an/NUL1.mod n/. After line 4 performs a squaring step, however,
the loop may terminate early if lines 5–6 detect that a nontrivial square root of 1
has just been discovered. (We shall explain these tests shortly.) If so, the algo-rithm stops and returns
TRUE . Lines 7–8 return TRUE if the value computed for
xt/DC1an/NUL1.mod n/is not equal to 1, just as the P SEUDOPRIME procedure returns
COMPOSITE in this case. Line 9 returns FALSE if we haven’t returned TRUE in
lines 6 or 8.
We now argue that if W ITNESS .a; n/ returns TRUE , then we can construct a
proof that nis composite using aas a witness.
If W ITNESS returns TRUE from line 8, then it has discovered that xtD
an/NUL1modn¤1.I fnis prime, however, we have by Fermat’s theorem (Theo-
rem 31.31) that an/NUL1/DC11.mod n/for all a2ZC
n. Therefore, ncannot be prime,
and the equation an/NUL1modn¤1proves this fact.
If W ITNESS returns TRUE from line 6, then it has discovered that xi/NUL1is a non-
trivial square root of 1, modulo n, since we have that xi/NUL16/DC1˙ 1.mod n/yet
xi/DC1x2
i/NUL1/DC11.mod n/. Corollary 31.35 states that only if nis composite can
there exist a nontrivial square root of 1modulo n, so that demonstrating that xi/NUL1
is a nontrivial square root of 1modulo nproves that nis composite.
This completes our proof of the correctness of W ITNESS . If we ﬁnd that the call
WITNESS .a; n/ returns TRUE ,t h e n nis surely composite, and the witness a, along
with the reason that the procedure returns TRUE (did it return from line 6 or from
line 8?), provides a proof that nis composite.970 Chapter 31 Number-Theoretic Algorithms
At this point, we brieﬂy present an alternative description of the behavior of
WITNESS as a function of the sequence XDhx0;x1;:::;x ti, which we shall ﬁnd
useful later on, when we analyze the efﬁciency of the Miller-Rabin primality test.Note that if x
iD1for some 0/DC4i<t ,W ITNESS might not compute the rest
of the sequence. If it were to do so, however, each value xiC1;xiC2;:::;x twould
be1, and we consider these positions in the sequence Xas being all 1s. We have
four cases:
1.XDh:::;di,w h e r e d¤1: the sequence Xdoes not end in 1.R e t u r n TRUE
in line 8; ais a witness to the compositeness of n(by Fermat’s Theorem).
2.XDh1; 1; : : : ; 1i: the sequence Xis all 1s. Return FALSE ;ais not a witness
to the compositeness of n.
3.XDh:::;/NUL1; 1; : : : ; 1i: the sequence Xends in 1, and the last non- 1is equal
to/NUL1.R e t u r n FALSE ;ais not a witness to the compositeness of n.
4.XDh:::;d;1 ;:::;1i,w h e r e d¤˙1: the sequence Xends in 1, but the last
non-1is not/NUL1.R e t u r n TRUE in line 6; ais a witness to the compositeness
ofn,s i n c e dis a nontrivial square root of 1.
We now examine the Miller-Rabin primality test based on the use of W ITNESS .
Again, we assume that nis an odd integer greater than 2.
MILLER -RABIN .n; s/
1forjD1tos
2 aDRANDOM .1; n/NUL1/
3 ifWITNESS .a; n/
4 return COMPOSITE //deﬁnitely
5return PRIME //almost surely
The procedure M ILLER -RABIN is a probabilistic search for a proof that nis
composite. The main loop (beginning on line 1) picks up to srandom values of a
from ZC
n(line 2). If one of the a’s picked is a witness to the compositeness of n,
then M ILLER -RABIN returns COMPOSITE on line 4. Such a result is always cor-
rect, by the correctness of W ITNESS .I f M ILLER -RABIN ﬁnds no witness in s
trials, then the procedure assumes that this is because no witnesses exist, and there-fore it assumes that nis prime. We shall see that this result is likely to be correct
ifsis large enough, but that there is still a tiny chance that the procedure may be
unlucky in its choice of a’s and that witnesses do exist even though none has been
found.
To illustrate the operation of M
ILLER -RABIN ,l e tnbe the Carmichael num-
ber561,s ot h a t n/NUL1D560D24/SOH35,tD4,a n d uD35. If the pro-
cedure chooses aD7as a base, Figure 31.4 in Section 31.6 shows that W IT-
NESS computes x0/DC1a35/DC1241 . mod 561/ and thus computes the sequence31.8 Primality testing 971
XDh241; 298; 166; 67; 1 i. Thus, W ITNESS discovers a nontrivial square root
of1in the last squaring step, since a280/DC167 . mod n/anda560/DC11.mod n/.
Therefore, aD7is a witness to the compositeness of n,W ITNESS .7; n/ returns
TRUE ,a n dM ILLER -RABIN returns COMPOSITE .
Ifnis aˇ-bit number, M ILLER -RABIN requires O.sˇ/ arithmetic operations
andO.sˇ3/bit operations, since it requires asymptotically no more work than s
modular exponentiations.
Error rate of the Miller-Rabin primality test
If M ILLER -RABIN returns PRIME , then there is a very slim chance that it has made
an error. Unlike P SEUDOPRIME , however, the chance of error does not depend
onn; there are no bad inputs for this procedure. Rather, it depends on the size of s
and the “luck of the draw” in choosing base values a. Moreover, since each test is
more stringent than a simple check of equation (31.40), we can expect on generalprinciples that the error rate should be small for randomly chosen integers n.T h e
following theorem presents a more precise argument.
Theorem 31.38
Ifnis an odd composite number, then the number of witnesses to the composite-
ness of nis at least .n/NUL1/=2 .
Proof The proof shows that the number of nonwitnesses is at most .n/NUL1/=2 ,
which implies the theorem.
We start by claiming that any nonwitness must be a member of Z
/ETX
n. Why?
Consider any nonwitness a. It must satisfy an/NUL1/DC11.mod n/or, equivalently,
a/SOHan/NUL2/DC11.mod n/. Thus, the equation ax/DC11.mod n/has a solution,
namely an/NUL2. By Corollary 31.21, gcd .a; n/j1, which in turn implies that
gcd.a; n/D1. Therefore, ai sam e m b e ro f Z/ETX
n; all nonwitnesses belong to Z/ETX
n.
To complete the proof, we show that not only are all nonwitnesses contained
inZ/ETX
n, they are all contained in a proper subgroup BofZ/ETX
n(recall that we say B
is aproper subgroup of Z/ETX
nwhen Bis subgroup of Z/ETX
nbutBis not equal to Z/ETX
n).
By Corollary 31.16, we then have jBj/DC4jZ/ETX
nj=2.S i n c ejZ/ETX
nj/DC4n/NUL1, we obtain
jBj/DC4.n/NUL1/=2 . Therefore, the number of nonwitnesses is at most .n/NUL1/=2 ,s o
that the number of witnesses must be at least .n/NUL1/=2 .
We now show how to ﬁnd a proper subgroup BofZ/ETX
ncontaining all of the
nonwitnesses. We break the proof into two cases.
Case 1: There exists an x2Z/ETX
nsuch that
xn/NUL16/DC11.mod n/ :972 Chapter 31 Number-Theoretic Algorithms
In other words, nis not a Carmichael number. Because, as we noted earlier,
Carmichael numbers are extremely rare, case 1 is the main case that arises “inpractice” (e.g., when nhas been chosen randomly and is being tested for primal-
ity).
LetBDfb2Z
/ETX
nWbn/NUL1/DC11.mod n/g. Clearly, Bis nonempty, since 12B.
Since Bis closed under multiplication modulo n,w eh a v et h a t Bis a subgroup
ofZ/ETX
nby Theorem 31.14. Note that every nonwitness belongs to B, since a non-
witness asatisﬁes an/NUL1/DC11.mod n/.S i n c e x2Z/ETX
n/NULB,w eh a v et h a t Bis a
proper subgroup of Z/ETX
n.
Case 2: For all x2Z/ETX
n,
xn/NUL1/DC11.mod n/ : (31.41)
In other words, nis a Carmichael number. This case is extremely rare in prac-
tice. However, the Miller-Rabin test (unlike a pseudo-primality test) can efﬁcientlydetermine that Carmichael numbers are composite, as we now show.
In this case, ncannot be a prime power. To see why, let us suppose to the
contrary that nDp
e,w h e r e pis a prime and e>1 . We derive a contradiction
as follows. Since we assume that nis odd, pmust also be odd. Theorem 31.32
implies that Z/ETX
nis a cyclic group: it contains a generator gsuch that ord n.g/D
jZ/ETX
njD/RS.n/Dpe.1/NUL1=p/D.p/NUL1/pe/NUL1. (The formula for /RS.n/ comes from
equation (31.20).) By equation (31.41), we have gn/NUL1/DC11.mod n/. Then the
discrete logarithm theorem (Theorem 31.33, taking yD0) implies that n/NUL1/DC10
.mod /RS.n// ,o r
.p/NUL1/pe/NUL1jpe/NUL1:
This is a contradiction for e>1 ,s i n c e .p/NUL1/pe/NUL1is divisible by the prime p
butpe/NUL1is not. Thus, nis not a prime power.
Since the odd composite number nis not a prime power, we decompose it into
a product n1n2,w h e r e n1andn2are odd numbers greater than 1 that are relatively
prime to each other. (There may be several ways to decompose n, and it does not
matter which one we choose. For example, if nDpe1
1pe2
2/SOH/SOH/SOHper
r, then we can
choose n1Dpe1
1andn2Dpe2
2pe3
3/SOH/SOH/SOHper
r.)
Recall that we deﬁne tanduso that n/NUL1D2tu,w h e r e t/NAK1anduis odd, and
that for an input a, the procedure W ITNESS computes the sequence
XDhau;a2u;a22u;:::;a2tui
(all computations are performed modulo n).
Let us call a pair ./ETB; j / of integers acceptable if/ETB2Z/ETX
n,j2f0; 1; : : : ; tg,a n d
/ETB2ju/DC1/NUL1.mod n/ :31.8 Primality testing 973
Acceptable pairs certainly exist since uis odd; we can choose /ETBDn/NUL1and
jD0,s ot h a t .n/NUL1; 0/ is an acceptable pair. Now pick the largest possible jsuch
that there exists an acceptable pair ./ETB; j / ,a n dﬁ x /ETBso that ./ETB; j / is an acceptable
pair. Let
BDfx2Z/ETX
nWx2ju/DC1˙1.mod n/g:
Since Bis closed under multiplication modulo n, it is a subgroup of Z/ETX
n. By Theo-
rem 31.15, therefore, jBjdividesjZ/ETX
nj. Every nonwitness must be a member of B,
since the sequence Xproduced by a nonwitness must either be all 1s or else contain
a/NUL1no later than the jth position, by the maximality of j. (If.a; j0/is acceptable,
where ais a nonwitness, we must have j0/DC4jby how we chose j.)
We now use the existence of /ETBto demonstrate that there exists a w2Z/ETX
n/NULB,
and hence that Bis a proper subgroup of Z/ETX
n.S i n c e /ETB2ju/DC1/NUL1.mod n/,w eh a v e
/ETB2ju/DC1/NUL1.mod n1/by Corollary 31.29 to the Chinese remainder theorem. By
Corollary 31.28, there exists a wsimultaneously satisfying the equations
w/DC1/ETB.mod n1/;
w/DC11.mod n2/:
Therefore,
w2ju/DC1/NUL 1.mod n1/;
w2ju/DC1 1.mod n2/:
By Corollary 31.29, w2ju6/DC11.mod n1/implies w2ju6/DC11.mod n/,a n d
w2ju6/DC1/NUL1.mod n2/implies w2ju6/DC1/NUL1.mod n/. Hence, we conclude that
w2ju6/DC1˙1.mod n/,a n ds o w62B.
It remains to show that w2Z/ETX
n, which we do by ﬁrst working separately mod-
ulon1and modulo n2. Working modulo n1, we observe that since /ETB2Z/ETX
n,w e
have that gcd ./ETB; n/D1, and so also gcd ./ETB; n 1/D1;i f/ETBdoes not have any com-
mon divisors with n, then it certainly does not have any common divisors with n1.
Since w/DC1/ETB.mod n1/, we see that gcd .w; n 1/D1. Working modulo n2,w e
observe that w/DC11.mod n2/implies gcd .w; n 2/D1. To combine these results,
we use Theorem 31.6, which implies that gcd .w; n 1n2/Dgcd.w; n/D1.T h a ti s ,
w2Z/ETX
n.
Therefore w2Z/ETX
n/NULB, and we ﬁnish case 2 with the conclusion that Bis a
proper subgroup of Z/ETX
n.
In either case, we see that the number of witnesses to the compositeness of nis
at least .n/NUL1/=2 .
Theorem 31.39
For any odd integer n>2 and positive integer s, the probability that M ILLER -
RABIN .n; s/ errs is at most 2/NULs.974 Chapter 31 Number-Theoretic Algorithms
Proof Using Theorem 31.38, we see that if nis composite, then each execution of
theforloop of lines 1–4 has a probability of at least 1=2of discovering a witness x
to the compositeness of n.M ILLER -RABIN makes an error only if it is so unlucky
as to miss discovering a witness to the compositeness of non each of the siterations
of the main loop. The probability of such a sequence of misses is at most 2/NULs.
Ifnis prime, M ILLER -RABIN always reports P RIME ,a n di f nis composite, the
chance that M ILLER -RABIN reports P RIME is at most 2/NULs.
When applying M ILLER -RABIN to a large randomly chosen integer n,h o w e v e r ,
we need to consider as well the prior probability that nis prime, in order to cor-
rectly interpret M ILLER -RABIN ’s result. Suppose that we ﬁx a bit length ˇand
choose at random an integer nof length ˇbits to be tested for primality. Let A
denote the event that nis prime. By the prime number theorem (Theorem 31.37),
the probability that nis prime is approximately
PrfAg/EM1=lnn
/EM1:443=ˇ :
Now let Bdenote the event that M ILLER -RABIN returns P RIME .W e h a v e t h a t
Pr˚
BjA/TAB
D0(or equivalently, that Pr fBjAgD1)a n dP r˚
Bj
A/TAB
/DC42/NULs(or
equivalently, that Pr˚
Bj
A/TAB
>1/NUL2/NULs).
But what is PrfAjBg, the probability that nis prime, given that M ILLER -
RABIN has returned P RIME ? By the alternate form of Bayes’s theorem (equa-
tion (C.18)) we have
PrfAjBgDPrfAgPrfBjAg
PrfAgPrfBjAgCPr˚
A/TAB
Pr˚
Bj
A/TAB
/EM1
1C2/NULs.lnn/NUL1/:
This probability does not exceed 1=2until sexceeds lg .lnn/NUL1/. Intuitively, that
many initial trials are needed just for the conﬁdence derived from failing to ﬁnd awitness to the compositeness of nto overcome the prior bias in favor of nbeing
composite. For a number with ˇD1024 bits, this initial testing requires about
lg.lnn/NUL1//EMlg.ˇ=1:443/
/EM9
trials. In any case, choosing sD50should sufﬁce for almost any imaginable
application.
In fact, the situation is much better. If we are trying to ﬁnd large primes by
applying M
ILLER -RABIN to large randomly chosen odd integers, then choosing
a small value of s(say3) is very unlikely to lead to erroneous results, though31.9 Integer factorization 975
we won’t prove it here. The reason is that for a randomly chosen odd composite
integer n, the expected number of nonwitnesses to the compositeness of nis likely
to be very much smaller than .n/NUL1/=2 .
If the integer nis not chosen randomly, however, the best that can be proven is
that the number of nonwitnesses is at most .n/NUL1/=4, using an improved version
of Theorem 31.38. Furthermore, there do exist integers nfor which the number of
nonwitnesses is .n/NUL1/=4.
Exercises
31.8-1
Prove that if an odd integer n>1 is not a prime or a prime power, then there exists
a nontrivial square root of 1modulo n.
31.8-2 ?
It is possible to strengthen Euler’s theorem slightly to the form
a/NAK.n//DC11.mod n/for all a2Z/ETX
n;
where nDpe1
1/SOH/SOH/SOHper
rand/NAK.n/ is deﬁned by
/NAK.n/Dlcm./RS.pe1
1/ ;:::;/RS. per
r// : (31.42)
Prove that /NAK.n/j/RS.n/ . A composite number nis a Carmichael number if
/NAK.n/jn/NUL1. The smallest Carmichael number is 561D3/SOH11/SOH17; here,
/NAK.n/Dlcm.2; 10; 16/D80, which divides 560. Prove that Carmichael num-
bers must be both “square-free” (not divisible by the square of any prime) and theproduct of at least three primes. (For this reason, they are not very common.)
31.8-3
Prove that if xis a nontrivial square root of 1, modulo n,t h e ng c d .x/NUL1; n/ and
gcd.xC1; n/ are both nontrivial divisors of n.
?31.9 Integer factorization
Suppose we have an integer nthat we wish to factor , that is, to decompose into a
product of primes. The primality test of the preceding section may tell us that nis
composite, but it does not tell us the prime factors of n. Factoring a large integer n
seems to be much more difﬁcult than simply determining whether nis prime or
composite. Even with today’s supercomputers and the best algorithms to date, wecannot feasibly factor an arbitrary 1024 -bit number.976 Chapter 31 Number-Theoretic Algorithms
Pollard’s rho heuristic
Trial division by all integers up to Ris guaranteed to factor completely any number
up to R2. For the same amount of work, the following procedure, P OLLARD -RHO,
factors any number up to R4(unless we are unlucky). Since the procedure is only
a heuristic, neither its running time nor its success is guaranteed, although theprocedure is highly effective in practice. Another advantage of the P
OLLARD -
RHOprocedure is that it uses only a constant number of memory locations. (If you
wanted to, you could easily implement P OLLARD -RHOon a programmable pocket
calculator to ﬁnd factors of small numbers.)
POLLARD -RHO.n/
1iD1
2x1DRANDOM .0; n/NUL1/
3yDx1
4kD2
5while TRUE
6 iDiC1
7 xiD.x2
i/NUL1/NUL1/modn
8 dDgcd.y/NULxi;n /
9 ifd¤1andd¤n
10 print d
11 ifi==k
12 yDxi
13 kD2k
The procedure works as follows. Lines 1–2 initialize ito1andx1to a randomly
chosen value in Zn.T h e while loop beginning on line 5 iterates forever, searching
for factors of n. During each iteration of the while loop, line 7 uses the recurrence
xiD.x2
i/NUL1/NUL1/modn (31.43)
to produce the next value of xiin the inﬁnite sequence
x1;x2;x3;x4;::: ; (31.44)
with line 6 correspondingly incrementing i. The pseudocode is written using sub-
scripted variables xifor clarity, but the program works the same if all of the sub-
scripts are dropped, since only the most recent value of xineeds to be maintained.
With this modiﬁcation, the procedure uses only a constant number of memory lo-
cations.
Every so often, the program saves the most recently generated xivalue in the
variable y. Speciﬁcally, the values that are saved are the ones whose subscripts are
powers of 2:31.9 Integer factorization 977
x1;x2;x4;x8;x16;::: :
Line 3 saves the value x1, and line 12 saves xkwhenever iis equal to k.T h e
variable kis initialized to 2 in line 4, and line 13 doubles it whenever line 12
updates y. Therefore, kfollows the sequence 1;2;4;8;::: and always gives the
subscript of the next value xkto be saved in y.
Lines 8–10 try to ﬁnd a factor of n, using the saved value of yand the cur-
rent value of xi. Speciﬁcally, line 8 computes the greatest common divisor
dDgcd.y/NULxi;n /. If line 9 ﬁnds dto be a nontrivial divisor of n, then line 10
prints d.
This procedure for ﬁnding a factor may seem somewhat mysterious at ﬁrst.
Note, however, that P OLLARD -RHOnever prints an incorrect answer; any num-
ber it prints is a nontrivial divisor of n.POLLARD -RHOmight not print anything
at all, though; it comes with no guarantee that it will print any divisors. We shallsee, however, that we have good reason to expect P
OLLARD -RHOto print a fac-
torpofnafter ‚.p
p/iterations of the while loop. Thus, if nis composite, we
can expect this procedure to discover enough divisors to factor ncompletely after
approximately n1=4updates, since every prime factor pofnexcept possibly the
largest one is less thanp
n.
We begin our analysis of how this procedure behaves by studying how long
it takes a random sequence modulo nto repeat a value. Since Znis ﬁnite, and
since each value in the sequence (31.44) depends only on the previous value, thesequence (31.44) eventually repeats itself. Once we reach an x
isuch that xiDxj
for some j< i , we are in a cycle, since xiC1DxjC1,xiC2DxjC2, and so on.
The reason for the name “rho heuristic” is that, as Figure 31.7 shows, we can draw
the sequence x1;x2;:::;x j/NUL1as the “tail” of the rho and the cycle xj;xjC1;:::;x i
as the “body” of the rho.
Let us consider the question of how long it takes for the sequence of xito repeat.
This information is not exactly what we need, but we shall see later how to modify
the argument. For the purpose of this estimation, let us assume that the function
fn.x/D.x2/NUL1/modn
behaves like a “random” function. Of course, it is not really random, but this as-
sumption yields results consistent with the observed behavior of P OLLARD -RHO.
We can then consider each xito have been independently drawn from Znaccording
to a uniform distribution on Zn. By the birthday-paradox analysis of Section 5.4.1,
we expect ‚.p
n/steps to be taken before the sequence cycles.
Now for the required modiﬁcation. Let pbe a nontrivial factor of nsuch that
gcd.p; n=p/D1. For example, if nhas the factorization nDpe1
1pe2
2/SOH/SOH/SOHper
r,t h e n
we may take pto be pe1
1. (Ife1D1,t h e n pis just the smallest prime factor of n,
a good example to keep in mind.)978 Chapter 31 Number-Theoretic Algorithms
996 310
396
84
120
529
1053 595339814
1194
63
8
3
2
(b) (c) (a)3
218
26
831
11
47177
1186
mod 1387 mod 19 mod 7386
1663
3
2 x1x2x3x4x5x6x7
x0
1x0
2x0
3x0
4
x0
5x0
6x0
7
x00
1x00
2x00
3x00
4x00
5x00
6x00
7
Figure 31.7 Pollard’s rho heuristic. (a)The values produced by the recurrence xiC1D
.x2
i/NUL1/mod1387 , starting with x1D2. The prime factorization of 1387 is19/SOH73. The heavy
arrows indicate the iteration steps that are executed before the factor 19 is discovered. The light
arrows point to unreached values in the iteration, to illustrate the “rho” shape. The shaded values are
theyvalues stored by P OLLARD -RHO. The factor 19 is discovered upon reaching x7D177,w h e n
gcd.63/NUL177; 1387/D19is computed. The ﬁrst xvalue that would be repeated is 1186, but the
factor 19 is discovered before this value is repeated. (b)The values produced by the same recurrence,
modulo 19. Every value xigiven in part (a) is equivalent, modulo 19, to the value x0
ishown here.
For example, both x4D63andx7D177are equivalent to 6, modulo 19. (c)The values produced
by the same recurrence, modulo 73. Every value xigiven in part (a) is equivalent, modulo 73, to the
value x00
ishown here. By the Chinese remainder theorem, each node in part (a) corresponds to a pair
of nodes, one from part (b) and one from part (c).
The sequencehxiiinduces a corresponding sequence hx0
iimodulo p,w h e r e
x0
iDximodp
for all i.
Furthermore, because fnis deﬁned using only arithmetic operations (squaring
and subtraction) modulo n, we can compute x0
iC1from x0
i; the “modulo p”v i e wo f31.9 Integer factorization 979
the sequence is a smaller version of what is happening modulo n:
x0
iC1DxiC1modp
Dfn.xi/modp
D..x2
i/NUL1/modn/modp
D.x2
i/NUL1/modp (by Exercise 31.1-7)
D..ximodp/2/NUL1/modp
D..x0
i/2/NUL1/modp
Dfp.x0
i/:
Thus, although we are not explicitly computing the sequence hx0
ii, this sequence is
well deﬁned and obeys the same recurrence as the sequence hxii.
Reasoning as before, we ﬁnd that the expected number of steps before the se-
quencehx0
iirepeats is ‚.p
p/.I fpis small compared to n, the sequencehx0
iimight
repeat much more quickly than the sequence hxii. Indeed, as parts (b) and (c) of
Figure 31.7 show, the hx0
iisequence repeats as soon as two elements of the se-
quencehxiiare merely equivalent modulo p, rather than equivalent modulo n.
Lettdenote the index of the ﬁrst repeated value in the hx0
iisequence, and let
u>0 denote the length of the cycle that has been thereby produced. That is, t
andu>0 are the smallest values such that x0
tCiDx0
tCuCifor all i/NAK0.B y t h e
above arguments, the expected values of tanduare both ‚.p
p/. Note that if
x0
tCiDx0
tCuCi,t h e n pj.xtCuCi/NULxtCi/. Thus, gcd .xtCuCi/NULxtCi;n />1 .
Therefore, once P OLLARD -RHOhas saved as yany value xksuch that k/NAKt,
thenymodpis always on the cycle modulo p. (If a new value is saved as y,
that value is also on the cycle modulo p.) Eventually, kis set to a value that
is greater than u, and the procedure then makes an entire loop around the cycle
modulo pwithout changing the value of y. The procedure then discovers a factor
ofnwhen xi“runs into” the previously stored value of y, modulo p, that is, when
xi/DC1y.mod p/.
Presumably, the factor found is the factor p, although it may occasionally hap-
pen that a multiple of pis discovered. Since the expected values of both tanduare
‚.p
p/, the expected number of steps required to produce the factor pis‚.p
p/.
This algorithm might not perform quite as expected, for two reasons. First, the
heuristic analysis of the running time is not rigorous, and it is possible that the cycleof values, modulo p, could be much larger thanp
p. In this case, the algorithm
performs correctly but much more slowly than desired. In practice, this issue seemsto be moot. Second, the divisors of nproduced by this algorithm might always be
one of the trivial factors 1orn. For example, suppose that nDpq,w h e r e p
andqare prime. It can happen that the values of tanduforpare identical with
the values of tanduforq, and thus the factor pis always revealed in the same
gcd operation that reveals the factor q. Since both factors are revealed at the same980 Chapter 31 Number-Theoretic Algorithms
time, the trivial factor pqDnis revealed, which is useless. Again, this problem
seems to be insigniﬁcant in practice. If necessary, we can restart the heuristic witha different recurrence of the form x
iC1D.x2
i/NULc/modn. (We should avoid the
values cD0andcD2for reasons we will not go into here, but other values are
ﬁne.)
Of course, this analysis is heuristic and not rigorous, since the recurrence is
not really “random.” Nonetheless, the procedure performs well in practice, andit seems to be as efﬁcient as this heuristic analysis indicates. It is the method of
choice for ﬁnding small prime factors of a large number. To factor a ˇ-bit compos-
ite number ncompletely, we only need to ﬁnd all prime factors less than bn
1=2c,
and so we expect P OLLARD -RHOto require at most n1=4D2ˇ=4arithmetic opera-
tions and at most n1=4ˇ2D2ˇ=4ˇ2bit operations. P OLLARD -RHO’s ability to ﬁnd
a small factor pofnwith an expected number ‚.p
p/of arithmetic operations is
often its most appealing feature.
Exercises
31.9-1
Referring to the execution history shown in Figure 31.7(a), when does P OLLARD -
RHOprint the factor 73 of 1387?
31.9-2
Suppose that we are given a function fWZn! Znand an initial value x02Zn.
Deﬁne xiDf. x i/NUL1/foriD1 ;2;::: .L e t tandu>0 be the smallest values such
thatxtCiDxtCuCiforiD0; 1; : : : . In the terminology of Pollard’s rho algorithm,
tis the length of the tail and uis the length of the cycle of the rho. Give an efﬁcient
algorithm to determine tanduexactly, and analyze its running time.
31.9-3
How many steps would you expect P OLLARD -RHOto require to discover a factor
of the form pe,w h e r e pis prime and e>1 ?
31.9-4 ?
One disadvantage of P OLLARD -RHOas written is that it requires one gcd compu-
tation for each step of the recurrence. Instead, we could batch the gcd computa-
tions by accumulating the product of several xivalues in a row and then using this
product instead of xiin the gcd computation. Describe carefully how you would
implement this idea, why it works, and what batch size you would pick as the most
effective when working on a ˇ-bit number n.Problems for Chapter 31 981
Problems
31-1 Binary gcd algorithm
Most computers can perform the operations of subtraction, testing the parity (oddor even) of a binary integer, and halving more quickly than computing remainders.This problem investigates the binary gcd algorithm , which avoids the remainder
computations used in Euclid’s algorithm.
a.Prove that if aandbare both even, then gcd .a; b/D2/SOHgcd.a=2; b=2/ .
b.Prove that if ais odd and bis even, then gcd .a; b/Dgcd.a; b=2/ .
c.Prove that if aandbare both odd, then gcd .a; b/Dgcd..a/NULb/=2;b/ .
d.Design an efﬁcient binary gcd algorithm for input integers aandb,w h e r e
a/NAKb, that runs in O.lga/time. Assume that each subtraction, parity test,
and halving takes unit time.
31-2 Analysis of bit operations in Euclid’s algorithm
a.Consider the ordinary “paper and pencil” algorithm for long division: dividing
abyb, which yields a quotient qand remainder r. Show that this method
requires O..1Clgq/lgb/bit operations.
b.Deﬁne /SYN.a; b/D.1Clga/.1Clgb/. Show that the number of bit operations
performed by E
UCLID in reducing the problem of computing gcd .a; b/ to that
of computing gcd .b; a modb/is at most c./SYN.a;b//NUL/SYN.b; a modb//for some
sufﬁciently large constant c>0 .
c.Show that E UCLID .a; b/ requires O./SYN.a;b// bit operations in general and
O.ˇ2/bit operations when applied to two ˇ-bit inputs.
31-3 Three algorithms for Fibonacci numbers
This problem compares the efﬁciency of three methods for computing the nth Fi-
bonacci number Fn,g i v e n n. Assume that the cost of adding, subtracting, or mul-
tiplying two numbers is O.1/ , independent of the size of the numbers.
a.Show that the running time of the straightforward recursive method for com-
puting Fnbased on recurrence (3.22) is exponential in n. (See, for example, the
FIBprocedure on page 775.)
b.Show how to compute FninO.n/ time using memoization.982 Chapter 31 Number-Theoretic Algorithms
c.Show how to compute FninO.lgn/time using only integer addition and mul-
tiplication. ( Hint: Consider the matrix
/DC201
11/DC3
and its powers.)
d.Assume now that adding two ˇ-bit numbers takes ‚.ˇ/ time and that multi-
plying two ˇ-bit numbers takes ‚.ˇ2/time. What is the running time of these
three methods under this more reasonable cost measure for the elementary arith-
metic operations?
31-4 Quadratic residues
Letpbe an odd prime. A number a2Z/ETX
pis aquadratic residue if the equation
x2Da.mod p/has a solution for the unknown x.
a.Show that there are exactly .p/NUL1/=2 quadratic residues, modulo p.
b.Ifpis prime, we deﬁne the Legendre symbol .a
p/,f o ra2Z/ETX
p,t ob e 1ifais a
quadratic residue modulo pand/NUL1otherwise. Prove that if a2Z/ETX
p,t h e n
/DLEa
p/DC1
/DC1a.p/NUL1/=2.mod p/ :
Give an efﬁcient algorithm that determines whether a given number ais a qua-
dratic residue modulo p. Analyze the efﬁciency of your algorithm.
c.Prove that if pis a prime of the form 4kC3andais a quadratic residue in Z/ETX
p,
thenakC1modpis a square root of a, modulo p. How much time is required
to ﬁnd the square root of a quadratic residue amodulo p?
d.Describe an efﬁcient randomized algorithm for ﬁnding a nonquadratic residue,
modulo an arbitrary prime p, that is, a member of Z/ETX
pthat is not a quadratic
residue. How many arithmetic operations does your algorithm require on aver-age?
Chapter notes
Niven and Zuckerman [265] provide an excellent introduction to elementary num-ber theory. Knuth [210] contains a good discussion of algorithms for ﬁnding theNotes for Chapter 31 983
greatest common divisor, as well as other basic number-theoretic algorithms. Bach
[30] and Riesel [295] provide more recent surveys of computational number the-ory. Dixon [91] gives an overview of factorization and primality testing. Theconference proceedings edited by Pomerance [280] contains several excellent sur-vey articles. More recently, Bach and Shallit [31] have provided an exceptionaloverview of the basics of computational number theory.
Knuth [210] discusses the origin of Euclid’s algorithm. It appears in Book 7,
Propositions 1 and 2, of the Greek mathematician Euclid’s Elements ,w h i c hw a s
written around 300
B.C. Euclid’s description may have been derived from an al-
gorithm due to Eudoxus around 375 B.C. Euclid’s algorithm may hold the honor
of being the oldest nontrivial algorithm; it is rivaled only by an algorithm for mul-tiplication known to the ancient Egyptians. Shallit [312] chronicles the history ofthe analysis of Euclid’s algorithm.
Knuth attributes a special case of the Chinese remainder theorem (Theo-
rem 31.27) to the Chinese mathematician Sun-Ts˘ u, who lived sometime between
200
B.C.a n d A.D. 200—the date is quite uncertain. The same special case was
given by the Greek mathematician Nichomachus around A.D. 100. It was gener-
alized by Chhin Chiu-Shao in 1247. The Chinese remainder theorem was ﬁnallystated and proved in its full generality by L. Euler in 1734.
The randomized primality-testing algorithm presented here is due to Miller [255]
and Rabin [289]; it is the fastest randomized primality-testing algorithm known,to within constant factors. The proof of Theorem 31.39 is a slight adaptation of
one suggested by Bach [29]. A proof of a stronger result for M
ILLER -RABIN
was given by Monier [258, 259]. For many years primality-testing was the classic
example of a problem where randomization appeared to be necessary to obtainan efﬁcient (polynomial-time) algorithm. In 2002, however, Agrawal, Kayal, andSaxema [4] surprised everyone with their deterministic polynomial-time primality-testing algorithm. Until then, the fastest deterministic primality testing algorithmknown, due to Cohen and Lenstra [73], ran in time .lgn/
O.lg lg lg n/on input n,w h i c h
is just slightly superpolynomial. Nonetheless, for practical purposes randomizedprimality-testing algorithms remain more efﬁcient and are preferred.
The problem of ﬁnding large “random” primes is nicely discussed in an article
by Beauchemin, Brassard, Cr´ epeau, Goutier, and Pomerance [36].
The concept of a public-key cryptosystem is due to Difﬁe and Hellman [87].
The RSA cryptosystem was proposed in 1977 by Rivest, Shamir, and Adleman[296]. Since then, the ﬁeld of cryptography has blossomed. Our understanding
of the RSA cryptosystem has deepened, and modern implementations use signif-
icant reﬁnements of the basic techniques presented here. In addition, many newtechniques have been developed for proving cryptosystems to be secure. For ex-ample, Goldwasser and Micali [142] show that randomization can be an effectivetool in the design of secure public-key encryption schemes. For signature schemes,984 Chapter 31 Number-Theoretic Algorithms
Goldwasser, Micali, and Rivest [143] present a digital-signature scheme for which
every conceivable type of forgery is provably as difﬁcult as factoring. Menezes,van Oorschot, and Vanstone [254] provide an overview of applied cryptography.
The rho heuristic for integer factorization was invented by Pollard [277]. The
version presented here is a variant proposed by Brent [56].
The best algorithms for factoring large numbers have a running time that grows
roughly exponentially with the cube root of the length of the number nto be fac-
tored. The general number-ﬁeld sieve factoring algorithm (as developed by Buh-
ler, Lenstra, and Pomerance [57] as an extension of the ideas in the number-ﬁeld
sieve factoring algorithm by Pollard [278] and Lenstra et al. [232] and reﬁned byCoppersmith [77] and others) is perhaps the most efﬁcient such algorithm in gen-eral for large inputs. Although it is difﬁcult to give a rigorous analysis of thisalgorithm, under reasonable assumptions we can derive a running-time estimate ofL.1=3; n/
1:902 Co.1/,w h e r e L.˛; n/De.lnn/˛.ln lnn/1/NUL˛.
The elliptic-curve method due to Lenstra [233] may be more effective for some
inputs than the number-ﬁeld sieve method, since, like Pollard’s rho method, it canﬁnd a small prime factor pquite quickly. With this method, the time to ﬁnd pis
estimated to be L.1=2; p/ p
2Co.1/.32 String Matching
Text-editing programs frequently need to ﬁnd all occurrences of a pattern in the
text. Typically, the text is a document being edited, and the pattern searched for is aparticular word supplied by the user. Efﬁcient algorithms for this problem—called“string matching”—can greatly aid the responsiveness of the text-editing program.Among their many other applications, string-matching algorithms search for par-ticular patterns in DNA sequences. Internet search engines also use them to ﬁndWeb pages relevant to queries.
We formalize the string-matching problem as follows. We assume that the
text is an array TŒ 1::n /c141 of length nand that the pattern is an array PŒ 1::m /c141
of length m/DC4n. We further assume that the elements of PandTare char-
acters drawn from a ﬁnite alphabet †. For example, we may have †Df0,1g
or†Dfa;b;:::;zg. The character arrays PandTare often called strings of
characters.
Referring to Figure 32.1, we say that pattern Poccurs with shift sin text T
(or, equivalently, that pattern Poccurs beginning at position sC1in text T)i f
0/DC4s/DC4n/NULmandTŒ sC1::sCm/c141DPŒ 1::m /c141 (that is, if TŒ sCj/c141DPŒ j/c141 ,f o r
1/DC4j/DC4m). IfPoccurs with shift sinT, then we call savalid shift ; otherwise,
we call saninvalid shift
.T h estring-matching problem is the problem of ﬁnding
all valid shifts with which a given pattern Poccurs in a given text T.
abcabaabcabac
abaa pattern Ptext T
s = 3
Figure 32.1 An example of the string-matching problem, where we want to ﬁnd all occurrences of
the pattern PDabaa in the text TDabcabaabcabac . The pattern occurs only once in the text,
at shift sD3, which we call a valid shift. A vertical line connects each character of the pattern to its
matching character in the text, and all matched characters are shaded.986 Chapter 32 String Matching
Algorithm Preprocessing time Matching time
Naive 0 O..n/NULmC1/m/
Rabin-Karp ‚.m/ O..n /NULmC1/m/
Finite automaton O.mj†j/‚ . n /
Knuth-Morris-Pratt ‚.m/ ‚.n/
Figure 32.2 The string-matching algorithms in this chapter and their preprocessing and matching
times.
Except for the naive brute-force algorithm, which we review in Section 32.1,
each string-matching algorithm in this chapter performs some preprocessing basedon the pattern and then ﬁnds all valid shifts; we call this latter phase “matching.”Figure 32.2 shows the preprocessing and matching times for each of the algorithmsin this chapter. The total running time of each algorithm is the sum of the prepro-
cessing and matching times. Section 32.2 presents an interesting string-matching
algorithm, due to Rabin and Karp. Although the ‚..n/NULmC1/m/ worst-case
running time of this algorithm is no better than that of the naive method, it worksmuch better on average and in practice. It also generalizes nicely to other pattern-matching problems. Section 32.3 then describes a string-matching algorithm thatbegins by constructing a ﬁnite automaton speciﬁcally designed to search for occur-rences of the given pattern Pin a text. This algorithm takes O.mj†j/preprocess-
ing time, but only ‚.n/ matching time. Section 32.4 presents the similar, but much
cleverer, Knuth-Morris-Pratt (or KMP) algorithm; it has the same ‚.n/ matching
time, and it reduces the preprocessing time to only ‚.m/ .
Notation and terminology
We denote by †
/ETX(read “sigma-star”) the set of all ﬁnite-length strings formed
using characters from the alphabet †. In this chapter, we consider only ﬁnite-
length strings. The zero-length empty string , denoted ", also belongs to †/ETX.T h e
length of a string xis denotedjxj.T h e concatenation of two strings xandy,
denoted xy, has lengthjxjCjyjand consists of the characters from xfollowed by
the characters from y.
We say that a string wis apreﬁx of a string x, denoted w<x,i fxDwyfor
some string y2†/ETX. Note that if w<x,t h e njwj/DC4jxj. Similarly, we say that a
string wis asufﬁx of a string x, denoted w=x,i fxDywfor some y2†/ETX.A s
with a preﬁx, w=ximpliesjwj/DC4jxj. For example, we have ab<abcca and
cca=abcca . The empty string "is both a sufﬁx and a preﬁx of every string. For
any strings xandyand any character a,w eh a v e x=yif and only if xa=ya.Chapter 32 String Matching 987
x
z
xy
y
(a)x
z
xy
y
(b)x
z
xy
y
(c)
Figure 32.3 A graphical proof of Lemma 32.1. We suppose that x=´andy=´. The three parts
of the ﬁgure illustrate the three cases of the lemma. Vertical lines connect matching regions (shownshaded) of the strings. (a)Ifjxj/DC4jyj,t h e n x=y.(b)Ifjxj/NAKjyj,t h e n y=x.(c)IfjxjDjyj,
thenxDy.
Also note that <and=are transitive relations. The following lemma will be useful
later.
Lemma 32.1 (Overlapping-sufﬁx lemma)
Suppose that x,y,a n d ´are strings such that x=´andy=´.I fjxj/DC4jyj,
thenx=y.I fjxj/NAKjyj,t h e n y=x.I fjxjDjyj,t h e n xDy.
Proof See Figure 32.3 for a graphical proof.
For brevity of notation, we denote the k-character preﬁx PŒ 1::k/c141 of the pattern
PŒ 1::m /c141 byPk. Thus, P0D"andPmDPDPŒ 1::m /c141 . Similarly, we denote
thek-character preﬁx of the text TbyTk. Using this notation, we can state the
string-matching problem as that of ﬁnding all shifts sin the range 0/DC4s/DC4n/NULm
such that P=TsCm.
In our pseudocode, we allow two equal-length strings to be compared for equal-
ity as a primitive operation. If the strings are compared from left to right and thecomparison stops when a mismatch is discovered, we assume that the time takenby such a test is a linear function of the number of matching characters discovered.To be precise, the test “ x
==y” is assumed to take time ‚.tC1/,w h e r e tis the
length of the longest string ´such that ´<xand´<y. (We write ‚.tC1/
rather than ‚.t/ to handle the case in which tD0; the ﬁrst characters compared
do not match, but it takes a positive amount of time to perform this comparison.)988 Chapter 32 String Matching
32.1 The naive string-matching algorithm
The naive algorithm ﬁnds all valid shifts using a loop that checks the condition
PŒ 1::m /c141DTŒ sC1::sCm/c141for each of the n/NULmC1possible values of s.
NAIVE -STRING -MATCHER .T; P /
1nDT:length
2mDP:length
3forsD0ton/NULm
4 ifPŒ 1::m /c141 ==TŒ sC1::sCm/c141
5 print “Pattern occurs with shift” s
Figure 32.4 portrays the naive string-matching procedure as sliding a “template”
containing the pattern over the text, noting for which shifts all of the characterson the template equal the corresponding characters in the text. The forloop of
lines 3–5 considers each possible shift explicitly. The test in line 4 determineswhether the current shift is valid; this test implicitly loops to check correspondingcharacter positions until all positions match successfully or a mismatch is found.
Line 5 prints out each valid shift s.
Procedure N
AIVE -STRING -MATCHER takes time O..n/NULmC1/m/ , and this
bound is tight in the worst case. For example, consider the text string an(a string
ofna’s) and the pattern am. For each of the n/NULmC1possible values of the shift s,
the implicit loop on line 4 to compare corresponding characters must execute m
times to validate the shift. The worst-case running time is thus ‚..n/NULmC1/m/ ,
which is ‚.n2/ifmDbn=2c. Because it requires no preprocessing, N AIVE -
STRING -MATCHER ’s running time equals its matching time.
acaabc
aabs = 0
(a)acaabc
aabs = 1
(b)acaabc
aabs = 2
(c)acaabc
aabs = 3
(d)
Figure 32.4 The operation of the naive string matcher for the pattern PDaab and the text
TDacaabc . We can imagine the pattern Pas a template that we slide next to the text. (a)–(d) The
four successive alignments tried by the naive string matcher. In each part, vertical lines connect cor-
responding regions found to match (shown shaded), and a jagged line connects the ﬁrst mismatched
character found, if any. The algorithm ﬁnds one occurrence of the pattern, at shift sD2,s h o w ni n
part (c).32.1 The naive string-matching algorithm 989
As we shall see, N AIVE -STRING -MATCHER is not an optimal procedure for this
problem. Indeed, in this chapter we shall see that the Knuth-Morris-Pratt algorithmis much better in the worst case. The naive string-matcher is inefﬁcient becauseit entirely ignores information gained about the text for one value of swhen it
considers other values of s. Such information can be quite valuable, however. For
example, if PDaaab and we ﬁnd that sD0is valid, then none of the shifts 1,2,
or3are valid, since TŒ 4 /c141Db. In the following sections, we examine several ways
to make effective use of this sort of information.
Exercises
32.1-1
Show the comparisons the naive string matcher makes for the pattern PD0001
in the text TD000010001010001 .
32.1-2
Suppose that all characters in the pattern Pare different. Show how to accelerate
N
AIVE -STRING -MATCHER to run in time O.n/ on an n-character text T.
32.1-3
Suppose that pattern Pand text Tarerandomly chosen strings of length mandn,
respectively, from the d-ary alphabet †dDf0; 1; : : : ; d/NUL1g,w h e r e d/NAK2.S h o w
that the expected number of character-to-character comparisons made by the im-
plicit loop in line 4 of the naive algorithm is
.n/NULmC1/1/NULd/NULm
1/NULd/NUL1/DC42.n/NULmC1/
over all executions of this loop. (Assume that the naive algorithm stops comparing
characters for a given shift once it ﬁnds a mismatch or matches the entire pattern.)Thus, for randomly chosen strings, the naive algorithm is quite efﬁcient.
32.1-4
Suppose we allow the pattern Pto contain occurrences of a gap character}that
can match an arbitrary string of characters (even one of zero length). For example,
the pattern ab}ba}coccurs in the text cabccbacbacab as
ca b’
abcc’
}ba’
bacba“
}c’
cab
and as
ca b’
abccbac—
}ba’
ba’
}c’
cab:990 Chapter 32 String Matching
Note that the gap character may occur an arbitrary number of times in the pattern
but not at all in the text. Give a polynomial-time algorithm to determine whethersuch a pattern Poccurs in a given text T, and analyze the running time of your
algorithm.
32.2 The Rabin-Karp algorithm
Rabin and Karp proposed a string-matching algorithm that performs well in prac-tice and that also generalizes to other algorithms for related problems, such astwo-dimensional pattern matching. The Rabin-Karp algorithm uses ‚.m/ prepro-
cessing time, and its worst-case running time is ‚..n/NULmC1/m/ . Based on certain
assumptions, however, its average-case running time is better.
This algorithm makes use of elementary number-theoretic notions such as the
equivalence of two numbers modulo a third number. You might want to refer toSection 31.1 for the relevant deﬁnitions.
For expository purposes, let us assume that †Df0;1;2;:::;9g, so that each
character is a decimal digit. (In the general case, we can assume that each charac-
ter is a digit in radix- dnotation, where dDj†j.) We can then view a string of k
consecutive characters as representing a length- kdecimal number. The character
string 31415 thus corresponds to the decimal number 31,415. Because we inter-
pret the input characters as both graphical symbols and digits, we ﬁnd it convenient
in this section to denote them as we would digits, in our standard text font.
Given a pattern PŒ 1::m /c141 ,l e tpdenote its corresponding decimal value. In a sim-
ilar manner, given a text TŒ 1::n /c141 ,l e tt
sdenote the decimal value of the length- m
substring TŒ sC1::sCm/c141,f o rsD0; 1; : : : ; n/NULm. Certainly, tsDpif and only
ifTŒ sC1::sCm/c141DPŒ 1::m /c141 ; thus, sis a valid shift if and only if tsDp.I fw e
could compute pin time ‚.m/ and all the tsvalues in a total of ‚.n/NULmC1/time,1
then we could determine all valid shifts sin time ‚.m/C‚.n/NULmC1/D‚.n/
by comparing pwith each of the tsvalues. (For the moment, let’s not worry about
the possibility that pand the tsvalues might be very large numbers.)
We can compute pin time ‚.m/ using Horner’s rule (see Section 30.1):
pDPŒ m /c141C10 .P Œm/NUL1/c141C10.P Œm/NUL2/c141C/SOH/SOH/SOHC 10.P Œ2/c141C10P Œ1/c141//SOH/SOH/SOH// :
Similarly, we can compute t0from TŒ 1::m /c141 in time ‚.m/ .
1We write ‚.n/NULmC1/instead of ‚.n/NULm/because stakes on n/NULmC1different values. The
“C1” is signiﬁcant in an asymptotic sense because when mDn, computing the lone tsvalue takes
‚.1/ time, not ‚.0/ time.32.2 The Rabin-Karp algorithm 991
To compute the remaining values t1;t2;:::;t n/NULmin time ‚.n/NULm/, we observe
that we can compute tsC1from tsin constant time, since
tsC1D10.t s/NUL10m/NUL1TŒ sC1/c141/CTŒ sCmC1/c141 : (32.1)
Subtracting 10m/NUL1TŒ sC1/c141removes the high-order digit from ts, multiplying the
result by 10shifts the number left by one digit position, and adding TŒ sCmC1/c141
brings in the appropriate low-order digit. For example, if mD5andtsD31415 ,
then we wish to remove the high-order digit TŒ sC1/c141D3a n db r i n gi nt h en e w
low-order digit (suppose it is TŒ sC5C1/c141D2) to obtain
tsC1D10.31415/NUL10000/SOH3/C2
D14152 :
If we precompute the constant 10m/NUL1(which we can do in time O.lgm/using the
techniques of Section 31.6, although for this application a straightforward O.m/ -
time method sufﬁces), then each execution of equation (32.1) takes a constant num-ber of arithmetic operations. Thus, we can compute pin time ‚.m/ , and we can
compute all of t
0;t1;:::;t n/NULmin time ‚.n/NULmC1/. Therefore, we can ﬁnd all
occurrences of the pattern PŒ 1::m /c141 in the text TŒ 1::n /c141 with‚.m/ preprocessing
time and ‚.n/NULmC1/matching time.
Until now, we have intentionally overlooked one problem: pandtsmay be
too large to work with conveniently. If Pcontains mcharacters, then we cannot
reasonably assume that each arithmetic operation on p(which is mdigits long)
takes “constant time.” Fortunately, we can solve this problem easily, as Figure 32.5
shows: compute pand the tsvalues modulo a suitable modulus q. We can compute
pmodulo qin‚.m/ time and all the tsvalues modulo qin‚.n/NULmC1/time.
If we choose the modulus qas a prime such that 10qjust ﬁts within one computer
word, then we can perform all the necessary computations with single-precisionarithmetic. In general, with a d-ary alphabetf0; 1; : : : ; d/NUL1g, we choose qso
thatdqﬁts within a computer word and adjust the recurrence equation (32.1) to
work modulo q,s ot h a ti tb e c o m e s
t
sC1D.d.t s/NULTŒ sC1/c141h/CTŒ sCmC1/c141/modq; (32.2)
where h/DC1dm/NUL1.mod q/is the value of the digit “ 1” in the high-order position
of an m-digit text window.
The solution of working modulo qis not perfect, however: ts/DC1p.mod q/
does not imply that tsDp. On the other hand, if ts6/DC1p.mod q/,t h e nw e
deﬁnitely have that ts¤p, so that shift sis invalid. We can thus use the test
ts/DC1p.mod q/as a fast heuristic test to rule out invalid shifts s. Any shift sfor
which ts/DC1p.mod q/must be tested further to see whether sis really valid or
we just have a spurious hit . This additional test explicitly checks the condition992 Chapter 32 String Matching
2359023141526739921
7
(a)mod 13
2359023141526739921
7
(b)mod 13123456789 1 0 1 1 1 2 1 3 1 4 1 5 1 6 1 7 1 8 1 9
8931101 8 5 11 911 7 10 4
valid
matchspurious
hit…… …
314152
78old
high-order
digitnew
low-order
digit
≡(31415 – 3·10000)·10 + 2  (mod 13) old
high-order
digitnew
low-order
digit shift
≡(7 – 3·3)·10 + 2  (mod 13) 
≡8  (mod 13) 
(c)14152
Figure 32.5 The Rabin-Karp algorithm. Each character is a decimal digit, and we compute values
modulo 13.(a)A text string. A window of length 5 is shown shaded. The numerical value of the
shaded number, computed modulo 13, yields the value 7.(b)The same text string with values com-
puted modulo 13for each possible position of a length-5 window. Assuming the pattern PD31415 ,
we look for windows whose value modulo 13is7,s i n c e 31415/DC17.mod 13/. The algorithm ﬁnds
two such windows, shown shaded in the ﬁgure. The ﬁrst, beginning at text position 7, is indeed an
occurrence of the pattern, while the second, beginning at text position 13, is a spurious hit. (c)How
to compute the value for a window in constant time, given the value for the previous window. The
ﬁrst window has value 31415. Dropping the high-order digit 3, shifting left (multiplying by 10), and
then adding in the low-order digit 2gives us the new value 14152 . Because all computations are
performed modulo 13, the value for the ﬁrst window is 7, and the value for the new window is 8.32.2 The Rabin-Karp algorithm 993
PŒ 1::m /c141DTŒ sC1::sCm/c141.I fqis large enough, then we hope that spurious
hits occur infrequently enough that the cost of the extra checking is low.
The following procedure makes these ideas precise. The inputs to the procedure
are the text T, the pattern P, the radix dto use (which is typically taken to be j†j),
and the prime qto use.
RABIN -KARP-MATCHER . T;P;d;q/
1nDT:length
2mDP:length
3hDdm/NUL1modq
4pD0
5t0D0
6foriD1tom //preprocessing
7 pD.dpCPŒ i/c141 / modq
8 t0D.dt0CTŒ i/c141 / modq
9forsD0ton/NULm //matching
10 ifp==ts
11 ifPŒ 1::m /c141 ==TŒ sC1::sCm/c141
12 print “Pattern occurs with shift” s
13 ifs<n/NULm
14 tsC1D.d.t s/NULTŒ sC1/c141h/CTŒ sCmC1/c141/modq
The procedure R ABIN -KARP-MATCHER works as follows. All characters are
interpreted as radix- ddigits. The subscripts on tare provided only for clarity; the
program works correctly if all the subscripts are dropped. Line 3 initializes hto the
value of the high-order digit position of an m-digit window. Lines 4–8 compute p
as the value of PŒ 1::m /c141 modqandt0as the value of TŒ 1::m /c141 modq.T h e for
loop of lines 9–14 iterates through all possible shifts s, maintaining the following
invariant:
Whenever line 10 is executed, tsDTŒ sC1::sCm/c141modq.
IfpDtsin line 10 (a “hit”), then line 11 checks to see whether PŒ 1::m /c141D
TŒ sC1::sCm/c141in order to rule out the possibility of a spurious hit. Line 12 prints
out any valid shifts that are found. If s<n/NULm(checked in line 13), then the for
loop will execute at least one more time, and so line 14 ﬁrst executes to ensure that
the loop invariant holds when we get back to line 10. Line 14 computes the value
oftsC1modqfrom the value of tsmodqin constant time using equation (32.2)
directly.
RABIN -KARP-MATCHER takes ‚.m/ preprocessing time, and its matching time
is‚..n/NULmC1/m/ in the worst case, since (like the naive string-matching algo-
rithm) the Rabin-Karp algorithm explicitly veriﬁes every valid shift. If PDam994 Chapter 32 String Matching
andTDan, then verifying takes time ‚..n/NULmC1/m/ , since each of the n/NULmC1
possible shifts is valid.
In many applications, we expect few valid shifts—perhaps some constant cof
them. In such applications, the expected matching time of the algorithm is onlyO..n/NULmC1/Ccm/DO.nCm/, plus the time required to process spurious
hits. We can base a heuristic analysis on the assumption that reducing values mod-uloqacts like a random mapping from †
/ETXtoZq. (See the discussion on the use of
division for hashing in Section 11.3.1. It is difﬁcult to formalize and prove such an
assumption, although one viable approach is to assume that qis chosen randomly
from integers of the appropriate size. We shall not pursue this formalization here.)We can then expect that the number of spurious hits is O.n=q/ , since we can es-
timate the chance that an arbitrary t
swill be equivalent to p, modulo q,a s1=q.
Since there are O.n/ positions at which the test of line 10 fails and we spend O.m/
time for each hit, the expected matching time taken by the Rabin-Karp algorithmis
O.n/CO.m./ETBCn=q// ;
where /ETBis the number of valid shifts. This running time is O.n/ if/ETBDO.1/ and
we choose q/NAKm. That is, if the expected number of valid shifts is small ( O.1/ )
and we choose the prime qto be larger than the length of the pattern, then we
can expect the Rabin-Karp procedure to use only O.nCm/matching time. Since
m/DC4n, this expected matching time is O.n/ .
Exercises
32.2-1
Working modulo qD11, how many spurious hits does the Rabin-Karp matcher en-
counter in the text TD3141592653589793 when looking for the pattern PD26?
32.2-2
How would you extend the Rabin-Karp method to the problem of searching a textstring for an occurrence of any one of a given set of kpatterns? Start by assuming
that all kpatterns have the same length. Then generalize your solution to allow the
patterns to have different lengths.
32.2-3
Show how to extend the Rabin-Karp method to handle the problem of looking forag i v e n m/STXmpattern in an n/STXnarray of characters. (The pattern may be shifted
vertically and horizontally, but it may not be rotated.)32.3 String matching with ﬁnite automata 995
32.2-4
Alice has a copy of a long n-bit ﬁle ADhan/NUL1;an/NUL2;:::;a 0i, and Bob similarly
has an n-bit ﬁle BDhbn/NUL1;bn/NUL2;:::;b 0i. Alice and Bob wish to know if their
ﬁles are identical. To avoid transmitting all of AorB, they use the following fast
probabilistic check. Together, they select a prime q > 1000n and randomly select
an integer xfromf0; 1; : : : ; q/NUL1g. Then, Alice evaluates
A.x/D n/NUL1X
iD0aixi!
modq
and Bob similarly evaluates B.x/ . Prove that if A¤B, there is at most one
chance in 1000 thatA.x/DB.x/ , whereas if the two ﬁles are the same, A.x/ is
necessarily the same as B.x/ .(Hint: See Exercise 31.4-4.)
32.3 String matching with ﬁnite automata
Many string-matching algorithms build a ﬁnite automaton—a simple machine for
processing information—that scans the text string Tfor all occurrences of the pat-
ternP. This section presents a method for building such an automaton. These
string-matching automata are very efﬁcient: they examine each text character ex-
actly once , taking constant time per text character. The matching time used—after
preprocessing the pattern to build the automaton—is therefore ‚.n/ . The time to
build the automaton, however, can be large if †is large. Section 32.4 describes a
clever way around this problem.
We begin this section with the deﬁnition of a ﬁnite automaton. We then examine
a special string-matching automaton and show how to use it to ﬁnd occurrences
of a pattern in a text. Finally, we shall show how to construct the string-matching
automaton for a given input pattern.
Finite automata
Aﬁnite automaton M, illustrated in Figure 32.6, is a 5-tuple .Q; q 0;A ;†;ı/ ,
where
/SIQis a ﬁnite set of states ,
/SIq02Qis thestart state ,
/SIA/DC2Qis a distinguished set of accepting states ,
/SI†is a ﬁnite input alphabet ,
/SIıis a function from Q/STX†intoQ, called the transition function ofM.996 Chapter 32 String Matching
10
00abinput
state
01
(a)a
a
bb
(b)0 1
Figure 32.6 A simple two-state ﬁnite automaton with state set QDf0; 1g,s t a r ts t a t e q0D0,
and input alphabet †Dfa;bg.(a)A tabular representation of the transition function ı.(b)An
equivalent state-transition diagram. State 1, shown blackend, is the only accepting state. Directed
edges represent transitions. For example, the edge from state 1to state 0labeled bindicates that
ı.1;b/D0. This automaton accepts those strings that end in an odd number of a’s. More precisely,
it accepts a string xif and only if xDy´,w h e r e yD"oryends with a b,a n d ´Dak,w h e r e kis
odd. For example, on input abaaa , including the start state, this automaton enters the sequence of
statesh0; 1; 0; 1; 0; 1i, and so it accepts this input. For input abbaa , it enters the sequence of states
h0; 1; 0; 0; 1; 0i, and so it rejects this input.
The ﬁnite automaton begins in state q0and reads the characters of its input string
one at a time. If the automaton is in state qand reads input character a, it moves
(“makes a transition”) from state qto state ı.q;a/ . Whenever its current state qis
am e m b e ro f A, the machine Mhasaccepted the string read so far. An input that
is not accepted is rejected .
A ﬁnite automaton Minduces a function /RS, called the ﬁnal-state function ,
from †/ETXtoQsuch that /RS.w/ is the state Mends up in after scanning the string w.
Thus, Maccepts a string wif and only if /RS.w/2A. We deﬁne the function /RS
recursively, using the transition function:
/RS."/Dq0;
/RS.wa/Dı./RS.w/;a/ forw2†/ETX;a2†.
String-matching automata
For a given pattern P, we construct a string-matching automaton in a preprocess-
ing step before using it to search the text string. Figure 32.7 illustrates how we
construct the automaton for the pattern PDababaca . From now on, we shall
assume that Pis a given ﬁxed pattern string; for brevity, we shall not indicate the
dependence upon Pin our notation.
In order to specify the string-matching automaton corresponding to a given pat-
ternPŒ 1::m /c141 , we ﬁrst deﬁne an auxiliary function /ESC, called the sufﬁx function
corresponding to P. The function /ESCmaps †/ETXtof0; 1; : : : ; mgsuch that /ESC.x/ is the
length of the longest preﬁx of Pthat is also a sufﬁx of x:
/ESC.x/DmaxfkWPk=xg: (32.3)32.3 String matching with ﬁnite automata 997
0 1 2 3 4 5 6 7ababaca
baaa
a
b
(a)
100
120
300
140
500
146
700
1200
1234567stateinput
abc
ababacaP
(b)123456789 1 0 1 1
abababacaba
012345456
723—
—
(c)i
TŒ i/c141
state/RS.T i/
Figure 32.7 (a) A state-transition diagram for the string-matching automaton that accepts all
strings ending in the string ababaca . State 0is the start state, and state 7(shown blackened) is
the only accepting state. A directed edge from state ito state jlabeled arepresents ı.i; a/Dj.T h e
right-going edges forming the “spine” of the automaton, shown heavy in the ﬁgure, correspond to
successful matches between pattern and input characters. The left-going edges correspond to failingmatches. Some edges corresponding to failing matches are omitted; by convention, if a state ihas
no outgoing edge labeled afor some a2†,t h e n ı.i; a/D0.(b)The corresponding transition
function ı, and the pattern string PDababaca . The entries corresponding to successful matches
between pattern and input characters are shown shaded. (c)The operation of the automaton on the
textTDabababacaba . Under each text character TŒ i/c141 appears the state /RS.T
i/that the automa-
ton is in after processing the preﬁx Ti. The automaton ﬁnds one occurrence of the pattern, ending in
position 9.
The sufﬁx function /ESCis well deﬁned since the empty string P0D"is a suf-
ﬁx of every string. As examples, for the pattern PDab,w eh a v e /ESC."/D0,
/ESC.ccaca /D1,a n d /ESC.ccab /D2. For a pattern Pof length m,w eh a v e
/ESC.x/Dmif and only if P=x. From the deﬁnition of the sufﬁx function,
x=yimplies /ESC.x//DC4/ESC.y/ .
We deﬁne the string-matching automaton that corresponds to a given pattern
PŒ 1::m /c141 as follows:998 Chapter 32 String Matching
/SIThe state set Qisf0; 1; : : : ; mg. The start state q0is state 0, and state mis the
only accepting state.
/SIThe transition function ıis deﬁned by the following equation, for any state q
and character a:
ı.q;a/D/ESC.P qa/ : (32.4)
We deﬁne ı.q;a/D/ESC.P qa/because we want to keep track of the longest pre-
ﬁx of the pattern Pthat has matched the text string Tso far. We consider the
most recently read characters of T. In order for a substring of T—let’s say the
substring ending at TŒ i/c141—to match some preﬁx PjofP,t h i sp r e ﬁ x Pjmust be a
sufﬁx of Ti. Suppose that qD/RS.T i/, so that after reading Ti, the automaton is in
stateq. We design the transition function ıso that this state number, q, tells us the
length of the longest preﬁx of Pthat matches a sufﬁx of Ti. That is, in state q,
Pq=TiandqD/ESC.T i/. (Whenever qDm,a l lmcharacters of Pmatch a sufﬁx
ofTi, and so we have found a match.) Thus, since /RS.T i/and/ESC.T i/both equal q,
we shall see (in Theorem 32.4, below) that the automaton maintains the followinginvariant:
/RS.T
i/D/ESC.T i/: (32.5)
If the automaton is in state qand reads the next character TŒ iC1/c141Da,t h e nw e
want the transition to lead to the state corresponding to the longest preﬁx of Pthat
is a sufﬁx of Tia, and that state is /ESC.T ia/. Because Pqis the longest preﬁx of P
that is a sufﬁx of Ti, the longest preﬁx of Pthat is a sufﬁx of Tiais not only /ESC.T ia/,
but also /ESC.P qa/. (Lemma 32.3, on page 1000, proves that /ESC.T ia/D/ESC.P qa/.)
Thus, when the automaton is in state q, we want the transition function on charac-
terato take the automaton to state /ESC.P qa/.
There are two cases to consider. In the ﬁrst case, aDPŒ qC1/c141, so that the
character acontinues to match the pattern; in this case, because ı.q;a/DqC1,t h e
transition continues to go along the “spine” of the automaton (the heavy edges inFigure 32.7). In the second case, a¤PŒ qC1/c141,s ot h a t adoes not continue to match
the pattern. Here, we must ﬁnd a smaller preﬁx of Pthat is also a sufﬁx of T
i.
Because the preprocessing step matches the pattern against itself when creating thestring-matching automaton, the transition function quickly identiﬁes the longestsuch smaller preﬁx of P.
Let’s look at an example. The string-matching automaton of Figure 32.7 has
ı.5;c/D6, illustrating the ﬁrst case, in which the match continues. To illus-
trate the second case, observe that the automaton of Figure 32.7 has ı.5;b/D4.
We make this transition because if the automaton reads a bin state qD5,t h e n
P
qbDababab , and the longest preﬁx of Pthat is also a sufﬁx of ababab is
P4Dabab .32.3 String matching with ﬁnite automata 999
x
a
PrPr–1
Figure 32.8 An illustration for the proof of Lemma 32.2. The ﬁgure shows that r/DC4/ESC.x/C1,
where rD/ESC.xa/ .
To clarify the operation of a string-matching automaton, we now give a simple,
efﬁcient program for simulating the behavior of such an automaton (representedby its transition function ı) in ﬁnding occurrences of a pattern Pof length min an
input text TŒ 1::n /c141 . As for any string-matching automaton for a pattern of length m,
the state set Qisf0; 1; : : : ; mg, the start state is 0, and the only accepting state is
statem.
F
INITE -AUTOMATON -MATCHER . T;ı;m /
1nDT:length
2qD0
3foriD1ton
4 qDı.q;T Œi/c141/
5 ifq==m
6 print “Pattern occurs with shift” i/NULm
From the simple loop structure of F INITE -AUTOMATON -MATCHER , we can easily
see that its matching time on a text string of length nis‚.n/ . This matching
time, however, does not include the preprocessing time required to compute thetransition function ı. We address this problem later, after ﬁrst proving that the
procedure F
INITE -AUTOMATON -MATCHER operates correctly.
Consider how the automaton operates on an input text TŒ 1::n /c141 . We shall prove
that the automaton is in state /ESC.T i/after scanning character TŒ i/c141.S i n c e /ESC.T i/Dm
if and only if P=Ti, the machine is in the accepting state mif and only if it has
just scanned the pattern P. To prove this result, we make use of the following two
lemmas about the sufﬁx function /ESC.
Lemma 32.2 (Sufﬁx-function inequality)
For any string xand character a,w eh a v e /ESC.xa//DC4/ESC.x/C1.
Proof Referring to Figure 32.8, let rD/ESC.xa/ .I frD0, then the conclusion
/ESC.xa/Dr/DC4/ESC.x/C1is trivially satisﬁed, by the nonnegativity of /ESC.x/ .N o w
assume that r>0 . Then, Pr=xa, by the deﬁnition of /ESC. Thus, Pr/NUL1=x,b y1000 Chapter 32 String Matching
x
a
a Pq
Pr
Figure 32.9 An illustration for the proof of Lemma 32.3. The ﬁgure shows that rD/ESC.P qa/,
where qD/ESC.x/ andrD/ESC.xa/ .
dropping the afrom the end of Prand from the end of xa. Therefore, r/NUL1/DC4/ESC.x/ ,
since /ESC.x/ is the largest ksuch that Pk=x, and thus /ESC.xa/Dr/DC4/ESC.x/C1.
Lemma 32.3 (Sufﬁx-function recursion lemma)
For any string xand character a,i fqD/ESC.x/ ,t h e n /ESC.xa/D/ESC.P qa/.
Proof From the deﬁnition of /ESC,w eh a v e Pq=x. As Figure 32.9 shows, we
also have Pqa=xa.I f w e l e t rD/ESC.xa/ ,t h e n Pr=xaand, by Lemma 32.2,
r/DC4qC1. Thus, we havejPrjDr/DC4qC1DjPqaj.S i n c e Pqa=xa,Pr=xa,
andjPrj/DC4jPqaj, Lemma 32.1 implies that Pr=Pqa. Therefore, r/DC4/ESC.P qa/,
that is, /ESC.xa//DC4/ESC.P qa/. But we also have /ESC.P qa//DC4/ESC.xa/ ,s i n c e Pqa=xa.
Thus, /ESC.xa/D/ESC.P qa/.
We are now ready to prove our main theorem characterizing the behavior of a
string-matching automaton on a given input text. As noted above, this theorem
shows that the automaton is merely keeping track, at each step, of the longest
preﬁx of the pattern that is a sufﬁx of what has been read so far. In other words,the automaton maintains the invariant (32.5).
Theorem 32.4
If/RSis the ﬁnal-state function of a string-matching automaton for a given pattern P
andTŒ 1::n /c141 is an input text for the automaton, then
/RS.T
i/D/ESC.T i/
foriD0; 1; : : : ; n .
Proof The proof is by induction on i.F o r iD0, the theorem is trivially true,
since T0D". Thus, /RS.T 0/D0D/ESC.T 0/.32.3 String matching with ﬁnite automata 1001
Now, we assume that /RS.T i/D/ESC.T i/and prove that /RS.T iC1/D/ESC.T iC1/.L e t q
denote /RS.T i/,a n dl e t adenote TŒ iC1/c141. Then,
/RS.T iC1/D/RS.T ia/ (by the deﬁnitions of TiC1anda)
Dı./RS.T i/; a/ (by the deﬁnition of /RS)
Dı.q;a/ (by the deﬁnition of q)
D/ESC.P qa/ (by the deﬁnition (32.4) of ı)
D/ESC.T ia/ (by Lemma 32.3 and induction)
D/ESC.T iC1/ (by the deﬁnition of TiC1).
By Theorem 32.4, if the machine enters state qon line 4, then qis the largest
value such that Pq=Ti. Thus, we have qDmon line 5 if and only if the ma-
chine has just scanned an occurrence of the pattern P. We conclude that F INITE -
AUTOMATON -MATCHER operates correctly.
Computing the transition function
The following procedure computes the transition function ıfrom a given pattern
PŒ 1::m /c141 .
COMPUTE -TRANSITION -FUNCTION .P; †/
1mDP:length
2forqD0tom
3 foreach character a2†
4 kDmin.mC1; qC2/
5 repeat
6 kDk/NUL1
7 until Pk=Pqa
8 ı.q;a/Dk
9return ı
This procedure computes ı.q;a/ in a straightforward manner according to its def-
inition in equation (32.4). The nested loops beginning on lines 2 and 3 considerall states qand all characters a, and lines 4–8 set ı.q;a/ to be the largest ksuch
thatP
k=Pqa. The code starts with the largest conceivable value of k,w h i c hi s
min.m; qC1/. It then decreases kuntilPk=Pqa, which must eventually occur,
since P0D"is a sufﬁx of every string.
The running time of C OMPUTE -TRANSITION -FUNCTION isO.m3j†j/,b e -
cause the outer loops contribute a factor of mj†j, the inner repeat loop can run
at most mC1times, and the test Pk=Pqaon line 7 can require comparing up1002 Chapter 32 String Matching
tomcharacters. Much faster procedures exist; by utilizing some cleverly com-
puted information about the pattern P(see Exercise 32.4-8), we can improve the
time required to compute ıfrom PtoO.mj†j/. With this improved procedure for
computing ı, we can ﬁnd all occurrences of a length- mpattern in a length- ntext
over an alphabet †withO.mj†j/preprocessing time and ‚.n/ matching time.
Exercises
32.3-1
Construct the string-matching automaton for the pattern PDaabab and illustrate
its operation on the text string TDaaababaabaababaab .
32.3-2
Draw a state-transition diagram for a string-matching automaton for the patternababbabbababbababbabb over the alphabet †Dfa;bg.
32.3-3
We call a pattern Pnonoverlappable ifP
k=Pqimplies kD0orkDq.D e -
scribe the state-transition diagram of the string-matching automaton for a nonover-lappable pattern.
32.3-4 ?
Given two patterns PandP
0, describe how to construct a ﬁnite automaton that
determines all occurrences of either pattern. Try to minimize the number of states
in your automaton.
32.3-5
Given a pattern Pcontaining gap characters (see Exercise 32.1-4), show how to
build a ﬁnite automaton that can ﬁnd an occurrence of Pin a text TinO.n/
matching time, where nDjTj.
?32.4 The Knuth-Morris-Pratt algorithm
We now present a linear-time string-matching algorithm due to Knuth, Morris, and
Pratt. This algorithm avoids computing the transition function ıaltogether, and its
matching time is ‚.n/ using just an auxiliary function /EM, which we precompute
from the pattern in time ‚.m/ and store in an array /EMŒ 1::m /c141 . The array /EMallows
us to compute the transition function ıefﬁciently (in an amortized sense) “on the
ﬂy” as needed. Loosely speaking, for any state qD0; 1; : : : ; m and any character32.4 The Knuth-Morris-Pratt algorithm 1003
a2†,t h ev a l u e /EMŒq/c141 contains the information we need to compute ı.q;a/ but
that does not depend on a. Since the array /EMhas only mentries, whereas ıhas
‚.mj†j/entries, we save a factor of j†jin the preprocessing time by computing /EM
rather than ı.
The preﬁx function for a pattern
The preﬁx function /EMfor a pattern encapsulates knowledge about how the pat-
tern matches against shifts of itself. We can take advantage of this information toavoid testing useless shifts in the naive pattern-matching algorithm and to avoidprecomputing the full transition function ıfor a string-matching automaton.
Consider the operation of the naive string matcher. Figure 32.10(a) shows a
particular shift sof a template containing the pattern PDababaca against a
textT. For this example, qD5of the characters have matched successfully, but
the6th pattern character fails to match the corresponding text character. The infor-
mation that qcharacters have matched successfully determines the corresponding
text characters. Knowing these qtext characters allows us to determine immedi-
ately that certain shifts are invalid. In the example of the ﬁgure, the shift sC1is
necessarily invalid, since the ﬁrst pattern character ( a) would be aligned with a text
character that we know does not match the ﬁrst pattern character, but does match
the second pattern character ( b). The shift s
0DsC2shown in part (b) of the ﬁg-
ure, however, aligns the ﬁrst three pattern characters with three text characters that
must necessarily match. In general, it is useful to know the answer to the followingquestion:
Given that pattern characters PŒ 1::q/c141 match text characters TŒ sC1::sCq/c141,
what is the least shift s
0>ssuch that for some k<q ,
PŒ 1::k/c141DTŒ s0C1::s0Ck/c141 ; (32.6)
where s0CkDsCq?
In other words, knowing that Pq=TsCq, we want the longest proper preﬁx Pk
ofPqthat is also a sufﬁx of TsCq.( S i n c e s0CkDsCq,i fw ea r eg i v e n s
andq, then ﬁnding the smallest shift s0is tantamount to ﬁnding the longest preﬁx
length k.) We add the difference q/NULkin the lengths of these preﬁxes of Pto the
shiftsto arrive at our new shift s0,s ot h a t s0DsC.q/NULk/. In the best case, kD0,
so that s0DsCq, and we immediately rule out shifts sC1; sC2;:::;sCq/NUL1.
In any case, at the new shift s0we don’t need to compare the ﬁrst kcharacters of P
with the corresponding characters of T, since equation (32.6) guarantees that they
match.
We can precompute the necessary information by comparing the pattern against
itself, as Figure 32.10(c) demonstrates. Since TŒ s0C1::s0Ck/c141is part of the1004 Chapter 32 String Matching
bacbab
aba
(a)abaabcbab
bacasT
P
q
bacbab
aba
(b)abaabcbab
bacas′ = s + 2T
P
k
ab
abaaba
(c)Pq
Pk
Figure 32.10 The preﬁx function /EM.(a)The pattern PDababaca aligns with a text Tso that
the ﬁrst qD5characters match. Matching characters, shown shaded, are connected by vertical lines.
(b)Using only our knowledge of the 5matched characters, we can deduce that a shift of sC1is
invalid, but that a shift of s0DsC2is consistent with everything we know about the text and therefore
is potentially valid. (c)We can precompute useful information for such deductions by comparing the
pattern with itself. Here, we see that the longest preﬁx of Pthat is also a proper sufﬁx of P5isP3.
We represent this precomputed information in the array /EM,s ot h a t /EMŒ5/c141D3. Given that qcharacters
have matched successfully at shift s, the next potentially valid shift is at s0DsC.q/NUL/EMŒq/c141/ as shown
in part (b).
known portion of the text, it is a sufﬁx of the string Pq. Therefore, we can interpret
equation (32.6) as asking for the greatest k<q such that Pk=Pq. Then, the new
shifts0DsC.q/NULk/is the next potentially valid shift. We will ﬁnd it convenient to
store, for each value of q, the number kof matching characters at the new shift s0,
rather than storing, say, s0/NULs.
We formalize the information that we precompute as follows. Given a pattern
PŒ 1::m /c141 ,t h epreﬁx function for the pattern Pis the function /EMWf1 ;2;:::;mg!
f0; 1; : : : ; m/NUL1gsuch that
/EMŒq/c141DmaxfkWk<q andPk=Pqg:
That is, /EMŒq/c141 is the length of the longest preﬁx of Pthat is a proper sufﬁx of Pq.
Figure 32.11(a) gives the complete preﬁx function /EMfor the pattern ababaca .32.4 The Knuth-Morris-Pratt algorithm 1005
1234567
0012301ababaca
(a)ababaca
ababaca
ababaca
ababaca
(b)"i
PŒ i/c141
/EMŒi/c141P5
P3
P1
P0/EMŒ5/c141D3
/EMŒ3/c141D1
/EMŒ1/c141D0
Figure 32.11 An illustration of Lemma 32.5 for the pattern PDababaca andqD5.(a)The/EM
function for the given pattern. Since /EMŒ5/c141D3,/EMŒ3/c141D1,a n d /EMŒ1/c141D0, by iterating /EMwe obtain
/EM/ETXŒ5/c141Df3; 1; 0g.(b)We slide the template containing the pattern Pto the right and note when some
preﬁx PkofPmatches up with some proper sufﬁx of P5; we get matches when kD3,1,a n d 0.I n
the ﬁgure, the ﬁrst row gives P, and the dotted vertical line is drawn just after P5. Successive rows
show all the shifts of Pthat cause some preﬁx PkofPto match some sufﬁx of P5. Successfully
matched characters are shown shaded. Vertical lines connect aligned matching characters. Thus,
fkWk<5 andPk=P5gDf3; 1; 0g. Lemma 32.5 claims that /EM/ETXŒq/c141DfkWk<q andPk=Pqg
for all q.
The pseudocode below gives the Knuth-Morris-Pratt matching algorithm as
the procedure KMP-M ATCHER . For the most part, the procedure follows from
FINITE -AUTOMATON -MATCHER , as we shall see. KMP-M ATCHER calls the aux-
iliary procedure C OMPUTE -PREFIX -FUNCTION to compute /EM.
KMP-M ATCHER .T; P /
1nDT:length
2mDP:length
3/EMDCOMPUTE -PREFIX -FUNCTION .P /
4qD0 //number of characters matched
5foriD1ton //scan the text from left to right
6 while q>0 andPŒ qC1/c141¤TŒ i/c141
7 qD/EMŒq/c141 //next character does not match
8 ifPŒ qC1/c141==TŒ i/c141
9 qDqC1 //next character matches
10 ifq==m //is all of Pmatched?
11 print “Pattern occurs with shift” i/NULm
12 qD/EMŒq/c141 //look for the next match1006 Chapter 32 String Matching
COMPUTE -PREFIX -FUNCTION .P /
1mDP:length
2l e t /EMŒ 1::m /c141 be a new array
3/EMŒ1/c141D0
4kD0
5forqD2tom
6 while k>0 andPŒ kC1/c141¤PŒ q/c141
7 kD/EMŒk/c141
8 ifPŒ kC1/c141==PŒ q/c141
9 kDkC1
10 /EMŒq/c141Dk
11return /EM
These two procedures have much in common, because both match a string against
the pattern P:K M P - M ATCHER matches the text Tagainst P,a n dC OMPUTE -
PREFIX -FUNCTION matches Pagainst itself.
We begin with an analysis of the running times of these procedures. Proving
these procedures correct will be more complicated.
Running-time analysis
The running time of C OMPUTE -PREFIX -FUNCTION is‚.m/ , which we show by
using the aggregate method of amortized analysis (see Section 17.1). The onlytricky part is showing that the while loop of lines 6–7 executes O.m/ times alto-
gether. We shall show that it makes at most m/NUL1iterations. We start by making
some observations about k. First, line 4 starts kat0, and the only way that k
increases is by the increment operation in line 9, which executes at most once periteration of the forloop of lines 5–10. Thus, the total increase in kis at most m/NUL1.
Second, since k<q upon entering the forloop and each iteration of the loop in-
crements q,w ea l w a y sh a v e k<q . Therefore, the assignments in lines 3 and 10
ensure that /EMŒq/c141 < q for all qD1 ;2;:::;m , which means that each iteration of
thewhile loop decreases k. Third, knever becomes negative. Putting these facts
together, we see that the total decrease in kfrom the while loop is bounded from
above by the total increase in kover all iterations of the forloop, which is m/NUL1.
Thus, the while loop iterates at most m/NUL1times in all, and C
OMPUTE -PREFIX -
FUNCTION runs in time ‚.m/ .
Exercise 32.4-4 asks you to show, by a similar aggregate analysis, that the match-
ing time of KMP-M ATCHER is‚.n/ .
Compared with F INITE -AUTOMATON -MATCHER ,b yu s i n g /EMrather than ı,w e
have reduced the time for preprocessing the pattern from O.mj†j/to‚.m/ , while
keeping the actual matching time bounded by ‚.n/ .32.4 The Knuth-Morris-Pratt algorithm 1007
Correctness of the preﬁx-function computation
We shall see a little later that the preﬁx function /EMhelps us simulate the transition
function ıin a string-matching automaton. But ﬁrst, we need to prove that the
procedure C OMPUTE -PREFIX -FUNCTION does indeed compute the preﬁx func-
tion correctly. In order to do so, we will need to ﬁnd all preﬁxes Pkthat are proper
sufﬁxes of a given preﬁx Pq.T h ev a l u eo f /EMŒq/c141 gives us the longest such preﬁx, but
the following lemma, illustrated in Figure 32.11, shows that by iterating the preﬁxfunction /EM, we can indeed enumerate all the preﬁxes P
kthat are proper sufﬁxes
ofPq.L e t
/EM/ETXŒq/c141Df/EMŒq/c141;/EM.2/Œq/c141; /EM.3/Œ q/c141 ;:::;/EM.t/Œq/c141g;
where /EM.i/Œq/c141is deﬁned in terms of functional iteration, so that /EM.0/Œq/c141Dqand
/EM.i/Œq/c141D/EMŒ/EM.i/NUL1/Œq/c141/c141fori/NAK1, and where the sequence in /EM/ETXŒq/c141stops upon
reaching /EM.t/Œq/c141D0.
Lemma 32.5 (Preﬁx-function iteration lemma)
LetPbe a pattern of length mwith preﬁx function /EM. Then, for qD1 ;2;:::;m ,
we have /EM/ETXŒq/c141DfkWk<q andPk=Pqg.
Proof We ﬁrst prove that /EM/ETXŒq/c141/DC2fkWk<q andPk=Pqgor, equivalently,
i2/EM/ETXŒq/c141implies Pi=Pq: (32.7)
Ifi2/EM/ETXŒq/c141,t h e n iD/EM.u/Œq/c141for some u>0 . We prove equation (32.7) by
induction on u.F o r uD1,w eh a v e iD/EMŒq/c141, and the claim follows since i<q
andP/EMŒq/c141=Pqby the deﬁnition of /EM. Using the relations /EMŒi/c141 < i andP/EMŒi/c141=Pi
and the transitivity of <and=establishes the claim for all iin/EM/ETXŒq/c141. Therefore,
/EM/ETXŒq/c141/DC2fkWk<q andPk=Pqg.
We now prove that fkWk<q andPk=Pqg/DC2/EM/ETXŒq/c141by contradiction. Sup-
pose to the contrary that the set fkWk<q andPk=Pqg/NUL/EM/ETXŒq/c141is nonempty,
and let jbe the largest number in the set. Because /EMŒq/c141 is the largest value in
fkWk<q andPk=Pqgand/EMŒq/c1412/EM/ETXŒq/c141,w em u s th a v e j</EM Œ q /c141 ,a n ds ow e
letj0denote the smallest integer in /EM/ETXŒq/c141that is greater than j. (We can choose
j0D/EMŒq/c141 if no other number in /EM/ETXŒq/c141is greater than j.) We have Pj=Pqbecause
j2fkWk<q andPk=Pqg, and from j02/EM/ETXŒq/c141and equation (32.7), we have
Pj0=Pq. Thus, Pj=Pj0by Lemma 32.1, and jis the largest value less than j0
with this property. Therefore, we must have /EMŒj0/c141Djand, since j02/EM/ETXŒq/c141,w e
must have j2/EM/ETXŒq/c141as well. This contradiction proves the lemma.
The algorithm C OMPUTE -PREFIX -FUNCTION computes /EMŒq/c141,i no r d e r ,f o r qD
1 ;2;:::;m . Setting /EMŒ1/c141 to0in line 3 of C OMPUTE -PREFIX -FUNCTION is cer-
tainly correct, since /EMŒq/c141 < q for all q. We shall use the following lemma and1008 Chapter 32 String Matching
its corollary to prove that C OMPUTE -PREFIX -FUNCTION computes /EMŒq/c141 correctly
forq>1 .
Lemma 32.6
LetPbe a pattern of length m,a n dl e t /EMbe the preﬁx function for P.F o r qD
1 ;2;:::;m ,i f/EMŒq/c141 > 0 ,t h e n /EMŒq/c141/NUL12/EM/ETXŒq/NUL1/c141.
Proof LetrD/EMŒq/c141 > 0 ,s ot h a t r<q andPr=Pq; thus, r/NUL1<q/NUL1and
Pr/NUL1=Pq/NUL1(by dropping the last character from PrandPq, which we can do
because r>0 ). By Lemma 32.5, therefore, r/NUL12/EM/ETXŒq/NUL1/c141. Thus, we have
/EMŒq/c141/NUL1Dr/NUL12/EM/ETXŒq/NUL1/c141.
ForqD2;3 ;:::;m , deﬁne the subset Eq/NUL1/DC2/EM/ETXŒq/NUL1/c141by
Eq/NUL1Dfk2/EM/ETXŒq/NUL1/c141WPŒ kC1/c141DPŒ q/c141g
DfkWk<q/NUL1andPk=Pq/NUL1andPŒ kC1/c141DPŒ q/c141g(by Lemma 32.5)
DfkWk<q/NUL1andPkC1=Pqg:
The set Eq/NUL1consists of the values k<q/NUL1for which Pk=Pq/NUL1and for which,
because PŒ kC1/c141DPŒ q/c141 ,w eh a v e PkC1=Pq. Thus, Eq/NUL1consists of those
values k2/EM/ETXŒq/NUL1/c141such that we can extend PktoPkC1and get a proper sufﬁx
ofPq.
Corollary 32.7
LetPbe a pattern of length m,a n dl e t /EMbe the preﬁx function for P.F o r qD
2;3 ;:::;m ,
/EMŒq/c141D(
0 ifEq/NUL1D;;
1Cmaxfk2Eq/NUL1gifEq/NUL1¤;:
Proof IfEq/NUL1is empty, there is no k2/EM/ETXŒq/NUL1/c141(including kD0)f o rw h i c h
we can extend PktoPkC1and get a proper sufﬁx of Pq. Therefore /EMŒq/c141D0.
IfEq/NUL1is nonempty, then for each k2Eq/NUL1we have kC1<q andPkC1=Pq.
Therefore, from the deﬁnition of /EMŒq/c141,w eh a v e
/EMŒq/c141/NAK1Cmaxfk2Eq/NUL1g: (32.8)
Note that /EMŒq/c141 > 0 .L e t rD/EMŒq/c141/NUL1,s ot h a t rC1D/EMŒq/c141 and there-
forePrC1=Pq.S i n c e rC1>0 ,w eh a v e PŒ rC1/c141DPŒ q/c141 . Furthermore,
by Lemma 32.6, we have r2/EM/ETXŒq/NUL1/c141. Therefore, r2Eq/NUL1,a n ds o r/DC4
maxfk2Eq/NUL1gor, equivalently,
/EMŒq/c141/DC41Cmaxfk2Eq/NUL1g: (32.9)
Combining equations (32.8) and (32.9) completes the proof.
32.4 The Knuth-Morris-Pratt algorithm 1009
We now ﬁnish the proof that C OMPUTE -PREFIX -FUNCTION computes /EMcor-
rectly. In the procedure C OMPUTE -PREFIX -FUNCTION , at the start of each iter-
ation of the forloop of lines 5–10, we have that kD/EMŒq/NUL1/c141. This condition
is enforced by lines 3 and 4 when the loop is ﬁrst entered, and it remains true ineach successive iteration because of line 10. Lines 6–9 adjust kso that it becomes
the correct value of /EMŒq/c141.T h e while loop of lines 6–7 searches through all values
k2/EM
/ETXŒq/NUL1/c141until it ﬁnds a value of kfor which PŒ kC1/c141DPŒ q/c141 ; at that point,
kis the largest value in the set Eq/NUL1, so that, by Corollary 32.7, we can set /EMŒq/c141
tokC1.I ft h e while loop cannot ﬁnd a k2/EM/ETXŒq/NUL1/c141such that PŒ kC1/c141DPŒ q/c141 ,
thenkequals 0at line 8. If PŒ 1 /c141DPŒ q/c141 , then we should set both kand/EMŒq/c141 to1;
otherwise we should leave kalone and set /EMŒq/c141 to0. Lines 8–10 set kand/EMŒq/c141
correctly in either case. This completes our proof of the correctness of C OMPUTE -
PREFIX -FUNCTION .
Correctness of the Knuth-Morris-Pratt algorithm
We can think of the procedure KMP-M ATCHER as a reimplemented version of
the procedure F INITE -AUTOMATON -MATCHER , but using the preﬁx function /EM
to compute state transitions. Speciﬁcally, we shall prove that in the ith iteration of
theforloops of both KMP-M ATCHER and F INITE -AUTOMATON -MATCHER ,t h e
state qhas the same value when we test for equality with m(at line 10 in KMP-
MATCHER and at line 5 in F INITE -AUTOMATON -MATCHER ). Once we have
argued that KMP-M ATCHER simulates the behavior of F INITE -AUTOMATON -
MATCHER , the correctness of KMP-M ATCHER follows from the correctness of
FINITE -AUTOMATON -MATCHER (though we shall see a little later why line 12 in
KMP-M ATCHER is necessary).
Before we formally prove that KMP-M ATCHER correctly simulates F INITE -
AUTOMATON -MATCHER , let’s take a moment to understand how the preﬁx func-
tion/EMreplaces the ıtransition function. Recall that when a string-matching
automaton is in state qand it scans a character aDTŒ i/c141, it moves to a new
state ı.q;a/ .I faDPŒ qC1/c141,s ot h a t acontinues to match the pattern, then
ı.q;a/DqC1. Otherwise, a¤PŒ qC1/c141,s ot h a t adoes not continue to match
the pattern, and 0/DC4ı.q;a//DC4q. In the ﬁrst case, when acontinues to match,
KMP-M ATCHER moves to state qC1without referring to the /EMfunction: the
while loop test in line 6 comes up false the ﬁrst time, the test in line 8 comes up
true, and line 9 increments q.
The/EMfunction comes into play when the character adoes not continue to match
the pattern, so that the new state ı.q;a/ is either qor to the left of qalong the spine
of the automaton. The while loop of lines 6–7 in KMP-M ATCHER iterates through
the states in /EM/ETXŒq/c141, stopping either when it arrives in a state, say q0, such that a
matches PŒ q0C1/c141orq0has gone all the way down to 0.I famatches PŒ q0C1/c141,1010 Chapter 32 String Matching
then line 9 sets the new state to q0C1, which should equal ı.q;a/ for the simulation
to work correctly. In other words, the new state ı.q;a/ should be either state 0or
one greater than some state in /EM/ETXŒq/c141.
Let’s look at the example in Figures 32.7 and 32.11, which are for the pattern
PDababaca . Suppose that the automaton is in state qD5; the states in
/EM/ETXŒ5/c141are, in descending order, 3,1,a n d 0. If the next character scanned is c,t h e n
we can easily see that the automaton moves to state ı.5;c/D6in both F INITE -
AUTOMATON -MATCHER and KMP-M ATCHER . Now suppose that the next char-
acter scanned is instead b, so that the automaton should move to state ı.5;b/D4.
Thewhile loop in KMP-M ATCHER exits having executed line 7 once, and it ar-
rives in state q0D/EMŒ5/c141D3.S i n c e PŒ q0C1/c141DPŒ 4 /c141Db, the test in line 8
comes up true, and KMP-M ATCHER moves to the new state q0C1D4Dı.5;b/.
Finally, suppose that the next character scanned is instead a, so that the automa-
ton should move to state ı.5;a/D1. The ﬁrst three times that the test in line 6
executes, the test comes up true. The ﬁrst time, we ﬁnd that PŒ 6 /c141Dc¤a,a n d
KMP-M ATCHER moves to state /EMŒ5/c141D3(the ﬁrst state in /EM/ETXŒ5/c141). The second
time, we ﬁnd that PŒ 4 /c141Db¤aand move to state /EMŒ3/c141D1(the second state
in/EM/ETXŒ5/c141). The third time, we ﬁnd that PŒ 2 /c141Db¤aand move to state /EMŒ1/c141D0
(the last state in /EM/ETXŒ5/c141). The while loop exits once it arrives in state q0D0.N o w ,
line 8 ﬁnds that PŒ q0C1/c141DPŒ 1 /c141Da, and line 9 moves the automaton to the new
stateq0C1D1Dı.5;a/.
Thus, our intuition is that KMP-M ATCHER iterates through the states in /EM/ETXŒq/c141in
decreasing order, stopping at some state q0and then possibly moving to state q0C1.
Although that might seem like a lot of work just to simulate computing ı.q;a/ ,
bear in mind that asymptotically, KMP-M ATCHER is no slower than F INITE -
AUTOMATON -MATCHER .
We are now ready to formally prove the correctness of the Knuth-Morris-Pratt
algorithm. By Theorem 32.4, we have that qD/ESC.T i/after each time we execute
line 4 of F INITE -AUTOMATON -MATCHER . Therefore, it sufﬁces to show that the
same property holds with regard to the forloop in KMP-M ATCHER . The proof
proceeds by induction on the number of loop iterations. Initially, both proceduressetqto0as they enter their respective forloops for the ﬁrst time. Consider itera-
tioniof the forloop in KMP-M
ATCHER ,a n dl e t q0be state at the start of this loop
iteration. By the inductive hypothesis, we have q0D/ESC.T i/NUL1/. We need to show
thatqD/ESC.T i/at line 10. (Again, we shall handle line 12 separately.)
When we consider the character TŒ i/c141, the longest preﬁx of Pthat is a sufﬁx of Ti
is either Pq0C1(ifPŒ q0C1/c141DTŒ i/c141) or some preﬁx (not necessarily proper, and
possibly empty) of Pq0. We consider separately the three cases in which /ESC.T i/D0,
/ESC.T i/Dq0C1,a n d 0</ESC . T i//DC4q0.32.4 The Knuth-Morris-Pratt algorithm 1011
/SIIf/ESC.T i/D0,t h e n P0D"is the only preﬁx of Pthat is a sufﬁx of Ti.T h ewhile
loop of lines 6–7 iterates through the values in /EM/ETXŒq0/c141, but although Pq=Tifor
every q2/EM/ETXŒq0/c141, the loop never ﬁnds a qsuch that PŒ qC1/c141DTŒ i/c141. The loop
terminates when qreaches 0, and of course line 9 does not execute. Therefore,
qD0at line 10, so that qD/ESC.T i/.
/SIIf/ESC.T i/Dq0C1,t h e n PŒ q0C1/c141DTŒ i/c141,a n dt h e while loop test in line 6
fails the ﬁrst time through. Line 9 executes, incrementing qso that afterward
we have qDq0C1D/ESC.T i/.
/SIIf0</ESC . T i//DC4q0, then the while loop of lines 6–7 iterates at least once,
checking in decreasing order each value q2/EM/ETXŒq0/c141until it stops at some q<q0.
Thus, Pqis the longest preﬁx of Pq0for which PŒ qC1/c141DTŒ i/c141, so that when the
while loop terminates, qC1D/ESC.P q0TŒ i/c141 / .S i n c e q0D/ESC.T i/NUL1/, Lemma 32.3
implies that /ESC.T i/NUL1TŒ i/c141 /D/ESC.P q0TŒ i/c141 / . Thus, we have
qC1D/ESC.P q0TŒ i/c141 /
D/ESC.T i/NUL1TŒ i/c141 /
D/ESC.T i/
when the while loop terminates. After line 9 increments q,w eh a v e qD/ESC.T i/.
Line 12 is necessary in KMP-M ATCHER , because otherwise, we might refer-
ence PŒ mC1/c141on line 6 after ﬁnding an occurrence of P. (The argument that
qD/ESC.T i/NUL1/upon the next execution of line 6 remains valid by the hint given in
Exercise 32.4-8: ı.m;a/Dı./EMŒm/c141;a/ or, equivalently, /ESC.Pa/D/ESC.P /EMŒm/c141a/for
anya2†.) The remaining argument for the correctness of the Knuth-Morris-
Pratt algorithm follows from the correctness of F INITE -AUTOMATON -MATCHER ,
s i n c ew eh a v es h o w nt h a tK M P - M ATCHER simulates the behavior of F INITE -
AUTOMATON -MATCHER .
Exercises
32.4-1
Compute the preﬁx function /EMfor the pattern ababbabbabbababbabb .
32.4-2
Give an upper bound on the size of /EM/ETXŒq/c141as a function of q. Give an example to
show that your bound is tight.
32.4-3
Explain how to determine the occurrences of pattern Pin the text Tby examining
the/EMfunction for the string PT(the string of length mCnthat is the concatenation
ofPandT).1012 Chapter 32 String Matching
32.4-4
Use an aggregate analysis to show that the running time of KMP-M ATCHER
is‚.n/ .
32.4-5
Use a potential function to show that the running time of KMP-M ATCHER is‚.n/ .
32.4-6
Show how to improve KMP-M ATCHER by replacing the occurrence of /EMin line 7
(but not line 12) by /EM0,w h e r e /EM0is deﬁned recursively for qD1 ;2;:::;m/NUL1by
the equation
/EM0Œq/c141D/c128
0 if/EMŒq/c141D0;
/EM0Œ/EMŒq/c141/c141 if/EMŒq/c141¤0andPŒ /EMŒ q/c141C1/c141DPŒ qC1/c141 ;
/EMŒq/c141 if/EMŒq/c141¤0andPŒ /EMŒ q/c141C1/c141¤PŒ qC1/c141 :
Explain why the modiﬁed algorithm is correct, and explain in what sense this
change constitutes an improvement.
32.4-7
Give a linear-time algorithm to determine whether a text Tis a cyclic rotation of
another string T0. For example, arc andcar are cyclic rotations of each other.
32.4-8 ?
Give an O.mj†j/-time algorithm for computing the transition function ıfor the
string-matching automaton corresponding to a given pattern P.(Hint: Prove that
ı.q;a/Dı./EMŒq/c141;a/ ifqDmorPŒ qC1/c141¤a.)
Problems
32-1 String matching based on repetition factors
Letyidenote the concatenation of string ywith itself itimes. For example,
.ab/3Dababab . We say that a string x2†/ETXhasrepetition factor rifxDyr
for some string y2†/ETXand some r>0 .L e t /SUB.x/ denote the largest rsuch that x
has repetition factor r.
a.Give an efﬁcient algorithm that takes as input a pattern PŒ 1::m /c141 and computes
the value /SUB.P i/foriD1 ;2;:::;m . What is the running time of your algo-
rithm?Notes for Chapter 32 1013
b.For any pattern PŒ 1::m /c141 ,l e t/SUB/ETX.P /be deﬁned as max 1/DC4i/DC4m/SUB.P i/. Prove that if
the pattern Pis chosen randomly from the set of all binary strings of length m,
then the expected value of /SUB/ETX.P /isO.1/ .
c.Argue that the following string-matching algorithm correctly ﬁnds all occur-
rences of pattern Pin a text TŒ 1::n /c141 in time O./SUB/ETX.P /nCm/:
REPETITION -MATCHER .P; T /
1mDP:length
2nDT:length
3kD1C/SUB/ETX.P /
4qD0
5sD0
6while s/DC4n/NULm
7 ifTŒ sCqC1/c141==PŒ qC1/c141
8 qDqC1
9 ifq==m
10 print “Pattern occurs with shift” s
11 ifq==morTŒ sCqC1/c141¤PŒ qC1/c141
12 sDsCmax.1;dq=ke/
13 qD0
This algorithm is due to Galil and Seiferas. By extending these ideas greatly,
they obtained a linear-time string-matching algorithm that uses only O.1/ stor-
age beyond what is required for PandT.
Chapter notes
The relation of string matching to the theory of ﬁnite automata is discussed by
Aho, Hopcroft, and Ullman [5]. The Knuth-Morris-Pratt algorithm [214] wasinvented independently by Knuth and Pratt and by Morris; they published theirwork jointly. Reingold, Urban, and Gries [294] give an alternative treatment of theKnuth-Morris-Pratt algorithm. The Rabin-Karp algorithm was proposed by Karpand Rabin [201]. Galil and Seiferas [126] give an interesting deterministic linear-time string-matching algorithm that uses only O.1/ space beyond that required to
store the pattern and text.33 Computational Geometry
Computational geometry is the branch of computer science that studies algorithms
for solving geometric problems. In modern engineering and mathematics, com-putational geometry has applications in such diverse ﬁelds as computer graphics,robotics, VLSI design, computer-aided design, molecular modeling, metallurgy,manufacturing, textile layout, forestry, and statistics. The input to a computational-geometry problem is typically a description of a set of geometric objects, such asa set of points, a set of line segments, or the vertices of a polygon in counterclock-wise order. The output is often a response to a query about the objects, such aswhether any of the lines intersect, or perhaps a new geometric object, such as theconvex hull (smallest enclosing convex polygon) of the set of points.
In this chapter, we look at a few computational-geometry algorithms in two
dimensions, that is, in the plane. We represent each input object by a set ofpointsfp
1;p2;p3;:::g, where each piD.xi;yi/andxi;yi2R.F o r e x a m -
ple, we represent an n-vertex polygon Pby a sequencehp0;p1;p2;:::;p n/NUL1i
of its vertices in order of their appearance on the boundary of P. Computational
geometry can also apply to three dimensions, and even higher-dimensional spaces,but such problems and their solutions can be very difﬁcult to visualize. Even intwo dimensions, however, we can see a good sample of computational-geometrytechniques.
Section 33.1 shows how to answer basic questions about line segments efﬁ-
ciently and accurately: whether one segment is clockwise or counterclockwisefrom another that shares an endpoint, which way we turn when traversing twoadjoining line segments, and whether two line segments intersect. Section 33.2presents a technique called “sweeping” that we use to develop an O.n lgn/-time
algorithm for determining whether a set of nline segments contains any inter-
sections. Section 33.3 gives two “rotational-sweep” algorithms that compute theconvex hull (smallest enclosing convex polygon) of a set of npoints: Graham’s
scan, which runs in time O.n lgn/, and Jarvis’s march, which takes O.nh/ time,
where his the number of vertices of the convex hull. Finally, Section 33.4 gives33.1 Line-segment properties 1015
anO.n lgn/-time divide-and-conquer algorithm for ﬁnding the closest pair of
points in a set of npoints in the plane.
33.1 Line-segment properties
Several of the computational-geometry algorithms in this chapter require answers
to questions about the properties of line segments. A convex combination of two
distinct points p1D.x1;y1/andp2D.x2;y2/is any point p3D.x3;y3/such
that for some ˛in the range 0/DC4˛/DC41,w eh a v e x3D˛x1C.1/NUL˛/x 2and
y3D˛y1C.1/NUL˛/y 2. We also write that p3D˛p1C.1/NUL˛/p 2. Intuitively, p3
is any point that is on the line passing through p1andp2and is on or between p1
andp2on the line. Given two distinct points p1andp2,t h eline segment
 p1p2
is the set of convex combinations of p1andp2. We call p1andp2theendpoints
of segment
 p1p2. Sometimes the ordering of p1andp2matters, and we speak of
thedirected segment/NUL/NUL/NUL!p1p2.I fp1is theorigin .0; 0/ , then we can treat the directed
segment/NUL/NUL/NUL!p1p2as the vector p2.
In this section, we shall explore the following questions:
1. Given two directed segments/NUL/NUL/NUL!p0p1and/NUL/NUL/NUL!p0p2,i s/NUL/NUL/NUL!p0p1clockwise from/NUL/NUL/NUL!p0p2
with respect to their common endpoint p0?
2. Given two line segments
 p0p1and
p1p2, if we traverse
 p0p1and then
 p1p2,
do we make a left turn at point p1?
3. Do line segments
 p1p2and
p3p4intersect?
There are no restrictions on the given points.
We can answer each question in O.1/ time, which should come as no surprise
since the input size of each question is O.1/ . Moreover, our methods use only ad-
ditions, subtractions, multiplications, and comparisons. We need neither divisionnor trigonometric functions, both of which can be computationally expensive andprone to problems with round-off error. For example, the “straightforward” methodof determining whether two segments intersect—compute the line equation of theform yDmxCbfor each segment ( mis the slope and bis the y-intercept),
ﬁnd the point of intersection of the lines, and check whether this point is on both
segments—uses division to ﬁnd the point of intersection. When the segments are
nearly parallel, this method is very sensitive to the precision of the division opera-
tion on real computers. The method in this section, which avoids division, is much
more accurate.1016 Chapter 33 Computational Geometry
p2
xy
(0,0)p1p1 +p2
(a) (b)y
x(0,0)p
Figure 33.1 (a) The cross product of vectors p1andp2is the signed area of the parallelogram.
(b)The lightly shaded region contains vectors that are clockwise from p. The darkly shaded region
contains vectors that are counterclockwise from p.
Cross products
Computing cross products lies at the heart of our line-segment methods. Consider
vectors p1andp2, shown in Figure 33.1(a). We can interpret the cross product
p1/STXp2as the signed area of the parallelogram formed by the points .0; 0/ ,p1,p2,
andp1Cp2D.x1Cx2;y1Cy2/. An equivalent, but more useful, deﬁnition gives
the cross product as the determinant of a matrix:1
p1/STXp2Ddet/DC2x1x2
y1y2/DC3
Dx1y2/NULx2y1
D/NUL p2/STXp1:
Ifp1/STXp2is positive, then p1is clockwise from p2with respect to the origin .0; 0/ ;
if this cross product is negative, then p1is counterclockwise from p2.( S e e E x e r -
cise 33.1-1.) Figure 33.1(b) shows the clockwise and counterclockwise regions
relative to a vector p. A boundary condition arises if the cross product is 0;i nt h i s
case, the vectors are colinear , pointing in either the same or opposite directions.
To determine whether a directed segment/NUL/NUL/NUL!p0p1is closer to a directed seg-
ment/NUL/NUL/NUL!p0p2in a clockwise direction or in a counterclockwise direction with respect
to their common endpoint p0, we simply translate to use p0as the origin. That
is, we let p1/NULp0denote the vector p0
1D.x0
1;y0
1/,w h e r e x0
1Dx1/NULx0and
y0
1Dy1/NULy0, and we deﬁne p2/NULp0similarly. We then compute the cross product
1Actually, the cross product is a three-dimensional concept. It is a vector that is perpendicular to
bothp1andp2according to the “right-hand rule” and whose magnitude is jx1y2/NULx2y1j.I n t h i s
chapter, however, we ﬁnd it convenient to treat the cross product simply as the value x1y2/NULx2y1.33.1 Line-segment properties 1017
p0p1p2
p0p1p2
counterclockwise
(a) (b)clockwise
Figure 33.2 Using the cross product to determine how consecutive line segments
 p0p1and
p1p2
turn at point p1. We check whether the directed segment/NUL/NUL/NUL!p0p2is clockwise or counterclockwise
relative to the directed segment/NUL/NUL/NUL!p0p1.(a)If counterclockwise, the points make a left turn. (b)If
clockwise, they make a right turn.
.p1/NULp0//STX.p2/NULp0/D.x1/NULx0/.y2/NULy0//NUL.x2/NULx0/.y1/NULy0/:
If this cross product is positive, then/NUL/NUL/NUL!p0p1is clockwise from/NUL/NUL/NUL!p0p2; if negative, it
is counterclockwise.
Determining whether consecutive segments turn left or right
Our next question is whether two consecutive line segments
 p0p1and
p1p2turn
left or right at point p1. Equivalently, we want a method to determine which way a
given angle†p0p1p2turns. Cross products allow us to answer this question with-
out computing the angle. As Figure 33.2 shows, we simply check whether directedsegment/NUL/NUL/NUL!p
0p2is clockwise or counterclockwise relative to directed segment/NUL/NUL/NUL!p0p1.
To do so, we compute the cross product .p2/NULp0//STX.p1/NULp0/. If the sign of
this cross product is negative, then/NUL/NUL/NUL!p0p2is counterclockwise with respect to/NUL/NUL/NUL!p0p1,
and thus we make a left turn at p1. A positive cross product indicates a clockwise
orientation and a right turn. A cross product of 0means that points p0,p1,a n d p2
are colinear.
Determining whether two line segments intersect
To determine whether two line segments intersect, we check whether each segment
straddles the line containing the other. A segment
 p1p2straddles a line if point p1
lies on one side of the line and point p2lies on the other side. A boundary case
arises if p1orp2lies directly on the line. Two line segments intersect if and only
if either (or both) of the following conditions holds:
1. Each segment straddles the line containing the other.
2. An endpoint of one segment lies on the other segment. (This condition comes
from the boundary case.)1018 Chapter 33 Computational Geometry
The following procedures implement this idea. S EGMENTS -INTERSECT returns
TRUE if segments
 p1p2and
p3p4intersect and FALSE if they do not. It calls
the subroutines D IRECTION , which computes relative orientations using the cross-
product method above, and O N-SEGMENT , which determines whether a point
known to be colinear with a segment lies on that segment.
SEGMENTS -INTERSECT .p1;p2;p3;p4/
1d1DDIRECTION .p3;p4;p1/
2d2DDIRECTION .p3;p4;p2/
3d3DDIRECTION .p1;p2;p3/
4d4DDIRECTION .p1;p2;p4/
5if..d1>0andd2<0 / or.d1<0andd2>0 / / and
..d3>0andd4<0 / or.d3<0andd4>0 / /
6 return TRUE
7elseif d1==0and O N-SEGMENT .p3;p4;p1/
8 return TRUE
9elseif d2==0and O N-SEGMENT .p3;p4;p2/
10 return TRUE
11elseif d3==0and O N-SEGMENT .p1;p2;p3/
12 return TRUE
13elseif d4==0and O N-SEGMENT .p1;p2;p4/
14 return TRUE
15else return FALSE
DIRECTION .pi;pj;pk/
1return .pk/NULpi//STX.pj/NULpi/
ON-SEGMENT .pi;pj;pk/
1ifmin.xi;xj//DC4xk/DC4max.xi;xj/and min .yi;yj//DC4yk/DC4max.yi;yj/
2 return TRUE
3else return FALSE
SEGMENTS -INTERSECT works as follows. Lines 1–4 compute the relative ori-
entation diof each endpoint piwith respect to the other segment. If all the relative
orientations are nonzero, then we can easily determine whether segments
 p1p2
and
p3p4intersect, as follows. Segment
 p1p2straddles the line containing seg-
ment
 p3p4if directed segments/NUL/NUL/NUL!p3p1and/NUL/NUL/NUL!p3p2have opposite orientations relative
to/NUL/NUL/NUL!p3p4. In this case, the signs of d1andd2differ. Similarly,
 p3p4straddles
the line containing
 p1p2if the signs of d3andd4differ. If the test of line 5 is
true, then the segments straddle each other, and S EGMENTS -INTERSECT returns
TRUE . Figure 33.3(a) shows this case. Otherwise, the segments do not straddle33.1 Line-segment properties 1019
p1
p2
p3p4 (p1–p3)× (p4–p3) < 0
(p4–p1)× (p2–p1) < 0
(p2–p3)× (p4–p3) > 0(p3–p1)× (p2–p1) > 0
(a)p1p2
p3p4 (p1–p3)× (p4–p3) < 0
(p4–p1)× (p2–p1) < 0
(p2–p3)× (p4–p3) < 0
(p3–p1)× (p2–p1) > 0
(b)
p1
p2p3p4
(c)p1
p2p3p4
(d)
Figure 33.3 Cases in the procedure S EGMENTS -INTERSECT .(a)The segments
 p1p2and
p3p4
straddle each other’s lines. Because
 p3p4straddles the line containing
 p1p2, the signs of the cross
products .p3/NULp1//STX.p2/NULp1/and.p4/NULp1//STX.p2/NULp1/differ. Because
 p1p2straddles the line
containing
 p3p4, the signs of the cross products .p1/NULp3//STX.p4/NULp3/and.p2/NULp3//STX.p4/NULp3/
differ. (b)Segment
 p3p4straddles the line containing
 p1p2,b u t
 p1p2does not straddle the line
containing
 p3p4. The signs of the cross products .p1/NULp3//STX.p4/NULp3/and.p2/NULp3//STX.p4/NULp3/
are the same. (c)Point p3is colinear with
 p1p2and is between p1andp2.(d)Point p3is colinear
with
p1p2, but it is not between p1andp2. The segments do not intersect.
each other’s lines, although a boundary case may apply. If all the relative orienta-
tions are nonzero, no boundary case applies. All the tests against 0in lines 7–13
then fail, and S EGMENTS -INTERSECT returns FALSE in line 15. Figure 33.3(b)
shows this case.
A boundary case occurs if any relative orientation dkis0. Here, we know that pk
is colinear with the other segment. It is directly on the other segment if and only
if it is between the endpoints of the other segment. The procedure O N-SEGMENT
returns whether pkis between the endpoints of segment
 pipj, which will be the
other segment when called in lines 7–13; the procedure assumes that pkis colinear
with segment
 pipj. Figures 33.3(c) and (d) show cases with colinear points. In
Figure 33.3(c), p3is on
 p1p2,a n ds oS EGMENTS -INTERSECT returns TRUE in
line 12. No endpoints are on other segments in Figure 33.3(d), and so S EGMENTS -
INTERSECT returns FALSE in line 15.1020 Chapter 33 Computational Geometry
Other applications of cross products
Later sections of this chapter introduce additional uses for cross products. In Sec-
tion 33.3, we shall need to sort a set of points according to their polar angles withrespect to a given origin. As Exercise 33.1-3 asks you to show, we can use crossproducts to perform the comparisons in the sorting procedure. In Section 33.2, weshall use red-black trees to maintain the vertical ordering of a set of line segments.Rather than keeping explicit key values which we compare to each other in thered-black tree code, we shall compute a cross-product to determine which of twosegments that intersect a given vertical line is above the other.
Exercises
33.1-1
Prove that if p
1/STXp2is positive, then vector p1is clockwise from vector p2with
respect to the origin .0; 0/ and that if this cross product is negative, then p1is
counterclockwise from p2.
33.1-2
Professor van Pelt proposes that only the x-dimension needs to be tested in line 1
of O N-SEGMENT . Show why the professor is wrong.
33.1-3
Thepolar angle of a point p1with respect to an origin point p0is the angle of the
vector p1/NULp0in the usual polar coordinate system. For example, the polar angle
of.3; 5/ with respect to .2; 4/ is the angle of the vector .1; 1/ ,w h i c hi s 45degrees
or/EM=4 radians. The polar angle of .3; 3/ with respect to .2; 4/ is the angle of the
vector .1;/NUL1/,w h i c hi s 315degrees or 7/EM=4 radians. Write pseudocode to sort a
sequencehp1;p2;:::;p niofnpoints according to their polar angles with respect
to a given origin point p0. Your procedure should take O.n lgn/time and use cross
products to compare angles.
33.1-4
Show how to determine in O.n2lgn/time whether any three points in a set of n
points are colinear.
33.1-5
Apolygon is a piecewise-linear, closed curve in the plane. That is, it is a curve
ending on itself that is formed by a sequence of straight-line segments, called thesides of the polygon. A point joining two consecutive sides is a vertex of the poly-
gon. If the polygon is simple , as we shall generally assume, it does not cross itself.
The set of points in the plane enclosed by a simple polygon forms the interior of33.2 Determining whether any pair of segments intersects 1021
the polygon, the set of points on the polygon itself forms its boundary , and the set
of points surrounding the polygon forms its exterior . A simple polygon is convex
if, given any two points on its boundary or in its interior, all points on the linesegment drawn between them are contained in the polygon’s boundary or interior.A vertex of a convex polygon cannot be expressed as a convex combination of anytwo distinct points on the boundary or in the interior of the polygon.
Professor Amundsen proposes the following method to determine whether a se-
quencehp
0;p1;:::;p n/NUL1iofnpoints forms the consecutive vertices of a convex
polygon. Output “yes” if the set f†pipiC1piC2WiD0; 1; : : : ; n/NUL1g, where sub-
script addition is performed modulo n, does not contain both left turns and right
turns; otherwise, output “no.” Show that although this method runs in linear time,it does not always produce the correct answer. Modify the professor’s method sothat it always produces the correct answer in linear time.
33.1-6
Given a point p
0D.x0;y0/,t h eright horizontal ray from p0is the set of points
fpiD.xi;yi/Wxi/NAKx0andyiDy0g, that is, it is the set of points due right of p0
along with p0itself. Show how to determine whether a given right horizontal ray
from p0intersects a line segment
 p1p2inO.1/ time by reducing the problem to
that of determining whether two line segments intersect.
33.1-7
One way to determine whether a point p0is in the interior of a simple, but not
necessarily convex, polygon Pis to look at any ray from p0and check that the ray
intersects the boundary of Pan odd number of times but that p0itself is not on
the boundary of P. Show how to compute in ‚.n/ time whether a point p0is in
the interior of an n-vertex polygon P.(Hint: Use Exercise 33.1-6. Make sure your
algorithm is correct when the ray intersects the polygon boundary at a vertex andwhen the ray overlaps a side of the polygon.)
33.1-8
Show how to compute the area of an n-vertex simple, but not necessarily convex,
polygon in ‚.n/ time. (See Exercise 33.1-5 for deﬁnitions pertaining to polygons.)
33.2 Determining whether any pair of segments intersects
This section presents an algorithm for determining whether any two line segments
in a set of segments intersect. The algorithm uses a technique known as “sweep-ing,” which is common to many computational-geometry algorithms. Moreover, as1022 Chapter 33 Computational Geometry
the exercises at the end of this section show, this algorithm, or simple variations of
it, can help solve other computational-geometry problems.
The algorithm runs in O.n lgn/time, where nis the number of segments we are
given. It determines only whether or not any intersection exists; it does not printall the intersections. (By Exercise 33.2-1, it takes /DEL.n
2/time in the worst case to
ﬁndallthe intersections in a set of nline segments.)
Insweeping , an imaginary vertical sweep line passes through the given set of
geometric objects, usually from left to right. We treat the spatial dimension that
the sweep line moves across, in this case the x-dimension, as a dimension of
time. Sweeping provides a method for ordering geometric objects, usually by plac-ing them into a dynamic data structure, and for taking advantage of relationshipsamong them. The line-segment-intersection algorithm in this section considers allthe line-segment endpoints in left-to-right order and checks for an intersection eachtime it encounters an endpoint.
To describe and prove correct our algorithm for determining whether any two
ofnline segments intersect, we shall make two simplifying assumptions. First, we
assume that no input segment is vertical. Second, we assume that no three input
segments intersect at a single point. Exercises 33.2-8 and 33.2-9 ask you to show
that the algorithm is robust enough that it needs only a slight modiﬁcation to workeven when these assumptions do not hold. Indeed, removing such simplifyingassumptions and dealing with boundary conditions often present the most difﬁcultchallenges when programming computational-geometry algorithms and proving
their correctness.
Ordering segments
Because we assume that there are no vertical segments, we know that any input
segment intersecting a given vertical sweep line intersects it at a single point. Thus,
we can order the segments that intersect a vertical sweep line according to the y-
coordinates of the points of intersection.
To be more precise, consider two segments s
1ands2. We say that these segments
arecomparable atxif the vertical sweep line with x-coordinate xintersects both of
them. We say that s1isabove s2atx, written s1<xs2,i fs1ands2are comparable
atxand the intersection of s1with the sweep line at xis higher than the intersection
ofs2with the same sweep line, or if s1ands2intersect at the sweep line. In
Figure 33.4(a), for example, we have the relationships a<rc,a<tb,b<tc,
a<tc,a n d b<uc.S e g m e n t dis not comparable with any other segment.
For any given x, the relation “ <x” is a total preorder (see Section B.2) for all
segments that intersect the sweep line at x. That is, the relation is transitive, and
if segments s1ands2each intersect the sweep line at x, then either s1<xs2
ors2<xs1, or both (if s1ands2intersect at the sweep line). (The relation <xis33.2 Determining whether any pair of segments intersects 1023
rt ua
cbd
(a) (b)vwe
fg
hi
z
Figure 33.4 The ordering among line segments at various vertical sweep lines. (a)We have a<rc,
a<tb,b<tc,a<tc,a n d b<uc.S e g m e n t dis comparable with no other segment shown.
(b)When segments eandfintersect, they reverse their orders: we have e</ETBfbutf <we.A n y
sweep line (such as ´) that passes through the shaded region has eandfconsecutive in the ordering
given by the relation <´.
also reﬂexive, but neither symmetric nor antisymmetric.) The total preorder may
differ for differing values of x, however, as segments enter and leave the ordering.
A segment enters the ordering when its left endpoint is encountered by the sweep,and it leaves the ordering when its right endpoint is encountered.
What happens when the sweep line passes through the intersection of two seg-
ments? As Figure 33.4(b) shows, the segments reverse their positions in the totalpreorder. Sweep lines /ETBandware to the left and right, respectively, of the point
of intersection of segments eandf, and we have e<
/ETBfandf <we.N o t e
that because we assume that no three segments intersect at the same point, theremust be some vertical sweep line xfor which intersecting segments eandfare
consecutive in the total preorder <
x. Any sweep line that passes through the shaded
region of Figure 33.4(b), such as ´,h a seandfconsecutive in its total preorder.
Moving the sweep line
Sweeping algorithms typically manage two sets of data:
1. The sweep-line status gives the relationships among the objects that the sweep
line intersects.
2. The event-point schedule is a sequence of points, called event points ,w h i c h
we order from left to right according to their x-coordinates. As the sweep
progresses from left to right, whenever the sweep line reaches the x-coordinate
of an event point, the sweep halts, processes the event point, and then resumes.
Changes to the sweep-line status occur only at event points.
For some algorithms (the algorithm asked for in Exercise 33.2-7, for example),
the event-point schedule develops dynamically as the algorithm progresses. The al-gorithm at hand, however, determines all the event points before the sweep, based1024 Chapter 33 Computational Geometry
solely on simple properties of the input data. In particular, each segment endpoint
is an event point. We sort the segment endpoints by increasing x-coordinate and
proceed from left to right. (If two or more endpoints are covertical , i.e., they have
the same x-coordinate, we break the tie by putting all the covertical left endpoints
before the covertical right endpoints. Within a set of covertical left endpoints, weput those with lower y-coordinates ﬁrst, and we do the same within a set of cover-
tical right endpoints.) When we encounter a segment’s left endpoint, we insert thesegment into the sweep-line status, and we delete the segment from the sweep-line
status upon encountering its right endpoint. Whenever two segments ﬁrst become
consecutive in the total preorder, we check whether they intersect.
The sweep-line status is a total preorder T, for which we require the following
operations:
/SIINSERT .T; s/ : insert segment sintoT.
/SIDELETE .T; s/ : delete segment sfrom T.
/SIABOVE .T; s/ : return the segment immediately above segment sinT.
/SIBELOW .T; s/ : return the segment immediately below segment sinT.
It is possible for segments s1ands2to be mutually above each other in the total
preorder T; this situation can occur if s1ands2intersect at the sweep line whose
total preorder is given by T. In this case, the two segments may appear in either
order in T.
If the input contains nsegments, we can perform each of the operations I NSERT ,
DELETE ,ABOVE ,a n dB ELOW inO.lgn/time using red-black trees. Recall that
the red-black-tree operations in Chapter 13 involve comparing keys. We can re-place the key comparisons by comparisons that use cross products to determine therelative ordering of two segments (see Exercise 33.2-2).
Segment-intersection pseudocode
The following algorithm takes as input a set Sofnline segments, returning the
boolean value
TRUE if any pair of segments in Sintersects, and FALSE otherwise.
A red-black tree maintains the total preorder T.33.2 Determining whether any pair of segments intersects 1025
ANY-SEGMENTS -INTERSECT .S/
1TD;
2 sort the endpoints of the segments in Sfrom left to right,
breaking ties by putting left endpoints before right endpointsand breaking further ties by putting points with lowery-coordinates ﬁrst
3foreach point pin the sorted list of endpoints
4 ifpis the left endpoint of a segment s
5I
NSERT .T; s/
6 if(ABOVE .T; s/ exists and intersects s)
or (B ELOW .T; s/ exists and intersects s)
7 return TRUE
8 ifpis the right endpoint of a segment s
9 ifboth A BOVE .T; s/ and B ELOW .T; s/ exist
and A BOVE .T; s/ intersects B ELOW .T; s/
10 return TRUE
11 D ELETE .T; s/
12return FALSE
Figure 33.5 illustrates how the algorithm works. Line 1 initializes the total preorder
to be empty. Line 2 determines the event-point schedule by sorting the 2nsegment
endpoints from left to right, breaking ties as described above. One way to perform
line 2 is by lexicographically sorting the endpoints on . x;e;y/ ,w h e r e xandyare
the usual coordinates, eD0for a left endpoint, and eD1for a right endpoint.
Each iteration of the forloop of lines 3–11 processes one event point p.I fpis
the left endpoint of a segment s, line 5 adds sto the total preorder, and lines 6–7
return TRUE ifsintersects either of the segments it is consecutive with in the total
preorder deﬁned by the sweep line passing through p. (A boundary condition
occurs if plies on another segment s0. In this case, we require only that sands0
be placed consecutively into T.) If pis the right endpoint of a segment s,t h e n
we need to delete sfrom the total preorder. But ﬁrst, lines 9–10 return TRUE if
there is an intersection between the segments surrounding sin the total preorder
deﬁned by the sweep line passing through p. If these segments do not intersect,
line 11 deletes segment sfrom the total preorder. If the segments surrounding
segment sintersect, they would have become consecutive after deleting shad the
return statement in line 10 not prevented line 11 from executing. The correctness
argument, which follows, will make it clear why it sufﬁces to check the segmentssurrounding s. Finally, if we never ﬁnd any intersections after having processed
all2nevent points, line 12 returns
FALSE .1026 Chapter 33 Computational Geometry
aa
ba
c
bd
a
c
bd
c
be
d
c
ba
bcde
f
time
Figure 33.5 The execution of A NY-SEGMENTS -INTERSECT . Each dashed line is the sweep line at
an event point. Except for the rightmost sweep line, the ordering of segment names below each sweep
line corresponds to the total preorder Tat the end of the forloop processing the corresponding event
point. The rightmost sweep line occurs when processing the right endpoint of segment c; because
segments dandbsurround cand intersect each other, the procedure returns TRUE .
Correctness
To show that A NY-SEGMENTS -INTERSECT is correct, we will prove that the call
ANY-SEGMENTS -INTERSECT .S/returns TRUE if and only if there is an intersec-
tion among the segments in S.
It is easy to see that A NY-SEGMENTS -INTERSECT returns TRUE (on lines 7
and 10) only if it ﬁnds an intersection between two of the input segments. Hence,if it returns
TRUE , there is an intersection.
We also need to show the converse: that if there is an intersection, then A NY-
SEGMENTS -INTERSECT returns TRUE . Let us suppose that there is at least one
intersection. Let pbe the leftmost intersection point, breaking ties by choosing the
point with the lowest y-coordinate, and let aandbbe the segments that intersect
atp. Since no intersections occur to the left of p, the order given by Tis correct at
all points to the left of p. Because no three segments intersect at the same point, a
andbbecome consecutive in the total preorder at some sweep line ´.2Moreover,
´is to the left of por goes through p. Some segment endpoint qon sweep line ´
2If we allow three segments to intersect at the same point, there may be an intervening segment cthat
intersects both aandbat point p.T h a ti s ,w em a yh a v e a<wcandc<wbfor all sweep lines wto
the left of pfor which a<wb. Exercise 33.2-8 asks you to show that A NY-SEGMENTS -INTERSECT
is correct even if three segments do intersect at the same point.33.2 Determining whether any pair of segments intersects 1027
is the event point at which aandbbecome consecutive in the total preorder. If p
is on sweep line ´,t h e n qDp.I fpis not on sweep line ´,t h e n qis to the left
ofp. In either case, the order given by Tis correct just before encountering q.
(Here is where we use the lexicographic order in which the algorithm processesevent points. Because pis the lowest of the leftmost intersection points, even if p
is on sweep line ´and some other intersection point p
0is on ´, event point qDp
is processed before the other intersection p0can interfere with the total preorder T.
Moreover, even if pis the left endpoint of one segment, say a, and the right end-
point of the other segment, say b, because left endpoint events occur before right
endpoint events, segment bis inTupon ﬁrst encountering segment a.) Either event
point qis processed by A NY-SEGMENTS -INTERSECT or it is not processed.
Ifqis processed by A NY-SEGMENTS -INTERSECT , only two possible actions
may occur:
1. Either aorbis inserted into T, and the other segment is above or below it in
the total preorder. Lines 4–7 detect this case.
2. Segments aandbare already in T, and a segment between them in the total
preorder is deleted, making aandbbecome consecutive. Lines 8–11 detect this
case.
In either case, we ﬁnd the intersection pand A NY-SEGMENTS -INTERSECT returns
TRUE .
If event point qis not processed by A NY-SEGMENTS -INTERSECT , the proce-
dure must have returned before processing all event points. This situation could
have occurred only if A NY-SEGMENTS -INTERSECT had already found an inter-
section and returned TRUE .
Thus, if there is an intersection, A NY-SEGMENTS -INTERSECT returns TRUE .
As we have already seen, if A NY-SEGMENTS -INTERSECT returns TRUE , there is
an intersection. Therefore, A NY-SEGMENTS -INTERSECT always returns a correct
answer.
Running time
If set Scontains nsegments, then A NY-SEGMENTS -INTERSECT runs in time
O.n lgn/. Line 1 takes O.1/ time. Line 2 takes O.n lgn/time, using merge
sort or heapsort. The forloop of lines 3–11 iterates at most once per event point,
a n ds ow i t h 2nevent points, the loop iterates at most 2ntimes. Each iteration takes
O.lgn/time, since each red-black-tree operation takes O.lgn/time and, using the
method of Section 33.1, each intersection test takes O.1/ time. The total time is
thusO.n lgn/.1028 Chapter 33 Computational Geometry
Exercises
33.2-1
Show that a set of nline segments may contain ‚.n2/intersections.
33.2-2
Given two segments aandbthat are comparable at x, show how to determine
inO.1/ time which of a<xborb<xaholds. Assume that neither segment
is vertical. ( Hint: Ifaandbdo not intersect, you can just use cross products.
Ifaandbintersect—which you can of course determine using only cross prod-
ucts—you can still use only addition, subtraction, and multiplication, avoidingdivision. Of course, in the application of the <
xrelation used here, if aandb
intersect, we can just stop and declare that we have found an intersection.)
33.2-3
Professor Mason suggests that we modify A NY-SEGMENTS -INTERSECT so that
instead of returning upon ﬁnding an intersection, it prints the segments that inter-sect and continues on to the next iteration of the forloop. The professor calls the
resulting procedure P
RINT -INTERSECTING -SEGMENTS and claims that it prints
all intersections, from left to right, as they occur in the set of line segments. Pro-fessor Dixon disagrees, claiming that Professor Mason’s idea is incorrect. Whichprofessor is right? Will P
RINT -INTERSECTING -SEGMENTS always ﬁnd the left-
most intersection ﬁrst? Will it always ﬁnd all the intersections?
33.2-4
Give an O.n lgn/-time algorithm to determine whether an n-vertex polygon is
simple.
33.2-5
Give an O.n lgn/-time algorithm to determine whether two simple polygons with
a total of nvertices intersect.
33.2-6
Adisk consists of a circle plus its interior and is represented by its center point and
radius. Two disks intersect if they have any point in common. Give an O.n lgn/-
time algorithm to determine whether any two disks in a set of nintersect.
33.2-7
Given a set of nline segments containing a total of kintersections, show how to
output all kintersections in O..nCk/lgn/time.33.3 Finding the convex hull 1029
33.2-8
Argue that A NY-SEGMENTS -INTERSECT works correctly even if three or more
segments intersect at the same point.
33.2-9
Show that A NY-SEGMENTS -INTERSECT works correctly in the presence of verti-
cal segments if we treat the bottom endpoint of a vertical segment as if it were aleft endpoint and the top endpoint as if it were a right endpoint. How does youranswer to Exercise 33.2-2 change if we allow vertical segments?
33.3 Finding the convex hull
Theconvex hull of a set Qof points, denoted by CH .Q/, is the smallest convex
polygon Pfor which each point in Qis either on the boundary of Por in its
interior. (See Exercise 33.1-5 for a precise deﬁnition of a convex polygon.) Weimplicitly assume that all points in the set Qare unique and that Qcontains at
least three points which are not colinear. Intuitively, we can think of each point
inQas being a nail sticking out from a board. The convex hull is then the shape
formed by a tight rubber band that surrounds all the nails. Figure 33.6 shows a setof points and its convex hull.
In this section, we shall present two algorithms that compute the convex hull
of a set of npoints. Both algorithms output the vertices of the convex hull in
counterclockwise order. The ﬁrst, known as Graham’s scan, runs in O.n lgn/time.
The second, called Jarvis’s march, runs in O.nh/ time, where his the number of
vertices of the convex hull. As Figure 33.6 illustrates, every vertex of CH .Q/ is a
p0p1p2p3p4p5p6 p7
p8p9p10
p11
p12
Figure 33.6 A set of points QDfp0;p1;:::;p 12gwith its convex hull CH .Q/ in gray.1030 Chapter 33 Computational Geometry
point in Q. Both algorithms exploit this property, deciding which vertices in Qto
keep as vertices of the convex hull and which vertices in Qto reject.
We can compute convex hulls in O.n lgn/time by any one of several methods.
Both Graham’s scan and Jarvis’s march use a technique called “rotational sweep,”processing vertices in the order of the polar angles they form with a referencevertex. Other methods include the following:
/SIIn the incremental method , we ﬁrst sort the points from left to right, yielding a
sequencehp1;p2;:::;p ni.A t t h e ith stage, we update the convex hull of the
i/NUL1leftmost points, CH .fp1;p2;:::;p i/NUL1g/, according to the ith point from
the left, thus forming CH .fp1;p2;:::;p ig/. Exercise 33.3-6 asks you how to
implement this method to take a total of O.n lgn/time.
/SIIn the divide-and-conquer method , we divide the set of npoints in ‚.n/ time
into two subsets, one containing the leftmost dn=2epoints and one containing
the rightmostbn=2cpoints, recursively compute the convex hulls of the subsets,
and then, by means of a clever method, combine the hulls in O.n/ time. The
running time is described by the familiar recurrence T .n/D2T .n=2/CO.n/ ,
and so the divide-and-conquer method runs in O.n lgn/time.
/SITheprune-and-search method is similar to the worst-case linear-time median
algorithm of Section 9.3. With this method, we ﬁnd the upper portion (or “upper
chain”) of the convex hull by repeatedly throwing out a constant fraction of the
remaining points until only the upper chain of the convex hull remains. We thendo the same for the lower chain. This method is asymptotically the fastest: ifthe convex hull contains hvertices, it runs in only O.n lgh/time.
Computing the convex hull of a set of points is an interesting problem in its own
right. Moreover, algorithms for some other computational-geometry problems startby computing a convex hull. Consider, for example, the two-dimensional farthest-
pair problem : we are given a set of npoints in the plane and wish to ﬁnd the
two points whose distance from each other is maximum. As Exercise 33.3-3 asksyou to prove, these two points must be vertices of the convex hull. Although wewon’t prove it here, we can ﬁnd the farthest pair of vertices of an n-vertex convex
polygon in O.n/ time. Thus, by computing the convex hull of the ninput points
inO.n lgn/time and then ﬁnding the farthest pair of the resulting convex-polygon
vertices, we can ﬁnd the farthest pair of points in any set of npoints in O.n lgn/
time.
Graham’s scan
Graham’s scan solves the convex-hull problem by maintaining a stack Sof can-
didate points. It pushes each point of the input set Qonto the stack one time,33.3 Finding the convex hull 1031
and it eventually pops from the stack each point that is not a vertex of CH .Q/.
When the algorithm terminates, stack Scontains exactly the vertices of CH .Q/,i n
counterclockwise order of their appearance on the boundary.
The procedure G RAHAM -SCAN takes as input a set Qof points, wherejQj/NAK3.
It calls the functions T OP.S/, which returns the point on top of stack Swithout
changing S,a n dN EXT-TO-TOP.S/, which returns the point one entry below the
top of stack Swithout changing S. As we shall prove in a moment, the stack S
returned by G RAHAM -SCAN contains, from bottom to top, exactly the vertices
of CH .Q/ in counterclockwise order.
GRAHAM -SCAN.Q/
1l e t p0be the point in Qwith the minimum y-coordinate,
or the leftmost such point in case of a tie
2l e thp1;p2;:::;p mibe the remaining points in Q,
sorted by polar angle in counterclockwise order around p0
(if more than one point has the same angle, remove all but
the one that is farthest from p0)
3l e t Sbe an empty stack
4P USH.p0;S/
5P USH.p1;S/
6P USH.p2;S/
7foriD3tom
8 while the angle formed by points N EXT-TO-TOP.S/,TOP.S/,
andpimakes a nonleft turn
9P OP.S/
10 P USH.pi;S/
11return S
Figure 33.7 illustrates the progress of G RAHAM -SCAN. Line 1 chooses point p0
as the point with the lowest y-coordinate, picking the leftmost such point in case
of a tie. Since there is no point in Qthat is below p0and any other points with
the same y-coordinate are to its right, p0m u s tb eav e r t e xo fC H .Q/.L i n e 2
sorts the remaining points of Qby polar angle relative to p0, using the same
method—comparing cross products—as in Exercise 33.1-3. If two or more pointshave the same polar angle relative to p
0, all but the farthest such point are convex
combinations of p0and the farthest point, and so we remove them entirely from
consideration. We let mdenote the number of points other than p0that remain.
The polar angle, measured in radians, of each point in Qrelative to p0is in the
half-open interval Œ0; /EM/ . Since the points are sorted according to polar angles,
they are sorted in counterclockwise order relative to p0. We designate this sorted
sequence of points by hp1;p2;:::;p mi. Note that points p1andpmare vertices1032 Chapter 33 Computational Geometry
p12p11p10
p9
p8p7p6
p5
p4 p3
p2
p1
p0 (a)p12p11p10
p9
p8p7p6
p5
p4 p3
p2
p1
p0 (b)
p12p11p10
p9
p8p7p6
p5
p4p3
p2
p1
p0 (c)p12p11p10
p9
p8p7p6
p5
p4p3
p2
p1
p0 (d)
p12p11p10
p9
p8p7p6
p5
p4 p3
p2
p1
p0 (e)p12p11p10
p9
p8p7p6
p5
p4 p3
p2
p1
p0 (f)
Figure 33.7 The execution of G RAHAM -SCAN on the set Qof Figure 33.6. The current convex
hull contained in stack Sis shown in gray at each step. (a)The sequencehp1;p2;:::;p 12iof points
numbered in order of increasing polar angle relative to p0, and the initial stack Scontaining p0,p1,
andp2.(b)–(k) Stack Safter each iteration of the forloop of lines 7–10. Dashed lines show nonleft
turns, which cause points to be popped from the stack. In part (h), for example, the right turn at
angle†p7p8p9causes p8to be popped, and then the right turn at angle †p6p7p9causes p7to be
popped.33.3 Finding the convex hull 1033
p12p11p10
p9
p8p7p6
p5
p4 p3
p2
p1
p0 (g)p12p11p10
p9
p8p6
p5
p4 p3
p2
p1
p0 (h)
p12p11p10
p9
p8p7p6 p5
p3
p2
p1
p0 (i)p12p11p10
p9
p8p7p6
p3
p2
p1
p0 (j)
p12p11p10
p9
p8p7p6
p5
p4 p3
p2
p1
p0 (k)p12p11p10
p9
p8p7p6
p5
p4 p3
p2
p1
p0 (l)p4 p4p5p7
Figure 33.7, continued (l) The convex hull returned by the procedure, which matches that of
Figure 33.6.1034 Chapter 33 Computational Geometry
of CH .Q/ (see Exercise 33.3-1). Figure 33.7(a) shows the points of Figure 33.6
sequentially numbered in order of increasing polar angle relative to p0.
The remainder of the procedure uses the stack S. Lines 3–6 initialize the stack
to contain, from bottom to top, the ﬁrst three points p0,p1,a n d p2. Figure 33.7(a)
shows the initial stack S.T h e forloop of lines 7–10 iterates once for each point
in the subsequence hp3;p4;:::;p mi. We shall see that after processing point pi,
stack Scontains, from bottom to top, the vertices of CH .fp0;p1;:::;p ig/in coun-
terclockwise order. The while loop of lines 8–9 removes points from the stack if
we ﬁnd them not to be vertices of the convex hull. When we traverse the convex
hull counterclockwise, we should make a left turn at each vertex. Thus, each timethewhile loop ﬁnds a vertex at which we make a nonleft turn, we pop the vertex
from the stack. (By checking for a nonleft turn, rather than just a right turn, thistest precludes the possibility of a straight angle at a vertex of the resulting convexhull. We want no straight angles, since no vertex of a convex polygon may be aconvex combination of other vertices of the polygon.) After we pop all verticesthat have nonleft turns when heading toward point p
i, we push pionto the stack.
Figures 33.7(b)–(k) show the state of the stack Safter each iteration of the for
loop. Finally, G RAHAM -SCAN returns the stack Sin line 11. Figure 33.7(l) shows
the corresponding convex hull.
The following theorem formally proves the correctness of G RAHAM -SCAN.
Theorem 33.1 (Correctness of Graham’s scan)
If G RAHAM -SCAN executes on a set Qof points, wherejQj/NAK3, then at termina-
tion, the stack Sconsists of, from bottom to top, exactly the vertices of CH .Q/ in
counterclockwise order.
Proof After line 2, we have the sequence of points hp1;p2;:::;p mi.L e t u s
deﬁne, for iD2;3 ;:::;m , the subset of points QiDfp0;p1;:::;p ig.T h e
points in Q/NULQmare those that were removed because they had the same polar
angle relative to p0as some point in Qm; these points are not in CH .Q/,a n d
so CH .Qm/DCH.Q/. Thus, it sufﬁces to show that when G RAHAM -SCAN
terminates, the stack Sconsists of the vertices of CH .Qm/in counterclockwise
order, when listed from bottom to top. Note that just as p0,p1,a n d pmare vertices
of CH .Q/, the points p0,p1,a n d piare all vertices of CH .Qi/.
The proof uses the following loop invariant:
At the start of each iteration of the forloop of lines 7–10, stack Sconsists of,
from bottom to top, exactly the vertices of CH .Qi/NUL1/in counterclockwise
order.
Initialization: The invariant holds the ﬁrst time we execute line 7, since at that
time, stack Sconsists of exactly the vertices of Q2DQi/NUL1, and this set of three33.3 Finding the convex hull 1035
p0p1p2pkpj
pi
Qj
(a)p0p1pj
pi
(b)ptpr
Figure 33.8 The proof of correctness of G RAHAM -SCAN.(a)Because pi’s polar angle relative
top0is greater than pj’s polar angle, and because the angle †pkpjpimakes a left turn, adding pi
to CH .Qj/gives exactly the vertices of CH .Qj[fpig/.(b)If the angle†prptpimakes a nonleft
turn, then ptis either in the interior of the triangle formed by p0,pr,a n d pior on a side of the
triangle, which means that it cannot be a vertex of CH .Qi/.
vertices forms its own convex hull. Moreover, they appear in counterclockwise
order from bottom to top.
Maintenance: Entering an iteration of the forloop, the top point on stack S
ispi/NUL1, which was pushed at the end of the previous iteration (or before the
ﬁrst iteration, when iD3). Let pjbe the top point on Safter executing the
while loop of lines 8–9 but before line 10 pushes pi,a n dl e t pkbe the point
just below pjonS. At the moment that pjis the top point on Sand we have
not yet pushed pi, stack Scontains exactly the same points it contained after
iteration jof the forloop. By the loop invariant, therefore, Scontains exactly
the vertices of CH .Qj/at that moment, and they appear in counterclockwise
order from bottom to top.
Let us continue to focus on this moment just before pushing pi. We know
thatpi’s polar angle relative to p0is greater than pj’s polar angle and that
the angle†pkpjpimakes a left turn (otherwise we would have popped pj).
Therefore, because Scontains exactly the vertices of CH .Qj/, we see from
Figure 33.8(a) that once we push pi, stack Swill contain exactly the vertices
of CH .Qj[fpig/, still in counterclockwise order from bottom to top.
We now show that CH .Qj[fpig/is the same set of points as CH .Qi/. Consider
any point ptthat was popped during iteration iof the forloop, and let prbe
the point just below pton stack Sat the time ptwas popped ( prmight be pj).
The angle†prptpimakes a nonleft turn, and the polar angle of ptrelative
top0is greater than the polar angle of pr. As Figure 33.8(b) shows, ptmust1036 Chapter 33 Computational Geometry
be either in the interior of the triangle formed by p0,pr,a n d pior on a side of
this triangle (but it is not a vertex of the triangle). Clearly, since ptis within a
triangle formed by three other points of Qi, it cannot be a vertex of CH .Qi/.
Since ptis not a vertex of CH .Qi/,w eh a v et h a t
CH.Qi/NULfptg/DCH.Qi/: (33.1)
LetPibe the set of points that were popped during iteration iof the forloop.
Since the equality (33.1) applies for all points in Pi, we can apply it repeatedly
to show that CH .Qi/NULPi/DCH.Qi/.B u t Qi/NULPiDQj[fpig,a n ds ow e
conclude that CH .Qj[fpig/DCH.Qi/NULPi/DCH.Qi/.
We have shown that once we push pi, stack Scontains exactly the vertices
of CH .Qi/in counterclockwise order from bottom to top. Incrementing iwill
then cause the loop invariant to hold for the next iteration.
Termination: When the loop terminates, we have iDmC1, and so the loop
invariant implies that stack Sconsists of exactly the vertices of CH .Qm/,w h i c h
is CH .Q/, in counterclockwise order from bottom to top. This completes the
proof.
We now show that the running time of G RAHAM -SCAN isO.n lgn/,w h e r e
nDjQj. Line 1 takes ‚.n/ time. Line 2 takes O.n lgn/time, using merge sort
or heapsort to sort the polar angles and the cross-product method of Section 33.1to compare angles. (We can remove all but the farthest point with the same polarangle in total of O.n/ time over all npoints.) Lines 3–6 take O.1/ time. Because
m/DC4n/NUL1,t h eforloop of lines 7–10 executes at most n/NUL3times. Since P
USH
takes O.1/ time, each iteration takes O.1/ time exclusive of the time spent in the
while loop of lines 8–9, and thus overall the forloop takes O.n/ time exclusive of
the nested while loop.
We use aggregate analysis to show that the while loop takes O.n/ time overall.
ForiD0; 1; : : : ; m , we push each point pionto stack Sexactly once. As in the
analysis of the M ULTIPOP procedure of Section 17.1, we observe that we can pop at
most the number of items that we push. At least three points— p0,p1,a n d pm—are
never popped from the stack, so that in fact at most m/NUL2POPoperations are
performed in total. Each iteration of the while loop performs one P OP,a n ds o
there are at most m/NUL2iterations of the while loop altogether. Since the test in
line 8 takes O.1/ time, each call of P OPtakes O.1/ time, and m/DC4n/NUL1, the total
time taken by the while loop is O.n/ . Thus, the running time of G RAHAM -SCAN
isO.n lgn/.33.3 Finding the convex hull 1037
p4 p2
p0p1right chain left chain
right chain left chainp3
Figure 33.9 The operation of Jarvis’s march. We choose the ﬁrst vertex as the lowest point p0.
The next vertex, p1, has the smallest polar angle of any point with respect to p0. Then, p2has the
smallest polar angle with respect to p1. The right chain goes as high as the highest point p3. Then,
we construct the left chain by ﬁnding smallest polar angles with respect to the negative x-axis.
Jarvis’s march
Jarvis’s march computes the convex hull of a set Qof points by a technique known
aspackage wrapping (orgift wrapping ). The algorithm runs in time O.nh/ ,
where his the number of vertices of CH .Q/.W h e n hiso.lgn/, Jarvis’s march is
asymptotically faster than Graham’s scan.
Intuitively, Jarvis’s march simulates wrapping a taut piece of paper around the
setQ. We start by taping the end of the paper to the lowest point in the set, that is,
to the same point p0with which we start Graham’s scan. We know that this point
must be a vertex of the convex hull. We pull the paper to the right to make it taut,and then we pull it higher until it touches a point. This point must also be a vertexof the convex hull. Keeping the paper taut, we continue in this way around the setof vertices until we come back to our original point p
0.
More formally, Jarvis’s march builds a sequence HDhp0;p1;:::;p h/NUL1iof the
vertices of CH .Q/. We start with p0. As Figure 33.9 shows, the next vertex p1
in the convex hull has the smallest polar angle with respect to p0. (In case of ties,
we choose the point farthest from p0.) Similarly, p2has the smallest polar angle1038 Chapter 33 Computational Geometry
with respect to p1, and so on. When we reach the highest vertex, say pk(breaking
ties by choosing the farthest such vertex), we have constructed, as Figure 33.9shows, the right chain of CH .Q/. To construct the left chain , we start at p
kand
choose pkC1as the point with the smallest polar angle with respect to pk,b u t from
the negative x-axis . We continue on, forming the left chain by taking polar angles
from the negative x-axis, until we come back to our original vertex p0.
We could implement Jarvis’s march in one conceptual sweep around the convex
hull, that is, without separately constructing the right and left chains. Such imple-
mentations typically keep track of the angle of the last convex-hull side chosen and
require the sequence of angles of hull sides to be strictly increasing (in the rangeof0to2/EMradians). The advantage of constructing separate chains is that we need
not explicitly compute angles; the techniques of Section 33.1 sufﬁce to compareangles.
If implemented properly, Jarvis’s march has a running time of O.nh/ . For each
of the hvertices of CH .Q/, we ﬁnd the vertex with the minimum polar angle. Each
comparison between polar angles takes O.1/ time, using the techniques of Sec-
tion 33.1. As Section 9.1 shows, we can compute the minimum of nvalues in O.n/
time if each comparison takes O.1/ time. Thus, Jarvis’s march takes O.nh/ time.
Exercises
33.3-1
Prove that in the procedure G
RAHAM -SCAN, points p1andpmmust be vertices
of CH .Q/.
33.3-2
Consider a model of computation that supports addition, comparison, and multipli-cation and for which there is a lower bound of /DEL.n lgn/to sort nnumbers. Prove
that/DEL.n lgn/is a lower bound for computing, in order, the vertices of the convex
hull of a set of npoints in such a model.
33.3-3
Given a set of points Q, prove that the pair of points farthest from each other must
be vertices of CH .Q/.
33.3-4
For a given polygon Pand a point qon its boundary, the shadow ofqis the set
of points rsuch that the segment
qris entirely on the boundary or in the interior
ofP. As Figure 33.10 illustrates, a polygon Pisstar-shaped if there exists a
point pin the interior of Pthat is in the shadow of every point on the boundary
ofP. The set of all such points pis called the kernel ofP. Given an n-vertex,33.4 Finding the closest pair of points 1039
p
(a) (b)qq′
Figure 33.10 The deﬁnition of a star-shaped polygon, for use in Exercise 33.3-4. (a)A star-shaped
polygon. The segment from point pto any point qon the boundary intersects the boundary only at q.
(b)A non-star-shaped polygon. The shaded region on the left is the shadow of q, and the shaded
region on the right is the shadow of q0. Since these regions are disjoint, the kernel is empty.
star-shaped polygon Pspeciﬁed by its vertices in counterclockwise order, show
how to compute CH .P /inO.n/ time.
33.3-5
In the on-line convex-hull problem , we are given the set Qofnpoints one point at
a time. After receiving each point, we compute the convex hull of the points seenso far. Obviously, we could run Graham’s scan once for each point, with a totalrunning time of O.n
2lgn/. Show how to solve the on-line convex-hull problem in
a total of O.n2/time.
33.3-6 ?
Show how to implement the incremental method for computing the convex hullofnpoints so that it runs in O.n lgn/time.
33.4 Finding the closest pair of points
We now consider the problem of ﬁnding the closest pair of points in a set Qof
n/NAK2points. “Closest” refers to the usual euclidean distance: the distance between
points p1D.x1;y1/andp2D.x2;y2/isp
.x1/NULx2/2C.y1/NULy2/2. Two points
in set Qmay be coincident, in which case the distance between them is zero. This
problem has applications in, for example, trafﬁc-control systems. A system for
controlling air or sea trafﬁc might need to identify the two closest vehicles in order
to detect potential collisions.
A brute-force closest-pair algorithm simply looks at all the/NULn
2/SOH
D‚.n2/pairs
of points. In this section, we shall describe a divide-and-conquer algorithm for1040 Chapter 33 Computational Geometry
this problem, whose running time is described by the familiar recurrence T .n/D
2T .n=2/CO.n/ . Thus, this algorithm uses only O.n lgn/time.
The divide-and-conquer algorithm
Each recursive invocation of the algorithm takes as input a subset P/DC2Qand
arrays XandY, each of which contains all the points of the input subset P.
The points in array Xare sorted so that their x-coordinates are monotonically
increasing. Similarly, array Yis sorted by monotonically increasing y-coordinate.
Note that in order to attain the O.n lgn/time bound, we cannot afford to sort
in each recursive call; if we did, the recurrence for the running time would beT .n/D2T .n=2/CO.n lgn/, whose solution is T .n/DO.n lg
2n/.( U s e t h e
version of the master method given in Exercise 4.6-2.) We shall see a little later
how to use “presorting” to maintain this sorted property without actually sorting in
each recursive call.
A given recursive invocation with inputs P,X,a n d Yﬁrst checks whether
jPj/DC43. If so, the invocation simply performs the brute-force method described
above: try all/NULjPj
2/SOH
pairs of points and return the closest pair. If jPj>3,t h e
recursive invocation carries out the divide-and-conquer paradigm as follows.
Divide: Find a vertical line lthat bisects the point set Pinto two sets PLandPR
such thatjPLjDdjPj=2e,jPRjDbjPj=2c, all points in PLare on or to the
left of line l, and all points in PRare on or to the right of l. Divide the array X
into arrays XLandXR, which contain the points of PLandPRrespectively,
sorted by monotonically increasing x-coordinate. Similarly, divide the array Y
into arrays YLandYR, which contain the points of PLandPRrespectively,
sorted by monotonically increasing y-coordinate.
Conquer: Having divided PintoPLandPR, make two recursive calls, one to ﬁnd
the closest pair of points in PLand the other to ﬁnd the closest pair of points
inPR. The inputs to the ﬁrst call are the subset PLand arrays XLandYL;t h e
second call receives the inputs PR,XR,a n d YR. Let the closest-pair distances
returned for PLandPRbeıLandıR, respectively, and let ıDmin.ıL;ıR/.
Combine: The closest pair is either the pair with distance ıfound by one of the
recursive calls, or it is a pair of points with one point in PLand the other in PR.
The algorithm determines whether there is a pair with one point in PLand the
other point in PRand whose distance is less than ı. Observe that if a pair of
points has distance less than ı, both points of the pair must be within ıunits
of line l. Thus, as Figure 33.11(a) shows, they both must reside in the 2ı-wide
vertical strip centered at line l. To ﬁnd such a pair, if one exists, we do the
following:33.4 Finding the closest pair of points 1041
1. Create an array Y0, which is the array Ywith all points not in the 2ı-wide
vertical strip removed. The array Y0is sorted by y-coordinate, just as Yis.
2. For each point pin the array Y0, try to ﬁnd points in Y0that are within ı
units of p. As we shall see shortly, only the 7points in Y0that follow pneed
be considered. Compute the distance from pto each of these 7points, and
keep track of the closest-pair distance ı0found over all pairs of points in Y0.
3. Ifı0<ı, then the vertical strip does indeed contain a closer pair than the
recursive calls found. Return this pair and its distance ı0. Otherwise, return
the closest pair and its distance ıfound by the recursive calls.
The above description omits some implementation details that are necessary to
achieve the O.n lgn/running time. After proving the correctness of the algorithm,
we shall show how to implement the algorithm to achieve the desired time bound.
Correctness
The correctness of this closest-pair algorithm is obvious, except for two aspects.
First, by bottoming out the recursion when jPj/DC43, we ensure that we never try to
solve a subproblem consisting of only one point. The second aspect is that we need
only check the 7points following each point pin array Y0; we shall now prove this
property.
Suppose that at some level of the recursion, the closest pair of points is pL2PL
andpR2PR. Thus, the distance ı0between pLandpRis strictly less than ı.
Point pLmust be on or to the left of line land less than ıunits away. Similarly, pR
is on or to the right of land less than ıunits away. Moreover, pLandpRare
within ıunits of each other vertically. Thus, as Figure 33.11(a) shows, pLandpR
are within a ı/STX2ırectangle centered at line l. (There may be other points within
this rectangle as well.)
We next show that at most 8points of Pcan reside within this ı/STX2ırectangle.
Consider the ı/STXısquare forming the left half of this rectangle. Since all points
within PLare at least ıunits apart, at most 4points can reside within this square;
Figure 33.11(b) shows how. Similarly, at most 4points in PRcan reside within
theı/STXısquare forming the right half of the rectangle. Thus, at most 8points of P
can reside within the ı/STX2ırectangle. (Note that since points on line lmay be in
either PLorPR,t h e r em a yb eu pt o 4points on l. This limit is achieved if there are
two pairs of coincident points such that each pair consists of one point from PLand
one point from PR, one pair is at the intersection of land the top of the rectangle,
and the other pair is where lintersects the bottom of the rectangle.)
Having shown that at most 8points of Pcan reside within the rectangle, we
can easily see why we need to check only the 7points following each point in the
array Y0. Still assuming that the closest pair is pLandpR, let us assume without1042 Chapter 33 Computational Geometry
lpLpRPLPR
δ2δ
(a)PR
PL
(b)lcoincident points,
  one in PL,
  one in PR
coincident points,
  one in PL,
  one in PRδ δ
δ
Figure 33.11 Key concepts in the proof that the closest-pair algorithm needs to check only 7points
following each point in the array Y0.(a)IfpL2PLandpR2PRare less than ıunits apart, they
must reside within a ı/STX2ırectangle centered at line l.(b)How 4points that are pairwise at least ı
units apart can all reside within a ı/STXısquare. On the left are 4points in PL, and on the right are 4
points in PR.T h e ı/STX2ırectangle can contain 8points if the points shown on line lare actually
pairs of coincident points with one point in PLand one in PR.
loss of generality that pLprecedes pRin array Y0. Then, even if pLoccurs as early
as possible in Y0andpRoccurs as late as possible, pRis in one of the 7positions
following pL. Thus, we have shown the correctness of the closest-pair algorithm.
Implementation and running time
As we have noted, our goal is to have the recurrence for the running time be T .n/D
2T .n=2/CO.n/ ,w h e r e T .n/ is the running time for a set of npoints. The main
difﬁculty comes from ensuring that the arrays XL,XR,YL,a n d YR, which are
passed to recursive calls, are sorted by the proper coordinate and also that thearray Y
0is sorted by y-coordinate. (Note that if the array Xthat is received by a
recursive call is already sorted, then we can easily divide set PintoPLandPRin
linear time.)
The key observation is that in each call, we wish to form a sorted subset of a
sorted array. For example, a particular invocation receives the subset Pand the
array Y, sorted by y-coordinate. Having partitioned PintoPLandPR, it needs to
form the arrays YLandYR, which are sorted by y-coordinate, in linear time. We
can view the method as the opposite of the M ERGE procedure from merge sort in33.4 Finding the closest pair of points 1043
Section 2.3.1: we are splitting a sorted array into two sorted arrays. The following
pseudocode gives the idea.
1l e t YLŒ 1::Y: length /c141andYRŒ1 : : Y: length /c141be new arrays
2YL:lengthDYR:lengthD0
3foriD1toY:length
4 ifYŒ i/c1412PL
5 YL:lengthDYL:lengthC1
6 YLŒYL:length /c141DYŒ i/c141
7 elseYR:lengthDYR:lengthC1
8 YRŒYR:length /c141DYŒ i/c141
We simply examine the points in array Yin order. If a point YŒ i/c141 is in PL,w e
append it to the end of array YL; otherwise, we append it to the end of array YR.
Similar pseudocode works for forming arrays XL,XR,a n d Y0.
The only remaining question is how to get the points sorted in the ﬁrst place. We
presort them; that is, we sort them once and for all before the ﬁrst recursive call.
We pass these sorted arrays into the ﬁrst recursive call, and from there we whittlethem down through the recursive calls as necessary. Presorting adds an additionalO.n lgn/term to the running time, but now each step of the recursion takes linear
time exclusive of the recursive calls. Thus, if we let T .n/ be the running time of
each recursive step and T
0.n/be the running time of the entire algorithm, we get
T0.n/DT .n/CO.n lgn/and
T .n/D(
2T .n=2/CO.n/ ifn>3;
O.1/ ifn/DC43:
Thus, T .n/DO.n lgn/andT0.n/DO.n lgn/.
Exercises
33.4-1
Professor Williams comes up with a scheme that allows the closest-pair algorithmto check only 5points following each point in array Y
0. The idea is always to place
points on line linto set PL. Then, there cannot be pairs of coincident points on
linelwith one point in PLand one in PR. Thus, at most 6points can reside in
theı/STX2ırectangle. What is the ﬂaw in the professor’s scheme?
33.4-2
Show that it actually sufﬁces to check only the points in the 5array positions fol-
lowing each point in the array Y0.1044 Chapter 33 Computational Geometry
33.4-3
We can deﬁne the distance between two points in ways other than euclidean. Inthe plane, the L
m-distance between points p1andp2is given by the expres-
sion.jx1/NULx2jmCjy1/NULy2jm/1=m. Euclidean distance, therefore, is L2-distance.
Modify the closest-pair algorithm to use the L1-distance, which is also known as
theManhattan distance .
33.4-4
Given two points p1andp2in the plane, the L1-distance between them is
given by max .jx1/NULx2j;jy1/NULy2j/. Modify the closest-pair algorithm to use the
L1-distance.
33.4-5
Suppose that /DEL.n/ of the points given to the closest-pair algorithm are covertical.
Show how to determine the sets PLandPRand how to determine whether each
point of Yis inPLorPRso that the running time for the closest-pair algorithm
remains O.n lgn/.
33.4-6
Suggest a change to the closest-pair algorithm that avoids presorting the Yarray
but leaves the running time as O.n lgn/.(Hint: Merge sorted arrays YLandYRto
form the sorted array Y.)
Problems
33-1 Convex layers
Given a set Qof points in the plane, we deﬁne the convex layers ofQinductively.
The ﬁrst convex layer of Qconsists of those points in Qthat are vertices of CH .Q/.
Fori>1 ,d e ﬁ n e Qito consist of the points of Qwith all points in convex layers
1 ;2;:::;i/NUL1removed. Then, the ith convex layer of Qis CH .Qi/ifQi¤; and
is undeﬁned otherwise.
a.Give an O.n2/-time algorithm to ﬁnd the convex layers of a set of npoints.
b.Prove that /DEL.n lgn/time is required to compute the convex layers of a set of n
points with any model of computation that requires /DEL.n lgn/time to sort nreal
numbers.Problems for Chapter 33 1045
33-2 Maximal layers
LetQbe a set of npoints in the plane. We say that point .x; y/ dominates
point .x0;y0/ifx/NAKx0andy/NAKy0. A point in Qthat is dominated by no other
points in Qis said to be maximal . Note that Qmay contain many maximal points,
which can be organized into maximal layers as follows. The ﬁrst maximal layer L1
is the set of maximal points of Q.F o r i>1 ,t h eith maximal layer Liis the set of
maximal points in Q/NULSi/NUL1
jD1Lj.
Suppose that Qhasknonempty maximal layers, and let yibe the y-coordinate
of the leftmost point in LiforiD1 ;2;:::;k . For now, assume that no two points
inQhave the same x-o ry-coordinate.
a.Show that y1>y 2>/SOH/SOH/SOH>y k.
Consider a point .x; y/ that is to the left of any point in Qand for which yis
distinct from the y-coordinate of any point in Q.L e t Q0DQ[f.x; y/g.
b.Letjbe the minimum index such that yj<y, unless y<y k, in which case
we let jDkC1. Show that the maximal layers of Q0are as follows:
/SIIfj/DC4k, then the maximal layers of Q0are the same as the maximal layers
ofQ, except that Ljalso includes .x; y/ as its new leftmost point.
/SIIfjDkC1, then the ﬁrst kmaximal layers of Q0are the same as for Q,b u t
in addition, Q0has a nonempty .kC1/st maximal layer: LkC1Df.x; y/g.
c.Describe an O.n lgn/-time algorithm to compute the maximal layers of a set Q
ofnpoints. ( Hint: Move a sweep line from right to left.)
d.Do any difﬁculties arise if we now allow input points to have the same x-o r
y-coordinate? Suggest a way to resolve such problems.
33-3 Ghostbusters and ghosts
A group of nGhostbusters is battling nghosts. Each Ghostbuster carries a proton
pack, which shoots a stream at a ghost, eradicating it. A stream goes in a straightline and terminates when it hits the ghost. The Ghostbusters decide upon the fol-lowing strategy. They will pair off with the ghosts, forming nGhostbuster-ghost
pairs, and then simultaneously each Ghostbuster will shoot a stream at his cho-
sen ghost. As we all know, it is very dangerous to let streams cross, and so the
Ghostbusters must choose pairings for which no streams will cross.
Assume that the position of each Ghostbuster and each ghost is a ﬁxed point in
the plane and that no three positions are colinear.
a.Argue that there exists a line passing through one Ghostbuster and one ghost
such that the number of Ghostbusters on one side of the line equals the number
of ghosts on the same side. Describe how to ﬁnd such a line in O.n lgn/time.1046 Chapter 33 Computational Geometry
b.Give an O.n2lgn/-time algorithm to pair Ghostbusters with ghosts in such a
way that no streams cross.
33-4 Picking up sticks
Professor Charon has a set of nsticks, which are piled up in some conﬁguration.
Each stick is speciﬁed by its endpoints, and each endpoint is an ordered triplegiving its . x;y;´ / coordinates. No stick is vertical. He wishes to pick up all the
sticks, one at a time, subject to the condition that he may pick up a stick only if
there is no other stick on top of it.
a.Give a procedure that takes two sticks aandband reports whether ais above,
below, or unrelated to b.
b.Describe an efﬁcient algorithm that determines whether it is possible to pick up
all the sticks, and if so, provides a legal order in which to pick them up.
33-5 Sparse-hulled distributions
Consider the problem of computing the convex hull of a set of points in the planethat have been drawn according to some known random distribution. Sometimes,the number of points, or size, of the convex hull of npoints drawn from such a
distribution has expectation O.n
1/NUL/SI/for some constant /SI>0 . We call such a
distribution sparse-hulled . Sparse-hulled distributions include the following:
/SIPoints drawn uniformly from a unit-radius disk. The convex hull has expected
size‚.n1=3/.
/SIPoints drawn uniformly from the interior of a convex polygon with ksides, for
any constant k. The convex hull has expected size ‚.lgn/.
/SIPoints drawn according to a two-dimensional normal distribution. The convex
hull has expected size ‚.p
lgn/.
a.Given two convex polygons with n1andn2vertices respectively, show how to
compute the convex hull of all n1Cn2points in O.n 1Cn2/time. (The polygons
may overlap.)
b.Show how to compute the convex hull of a set of npoints drawn independently
according to a sparse-hulled distribution in O.n/ average-case time. ( Hint:
Recursively ﬁnd the convex hulls of the ﬁrst n=2 points and the second n=2
points, and then combine the results.)Notes for Chapter 33 1047
Chapter notes
This chapter barely scratches the surface of computational-geometry algorithms
and techniques. Books on computational geometry include those by Preparata andShamos [282], Edelsbrunner [99], and O’Rourke [269].
Although geometry has been studied since antiquity, the development of algo-
rithms for geometric problems is relatively new. Preparata and Shamos note that
the earliest notion of the complexity of a problem was given by E. Lemoine in 1902.He was studying euclidean constructions—those using a compass and a ruler—anddevised a set of ﬁve primitives: placing one leg of the compass on a given point,placing one leg of the compass on a given line, drawing a circle, passing the ruler’sedge through a given point, and drawing a line. Lemoine was interested in thenumber of primitives needed to effect a given construction; he called this amountthe “simplicity” of the construction.
The algorithm of Section 33.2, which determines whether any segments inter-
sect, is due to Shamos and Hoey [313].
The original version of Graham’s scan is given by Graham [150]. The package-
wrapping algorithm is due to Jarvis [189]. Using a decision-tree model of com-putation, Yao [359] proved a worst-case lower bound of /DEL.n lgn/for the running
time of any convex-hull algorithm. When the number of vertices hof the con-
vex hull is taken into account, the prune-and-search algorithm of Kirkpatrick and
Seidel [206], which takes O.n lgh/time, is asymptotically optimal.
TheO.n lgn/-time divide-and-conquer algorithm for ﬁnding the closest pair of
points is by Shamos and appears in Preparata and Shamos [282]. Preparata andShamos also show that the algorithm is asymptotically optimal in a decision-treemodel.34 NP-Completeness
Almost all the algorithms we have studied thus far have been polynomial-time al-
gorithms : on inputs of size n, their worst-case running time is O.nk/for some con-
stant k. You might wonder whether allproblems can be solved in polynomial time.
The answer is no. For example, there are problems, such as Turing’s famous “Halt-ing Problem,” that cannot be solved by any computer, no matter how much time weallow. There are also problems that can be solved, but not in time O.n
k/for any
constant k. Generally, we think of problems that are solvable by polynomial-time
algorithms as being tractable, or easy, and problems that require superpolynomial
time as being intractable, or hard.
The subject of this chapter, however, is an interesting class of problems, called
the “NP-complete” problems, whose status is unknown. No polynomial-time al-
gorithm has yet been discovered for an NP-complete problem, nor has anyone yetbeen able to prove that no polynomial-time algorithm can exist for any one of them.This so-called P¤NP question has been one of the deepest, most perplexing open
research problems in theoretical computer science since it was ﬁrst posed in 1971.
Several NP-complete problems are particularly tantalizing because they seem
on the surface to be similar to problems that we know how to solve in polynomialtime. In each of the following pairs of problems, one is solvable in polynomialtime and the other is NP-complete, but the difference between problems appears tobe slight:
Shortest vs. longest simple paths: In Chapter 24, we saw that even with negative
edge weights, we can ﬁnd shortest paths from a single source in a directed
graph GD.V; E/ inO.VE/ time. Finding a longest simple path between two
vertices is difﬁcult, however. Merely determining whether a graph contains asimple path with at least a given number of edges is NP-complete.
Euler tour vs. hamiltonian cycle: AnEuler tour of a connected, directed graph
GD.V; E/ is a cycle that traverses each edge ofGexactly once, although
it is allowed to visit each vertex more than once. By Problem 22-3, we candetermine whether a graph has an Euler tour in only O.E/ time and, in fact,Chapter 34 NP-Completeness 1049
we can ﬁnd the edges of the Euler tour in O.E/ time. A hamiltonian cycle of
a directed graph GD.V; E/ is a simple cycle that contains each vertex inV.
Determining whether a directed graph has a hamiltonian cycle is NP-complete.(Later in this chapter, we shall prove that determining whether an undirected
graph has a hamiltonian cycle is NP-complete.)
2-CNF satisﬁability vs. 3-CNF satisﬁability: A boolean formula contains vari-
a b l e sw h o s ev a l u e sa r e 0or1; boolean connectives such as ^(AND),_(OR),
and:(NOT); and parentheses. A boolean formula is satisﬁable if there exists
some assignment of the values 0and1to its variables that causes it to evaluate
to1. We shall deﬁne terms more formally later in this chapter, but informally, a
boolean formula is in k-conjunctive normal form ,o rk-CNF, if it is the AND
of clauses of ORs of exactly kvariables or their negations. For example, the
boolean formula .x
1_:x2/^.:x1_x3/^.:x2_:x3/is in 2-CNF. (It has
the satisfying assignment x1D1; x 2D0; x 3D1.) Although we can deter-
mine in polynomial time whether a 2-CNF formula is satisﬁable, we shall seelater in this chapter that determining whether a 3-CNF formula is satisﬁable isNP-complete.
NP-completeness and the classes P and NP
Throughout this chapter, we shall refer to three classes of problems: P, NP, and
NPC, the latter class being the NP-complete problems. We describe them infor-mally here, and we shall deﬁne them more formally later on.
The class P consists of those problems that are solvable in polynomial time.
More speciﬁcally, they are problems that can be solved in time O.n
k/for some
constant k,w h e r e nis the size of the input to the problem. Most of the problems
examined in previous chapters are in P.
The class NP consists of those problems that are “veriﬁable” in polynomial time.
What do we mean by a problem being veriﬁable? If we were somehow given a“certiﬁcate” of a solution, then we could verify that the certiﬁcate is correct in timepolynomial in the size of the input to the problem. For example, in the hamiltonian-cycle problem, given a directed graph GD.V; E/ , a certiﬁcate would be a se-
quenceh/ETB
1;/ETB2;/ETB3;:::;/ETB jVjiofjVjvertices. We could easily check in polynomial
time that ./ETBi;/ETBiC1/2EforiD1; 2; 3; : : : ;jVj/NUL1and that ./ETBjVj;/ETB1/2Eas well.
As another example, for 3-CNF satisﬁability, a certiﬁcate would be an assignmentof values to variables. We could check in polynomial time that this assignmentsatisﬁes the boolean formula.
Any problem in P is also in NP, since if a problem is in P then we can solve it
in polynomial time without even being supplied a certiﬁcate. We shall formalizethis notion later in this chapter, but for now we can believe that P /DC2NP. The open
question is whether or not P is a proper subset of NP.1050 Chapter 34 NP-Completeness
Informally, a problem is in the class NPC—and we refer to it as being NP-
complete —if it is in NP and is as “hard” as any problem in NP. We shall formally
deﬁne what it means to be as hard as any problem in NP later in this chapter.In the meantime, we will state without proof that if anyNP-complete problem
can be solved in polynomial time, then every problem in NP has a polynomial-
time algorithm. Most theoretical computer scientists believe that the NP-completeproblems are intractable, since given the wide range of NP-complete problemsthat have been studied to date—without anyone having discovered a polynomial-
time solution to any of them—it would be truly astounding if all of them could
be solved in polynomial time. Yet, given the effort devoted thus far to provingthat NP-complete problems are intractable—without a conclusive outcome—wecannot rule out the possibility that the NP-complete problems are in fact solvablein polynomial time.
To become a good algorithm designer, you must understand the rudiments of the
theory of NP-completeness. If you can establish a problem as NP-complete, you
provide good evidence for its intractability. As an engineer, you would then do
better to spend your time developing an approximation algorithm (see Chapter 35)
or solving a tractable special case, rather than searching for a fast algorithm that
solves the problem exactly. Moreover, many natural and interesting problems thaton the surface seem no harder than sorting, graph searching, or network ﬂow arein fact NP-complete. Therefore, you should become familiar with this remarkableclass of problems.
Overview of showing problems to be NP-complete
The techniques we use to show that a particular problem is NP-complete differ
fundamentally from the techniques used throughout most of this book to design
and analyze algorithms. When we demonstrate that a problem is NP-complete,
we are making a statement about how hard it is (or at least how hard we think itis), rather than about how easy it is. We are not trying to prove the existence ofan efﬁcient algorithm, but instead that no efﬁcient algorithm is likely to exist. Inthis way, NP-completeness proofs bear some similarity to the proof in Section 8.1of an /DEL.n lgn/-time lower bound for any comparison sort algorithm; the speciﬁc
techniques used for showing NP-completeness differ from the decision-tree methodused in Section 8.1, however.
We rely on three key concepts in showing a problem to be NP-complete:
Decision problems vs. optimization problems
Many problems of interest are optimization problems , in which each feasible (i.e.,
“legal”) solution has an associated value, and we wish to ﬁnd a feasible solution
with the best value. For example, in a problem that we call SHORTEST-PATH,Chapter 34 NP-Completeness 1051
we are given an undirected graph Gand vertices uand/ETB, and we wish to ﬁnd a
path from uto/ETBthat uses the fewest edges. In other words, SHORTEST-PATH
is the single-pair shortest-path problem in an unweighted, undirected graph. NP-completeness applies directly not to optimization problems, however, but to deci-
sion problems , in which the answer is simply “yes” or “no” (or, more formally, “1”
or “0”).
Although NP-complete problems are conﬁned to the realm of decision problems,
we can take advantage of a convenient relationship between optimization problems
and decision problems. We usually can cast a given optimization problem as a re-
lated decision problem by imposing a bound on the value to be optimized. Forexample, a decision problem related to SHORTEST-PATH is PATH: given a di-rected graph G, vertices uand/ETB, and an integer k, does a path exist from uto/ETB
consisting of at most kedges?
The relationship between an optimization problem and its related decision prob-
lem works in our favor when we try to show that the optimization problem is
“hard.” That is because the decision problem is in a sense “easier,” or at least “no
harder.” As a speciﬁc example, we can solve PATH by solving SHORTEST-PATH
and then comparing the number of edges in the shortest path found to the value
of the decision-problem parameter k. In other words, if an optimization prob-
lem is easy, its related decision problem is easy as well. Stated in a way that hasmore relevance to NP-completeness, if we can provide evidence that a decisionproblem is hard, we also provide evidence that its related optimization problem is
hard. Thus, even though it restricts attention to decision problems, the theory of
NP-completeness often has implications for optimization problems as well.
Reductions
The above notion of showing that one problem is no harder or no easier than an-other applies even when both problems are decision problems. We take advantageof this idea in almost every NP-completeness proof, as follows. Let us consider adecision problem A, which we would like to solve in polynomial time. We call the
input to a particular problem an instance of that problem; for example, in PATH,
an instance would be a particular graph G, particular vertices uand/ETBofG,a n da
particular integer k. Now suppose that we already know how to solve a different
decision problem Bin polynomial time. Finally, suppose that we have a procedure
that transforms any instance ˛ofAinto some instance ˇofBwith the following
characteristics:
/SIThe transformation takes polynomial time.
/SIThe answers are the same. That is, the answer for ˛is “yes” if and only if the
answer for ˇis also “yes.”1052 Chapter 34 NP-Completeness
polynomial-time
reduction algorithminstance β polynomial-time
algorithm to decide Byesyes
polynomial-time algorithm to decide Ano no ofBinstance α
ofA
Figure 34.1 How to use a polynomial-time reduction algorithm to solve a decision problem Ain
polynomial time, given a polynomial-time decision algorithm for another problem B. In polynomial
time, we transform an instance ˛ofAinto an instance ˇofB,w es o l v e Bin polynomial time, and
we use the answer for ˇas the answer for ˛.
We call such a procedure a polynomial-time reduction algorithm and, as Fig-
ure 34.1 shows, it provides us a way to solve problem Ain polynomial time:
1. Given an instance ˛of problem A, use a polynomial-time reduction algorithm
to transform it to an instance ˇof problem B.
2. Run the polynomial-time decision algorithm for Bon the instance ˇ.
3. Use the answer for ˇas the answer for ˛.
As long as each of these steps takes polynomial time, all three together do also, and
so we have a way to decide on ˛in polynomial time. In other words, by “reducing”
solving problem Ato solving problem B, we use the “easiness” of Bto prove the
“easiness” of A.
Recalling that NP-completeness is about showing how hard a problem is rather
than how easy it is, we use polynomial-time reductions in the opposite way to showthat a problem is NP-complete. Let us take the idea a step further, and show how wecould use polynomial-time reductions to show that no polynomial-time algorithmcan exist for a particular problem B. Suppose we have a decision problem Afor
which we already know that no polynomial-time algorithm can exist. (Let us notconcern ourselves for now with how to ﬁnd such a problem A.) Suppose further
that we have a polynomial-time reduction transforming instances of Ato instances
ofB. Now we can use a simple proof by contradiction to show that no polynomial-
time algorithm can exist for B. Suppose otherwise; i.e., suppose that Bhas a
polynomial-time algorithm. Then, using the method shown in Figure 34.1, we
would have a way to solve problem Ain polynomial time, which contradicts our
assumption that there is no polynomial-time algorithm for A.
For NP-completeness, we cannot assume that there is absolutely no polynomial-
time algorithm for problem A. The proof methodology is similar, however, in that
we prove that problem Bis NP-complete on the assumption that problem Ais also
NP-complete.34.1 Polynomial time 1053
A ﬁrst NP-complete problem
Because the technique of reduction relies on having a problem already known tobe NP-complete in order to prove a different problem NP-complete, we need a“ﬁrst” NP-complete problem. The problem we shall use is the circuit-satisﬁabilityproblem, in which we are given a boolean combinational circuit composed of AND,OR, and NOT gates, and we wish to know whether there exists some set of booleaninputs to this circuit that causes its output to be 1. We shall prove that this ﬁrst
problem is NP-complete in Section 34.3.
Chapter outline
This chapter studies the aspects of NP-completeness that bear most directly on the
analysis of algorithms. In Section 34.1, we formalize our notion of “problem” and
deﬁne the complexity class P of polynomial-time solvable decision problems. We
also see how these notions ﬁt into the framework of formal-language theory. Sec-tion 34.2 deﬁnes the class NP of decision problems whose solutions are veriﬁablein polynomial time. It also formally poses the P ¤NP question.
Section 34.3 shows we can relate problems via polynomial-time “reductions.”
It deﬁnes NP-completeness and sketches a proof that one problem, called “circuit
satisﬁability,” is NP-complete. Having found one NP-complete problem, we show
in Section 34.4 how to prove other problems to be NP-complete much more simply
by the methodology of reductions. We illustrate this methodology by showing that
two formula-satisﬁability problems are NP-complete. With additional reductions,we show in Section 34.5 a variety of other problems to be NP-complete.
34.1 Polynomial time
We begin our study of NP-completeness by formalizing our notion of polynomial-time solvable problems. We generally regard these problems as tractable, but forphilosophical, not mathematical, reasons. We can offer three supporting argu-ments.
First, although we may reasonably regard a problem that requires time ‚.n
100/
to be intractable, very few practical problems require time on the order of such ahigh-degree polynomial. The polynomial-time computable problems encounteredin practice typically require much less time. Experience has shown that once theﬁrst polynomial-time algorithm for a problem has been discovered, more efﬁcientalgorithms often follow. Even if the current best algorithm for a problem has arunning time of ‚.n
100/, an algorithm with a much better running time will likely
soon be discovered.1054 Chapter 34 NP-Completeness
Second, for many reasonable models of computation, a problem that can be
solved in polynomial time in one model can be solved in polynomial time in an-other. For example, the class of problems solvable in polynomial time by the serialrandom-access machine used throughout most of this book is the same as the classof problems solvable in polynomial time on abstract Turing machines.
1It is also
the same as the class of problems solvable in polynomial time on a parallel com-puter when the number of processors grows polynomially with the input size.
Third, the class of polynomial-time solvable problems has nice closure proper-
ties, since polynomials are closed under addition, multiplication, and composition.
For example, if the output of one polynomial-time algorithm is fed into the input ofanother, the composite algorithm is polynomial. Exercise 34.1-5 asks you to showthat if an algorithm makes a constant number of calls to polynomial-time subrou-tines and performs an additional amount of work that also takes polynomial time,then the running time of the composite algorithm is polynomial.
Abstract problems
To understand the class of polynomial-time solvable problems, we must ﬁrst have
a formal notion of what a “problem” is. We deﬁne an abstract problem Qto be a
binary relation on a set Iof problem instances and a set Sof problem solutions .
For example, an instance for SHORTEST-PATH is a triple consisting of a graph
and two vertices. A solution is a sequence of vertices in the graph, with perhaps
the empty sequence denoting that no path exists. The problem SHORTEST-PATHitself is the relation that associates each instance of a graph and two vertices witha shortest path in the graph that connects the two vertices. Since shortest paths arenot necessarily unique, a given problem instance may have more than one solution.
This formulation of an abstract problem is more general than we need for our
purposes. As we saw above, the theory of NP-completeness restricts attention to
decision problems : those having a yes/no solution. In this case, we can view an
abstract decision problem as a function that maps the instance set Ito the solution
setf0; 1g. For example, a decision problem related to SHORTEST-PATH is the
problem PATH that we saw earlier. If iDhG;u;/ETB;kiis an instance of the decision
problem PATH, then PATH .i/D1(yes) if a shortest path from uto/ETBhas at
most kedges, and PATH .i/D0(no) otherwise. Many abstract problems are not
decision problems, but rather optimization problems , which require some value to
be minimized or maximized. As we saw above, however, we can usually recast anoptimization problem as a decision problem that is no harder.
1See Hopcroft and Ullman [180] or Lewis and Papadimitriou [236] for a thorough treatment of the
Turing-machine model.34.1 Polynomial time 1055
Encodings
In order for a computer program to solve an abstract problem, we must represent
problem instances in a way that the program understands. An encoding of a set S
of abstract objects is a mapping efrom Sto the set of binary strings.2For example,
we are all familiar with encoding the natural numbers NDf0; 1; 2; 3; 4; : : :gas
the stringsf0; 1; 10; 11; 100; : : : g. Using this encoding, e.17/D10001 . If you
have looked at computer representations of keyboard characters, you probably haveseen the ASCII code, where, for example, the encoding of Ais1000001 . We can
encode a compound object as a binary string by combining the representations ofits constituent parts. Polygons, graphs, functions, ordered pairs, programs—all canbe encoded as binary strings.
Thus, a computer algorithm that “solves” some abstract decision problem actu-
ally takes an encoding of a problem instance as input. We call a problem whoseinstance set is the set of binary strings a concrete problem . We say that an algo-
rithmsolves a concrete problem in time O.T .n// if, when it is provided a problem
instance iof length nDjij, the algorithm can produce the solution in O.T .n//
time.
3A concrete problem is polynomial-time solvable , therefore, if there exists
an algorithm to solve it in time O.nk/for some constant k.
We can now formally deﬁne the complexity class Pas the set of concrete deci-
sion problems that are polynomial-time solvable.
We can use encodings to map abstract problems to concrete problems. Given
an abstract decision problem Qmapping an instance set Itof0; 1g, an encoding
eWI!f0; 1g/ETXcan induce a related concrete decision problem, which we denote
bye.Q/ .4If the solution to an abstract-problem instance i2IisQ.i/2f0; 1g,
then the solution to the concrete-problem instance e.i/2f0; 1g/ETXis also Q.i/ .A sa
technicality, some binary strings might represent no meaningful abstract-probleminstance. For convenience, we shall assume that any such string maps arbitrarilyto0. Thus, the concrete problem produces the same solutions as the abstract prob-
lem on binary-string instances that represent the encodings of abstract-problem
instances.
We would like to extend the deﬁnition of polynomial-time solvability from con-
crete problems to abstract problems by using encodings as the bridge, but we would
2The codomain of eneed not be binary strings; any set of strings over a ﬁnite alphabet having at
least 2 symbols will do.
3We assume that the algorithm’s output is separate from its input. Because it takes at least one time
step to produce each bit of the output and the algorithm takes O.T.n// time steps, the size of the
output is O.T.n// .
4We denote byf0; 1g/ETXthe set of all strings composed of symbols from the set f0; 1g.1056 Chapter 34 NP-Completeness
like the deﬁnition to be independent of any particular encoding. That is, the ef-
ﬁciency of solving a problem should not depend on how the problem is encoded.Unfortunately, it depends quite heavily on the encoding. For example, suppose thatan integer kis to be provided as the sole input to an algorithm, and suppose that
the running time of the algorithm is ‚.k/ . If the integer kis provided in unary —a
string of k1s—then the running time of the algorithm is O.n/ on length- ninputs,
which is polynomial time. If we use the more natural binary representation of theinteger k, however, then the input length is nDblgkcC1. In this case, the run-
ning time of the algorithm is ‚.k/D‚.2
n/, which is exponential in the size of the
input. Thus, depending on the encoding, the algorithm runs in either polynomialor superpolynomial time.
How we encode an abstract problem matters quite a bit to how we understand
polynomial time. We cannot really talk about solving an abstract problem withoutﬁrst specifying an encoding. Nevertheless, in practice, if we rule out “expensive”encodings such as unary ones, the actual encoding of a problem makes little dif-ference to whether the problem can be solved in polynomial time. For example,representing integers in base 3instead of binary has no effect on whether a prob-
lem is solvable in polynomial time, since we can convert an integer represented in
base3to an integer represented in base 2in polynomial time.
We say that a function fWf0; 1g
/ETX!f0; 1g/ETXispolynomial-time computable
if there exists a polynomial-time algorithm Athat, given any input x2f0; 1g/ETX,
produces as output f. x/ . For some set Iof problem instances, we say that two en-
codings e1ande2arepolynomially related if there exist two polynomial-time com-
putable functions f12andf21such that for any i2I,w eh a v e f12.e1.i//De2.i/
andf21.e2.i//De1.i/.5That is, a polynomial-time algorithm can compute the en-
coding e2.i/from the encoding e1.i/, and vice versa. If two encodings e1ande2of
an abstract problem are polynomially related, whether the problem is polynomial-time solvable or not is independent of which encoding we use, as the followinglemma shows.
Lemma 34.1
LetQbe an abstract decision problem on an instance set I,a n dl e t e
1ande2be
polynomially related encodings on I. Then, e1.Q/2P if and only if e2.Q/2P.
5Technically, we also require the functions f12andf21to “map noninstances to noninstances.”
Anoninstance of an encoding eis a string x2f0; 1g/ETXsuch that there is no instance ifor which
e.i/Dx. We require that f12.x/Dyfor every noninstance xof encoding e1,w h e r e yis some non-
instance of e2,a n dt h a t f21.x0/Dy0for every noninstance x0ofe2,w h e r e y0is some noninstance
ofe1.34.1 Polynomial time 1057
Proof We need only prove the forward direction, since the backward direction is
symmetric. Suppose, therefore, that e1.Q/ can be solved in time O.nk/for some
constant k. Further, suppose that for any problem instance i, the encoding e1.i/
can be computed from the encoding e2.i/in time O.nc/for some constant c,w h e r e
nDje2.i/j. To solve problem e2.Q/, on input e2.i/, we ﬁrst compute e1.i/and
then run the algorithm for e1.Q/ one1.i/. How long does this take? Converting
encodings takes time O.nc/, and thereforeje1.i/jDO.nc/, since the output of
a serial computer cannot be longer than its running time. Solving the problem
one1.i/takes time O.je1.i/jk/DO.nck/, which is polynomial since both candk
are constants.
Thus, whether an abstract problem has its instances encoded in binary or base 3
does not affect its “complexity,” that is, whether it is polynomial-time solvable or
not; but if instances are encoded in unary, its complexity may change. In order to
be able to converse in an encoding-independent fashion, we shall generally assumethat problem instances are encoded in any reasonable, concise fashion, unless wespeciﬁcally say otherwise. To be precise, we shall assume that the encoding of aninteger is polynomially related to its binary representation, and that the encoding ofa ﬁnite set is polynomially related to its encoding as a list of its elements, enclosedin braces and separated by commas. (ASCII is one such encoding scheme.) Withsuch a “standard” encoding in hand, we can derive reasonable encodings of othermathematical objects, such as tuples, graphs, and formulas. To denote the standardencoding of an object, we shall enclose the object in angle braces. Thus, hGi
denotes the standard encoding of a graph G.
As long as we implicitly use an encoding that is polynomially related to this
standard encoding, we can talk directly about abstract problems without referenceto any particular encoding, knowing that the choice of encoding has no effect on
whether the abstract problem is polynomial-time solvable. Henceforth, we shall
generally assume that all problem instances are binary strings encoded using the
standard encoding, unless we explicitly specify the contrary. We shall also typically
neglect the distinction between abstract and concrete problems. You should watch
out for problems that arise in practice, however, in which a standard encoding is
not obvious and the encoding does make a difference.
A formal-language framework
By focusing on decision problems, we can take advantage of the machinery of
formal-language theory. Let’s review some deﬁnitions from that theory. An
alphabet †is a ﬁnite set of symbols. A language Lover †is any set of
strings made up of symbols from †. For example, if †Df0; 1g, the set
LDf10; 11; 101; 111; 1011; 1101; 10001; : : : gis the language of binary represen-1058 Chapter 34 NP-Completeness
tations of prime numbers. We denote the empty string by",t h eempty language
by;, and the language of all strings over †by†/ETX. For example, if †Df0; 1g,
then†/ETXDf"; 0; 1; 00; 01; 10; 11; 000; : : : gis the set of all binary strings. Every
language Lover†is a subset of †/ETX.
We can perform a variety of operations on languages. Set-theoretic operations,
such as union andintersection , follow directly from the set-theoretic deﬁnitions.
We deﬁne the complement ofLby
LD†/ETX/NULL.T h econcatenation L1L2of two
languages L1andL2is the language
LDfx1x2Wx12L1andx22L2g:
Theclosure orKleene star of a language Lis the language
L/ETXDf"g[L[L2[L3[/SOH/SOH/SOH ;
where Lkis the language obtained by concatenating Lto itself ktimes.
From the point of view of language theory, the set of instances for any decision
problem Qis simply the set †/ETX,w h e r e †Df0; 1g.S i n c e Qis entirely character-
ized by those problem instances that produce a 1 (yes) answer, we can view Qas
a language Lover†Df0; 1g,w h e r e
LDfx2†/ETXWQ.x/D1g:
For example, the decision problem PATH has the corresponding language
PATHDfhG; u; /ETB; kiWGD.V; E/ is an undirected graph,
u; /ETB2V;
k/NAK0is an integer, and
there exists a path from uto/ETBinG
consisting of at most kedgesg:
(Where convenient, we shall sometimes use the same name—PATH in this case—
to refer to both a decision problem and its corresponding language.)
The formal-language framework allows us to express concisely the relation be-
tween decision problems and algorithms that solve them. We say that an al-gorithm Aaccepts a string x2f0; 1g
/ETXif, given input x, the algorithm’s out-
putA.x/ is1. The language accepted by an algorithm Ais the set of strings
LDfx2f0; 1g/ETXWA.x/D1g, that is, the set of strings that the algorithm accepts.
An algorithm Arejects a string xifA.x/D0.
Even if language Lis accepted by an algorithm A, the algorithm will not neces-
sarily reject a string x62Lprovided as input to it. For example, the algorithm may
loop forever. A language Lisdecided by an algorithm Aif every binary string
inLis accepted by Aand every binary string not in Lis rejected by A.A l a n -
guage Lisaccepted in polynomial time by an algorithm Aif it is accepted by A
and if in addition there exists a constant ksuch that for any length- nstring x2L,34.1 Polynomial time 1059
algorithm Aaccepts xin time O.nk/. A language Lisdecided in polynomial
time by an algorithm Aif there exists a constant ksuch that for any length- nstring
x2f0; 1g/ETX, the algorithm correctly decides whether x2Lin time O.nk/. Thus,
to accept a language, an algorithm need only produce an answer when provided astring in L, but to decide a language, it must correctly accept or reject every string
inf0; 1g
/ETX.
As an example, the language PATH can be accepted in polynomial time. One
polynomial-time accepting algorithm veriﬁes that Gencodes an undirected graph,
veriﬁes that uand/ETBare vertices in G, uses breadth-ﬁrst search to compute a short-
est path from uto/ETBinG, and then compares the number of edges on the shortest
path obtained with k.I fGencodes an undirected graph and the path found from u
to/ETBhas at most kedges, the algorithm outputs 1and halts. Otherwise, the algo-
rithm runs forever. This algorithm does not decide PATH, however, since it doesnot explicitly output 0for instances in which a shortest path has more than kedges.
A decision algorithm for PATH must explicitly reject binary strings that do not be-
long to PATH. For a decision problem such as PATH, such a decision algorithm is
easy to design: instead of running forever when there is not a path from uto/ETBwith
at most kedges, it outputs 0and halts. (It must also output 0and halt if the input
encoding is faulty.) For other problems, such as Turing’s Halting Problem, thereexists an accepting algorithm, but no decision algorithm exists.
We can informally deﬁne a complexity class as a set of languages, membership
in which is determined by a complexity measure , such as running time, of an
algorithm that determines whether a given string xbelongs to language L.T h e
actual deﬁnition of a complexity class is somewhat more technical.
6
Using this language-theoretic framework, we can provide an alternative deﬁni-
tion of the complexity class P:
PDfL/DC2f0; 1g/ETXWthere exists an algorithm Athat decides L
in polynomial timeg:
In fact, P is also the class of languages that can be accepted in polynomial time.
Theorem 34.2
PDfLWLis accepted by a polynomial-time algorithm g:
Proof Because the class of languages decided by polynomial-time algorithms is
a subset of the class of languages accepted by polynomial-time algorithms, weneed only show that if Lis accepted by a polynomial-time algorithm, it is de-
cided by a polynomial-time algorithm. Let Lbe the language accepted by some
6For more on complexity classes, see the seminal paper by Hartmanis and Stearns [162].1060 Chapter 34 NP-Completeness
polynomial-time algorithm A. We shall use a classic “simulation” argument to
construct another polynomial-time algorithm A0that decides L. Because Aac-
cepts Lin time O.nk/for some constant k, there also exists a constant csuch
thatAaccepts Lin at most cnksteps. For any input string x, the algorithm A0
simulates cnksteps of A. After simulating cnksteps, algorithm A0inspects the be-
havior of A.I fAhas accepted x,t h e n A0accepts xby outputting a 1.I fAhas not
accepted x,t h e n A0rejects xby outputting a 0. The overhead of A0simulating A
does not increase the running time by more than a polynomial factor, and thus A0
is a polynomial-time algorithm that decides L.
Note that the proof of Theorem 34.2 is nonconstructive. For a given language
L2P, we may not actually know a bound on the running time for the algorithm A
that accepts L. Nevertheless, we know that such a bound exists, and therefore, that
an algorithm A0exists that can check the bound, even though we may not be able
to ﬁnd the algorithm A0easily.
Exercises
34.1-1
Deﬁne the optimization problem LONGEST -PATH-LENGTH as the relation that
associates each instance of an undirected graph and two vertices with the num-ber of edges in a longest simple path between the two vertices. Deﬁne the de-cision problem LONGEST-PATH Df h G; u; /ETB; kiWGD.V; E/ is an undi-
rected graph, u; /ETB2V,k/NAK0is an integer, and there exists a simple path
from uto/ETBinGconsisting of at least kedgesg. Show that the optimization prob-
lem LONGEST-PATH-LENGTH can be solved in polynomial time if and only if
LONGEST-PATH2P.
34.1-2
Give a formal deﬁnition for the problem of ﬁnding the longest simple cycle in an
undirected graph. Give a related decision problem. Give the language correspond-
ing to the decision problem.
34.1-3
Give a formal encoding of directed graphs as binary strings using an adjacency-
matrix representation. Do the same using an adjacency-list representation. Arguethat the two representations are polynomially related.
34.1-4
Is the dynamic-programming algorithm for the 0-1 knapsack problem that is asked
for in Exercise 16.2-2 a polynomial-time algorithm? Explain your answer.34.2 Polynomial-time veriﬁcation 1061
34.1-5
Show that if an algorithm makes at most a constant number of calls to polynomial-time subroutines and performs an additional amount of work that also takes polyno-mial time, then it runs in polynomial time. Also show that a polynomial number ofcalls to polynomial-time subroutines may result in an exponential-time algorithm.
34.1-6
Show that the class P, viewed as a set of languages, is closed under union, inter-section, concatenation, complement, and Kleene star. That is, if L
1;L22P, then
L1[L22P,L1\L22P,L1L22P,
L12P, and L/ETX
12P.
34.2 Polynomial-time veriﬁcation
We now look at algorithms that verify membership in languages. For example,
suppose that for a given instance hG; u; /ETB; kiof the decision problem PATH, we
are also given a path pfrom uto/ETB. We can easily check whether pi sap a t hi n G
and whether the length of pis at most k, and if so, we can view pas a “certiﬁcate”
that the instance indeed belongs to PATH. For the decision problem PATH, this
certiﬁcate doesn’t seem to buy us much. After all, PATH belongs to P—in fact,we can solve PATH in linear time—and so verifying membership from a givencertiﬁcate takes as long as solving the problem from scratch. We shall now examinea problem for which we know of no polynomial-time decision algorithm and yet,given a certiﬁcate, veriﬁcation is easy.
Hamiltonian cycles
The problem of ﬁnding a hamiltonian cycle in an undirected graph has been stud-
ied for over a hundred years. Formally, a hamiltonian cycle of an undirected graph
GD.V; E/ is a simple cycle that contains each vertex in V. A graph that con-
tains a hamiltonian cycle is said to be hamiltonian ; otherwise, it is nonhamilto-
nian . The name honors W. R. Hamilton, who described a mathematical game on
the dodecahedron (Figure 34.2(a)) in which one player sticks ﬁve pins in any ﬁveconsecutive vertices and the other player must complete the path to form a cycle1062 Chapter 34 NP-Completeness
(a) (b)
Figure 34.2 (a) A graph representing the vertices, edges, and faces of a dodecahedron, with a
hamiltonian cycle shown by shaded edges. (b)A bipartite graph with an odd number of vertices.
Any such graph is nonhamiltonian.
containing all the vertices.7The dodecahedron is hamiltonian, and Figure 34.2(a)
shows one hamiltonian cycle. Not all graphs are hamiltonian, however. For ex-ample, Figure 34.2(b) shows a bipartite graph with an odd number of vertices.Exercise 34.2-2 asks you to show that all such graphs are nonhamiltonian.
We can deﬁne the hamiltonian-cycle problem ,“ D o e sag r a p h Ghave a hamil-
tonian cycle?” as a formal language:
HAM-CYCLEDfhGiWGis a hamiltonian graph g:
How might an algorithm decide the language HAM-CYCLE? Given a problem
instancehGi, one possible decision algorithm lists all permutations of the vertices
ofGand then checks each permutation to see if it is a hamiltonian path. What is
the running time of this algorithm? If we use the “reasonable” encoding of a graphas its adjacency matrix, the number mof vertices in the graph is /DEL.p
n/,w h e r e
nDjhGijis the length of the encoding of G.T h e r ea r e mŠpossible permutations
7In a letter dated 17 October 1856 to his friend John T. Graves, Hamilton [157, p. 624] wrote, “I
have found that some young persons have been much amused by trying a new mathematical game
w h i c h t h e I c o s i o n f u r n i s h e s , o n e p e r s o n s t i c k i n g ﬁ v e p i n s i n a n y ﬁ v e c o n s e c u t i v e p o i n t s ...a n d t h e
other player then aiming to insert, which by the theory in this letter can always be done, ﬁfteen other
pins, in cyclical succession, so as to cover all the other points, and to end in immediate proximity to
the pin wherewith his antagonist had begun.”34.2 Polynomial-time veriﬁcation 1063
of the vertices, and therefore the running time is /DEL.mŠ/D/DEL.p
nŠ /D/DEL.2p
n/,
which is not O.nk/for any constant k. Thus, this naive algorithm does not run
in polynomial time. In fact, the hamiltonian-cycle problem is NP-complete, as weshall prove in Section 34.5.
Veriﬁcation algorithms
Consider a slightly easier problem. Suppose that a friend tells you that a given
graph Gis hamiltonian, and then offers to prove it by giving you the vertices in
order along the hamiltonian cycle. It would certainly be easy enough to verify theproof: simply verify that the provided cycle is hamiltonian by checking whetherit is a permutation of the vertices of Vand whether each of the consecutive edges
along the cycle actually exists in the graph. You could certainly implement thisveriﬁcation algorithm to run in O.n
2/time, where nis the length of the encoding
ofG. Thus, a proof that a hamiltonian cycle exists in a graph can be veriﬁed in
polynomial time.
We deﬁne a veriﬁcation algorithm as being a two-argument algorithm A,w h e r e
one argument is an ordinary input string xand the other is a binary string ycalled
acertiﬁcate . A two-argument algorithm Averiﬁes an input string xif there exists
a certiﬁcate ysuch that A.x; y/D1.T h e language veriﬁed by a veriﬁcation
algorithm Ais
LDfx2f0; 1g/ETXWthere exists y2f0; 1g/ETXsuch that A.x; y/D1g:
Intuitively, an algorithm Averiﬁes a language Lif for any string x2L,t h e r e
exists a certiﬁcate ythatAcan use to prove that x2L. Moreover, for any string
x62L, there must be no certiﬁcate proving that x2L. For example, in the
hamiltonian-cycle problem, the certiﬁcate is the list of vertices in some hamilto-nian cycle. If a graph is hamiltonian, the hamiltonian cycle itself offers enoughinformation to verify this fact. Conversely, if a graph is not hamiltonian, therecan be no list of vertices that fools the veriﬁcation algorithm into believing that thegraph is hamiltonian, since the veriﬁcation algorithm carefully checks the proposed“cycle” to be sure.1064 Chapter 34 NP-Completeness
The complexity class NP
Thecomplexity class NPis the class of languages that can be veriﬁed by a poly-
nomial-time algorithm.8More precisely, a language Lbelongs to NP if and only if
there exist a two-input polynomial-time algorithm Aand a constant csuch that
LDfx2f0; 1g/ETXWthere exists a certiﬁcate ywithjyjDO.jxjc/
such that A.x; y/D1g:
We say that algorithm Averiﬁes language Lin polynomial time .
From our earlier discussion on the hamiltonian-cycle problem, we now see that
HAM-CYCLE2NP. (It is always nice to know that an important set is nonempty.)
Moreover, if L2P, then L2NP, since if there is a polynomial-time algorithm
to decide L, the algorithm can be easily converted to a two-argument veriﬁcation
algorithm that simply ignores any certiﬁcate and accepts exactly those input stringsit determines to be in L. Thus, P/DC2NP.
It is unknown whether P DNP, but most researchers believe that P and NP are
not the same class. Intuitively, the class P consists of problems that can be solvedquickly. The class NP consists of problems for which a solution can be veriﬁedquickly. You may have learned from experience that it is often more difﬁcult tosolve a problem from scratch than to verify a clearly presented solution, especiallywhen working under time constraints. Theoretical computer scientists generallybelieve that this analogy extends to the classes P and NP, and thus that NP includeslanguages that are not in P.
There is more compelling, though not conclusive, evidence that P ¤NP—the
existence of languages that are “NP-complete.” We shall study this class in Sec-tion 34.3.
Many other fundamental questions beyond the P ¤NP question remain unre-
solved. Figure 34.3 shows some possible scenarios. Despite much work by manyresearchers, no one even knows whether the class NP is closed under comple-ment. That is, does L2NP imply
L2NP? We can deﬁne the complexity class
co-NP as the set of languages Lsuch that
 L2NP. We can restate the question
of whether NP is closed under complement as whether NP Dco-NP. Since P is
closed under complement (Exercise 34.1-6), it follows from Exercise 34.2-9 thatP/DC2NP\co-NP. Once again, however, no one knows whether P DNP\co-NP
or whether there is some language in NP \co-NP/NULP.
8The name “NP” stands for “nondeterministic polynomial time.” The class NP was originally studied
in the context of nondeterminism, but this book uses the somewhat simpler yet equivalent notion of
veriﬁcation. Hopcroft and Ullman [180] give a good presentation of NP-completeness in terms of
nondeterministic models of computation.34.2 Polynomial-time veriﬁcation 1065
co-NP NP
(c)P = NP ∩ co-NPco-NP NP
(d)PP = NP = co-NP
(a)NP = co-NP
(b)P
NP∩ co-NP
Figure 34.3 Four possibilities for relationships among complexity classes. In each diagram, one
region enclosing another indicates a proper-subset relation. (a)PDNPDco-NP. Most researchers
regard this possibility as the most unlikely. (b)If NP is closed under complement, then NP Dco-NP,
but it need not be the case that P DNP.(c)PDNP\co-NP, but NP is not closed under complement.
(d)NP¤co-NP and P¤NP\co-NP. Most researchers regard this possibility as the most likely.
Thus, our understanding of the precise relationship between P and NP is woe-
fully incomplete. Nevertheless, even though we might not be able to prove that aparticular problem is intractable, if we can prove that it is NP-complete, then wehave gained valuable information about it.
Exercises
34.2-1
Consider the language GRAPH-ISOMORPHISM DfhG
1;G2iWG1andG2are
isomorphic graphsg. Prove that GRAPH-ISOMORPHISM 2NP by describing a
polynomial-time algorithm to verify the language.
34.2-2
Prove that if Gis an undirected bipartite graph with an odd number of vertices,
thenGis nonhamiltonian.
34.2-3
Show that if HAM-CYCLE 2P, then the problem of listing the vertices of a
hamiltonian cycle, in order, is polynomial-time solvable.1066 Chapter 34 NP-Completeness
34.2-4
Prove that the class NP of languages is closed under union, intersection, concate-nation, and Kleene star. Discuss the closure of NP under complement.
34.2-5
Show that any language in NP can be decided by an algorithm running in
time2
O.nk/for some constant k.
34.2-6
Ahamiltonian path in a graph is a simple path that visits every vertex exactly
once. Show that the language HAM-PATH DfhG; u; /ETBiWthere is a hamiltonian
path from uto/ETBin graph Ggbelongs to NP.
34.2-7
Show that the hamiltonian-path problem from Exercise 34.2-6 can be solved inpolynomial time on directed acyclic graphs. Give an efﬁcient algorithm for theproblem.
34.2-8
Let/RSbe a boolean formula constructed from the boolean input variables x
1;x2;
:::;x k, negations (:), ANDs (^), ORs (_), and parentheses. The formula /RSis a
tautology if it evaluates to 1for every assignment of 1and0to the input variables.
Deﬁne TAUTOLOGY as the language of boolean formulas that are tautologies.Show that TAUTOLOGY 2co-NP.
34.2-9
Prove that P/DC2co-NP.
34.2-10
Prove that if NP¤co-NP, then P¤NP.
34.2-11
LetGbe a connected, undirected graph with at least 3vertices, and let G
3be the
graph obtained by connecting all pairs of vertices that are connected by a path in G
of length at most 3. Prove that G3is hamiltonian. ( Hint: Construct a spanning tree
forG, and use an inductive argument.)34.3 NP-completeness and reducibility 1067
34.3 NP-completeness and reducibility
Perhaps the most compelling reason why theoretical computer scientists believe
that P¤NP comes from the existence of the class of “NP-complete” problems.
This class has the intriguing property that if anyNP-complete problem can be
solved in polynomial time, then every problem in NP has a polynomial-time solu-
tion, that is, PDNP. Despite years of study, though, no polynomial-time algorithm
has ever been discovered for any NP-complete problem.
The language HAM-CYCLE is one NP-complete problem. If we could decide
HAM-CYCLE in polynomial time, then we could solve every problem in NP inpolynomial time. In fact, if NP /NULP should turn out to be nonempty, we could say
with certainty that HAM-CYCLE 2NP/NULP.
The NP-complete languages are, in a sense, the “hardest” languages in NP. In
this section, we shall show how to compare the relative “hardness” of languagesusing a precise notion called “polynomial-time reducibility.” Then we formallydeﬁne the NP-complete languages, and we ﬁnish by sketching a proof that onesuch language, called CIRCUIT-SAT, is NP-complete. In Sections 34.4 and 34.5,we shall use the notion of reducibility to show that many other problems are NP-complete.
Reducibility
Intuitively, a problem Qcan be reduced to another problem Q
0if any instance of Q
can be “easily rephrased” as an instance of Q0, the solution to which provides a
solution to the instance of Q. For example, the problem of solving linear equations
in an indeterminate xreduces to the problem of solving quadratic equations. Given
an instance axCbD0, we transform it to 0x2CaxCbD0, whose solution
provides a solution to axCbD0. Thus, if a problem Qreduces to another
problem Q0,t h e n Qis, in a sense, “no harder to solve” than Q0.
Returning to our formal-language framework for decision problems, we say that
a language L1ispolynomial-time reducible to a language L2, written L1/DC4PL2,
if there exists a polynomial-time computable function fWf0; 1g/ETX!f0; 1g/ETXsuch
that for all x2f0; 1g/ETX,
x2L1if and only if f. x/2L2: (34.1)
We call the function fthereduction function , and a polynomial-time algorithm F
that computes fis areduction algorithm .
Figure 34.4 illustrates the idea of a polynomial-time reduction from a lan-
guage L1to another language L2. Each language is a subset of f0; 1g/ETX.T h e
reduction function fprovides a polynomial-time mapping such that if x2L1,1068 Chapter 34 NP-Completeness
L2L1{0,1}* {0,1}* f
Figure 34.4 An illustration of a polynomial-time reduction from a language L1to a language L2
via a reduction function f. For any input x2f0; 1g/ETX, the question of whether x2L1has the same
answer as the question of whether f. x/2L2.
thenf. x/2L2. Moreover, if x62L1,t h e n f. x/62L2. Thus, the reduction func-
tion maps any instance xof the decision problem represented by the language L1
to an instance f. x/ of the problem represented by L2. Providing an answer to
whether f. x/2L2directly provides the answer to whether x2L1.
Polynomial-time reductions give us a powerful tool for proving that various lan-
guages belong to P.
Lemma 34.3
IfL1;L2/DC2f0; 1g/ETXare languages such that L1/DC4PL2,t h e n L22P implies
L12P.
Proof LetA2be a polynomial-time algorithm that decides L2,a n dl e t Fbe a
polynomial-time reduction algorithm that computes the reduction function f.W e
shall construct a polynomial-time algorithm A1that decides L1.
Figure 34.5 illustrates how we construct A1. For a given input x2f0; 1g/ETX,
algorithm A1usesFto transform xintof. x/ , and then it uses A2to test whether
f. x/2L2. Algorithm A1takes the output from algorithm A2and produces that
answer as its own output.
The correctness of A1follows from condition (34.1). The algorithm runs in poly-
nomial time, since both FandA2run in polynomial time (see Exercise 34.1-5).
NP-completeness
Polynomial-time reductions provide a formal means for showing that one prob-
lem is at least as hard as another, to within a polynomial-time factor. That is, ifL
1/DC4PL2,t h e n L1is not more than a polynomial factor harder than L2,w h i c hi s34.3 NP-completeness and reducibility 1069
x
Ff. x/
A1A2yes,f. x/2L2
no,f. x/62L2yes,x2L1
no,x62L1
Figure 34.5 The proof of Lemma 34.3. The algorithm Fis a reduction algorithm that computes the
reduction function ffrom L1toL2in polynomial time, and A2is a polynomial-time algorithm that
decides L2. Algorithm A1decides whether x2L1by using Fto transform any input xintof. x/
and then using A2to decide whether f. x/2L2.
why the “less than or equal to” notation for reduction is mnemonic. We can now
deﬁne the set of NP-complete languages, which are the hardest problems in NP.
A language L/DC2f0; 1g/ETXisNP-complete if
1.L2NP, and
2.L0/DC4PLfor every L02NP.
If a language Lsatisﬁes property 2, but not necessarily property 1, we say that L
isNP-hard . We also deﬁne NPC to be the class of NP-complete languages.
As the following theorem shows, NP-completeness is at the crux of deciding
whether P is in fact equal to NP.
Theorem 34.4
If any NP-complete problem is polynomial-time solvable, then P DNP. Equiva-
lently, if any problem in NP is not polynomial-time solvable, then no NP-complete
problem is polynomial-time solvable.
Proof Suppose that L2P and also that L2NPC. For any L02NP, we
have L0/DC4PLby property 2 of the deﬁnition of NP-completeness. Thus, by
Lemma 34.3, we also have that L02P, which proves the ﬁrst statement of the
theorem.
To prove the second statement, note that it is the contrapositive of the ﬁrst state-
ment.
It is for this reason that research into the P ¤NP question centers around the
NP-complete problems. Most theoretical computer scientists believe that P ¤NP,
which leads to the relationships among P, NP, and NPC shown in Figure 34.6.But, for all we know, someone may yet come up with a polynomial-time algo-
rithm for an NP-complete problem, thus proving that P DNP. Nevertheless, since
no polynomial-time algorithm for any NP-complete problem has yet been discov-1070 Chapter 34 NP-Completeness
NPC
PNP
Figure 34.6 How most theoretical computer scientists view the relationships among P, NP,
and NPC. Both P and NPC are wholly contained within NP, and P \NPCD;.
ered, a proof that a problem is NP-complete provides excellent evidence that it is
intractable.
Circuit satisﬁability
We have deﬁned the notion of an NP-complete problem, but up to this point, we
have not actually proved that any problem is NP-complete. Once we prove that atleast one problem is NP-complete, we can use polynomial-time reducibility as atool to prove other problems to be NP-complete. Thus, we now focus on demon-strating the existence of an NP-complete problem: the circuit-satisﬁability prob-lem.
Unfortunately, the formal proof that the circuit-satisﬁability problem is NP-
complete requires technical detail beyond the scope of this text. Instead, we shallinformally describe a proof that relies on a basic understanding of boolean combi-national circuits.
Boolean combinational circuits are built from boolean combinational elements
that are interconnected by wires. A boolean combinational element is any circuit
element that has a constant number of boolean inputs and outputs and that performsa well-deﬁned function. Boolean values are drawn from the set f0; 1g,w h e r e 0
represents
FALSE and1represents TRUE .
The boolean combinational elements that we use in the circuit-satisﬁability prob-
lem compute simple boolean functions, and they are known as logic gates .F i g -
ure 34.7 shows the three basic logic gates that we use in the circuit-satisﬁabilityproblem: the NOT gate (orinverter ), theAND gate ,a n dt h e OR gate .T h e N O T
gate takes a single binary input x, whose value is either 0or1, and produces a
binary output ´whose value is opposite that of the input value. Each of the other
two gates takes two binary inputs xandyand produces a single binary output ´.
We can describe the operation of each gate, and of any boolean combinational
element, by a truth table , shown under each gate in Figure 34.7. A truth table gives
the outputs of the combinational element for each possible setting of the inputs. For34.3 NP-completeness and reducibility 1071
x
yzx
yz
00 0
01 0
10 011 100 0
01 1
10 111 1
(b) (c)xz
01
10
(a)x x xy y :xx ^yx _y
Figure 34.7 Three basic logic gates, with binary inputs and outputs. Under each gate is the truth
table that describes the gate’s operation. (a)The NOT gate. (b)The AND gate. (c)The OR gate.
example, the truth table for the OR gate tells us that when the inputs are xD0
andyD1, the output value is ´D1. We use the symbols :to denote the NOT
function,^to denote the AND function, and _to denote the OR function. Thus,
for example, 0_1D1.
We can generalize AND and OR gates to take more than two inputs. An AND
gate’s output is 1if all of its inputs are 1, and its output is 0otherwise. An OR gate’s
output is 1if any of its inputs are 1, and its output is 0otherwise.
Aboolean combinational circuit consists of one or more boolean combinational
elements interconnected by wires . A wire can connect the output of one element
to the input of another, thereby providing the output value of the ﬁrst element as aninput value of the second. Figure 34.8 shows two similar boolean combinationalcircuits, differing in only one gate. Part (a) of the ﬁgure also shows the values onthe individual wires, given the input hx
1D1; x 2D1; x 3D0i. Although a single
wire may have no more than one combinational-element output connected to it, it
can feed several element inputs. The number of element inputs fed by a wire is
called the fan-out of the wire. If no element output is connected to a wire, the wire
is acircuit input , accepting input values from an external source. If no element
input is connected to a wire, the wire is a circuit output , providing the results of
the circuit’s computation to the outside world. (An internal wire can also fan out
to a circuit output.) For the purpose of deﬁning the circuit-satisﬁability problem,we limit the number of circuit outputs to 1, though in actual hardware design, a
boolean combinational circuit may have multiple outputs.
Boolean combinational circuits contain no cycles. In other words, suppose we
create a directed graph GD.V; E/ with one vertex for each combinational element
and with kdirected edges for each wire whose fan-out is k; the graph contains
a directed edge .u; /ETB/ if a wire connects the output of element uto an input of
element /ETB.T h e n Gmust be acyclic.1072 Chapter 34 NP-Completeness
x3x2x1
(a)1
1
0
111111
11
0
0
1111
x3x2x1
(b)
Figure 34.8 Two instances of the circuit-satisﬁability problem. (a)The assignmenthx1D1;
x2D1; x3D0ito the inputs of this circuit causes the output of the circuit to be 1. The circuit
is therefore satisﬁable. (b)No assignment to the inputs of this circuit can cause the output of the
circuit to be 1. The circuit is therefore unsatisﬁable.
Atruth assignment for a boolean combinational circuit is a set of boolean input
values. We say that a one-output boolean combinational circuit is satisﬁable if it
has asatisfying assignment : a truth assignment that causes the output of the circuit
to be 1. For example, the circuit in Figure 34.8(a) has the satisfying assignment
hx1D1; x 2D1; x 3D0i, and so it is satisﬁable. As Exercise 34.3-1 asks you to
show, no assignment of values to x1,x2,a n d x3causes the circuit in Figure 34.8(b)
to produce a 1output; it always produces 0, and so it is unsatisﬁable.
Thecircuit-satisﬁability problem is, “Given a boolean combinational circuit
composed of AND, OR, and NOT gates, is it satisﬁable?” In order to pose this
question formally, however, we must agree on a standard encoding for circuits.Thesizeof a boolean combinational circuit is the number of boolean combina-
tional elements plus the number of wires in the circuit. We could devise a graphlike
encoding that maps any given circuit Cinto a binary string hCiwhose length is
polynomial in the size of the circuit itself. As a formal language, we can therefore
deﬁne
CIRCUIT-SATDfhCiWCis a satisﬁable boolean combinational circuit g:
The circuit-satisﬁability problem arises in the area of computer-aided hardware
optimization. If a subcircuit always produces 0, that subcircuit is unnecessary;
the designer can replace it by a simpler subcircuit that omits all logic gates andprovides the constant 0value as its output. You can see why we would like to have
a polynomial-time algorithm for this problem.
Given a circuit C, we might attempt to determine whether it is satisﬁable by
simply checking all possible assignments to the inputs. Unfortunately, if the circuithaskinputs, then we would have to check up to 2
kpossible assignments. When34.3 NP-completeness and reducibility 1073
the size of Cis polynomial in k, checking each one takes /DEL.2k/time, which is
superpolynomial in the size of the circuit.9In fact, as we have claimed, there is
strong evidence that no polynomial-time algorithm exists that solves the circuit-satisﬁability problem because circuit satisﬁability is NP-complete. We break theproof of this fact into two parts, based on the two parts of the deﬁnition of NP-completeness.
Lemma 34.5
The circuit-satisﬁability problem belongs to the class NP.
Proof We shall provide a two-input, polynomial-time algorithm Athat can verify
CIRCUIT-SAT. One of the inputs to Ais (a standard encoding of) a boolean com-
binational circuit C. The other input is a certiﬁcate corresponding to an assignment
of boolean values to the wires in C. (See Exercise 34.3-4 for a smaller certiﬁcate.)
We construct the algorithm Aas follows. For each logic gate in the circuit, it
checks that the value provided by the certiﬁcate on the output wire is correctlycomputed as a function of the values on the input wires. Then, if the output of theentire circuit is 1, the algorithm outputs 1, since the values assigned to the inputs
ofCprovide a satisfying assignment. Otherwise, Aoutputs 0.
Whenever a satisﬁable circuit Cis input to algorithm A, there exists a certiﬁcate
whose length is polynomial in the size of Cand that causes Ato output a 1. When-
ever an unsatisﬁable circuit is input, no certiﬁcate can fool Ainto believing that
the circuit is satisﬁable. Algorithm Aruns in polynomial time: with a good imple-
mentation, linear time sufﬁces. Thus, we can verify CIRCUIT -SAT in polynomial
time, and CIRCUIT-SAT 2NP.
The second part of proving that CIRCUIT-SAT is NP-complete is to show that
the language is NP-hard. That is, we must show that every language in NP ispolynomial-time reducible to CIRCUIT-SAT. The actual proof of this fact is fullof technical intricacies, and so we shall settle for a sketch of the proof based onsome understanding of the workings of computer hardware.
A computer program is stored in the computer memory as a sequence of in-
structions. A typical instruction encodes an operation to be performed, addresses
of operands in memory, and an address where the result is to be stored. A spe-
cial memory location, called the program counter , keeps track of which instruc-
9On the other hand, if the size of the circuit Cis‚.2k/, then an algorithm whose running time
isO.2k/has a running time that is polynomial in the circuit size. Even if P ¤NP, this situa-
tion would not contradict the NP-completeness of the problem; the existence of a polynomial-time
algorithm for a special case does not imply that there is a polynomial-time algorithm for all cases.1074 Chapter 34 NP-Completeness
tion is to be executed next. The program counter automatically increments upon
fetching each instruction, thereby causing the computer to execute instructions se-quentially. The execution of an instruction can cause a value to be written to theprogram counter, however, which alters the normal sequential execution and allowsthe computer to loop and perform conditional branches.
At any point during the execution of a program, the computer’s memory holds
the entire state of the computation. (We take the memory to include the programitself, the program counter, working storage, and any of the various bits of state
that a computer maintains for bookkeeping.) We call any particular state of com-
puter memory a conﬁguration . We can view the execution of an instruction as
mapping one conﬁguration to another. The computer hardware that accomplishesthis mapping can be implemented as a boolean combinational circuit, which wedenote by Min the proof of the following lemma.
Lemma 34.6
The circuit-satisﬁability problem is NP-hard.
Proof LetLbe any language in NP. We shall describe a polynomial-time algo-
rithm Fcomputing a reduction function fthat maps every binary string xto a
circuit CDf. x/ such that x2Lif and only if C2CIRCUIT-SAT.
Since L2NP, there must exist an algorithm Athat veriﬁes Lin polynomial
time. The algorithm Fthat we shall construct uses the two-input algorithm Ato
compute the reduction function f.
LetT .n/ denote the worst-case running time of algorithm Aon length- ninput
strings, and let k/NAK1be a constant such that T .n/DO.n
k/and the length of the
certiﬁcate is O.nk/. (The running time of Ais actually a polynomial in the total
input size, which includes both an input string and a certiﬁcate, but since the length
of the certiﬁcate is polynomial in the length nof the input string, the running time
is polynomial in n.)
The basic idea of the proof is to represent the computation of Aas a sequence
of conﬁgurations. As Figure 34.9 illustrates, we can break each conﬁguration intoparts consisting of the program for A, the program counter and auxiliary machine
state, the input x, the certiﬁcate y, and working storage. The combinational cir-
cuitM, which implements the computer hardware, maps each conﬁguration c
ito
the next conﬁguration ciC1, starting from the initial conﬁguration c0. Algorithm A
writes its output— 0or1—to some designated location by the time it ﬁnishes ex-
ecuting, and if we assume that thereafter Ahalts, the value never changes. Thus,
if the algorithm runs for at most T .n/ steps, the output appears as one of the bits
incT. n /.
The reduction algorithm Fconstructs a single combinational circuit that com-
putes all conﬁgurations produced by a given initial conﬁguration. The idea is to34.3 NP-completeness and reducibility 1075
M
A PC aux machine state xy working storageA PC aux machine state xy working storage
M
A PC aux machine state xy working storage
M
A PC aux machine state xy
…
working storage
0/1 outputMc0
c1
c2
cT(n)
Figure 34.9 The sequence of conﬁgurations produced by an algorithm Arunning on an input xand
certiﬁcate y. Each conﬁguration represents the state of the computer for one step of the computation
and, besides A,x,a n d y, includes the program counter (PC), auxiliary machine state, and working
storage. Except for the certiﬁcate y, the initial conﬁguration c0is constant. A boolean combinational
circuit Mmaps each conﬁguration to the next conﬁguration. The output is a distinguished bit in the
working storage.1076 Chapter 34 NP-Completeness
paste together T .n/ copies of the circuit M. The output of the ith circuit, which
produces conﬁguration ci, feeds directly into the input of the .iC1/st circuit. Thus,
the conﬁgurations, rather than being stored in the computer’s memory, simply re-side as values on the wires connecting copies of M.
Recall what the polynomial-time reduction algorithm Fmust do. Given an in-
putx, it must compute a circuit CDf. x/ that is satisﬁable if and only if there
exists a certiﬁcate ysuch that A.x; y/D1.W h e n Fobtains an input x,i tﬁ r s t
computes nDjxjand constructs a combinational circuit C
0consisting of T .n/
copies of M. The input to C0is an initial conﬁguration corresponding to a compu-
tation on A.x; y/ , and the output is the conﬁguration cT. n /.
Algorithm Fmodiﬁes circuit C0slightly to construct the circuit CDf. x/ .
First, it wires the inputs to C0corresponding to the program for A, the initial pro-
gram counter, the input x, and the initial state of memory directly to these known
values. Thus, the only remaining inputs to the circuit correspond to the certiﬁ-catey. Second, it ignores all outputs from C
0, except for the one bit of cT. n /
corresponding to the output of A. This circuit C, so constructed, computes
C.y/DA.x; y/ for any input yof length O.nk/. The reduction algorithm F,
when provided an input string x, computes such a circuit Cand outputs it.
We need to prove two properties. First, we must show that Fcorrectly computes
a reduction function f. That is, we must show that Cis satisﬁable if and only if
there exists a certiﬁcate ysuch that A.x; y/D1. Second, we must show that F
runs in polynomial time.
To show that Fcorrectly computes a reduction function, let us suppose that there
exists a certiﬁcate yof length O.nk/such that A.x; y/D1. Then, if we apply the
bits of yto the inputs of C, the output of CisC.y/DA.x; y/D1. Thus, if a
certiﬁcate exists, then Cis satisﬁable. For the other direction, suppose that Cis
satisﬁable. Hence, there exists an input ytoCsuch that C.y/D1, from which
we conclude that A.x; y/D1. Thus, Fcorrectly computes a reduction function.
To complete the proof sketch, we need only show that Fruns in time polynomial
innDjxj. The ﬁrst observation we make is that the number of bits required to
represent a conﬁguration is polynomial in n. The program for Aitself has constant
size, independent of the length of its input x. The length of the input xisn,a n d
the length of the certiﬁcate yisO.nk/. Since the algorithm runs for at most O.nk/
steps, the amount of working storage required by Ais polynomial in nas well.
(We assume that this memory is contiguous; Exercise 34.3-5 asks you to extendthe argument to the situation in which the locations accessed by Aare scattered
across a much larger region of memory and the particular pattern of scattering can
differ for each input x.)
The combinational circuit Mimplementing the computer hardware has size
polynomial in the length of a conﬁguration, which is O.n
k/; hence, the size of M
is polynomial in n. (Most of this circuitry implements the logic of the memory34.3 NP-completeness and reducibility 1077
system.) The circuit Cconsists of at most tDO.nk/copies of M, and hence it
has size polynomial in n. The reduction algorithm Fcan construct Cfrom xin
polynomial time, since each step of the construction takes polynomial time.
The language CIRCUIT-SAT is therefore at least as hard as any language in NP,
and since it belongs to NP, it is NP-complete.
Theorem 34.7
The circuit-satisﬁability problem is NP-complete.
Proof Immediate from Lemmas 34.5 and 34.6 and from the deﬁnition of NP-
completeness.
Exercises
34.3-1
Verify that the circuit in Figure 34.8(b) is unsatisﬁable.
34.3-2
Show that the/DC4Prelation is a transitive relation on languages. That is, show that if
L1/DC4PL2andL2/DC4PL3,t h e n L1/DC4PL3.
34.3-3
Prove that L/DC4P
Lif and only if
 L/DC4PL.
34.3-4
Show that we could have used a satisfying assignment as a certiﬁcate in an alter-native proof of Lemma 34.5. Which certiﬁcate makes for an easier proof?
34.3-5
The proof of Lemma 34.6 assumes that the working storage for algorithm Aoccu-
pies a contiguous region of polynomial size. Where in the proof do we exploit thisassumption? Argue that this assumption does not involve any loss of generality.
34.3-6
A language Liscomplete for a language class Cwith respect to polynomial-time
reductions if L2CandL
0/DC4PLfor all L02C. Show that;andf0; 1g/ETXare the
only languages in P that are not complete for P with respect to polynomial-timereductions.1078 Chapter 34 NP-Completeness
34.3-7
Show that, with respect to polynomial-time reductions (see Exercise 34.3-6), Lis
complete for NP if and only if
 Lis complete for co-NP.
34.3-8
The reduction algorithm Fin the proof of Lemma 34.6 constructs the circuit
CDf. x/ based on knowledge of x,A,a n d k. Professor Sartre observes that
the string xis input to F, but only the existence of A,k, and the constant factor
implicit in the O.nk/running time is known to F(since the language Lbelongs
to NP), not their actual values. Thus, the professor concludes that Fcan’t possi-
bly construct the circuit Cand that the language CI RCUIT-SAT is not necessarily
NP-hard. Explain the ﬂaw in the professor’s reasoning.
34.4 NP-completeness proofs
We proved that the circuit-satisﬁability problem is NP-complete by a direct proofthatL/DC4
PCIRCUIT-SAT for every language L2NP. In this section, we shall
show how to prove that languages are NP-complete without directly reducing every
language in NP to the given language. We shall illustrate this methodology byproving that various formula-satisﬁability problems are NP-complete. Section 34.5provides many more examples of the methodology.
The following lemma is the basis of our method for showing that a language is
NP-complete.
Lemma 34.8
IfLis a language such that L
0/DC4PLfor some L02NPC, then Lis NP-hard. If, in
addition, L2NP, then L2NPC.
Proof Since L0is NP-complete, for all L002NP, we have L00/DC4PL0. By sup-
position, L0/DC4PL, and thus by transitivity (Exercise 34.3-2), we have L00/DC4PL,
which shows that Lis NP-hard. If L2NP, we also have L2NPC.
In other words, by reducing a known NP-complete language L0toL, we implic-
itly reduce every language in NP to L. Thus, Lemma 34.8 gives us a method for
proving that a language Lis NP-complete:
1. Prove L2NP.
2. Select a known NP-complete language L0.34.4 NP-completeness proofs 1079
3. Describe an algorithm that computes a function fmapping every instance
x2f0; 1g/ETXofL0to an instance f. x/ ofL.
4. Prove that the function fsatisﬁes x2L0if and only if f. x/2Lfor all
x2f0; 1g/ETX.
5. Prove that the algorithm computing fruns in polynomial time.
(Steps 2–5 show that Lis NP-hard.) This methodology of reducing from a sin-
gle known NP-complete language is far simpler than the more complicated pro-cess of showing directly how to reduce from every language in NP. ProvingCIRCUIT-SAT2NPC has given us a “foot in the door.” Because we know that the
circuit-satisﬁability problem is NP-complete, we now can prove much more easilythat other problems are NP-complete. Moreover, as we develop a catalog of knownNP-complete problems, we will have more and more choices for languages from
which to reduce.
Formula satisﬁability
We illustrate the reduction methodology by giving an NP-completeness proof for
the problem of determining whether a boolean formula, not a circuit, is satisﬁable.
This problem has the historical honor of being the ﬁrst problem ever shown to be
NP-complete.
We formulate the (formula) satisﬁability problem in terms of the language SAT
as follows. An instance of SAT is a boolean formula /RScomposed of
1.nboolean variables: x
1;x2;:::;x n;
2.mboolean connectives: any boolean function with one or two inputs and one
output, such as^(AND),_(OR),:(NOT),!(implication),$(if and only
if); and
3. parentheses. (Without loss of generality, we assume that there are no redundant
parentheses, i.e., a formula contains at most one pair of parentheses per booleanconnective.)
We can easily encode a boolean formula /RSin a length that is polynomial in nCm.
As in boolean combinational circuits, a truth assignment for a boolean formula /RS
is a set of values for the variables of /RS,a n da satisfying assignment is a truth
assignment that causes it to evaluate to 1. A formula with a satisfying assignment
is asatisﬁable formula. The satisﬁability problem asks whether a given boolean
formula is satisﬁable; in formal-language terms,
SATDfh/RSiW/RSis a satisﬁable boolean formula g:
As an example, the formula1080 Chapter 34 NP-Completeness
/RSD..x1!x2/_:..:x1$x3/_x4//^:x2
has the satisfying assignment hx1D0; x 2D0; x 3D1; x 4D1i,s i n c e
/RSD..0!0/_:..:0$1/_1//^:0 (34.2)
D.1_:.1_1//^1
D.1_0/^1
D1;
and thus this formula /RSbelongs to SAT.
The naive algorithm to determine whether an arbitrary boolean formula is satis-
ﬁable does not run in polynomial time. A formula with nvariables has 2npossible
assignments. If the length of h/RSiis polynomial in n, then checking every assign-
ment requires /DEL.2n/time, which is superpolynomial in the length of h/RSi.A s t h e
following theorem shows, a polynomial-time algorithm is unlikely to exist.
Theorem 34.9
Satisﬁability of boolean formulas is NP-complete.
Proof We start by arguing that SAT 2NP. Then we prove that SAT is NP-hard by
showing that CIRCUIT-SAT /DC4PSAT; by Lemma 34.8, this will prove the theorem.
To show that SAT belongs to NP, we show that a certiﬁcate consisting of a
satisfying assignment for an input formula /RScan be veriﬁed in polynomial time.
The verifying algorithm simply replaces each variable in the formula with its cor-
responding value and then evaluates the expression, much as we did in equa-
tion (34.2) above. This task is easy to do in polynomial time. If the expressionevaluates to 1, then the algorithm has veriﬁed that the formula is satisﬁable. Thus,
the ﬁrst condition of Lemma 34.8 for NP-completeness holds.
To prove that SAT is NP-hard, we show that CIRCUIT-SAT /DC4
PSAT. In other
words, we need to show how to reduce any instance of circuit satisﬁability to aninstance of formula satisﬁability in polynomial time. We can use induction toexpress any boolean combinational circuit as a boolean formula. We simply lookat the gate that produces the circuit output and inductively express each of thegate’s inputs as formulas. We then obtain the formula for the circuit by writing anexpression that applies the gate’s function to its inputs’ formulas.
Unfortunately, this straightforward method does not amount to a polynomial-
time reduction. As Exercise 34.4-1 asks you to show, shared subformulas—whicharise from gates whose output wires have fan-out of 2 or more—can cause thesize of the generated formula to grow exponentially. Thus, the reduction algorithmmust be somewhat more clever.
Figure 34.10 illustrates how we overcome this problem, using as an example
the circuit from Figure 34.8(a). For each wire x
iin the circuit C, the formula /RS34.4 NP-completeness proofs 1081
x6
x3x4 x7x10 x9x8x5
x2x1
Figure 34.10 Reducing circuit satisﬁability to formula satisﬁability. The formula produced by the
reduction algorithm has a variable for each wire in the circuit.
has a variable xi. We can now express how each gate operates as a small formula
involving the variables of its incident wires. For example, the operation of theoutput AND gate is x
10$.x7^x8^x9/. We call each of these small formulas a
clause .
The formula /RSproduced by the reduction algorithm is the AND of the circuit-
output variable with the conjunction of clauses describing the operation of eachgate. For the circuit in the ﬁgure, the formula is
/RSDx
10^.x4$:x3/
^.x5$.x1_x2//
^.x6$:x4/
^.x7$.x1^x2^x4//
^.x8$.x5_x6//
^.x9$.x6_x7//
^.x10$.x7^x8^x9// :
Given a circuit C, it is straightforward to produce such a formula /RSin polynomial
time.
Why is the circuit Csatisﬁable exactly when the formula /RSis satisﬁable? If C
has a satisfying assignment, then each wire of the circuit has a well-deﬁned value,and the output of the circuit is 1. Therefore, when we assign wire values to
variables in /RS, each clause of /RSevaluates to 1, and thus the conjunction of all
evaluates to 1. Conversely, if some assignment causes /RSto evaluate to 1,t h e
circuit Cis satisﬁable by an analogous argument. Thus, we have shown that
CIRCUIT-SAT/DC4
PSAT, which completes the proof.
1082 Chapter 34 NP-Completeness
3-CNF satisﬁability
We can prove many problems NP-complete by reducing from formula satisﬁability.
The reduction algorithm must handle any input formula, though, and this require-ment can lead to a huge number of cases that we must consider. We often preferto reduce from a restricted language of boolean formulas, so that we need to con-sider fewer cases. Of course, we must not restrict the language so much that itbecomes polynomial-time solvable. One convenient language is 3-CNF satisﬁabil-ity, or 3-CNF-SAT.
We deﬁne 3-CNF satisﬁability using the following terms. A literal in a boolean
formula is an occurrence of a variable or its negation. A boolean formula is inconjunctive normal form ,o rCNF , if it is expressed as an AND of clauses , each
of which is the OR of one or more literals. A boolean formula is in 3-conjunctive
normal form ,o r3-CNF , if each clause has exactly three distinct literals.
For example, the boolean formula
.x
1_:x1_:x2/^.x3_x2_x4/^.:x1_:x3_:x4/
is in 3-CNF. The ﬁrst of its three clauses is .x1_:x1_:x2/, which contains the
three literals x1,:x1,a n d:x2.
In 3-CNF-SAT, we are asked whether a given boolean formula /RSin 3-CNF is
satisﬁable. The following theorem shows that a polynomial-time algorithm thatcan determine the satisﬁability of boolean formulas is unlikely to exist, even whenthey are expressed in this simple normal form.
Theorem 34.10
Satisﬁability of boolean formulas in 3-conjunctive normal form is NP-complete.
Proof The argument we used in the proof of Theorem 34.9 to show that SAT 2
NP applies equally well here to show that 3-CNF-SAT 2NP. By Lemma 34.8,
therefore, we need only show that SAT /DC4
P3-CNF-SAT.
We break the reduction algorithm into three basic steps. Each step progressively
transforms the input formula /RScloser to the desired 3-conjunctive normal form.
The ﬁrst step is similar to the one used to prove CIRCUIT-SAT /DC4PSAT in
Theorem 34.9. First, we construct a binary “parse” tree for the input formula /RS,
with literals as leaves and connectives as internal nodes. Figure 34.11 shows sucha parse tree for the formula
/RSD..x
1!x2/_:..:x1$x3/_x4//^:x2: (34.3)
Should the input formula contain a clause such as the OR of several literals, we use
associativity to parenthesize the expression fully so that every internal node in theresulting tree has 1or2children. We can now think of the binary parse tree as a
circuit for computing the function.34.4 NP-completeness proofs 1083
:x1x1:x2
x2
x3x4y1
y2
y3 y4
y5
y6^
$_
_: !
Figure 34.11 The tree corresponding to the formula /RSD..x1!x2/_:..:x1$x3/_x4//^:x2:
Mimicking the reduction in the proof of Theorem 34.9, we introduce a vari-
ableyifor the output of each internal node. Then, we rewrite the original for-
mula /RSas the AND of the root variable and a conjunction of clauses describing the
operation of each node. For the formula (34.3), the resulting expression is
/RS0Dy1^.y1$.y2^:x2//
^.y2$.y3_y4//
^.y3$.x1!x2//
^.y4$:y5/
^.y5$.y6_x4//
^.y6$.:x1$x3// :
Observe that the formula /RS0thus obtained is a conjunction of clauses /RS0
i, each of
which has at most 3 literals. The only requirement that we might fail to meet isthat each clause has to be an OR of 3literals.
The second step of the reduction converts each clause /RS
0
iinto conjunctive normal
form. We construct a truth table for /RS0
iby evaluating all possible assignments to
its variables. Each row of the truth table consists of a possible assignment of thevariables of the clause, together with the value of the clause under that assignment.Using the truth-table entries that evaluate to 0, we build a formula in disjunctive
normal form (orDNF )—an OR of ANDs—that is equivalent to :/RS
0
i.W e t h e n
negate this formula and convert it into a CNF formula /RS00
iby using DeMorgan’s1084 Chapter 34 NP-Completeness
y1y2x2
.y1$.y2^:x2//
111
 0
110
 1
101
 0
100
 0
011
 1
010
 0
001
 1
000
 1
Figure 34.12 The truth table for the clause .y1$.y2^:x2//.
laws for propositional logic,
:.a^b/D: a_:b;
:.a_b/D: a^:b;
to complement all literals, change ORs into ANDs, and change ANDs into ORs.
In our example, we convert the clause /RS0
1D.y1$.y2^:x2//into CNF
as follows. The truth table for /RS0
1appears in Figure 34.12. The DNF formula
equivalent to:/RS0
1is
.y1^y2^x2/_.y1^:y2^x2/_.y1^:y2^:x2/_.:y1^y2^:x2/:
Negating and applying DeMorgan’s laws, we get the CNF formula/RS
00
1D.:y1_:y2_:x2/^.:y1_y2_:x2/
^.:y1_y2_x2/^.y1_:y2_x2/;
which is equivalent to the original clause /RS0
1.
At this point, we have converted each clause /RS0
iof the formula /RS0into a CNF
formula /RS00
i, and thus /RS0is equivalent to the CNF formula /RS00consisting of the
conjunction of the /RS00
i. Moreover, each clause of /RS00has at most 3literals.
The third and ﬁnal step of the reduction further transforms the formula so that
each clause has exactly 3distinct literals. We construct the ﬁnal 3-CNF formula /RS000
from the clauses of the CNF formula /RS00. The formula /RS000also uses two auxiliary
variables that we shall call pandq. For each clause Ciof/RS00, we include the
following clauses in /RS000:
/SIIfCihas3distinct literals, then simply include Cia sac l a u s eo f /RS000.
/SIIfCihas 2 distinct literals, that is, if CiD.l1_l2/,w h e r e l1andl2are literals,
then include .l1_l2_p/^.l1_l2_:p/as clauses of /RS000. The literals
pand:pmerely fulﬁll the syntactic requirement that each clause of /RS000has34.4 NP-completeness proofs 1085
exactly 3distinct literals. Whether pD0orpD1, one of the clauses is
equivalent to l1_l2, and the other evaluates to 1, which is the identity for AND.
/SIIfCihas just 1distinct literal l, then include .l_p_q/^.l_p_:q/^
.l_:p_q/^.l_:p_:q/as clauses of /RS000. Regardless of the values of p
andq, one of the four clauses is equivalent to l, and the other 3evaluate to 1.
We can see that the 3-CNF formula /RS000is satisﬁable if and only if /RSis satisﬁable
by inspecting each of the three steps. Like the reduction from C IRCUIT-SAT to
SAT, the construction of /RS0from /RSin the ﬁrst step preserves satisﬁability. The
second step produces a CNF formula /RS00that is algebraically equivalent to /RS0.T h e
third step produces a 3-CNF formula /RS000that is effectively equivalent to /RS00,s i n c e
any assignment to the variables pandqproduces a formula that is algebraically
equivalent to /RS00.
We must also show that the reduction can be computed in polynomial time. Con-
structing /RS0from /RSintroduces at most 1variable and 1clause per connective in /RS.
Constructing /RS00from /RS0can introduce at most 8clauses into /RS00for each clause
from /RS0, since each clause of /RS0has at most 3variables, and the truth table for
each clause has at most 23D8rows. The construction of /RS000from /RS00introduces
at most 4clauses into /RS000for each clause of /RS00. Thus, the size of the resulting
formula /RS000is polynomial in the length of the original formula. Each of the con-
structions can easily be accomplished in polynomial time.
Exercises
34.4-1
Consider the straightforward (nonpolynomial-time) reduction in the proof of The-orem 34.9. Describe a circuit of size nthat, when converted to a formula by this
method, yields a formula whose size is exponential in n.
34.4-2
Show the 3-CNF formula that results when we use the method of Theorem 34.10
on the formula (34.3).
34.4-3
Professor Jagger proposes to show that SAT /DC4
P3-CNF-SAT by using only the
truth-table technique in the proof of Theorem 34.10, and not the other steps. Thatis, the professor proposes to take the boolean formula /RS, form a truth table for
its variables, derive from the truth table a formula in 3-DNF that is equivalentto:/RS, and then negate and apply DeMorgan’s laws to produce a 3-CNF formula
equivalent to /RS. Show that this strategy does not yield a polynomial-time reduction.1086 Chapter 34 NP-Completeness
34.4-4
Show that the problem of determining whether a boolean formula is a tautology iscomplete for co-NP. ( Hint: See Exercise 34.3-7.)
34.4-5
Show that the problem of determining the satisﬁability of boolean formulas in dis-junctive normal form is polynomial-time solvable.
34.4-6
Suppose that someone gives you a polynomial-time algorithm to decide formula
satisﬁability. Describe how to use this algorithm to ﬁnd satisfying assignments in
polynomial time.
34.4-7
Let 2-CNF-SAT be the set of satisﬁable boolean formulas in CNF with exactly 2literals per clause. Show that 2-CNF-SAT 2P. Make your algorithm as efﬁcient as
possible. ( Hint: Observe that x_yis equivalent to:x!y. Reduce 2-CNF-SAT
to an efﬁciently solvable problem on a directed graph.)
34.5 NP-complete problems
NP-complete problems arise in diverse domains: boolean logic, graphs, arithmetic,network design, sets and partitions, storage and retrieval, sequencing and schedul-ing, mathematical programming, algebra and number theory, games and puzzles,automata and language theory, program optimization, biology, chemistry, physics,and more. In this section, we shall use the reduction methodology to provide NP-completeness proofs for a variety of problems drawn from graph theory and setpartitioning.
Figure 34.13 outlines the structure of the NP-completeness proofs in this section
and Section 34.4. We prove each language in the ﬁgure to be NP-complete by
reduction from the language that points to it. At the root is CIRCUIT-SAT, which
we proved NP-complete in Theorem 34.7.
34.5.1 The clique problem
Aclique in an undirected graph GD.V; E/ is a subset V
0/DC2Vof vertices, each
pair of which is connected by an edge in E. In other words, a clique is a complete
subgraph of G.T h e sizeof a clique is the number of vertices it contains. The
clique problem is the optimization problem of ﬁnding a clique of maximum size in34.5 NP-complete problems 1087
CIRCUIT-SAT
SAT
3-CNF-SAT
CLIQUE
VERTEX-COVERSUBSET-SUM
HAM-CYCLE
TSP
Figure 34.13 The structure of NP-completeness proofs in Sections 34.4 and 34.5. All proofs ulti-
mately follow by reduction from the NP-completeness of CIRCUIT-SAT.
a graph. As a decision problem, we ask simply whether a clique of a given size k
exists in the graph. The formal deﬁnition is
CLIQUEDfhG; kiWGis a graph containing a clique of size kg:
A naive algorithm for determining whether a graph GD.V; E/ withjVjver-
tices has a clique of size kis to list all k-subsets of V, and check each one to
see whether it forms a clique. The running time of this algorithm is /DEL.k2/NULjVj
k/SOH
/,
which is polynomial if kis a constant. In general, however, kcould be nearjVj=2,
in which case the algorithm runs in superpolynomial time. Indeed, an efﬁcientalgorithm for the clique problem is unlikely to exist.
Theorem 34.11
The clique problem is NP-complete.
Proof To show that CLIQUE 2NP, for a given graph GD.V; E/ ,w eu s et h e
setV
0/DC2Vof vertices in the clique as a certiﬁcate for G. We can check whether V0
is a clique in polynomial time by checking whether, for each pair u; /ETB2V0,t h e
edge .u; /ETB/ belongs to E.
We next prove that 3-CNF-SAT /DC4PCLIQUE, which shows that the clique prob-
lem is NP-hard. You might be surprised that we should be able to prove such aresult, since on the surface logical formulas seem to have little to do with graphs.
The reduction algorithm begins with an instance of 3-CNF-SAT. Let /RSD
C
1^C2^/SOH/SOH/SOH^ Ckbe a boolean formula in 3-CNF with kclauses. For rD1088 Chapter 34 NP-Completeness
x1x1
x2 x2
x3 x3:x1:x2:x3C1Dx1_:x2_:x3
C2D:x1_x2_x3 C3Dx1_x2_x3
Figure 34.14 The graph Gderived from the 3-CNF formula /RSDC1^C2^C3,w h e r e C1D
.x1_:x2_:x3/,C2D.:x1_x2_x3/,a n d C3D.x1_x2_x3/, in reducing 3-CNF-SAT to
CLIQUE. A satisfying assignment of the formula has x2D0,x3D1,a n d x1either 0or1.T h i s
assignment satisﬁes C1with:x2, and it satisﬁes C2andC3withx3, corresponding to the clique
with lightly shaded vertices.
1 ;2;:::;k , each clause Crhas exactly three distinct literals lr
1,lr
2,a n d lr
3.W es h a l l
construct a graph Gsuch that /RSis satisﬁable if and only if Ghas a clique of size k.
We construct the graph GD.V; E/ as follows. For each clause CrD
.lr
1_lr
2_lr
3/in/RS, we place a triple of vertices /ETBr
1,/ETBr
2,a n d /ETBr
3intoV. We put
an edge between two vertices /ETBr
iand/ETBs
jif both of the following hold:
/SI/ETBr
iand/ETBs
jare in different triples, that is, r¤s,a n d
/SItheir corresponding literals are consistent ,t h a ti s , lr
iis not the negation of ls
j.
We can easily build this graph from /RSin polynomial time. As an example of this
construction, if we have
/RSD.x1_:x2_:x3/^.:x1_x2_x3/^.x1_x2_x3/;
thenGis the graph shown in Figure 34.14.
We must show that this transformation of /RSintoGis a reduction. First, suppose
that/RShas a satisfying assignment. Then each clause Crcontains at least one
literal lr
ithat is assigned 1, and each such literal corresponds to a vertex /ETBr
i. Picking
one such “true” literal from each clause yields a set V0ofkvertices. We claim that
V0is a clique. For any two vertices /ETBr
i;/ETBs
j2V0,w h e r e r¤s, both corresponding
literals lr
iandls
jmap to 1by the given satisfying assignment, and thus the literals34.5 NP-complete problems 1089
cannot be complements. Thus, by the construction of G, the edge ./ETBr
i;/ETBs
j/belongs
toE.
Conversely, suppose that Ghas a clique V0of size k. No edges in Gconnect
vertices in the same triple, and so V0contains exactly one vertex per triple. We can
assign 1to each literal lr
isuch that /ETBr
i2V0without fear of assigning 1to both a
literal and its complement, since Gcontains no edges between inconsistent literals.
Each clause is satisﬁed, and so /RSis satisﬁed. (Any variables that do not correspond
to a vertex in the clique may be set arbitrarily.)
In the example of Figure 34.14, a satisfying assignment of /RShasx2D0and
x3D1. A corresponding clique of size kD3consists of the vertices correspond-
ing to:x2from the ﬁrst clause, x3from the second clause, and x3from the third
clause. Because the clique contains no vertices corresponding to either x1or:x1,
we can set x1to either 0or1in this satisfying assignment.
Observe that in the proof of Theorem 34.11, we reduced an arbitrary instance
of 3-CNF-SAT to an instance of CLIQUE with a particular structure. You mightthink that we have shown only that CLIQUE is NP-hard in graphs in which thevertices are restricted to occur in triples and in which there are no edges betweenvertices in the same triple. Indeed, we have shown that CLIQUE is NP-hard onlyin this restricted case, but this proof sufﬁces to show that CLIQUE is NP-hard ingeneral graphs. Why? If we had a polynomial-time algorithm that solved CLIQUEon general graphs, it would also solve CLIQUE on restricted graphs.
The opposite approach—reducing instances of 3-CNF-SAT with a special struc-
ture to general instances of CLIQUE—would not have sufﬁced, however. Whynot? Perhaps the instances of 3-CNF-SAT that we chose to reduce from were“easy,” and so we would not have reduced an NP-hard problem to CLIQUE.
Observe also that the reduction used the instance of 3-CNF-SAT, but not the
solution. We would have erred if the polynomial-time reduction had relied on
knowing whether the formula /RSis satisﬁable, since we do not know how to decide
whether /RSis satisﬁable in polynomial time.
34.5.2 The vertex-cover problem
Avertex cover of an undirected graph GD.V; E/ is a subset V
0/DC2Vsuch that
if.u; /ETB/2E,t h e n u2V0or/ETB2V0(or both). That is, each vertex “covers” its
incident edges, and a vertex cover for Gis a set of vertices that covers all the edges
inE.T h e sizeof a vertex cover is the number of vertices in it. For example, the
graph in Figure 34.15(b) has a vertex cover fw;´gof size 2.
Thevertex-cover problem is to ﬁnd a vertex cover of minimum size in a given
graph. Restating this optimization problem as a decision problem, we wish to1090 Chapter 34 NP-Completeness
u v
y xz w
(a)u v
y xz w
(b)
Figure 34.15 Reducing CLIQUE to VERTEX-COVER. (a)An undirected graph GD.V; E/ with
clique V0Dfu; /ETB; x; yg.(b)The graph
 Gproduced by the reduction algorithm that has vertex cover
V/NULV0Dfw;´g.
determine whether a graph has a vertex cover of a given size k. As a language, we
deﬁne
VERTEX-COVER DfhG; kiWgraph Ghas a vertex cover of size kg:
The following theorem shows that this problem is NP-complete.
Theorem 34.12
The vertex-cover problem is NP-complete.
Proof We ﬁrst show that VERTEX-COVER 2NP. Suppose we are given a graph
GD.V; E/ and an integer k. The certiﬁcate we choose is the vertex cover V0/DC2V
itself. The veriﬁcation algorithm afﬁrms that jV0jDk, and then it checks, for each
edge .u; /ETB/2E,t h a t u2V0or/ETB2V0. We can easily verify the certiﬁcate in
polynomial time.
We prove that the vertex-cover problem is NP-hard by showing that CLIQUE /DC4P
VERTEX-COVER. This reduction relies on the notion of the “complement” of a
graph. Given an undirected graph GD.V; E/ ,w ed e ﬁ n et h e complement ofG
as
GD.V;
E/,w h e r e
 EDf.u; /ETB/Wu; /ETB2V;u¤/ETB;and.u; /ETB/62Eg.I n o t h e r
words,
 Gis the graph containing exactly those edges that are not in G. Figure 34.15
shows a graph and its complement and illustrates the reduction from CLIQUE toVERTEX-COVER.
The reduction algorithm takes as input an instance hG;kiof the clique problem.
It computes the complement
G, which we can easily do in polynomial time. The
output of the reduction algorithm is the instance h
G;jVj/NULkiof the vertex-cover
problem. To complete the proof, we show that this transformation is indeed a34.5 NP-complete problems 1091
reduction: the graph Ghas a clique of size kif and only if the graph
 Gh a sav e r t e x
cover of sizejVj/NULk.
Suppose that Ghas a clique V0/DC2VwithjV0jDk. We claim that V/NULV0is a
vertex cover in
 G.L e t .u; /ETB/ be any edge in
 E. Then, .u; /ETB/62E, which implies
that at least one of uor/ETBdoes not belong to V0, since every pair of vertices in V0is
connected by an edge of E. Equivalently, at least one of uor/ETBis inV/NULV0,w h i c h
means that edge .u; /ETB/ is covered by V/NULV0.S i n c e .u; /ETB/ was chosen arbitrarily
from
 E, every edge of
 Eis covered by a vertex in V/NULV0. Hence, the set V/NULV0,
which has sizejVj/NULk, forms a vertex cover for
 G.
Conversely, suppose that
 Ghas a vertex cover V0/DC2V,w h e r ejV0jDjVj/NULk.
Then, for all u; /ETB2V,i f.u; /ETB/2
E,t h e n u2V0or/ETB2V0or both. The
contrapositive of this implication is that for all u; /ETB2V,i fu62V0and/ETB62V0,
then.u; /ETB/2E. In other words, V/NULV0is a clique, and it has size jVj/NULjV0jDk.
Since VERTEX-COVER is NP-complete, we don’t expect to ﬁnd a polynomial-
time algorithm for ﬁnding a minimum-size vertex cover. Section 35.1 presents apolynomial-time “approximation algorithm,” however, which produces “approxi-mate” solutions for the vertex-cover problem. The size of a vertex cover producedby the algorithm is at most twice the minimum size of a vertex cover.
Thus, we shouldn’t give up hope just because a problem is NP-complete. We
may be able to design a polynomial-time approximation algorithm that obtainsnear-optimal solutions, even though ﬁnding an optimal solution is NP-complete.Chapter 35 gives several approximation algorithms for NP-complete problems.
34.5.3 The hamiltonian-cycle problem
We now return to the hamiltonian-cycle problem deﬁned in Section 34.2.
Theorem 34.13
The hamiltonian cycle problem is NP-complete.
Proof We ﬁrst show that HAM-CYCLE belongs to NP. Given a graph GD
.V; E/ , our certiﬁcate is the sequence of jVjvertices that makes up the hamiltonian
cycle. The veriﬁcation algorithm checks that this sequence contains each vertex
inVexactly once and that with the ﬁrst vertex repeated at the end, it forms a cycle
inG. That is, it checks that there is an edge between each pair of consecutive
vertices and between the ﬁrst and last vertices. We can verify the certiﬁcate inpolynomial time.
We now prove that VERTEX-COVER /DC4
PHAM-CYCLE, which shows that
HAM-CYCLE is NP-complete. Given an undirected graph GD.V; E/ and an1092 Chapter 34 NP-Completeness
[u,v,1]
[u,v,2]
[u,v,3]
[u,v,4]
[u,v,5]
[u,v,6][v,u,1]
[v,u,2]
[v,u,3]
[v,u,4]
[v,u,5]
[v,u,6]Wuv
(a)Wuv
(b)[u,v,1]
[u,v,6][v,u,1]
[v,u,6]Wuv
(c)[u,v,1]
[u,v,6][v,u,1]
[v,u,6]Wuv
(d)[u,v,1]
[u,v,6][v,u,1]
[v,u,6]
Figure 34.16 The widget used in reducing the vertex-cover problem to the hamiltonian-cycle prob-
lem. An edge .u; /ETB/ of graph Gcorresponds to widget Wu/ETBin the graph G0created in the reduction.
(a)The widget, with individual vertices labeled. (b)–(d) The shaded paths are the only possible ones
through the widget that include all vertices, assuming that the only connections from the widget to
the remainder of G0are through vertices Œu; /ETB; 1/c141 ,Œu; /ETB; 6/c141 ,Œ/ETB; u; 1/c141 ,a n d Œ/ETB; u; 6/c141 .
integer k, we construct an undirected graph G0D.V0;E0/that has a hamiltonian
cycle if and only if Ghas a vertex cover of size k.
Our construction uses a widget , which is a piece of a graph that enforces certain
properties. Figure 34.16(a) shows the widget we use. For each edge .u; /ETB/2E,t h e
graph G0that we construct will contain one copy of this widget, which we denote
byWu/ETB. We denote each vertex in Wu/ETBbyŒ u ;/ETB;i/c141 orŒ /ETB;u ;i/c141 ,w h e r e 1/DC4i/DC46,s o
that each widget Wu/ETBcontains 12vertices. Widget Wu/ETBalso contains the 14edges
shown in Figure 34.16(a).
Along with the internal structure of the widget, we enforce the properties we
want by limiting the connections between the widget and the remainder of thegraph G
0that we construct. In particular, only vertices Œ u ;/ETB;1 /c141 ,Œ u ;/ETB;6 /c141 ,Œ /ETB;u ;1 /c141 ,
andŒ /ETB;u ;6 /c141 will have edges incident from outside Wu/ETB. Any hamiltonian cycle
ofG0must traverse the edges of Wu/ETBin one of the three ways shown in Fig-
ures 34.16(b)–(d). If the cycle enters through vertex Œ u ;/ETB;1 /c141 , it must exit through
vertex Œ u ;/ETB;6 /c141 , and it either visits all 12of the widget’s vertices (Figure 34.16(b))
or the six vertices Œ u ;/ETB;1 /c141 through Œ u ;/ETB;6 /c141 (Figure 34.16(c)). In the latter case,
the cycle will have to reenter the widget to visit vertices Œ /ETB;u ;1 /c141 through Œ /ETB;u ;6 /c141 .
Similarly, if the cycle enters through vertex Œ /ETB;u ;1 /c141 , it must exit through ver-
texŒ /ETB;u ;6 /c141 , and it either visits all 12of the widget’s vertices (Figure 34.16(d)) or
the six vertices Œ /ETB;u ;1 /c141 through Œ /ETB;u ;6 /c141 (Figure 34.16(c)). No other paths through
the widget that visit all 12vertices are possible. In particular, it is impossible to
construct two vertex-disjoint paths, one of which connects Œ u ;/ETB;1 /c141 toŒ /ETB;u ;6 /c141 and
the other of which connects Œ /ETB;u ;1 /c141 toŒ u ;/ETB;6 /c141 , such that the union of the two paths
contains all of the widget’s vertices.34.5 NP-complete problems 1093
[w,x,1]
[w,x,6][x,w,1]
[x,w,6]Wwx(b)
[x,y,1]
[x,y,6][y,x,1]
[y,x,6]Wxy[w,y,1]
[w,y,6][y,w,1]
[y,w,6]Wwy[w,z,1]
[w,z,6][z,w,1]
[z,w,6]Wwzs1
s2w x
z y(a)
Figure 34.17 Reducing an instance of the vertex-cover problem to an instance of the hamiltonian-
cycle problem. (a)An undirected graph Gwith a vertex cover of size 2, consisting of the lightly
shaded vertices wandy.(b)The undirected graph G0produced by the reduction, with the hamilto-
nian path corresponding to the vertex cover shaded. The vertex cover fw;ygcorresponds to edges
.s1;Œ w;x;1 /c141 / and.s2;Œ y;x;1 /c141 / appearing in the hamiltonian cycle.
The only other vertices in V0other than those of widgets are selector vertices
s1;s2;:::;s k. We use edges incident on selector vertices in G0to select the k
vertices of the cover in G.
In addition to the edges in widgets, E0contains two other types of edges, which
Figure 34.17 shows. First, for each vertex u2V, we add edges to join pairs
of widgets in order to form a path containing all widgets corresponding to edges
incident on uinG. We arbitrarily order the vertices adjacent to each vertex
u2Vasu.1/;u.2/;:::;u.degree .u//, where degree .u/is the number of vertices
adjacent to u. We create a path in G0through all the widgets corresponding
to edges incident on uby adding to E0the edgesf.Œu; u.i/; 6/c141; Œu; u.iC1/;1 /c141 /W
1/DC4i/DC4degree .u//NUL1g. In Figure 34.17, for example, we order the vertices ad-
jacent to wasx;y;´ , and so graph G0in part (b) of the ﬁgure includes the edges1094 Chapter 34 NP-Completeness
.Œw;x;6/c141;Œw;y;1/c141/ and.Œw;y;6/c141;Œw;´;1/c141/ . For each vertex u2V, these edges
inG0ﬁll in a path containing all widgets corresponding to edges incident on u
inG.
The intuition behind these edges is that if we choose a vertex u2Vin the vertex
cover of G, we can construct a path from Œu; u.1/;1 /c141toŒu; u.degree .u//;6 /c141inG0that
“covers” all widgets corresponding to edges incident on u. That is, for each of these
widgets, say Wu;u.i/, the path either includes all 12vertices (if uis in the vertex
cover but u.i/is not) or just the six vertices Œu; u.i/; 1/c141; Œu; u.i/;2 /c141 ;:::;Œ u ;u.i/;6 /c141(if
bothuandu.i/are in the vertex cover).
The ﬁnal type of edge in E0joins the ﬁrst vertex Œu; u.1/;1 /c141and the last vertex
Œu; u.degree .u//;6 /c141of each of these paths to each of the selector vertices. That is, we
include the edges
f.sj;Œ u ;u.1/;1 /c141 /Wu2Vand1/DC4j/DC4kg
[f.sj;Œ u ;u.degree .u//;6 /c141 /Wu2Vand1/DC4j/DC4kg:
Next, we show that the size of G0is polynomial in the size of G, and hence we
can construct G0in time polynomial in the size of G. The vertices of G0are those
in the widgets, plus the selector vertices. With 12vertices per widget, plus k/DC4jVj
selector vertices, we have a total of
jV0jD12jEjCk
/DC412jEjCjVj
vertices. The edges of G0are those in the widgets, those that go between widgets,
and those connecting selector vertices to widgets. Each widget contains 14edges,
totaling 14jEjin all widgets. For each vertex u2V,g r a p h G0has degree .u//NUL1
edges going between widgets, so that summed over all vertices in V,
X
u2V.degree .u//NUL1/D2jEj/NULjVj
edges go between widgets. Finally, G0has two edges for each pair consisting of a
selector vertex and a vertex of V, totaling 2kjVjsuch edges. The total number of
edges of G0is therefore
jE0jD.14jEj/C.2jEj/NULjVj/C.2kjVj/
D16jEjC.2k/NUL1/jVj
/DC416jEjC.2jVj/NUL1/jVj:
Now we show that the transformation from graph GtoG0is a reduction. That is,
we must show that Ghas a vertex cover of size kif and only if G0has a hamiltonian
cycle.34.5 NP-complete problems 1095
Suppose that GD.V; E/ h a sav e r t e xc o v e r V/ETX/DC2Vof size k.L e t
V/ETXDfu1;u2;:::;u kg. As Figure 34.17 shows, we form a hamiltonian cy-
cle in G0by including the following edges10for each vertex uj2V/ETX. Include
edges˚
.Œuj;u.i/
j; 6/c141; Œu j;u.iC1/
j;1 /c141 /W1/DC4i/DC4degree .uj//NUL1/TAB
, which connect all
widgets corresponding to edges incident on uj. We also include the edges within
these widgets as Figures 34.16(b)–(d) show, depending on whether the edge is cov-ered by one or two vertices in V
/ETX. The hamiltonian cycle also includes the edges
f.sj;Œ uj;u.1/
j;1 /c141 /W1/DC4j/DC4kg
[f.sjC1;Œ uj;u.degree .uj//
j ;6 /c141 /W1/DC4j/DC4k/NUL1g
[f.s1;Œ uk;u.degree .uk//
k;6 /c141 /g:
By inspecting Figure 34.17, you can verify that these edges form a cycle. The cycle
starts at s1, visits all widgets corresponding to edges incident on u1, then visits s2,
visits all widgets corresponding to edges incident on u2, and so on, until it returns
tos1. The cycle visits each widget either once or twice, depending on whether one
or two vertices of V/ETXcover its corresponding edge. Because V/ETXis a vertex cover
forG, each edge in Eis incident on some vertex in V/ETX, and so the cycle visits each
vertex in each widget of G0. Because the cycle also visits every selector vertex, it
is hamiltonian.
Conversely, suppose that G0D.V0;E0/has a hamiltonian cycle C/DC2E0.W e
claim that the set
V/ETXDfu2VW.sj;Œ u ;u.1/;1 /c141 /2Cfor some 1/DC4j/DC4kg (34.4)
is a vertex cover for G. To see why, partition Cinto maximal paths that start at
some selector vertex si, traverse an edge .si;Œ u ;u.1/;1 /c141 /for some u2V,a n de n d
at a selector vertex sjwithout passing through any other selector vertex. Let us call
each such path a “cover path.” From how G0is constructed, each cover path must
start at some si, take the edge .si;Œ u ;u.1/;1 /c141 /for some vertex u2V, pass through
all the widgets corresponding to edges in Eincident on u, and then end at some
selector vertex sj. We refer to this cover path as pu, and by equation (34.4), we
putuintoV/ETX. Each widget visited by pumust be Wu/ETBorW/ETBufor some /ETB2V.
For each widget visited by pu, its vertices are visited by either one or two cover
paths. If they are visited by one cover path, then edge .u; /ETB/2Eis covered in G
by vertex u. If two cover paths visit the widget, then the other cover path must
bep/ETB, which implies that /ETB2V/ETX, and edge .u; /ETB/2Eis covered by both uand/ETB.
10Technically, we deﬁne a cycle in terms of vertices rather than edges (see Section B.4). In the
interest of clarity, we abuse notation here and deﬁne the hamiltonian cycle in terms of edges.1096 Chapter 34 NP-Completeness
u v
x w4
2 3
51
1
Figure 34.18 An instance of the traveling-salesman problem. Shaded edges represent a minimum-
cost tour, with cost 7.
Because each vertex in each widget is visited by some cover path, we see that each
edge in Eis covered by some vertex in V/ETX.
34.5.4 The traveling-salesman problem
In the traveling-salesman problem , which is closely related to the hamiltonian-
cycle problem, a salesman must visit ncities. Modeling the problem as a complete
graph with nvertices, we can say that the salesman wishes to make a tour,o r
hamiltonian cycle, visiting each city exactly once and ﬁnishing at the city he startsfrom. The salesman incurs a nonnegative integer cost c.i;j/ to travel from city i
to city j, and the salesman wishes to make the tour whose total cost is minimum,
where the total cost is the sum of the individual costs along the edges of the tour.For example, in Figure 34.18, a minimum-cost tour is hu; w; /ETB; x; ui, with cost 7.
The formal language for the corresponding decision problem is
TSPDfhG; c; kiWGD.V; E/ is a complete graph ;
cis a function from V/STXV!Z;
k2Z,a n d
Ghas a traveling-salesman tour with cost at most kg:
The following theorem shows that a fast algorithm for the traveling-salesman
problem is unlikely to exist.
Theorem 34.14
The traveling-salesman problem is NP-complete.
Proof We ﬁrst show that TSP belongs to NP. Given an instance of the problem,
we use as a certiﬁcate the sequence of nvertices in the tour. The veriﬁcation
algorithm checks that this sequence contains each vertex exactly once, sums up the
edge costs, and checks whether the sum is at most k. This process can certainly be
done in polynomial time.34.5 NP-complete problems 1097
To prove that TSP is NP-hard, we show that HAM-CYCLE /DC4PTSP. Let
GD.V; E/ be an instance of HAM-CYCLE. We construct an instance of TSP as
follows. We form the complete graph G0D.V; E0/,w h e r e E0Df.i; j /Wi;j2V
andi¤jg, and we deﬁne the cost function cby
c.i;j/D(
0if.i; j /2E;
1if.i; j /62E:
(Note that because Gis undirected, it has no self-loops, and so c./ETB;/ETB/D1for all
vertices /ETB2V.) The instance of TSP is then hG0;c;0i, which we can easily create
in polynomial time.
We now show that graph Ghas a hamiltonian cycle if and only if graph G0has a
tour of cost at most 0. Suppose that graph Ghas a hamiltonian cycle h. Each edge
inhbelongs to Eand thus has cost 0inG0. Thus, his a tour in G0with cost 0.
Conversely, suppose that graph G0has a tour h0of cost at most 0. Since the costs
of the edges in E0are0and1, the cost of tour h0is exactly 0and each edge on the
tour must have cost 0. Therefore, h0contains only edges in E. We conclude that h0
is a hamiltonian cycle in graph G.
34.5.5 The subset-sum problem
We next consider an arithmetic NP-complete problem. In the subset-sum problem ,
we are given a ﬁnite set Sof positive integers and an integer target t>0 . We ask
whether there exists a subset S0/DC2Swhose elements sum to t. For example,
ifSDf1; 2; 7; 14; 49; 98; 343; 686; 2409; 2793; 16808; 17206; 117705; 117993 g
andtD138457 , then the subset S0Df1; 2; 7; 98; 343; 686; 2409; 17206; 117705 g
is a solution.
As usual, we deﬁne the problem as a language:
SUBSET-SUMDfhS;tiWthere exists a subset S0/DC2Ssuch that tDP
s2S0sg:
As with any arithmetic problem, it is important to recall that our standard encoding
assumes that the input integers are coded in binary. With this assumption in mind,
we can show that the subset-sum problem is unlikely to have a fast algorithm.
Theorem 34.15
The subset-sum problem is NP-complete.
Proof To show that SUBSET-SUM is in NP, for an instance hS;tiof the problem,
we let the subset S0be the certiﬁcate. A veriﬁcation algorithm can check whether
tDP
s2S0sin polynomial time.
We now show that 3-CNF-SAT /DC4PSUBSET-SUM. Given a 3-CNF formula /RS
over variables x1;x2;:::;x nwith clauses C1;C2;:::;C k, each containing exactly1098 Chapter 34 NP-Completeness
three distinct literals, the reduction algorithm constructs an instance hS; tiof the
subset-sum problem such that /RSis satisﬁable if and only if there exists a subset
ofSwhose sum is exactly t. Without loss of generality, we make two simplifying
assumptions about the formula /RS. First, no clause contains both a variable and its
negation, for such a clause is automatically satisﬁed by any assignment of valuesto the variables. Second, each variable appears in at least one clause, because itdoes not matter what value is assigned to a variable that appears in no clauses.
The reduction creates two numbers in set Sfor each variable x
iand two numbers
inSfor each clause Cj. We shall create numbers in base 10, where each number
contains nCkdigits and each digit corresponds to either one variable or one clause.
Base 10(and other bases, as we shall see) has the property we need of preventing
carries from lower digits to higher digits.
As Figure 34.19 shows, we construct set Sand target tas follows. We label
each digit position by either a variable or a clause. The least signiﬁcant kdigits are
labeled by the clauses, and the most signiﬁcant ndigits are labeled by variables.
/SIThe target thas a 1in each digit labeled by a variable and a 4in each digit
labeled by a clause.
/SIFor each variable xi, setScontains two integers /ETBiand/ETB0
i. Each of /ETBiand/ETB0
i
has a 1in the digit labeled by xiand0s in the other variable digits. If literal xi
appears in clause Cj, then the digit labeled by Cjin/ETBicontains a 1. If lit-
eral:xiappears in clause Cj, then the digit labeled by Cjin/ETB0
icontains a 1.
All other digits labeled by clauses in /ETBiand/ETB0
iare0.
All/ETBiand/ETB0
ivalues in set Sare unique. Why? For l¤i,n o/ETBlor/ETB0
lvalues can
equal /ETBiand/ETB0
iin the most signiﬁcant ndigits. Furthermore, by our simplifying
assumptions above, no /ETBiand/ETB0
ican be equal in all kleast signiﬁcant digits.
If/ETBiand/ETB0
iwere equal, then xiand:xiwould have to appear in exactly the
same set of clauses. But we assume that no clause contains both xiand:xi
and that either xior:xiappears in some clause, and so there must be some
clause Cjfor which /ETBiand/ETB0
idiffer.
/SIFor each clause Cj, setScontains two integers sjands0
j. Each of sjands0
jhas
0s in all digits other than the one labeled by Cj.F o r sj, there is a 1in the Cj
digit, and s0
jhas a 2in this digit. These integers are “slack variables,” which we
use to get each clause-labeled digit position to add to the target value of 4.
Simple inspection of Figure 34.19 demonstrates that all sjands0
jvalues in S
are unique in set S.
Note that the greatest sum of digits in any one digit position is 6, which occurs in
the digits labeled by clauses (three 1s from the /ETBiand/ETB0
ivalues, plus 1and2from34.5 NP-complete problems 1099
=1001001
=1000110=0100001=0101110=0010011=0011100=0001000=0002000=0000100=0000200=0000010=0000020=0000001=0000002
=1114444x1x2x3C1C2C3C4
/ETB1
/ETB0
1
/ETB2
/ETB0
2
/ETB3
/ETB0
3
s1
s0
1
s2
s0
2
s3
s0
3
s4
s0
4
t
Figure 34.19 The reduction of 3-CNF-SAT to SUBSET-SUM. The formula in 3-CNF is /RSD
C1^C2^C3^C4,w h e r e C1D.x1_:x2_:x3/,C2D.:x1_:x2_:x3/,C3D.:x1_:x2_x3/,
andC4D.x1_x2_x3/. A satisfying assignment of /RSishx1D0; x2D0; x3D1i.T h e s e t S
produced by the reduction consists of the base- 10numbers shown; reading from top to bottom, SD
f1001001; 1000110; 100001; 101110; 10011; 11100; 1000; 2000; 100; 200; 10; 20; 1; 2 g.T h et a r g e t t
is1114444 . The subset S0/DC2Sis lightly shaded, and it contains /ETB0
1,/ETB0
2,a n d /ETB3, corresponding to the
satisfying assignment. It also contains slack variables s1,s0
1,s0
2,s3,s4,a n d s0
4to achieve the target
value of 4in the digits labeled by C1through C4.
thesjands0
jvalues). Interpreting these numbers in base 10, therefore, no carries
can occur from lower digits to higher digits.11
We can perform the reduction in polynomial time. The set Scontains 2nC2k
values, each of which has nCkdigits, and the time to produce each digit is poly-
nomial in nCk.T h et a r g e t thasnCkdigits, and the reduction produces each in
constant time.
We now show that the 3-CNF formula /RSis satisﬁable if and only if there exists
a subset S0/DC2Swhose sum is t. First, suppose that /RShas a satisfying assignment.
ForiD1 ;2;:::;n ,i fxiD1in this assignment, then include /ETBiinS0. Otherwise,
include /ETB0
i. In other words, we include in S0exactly the /ETBiand/ETB0
ivalues that cor-
11In fact, any base b,w h e r e b/NAK7, would work. The instance at the beginning of this subsection is
the set Sand target tin Figure 34.19 interpreted in base 7,w i t h Slisted in sorted order.1100 Chapter 34 NP-Completeness
respond to literals with the value 1in the satisfying assignment. Having included
either /ETBior/ETB0
i, but not both, for all i, and having put 0in the digits labeled by
variables in all sjands0
j, we see that for each variable-labeled digit, the sum of the
values of S0must be 1, which matches those digits of the target t. Because each
clause is satisﬁed, the clause contains some literal with the value 1. Therefore,
each digit labeled by a clause has at least one 1contributed to its sum by a /ETBior/ETB0
i
value in S0. In fact, 1,2,o r3literals may be 1in each clause, and so each clause-
labeled digit has a sum of 1,2,o r3from the /ETBiand/ETB0
ivalues in S0. In Figure 34.19
for example, literals :x1,:x2,a n d x3have the value 1in a satisfying assignment.
Each of clauses C1andC4contains exactly one of these literals, and so together /ETB0
1,
/ETB0
2,a n d /ETB3contribute 1to the sum in the digits for C1andC4.C l a u s e C2contains
two of these literals, and /ETB0
1,/ETB0
2,a n d /ETB3contribute 2to the sum in the digit for C2.
Clause C3contains all three of these literals, and /ETB0
1,/ETB0
2,a n d /ETB3contribute 3to the
sum in the digit for C3. We achieve the target of 4in each digit labeled by clause Cj
by including in S0the appropriate nonempty subset of slack variables fsj;s0
jg.I n
Figure 34.19, S0includes s1,s0
1,s0
2,s3,s4,a n d s0
4. Since we have matched the target
in all digits of the sum, and no carries can occur, the values of S0sum to t.
Now, suppose that there is a subset S0/DC2Sthat sums to t. The subset S0must
include exactly one of /ETBiand/ETB0
ifor each iD1 ;2;:::;n , for otherwise the digits
labeled by variables would not sum to 1.I f/ETBi2S0, we set xiD1. Otherwise,
/ETB0
i2S0, and we set xiD0. We claim that every clause Cj,f o rjD1 ;2;:::;k ,i s
satisﬁed by this assignment. To prove this claim, note that to achieve a sum of 4in
the digit labeled by Cj, the subset S0must include at least one /ETBior/ETB0
ivalue that
has a 1 in the digit labeled by Cj, since the contributions of the slack variables sj
ands0
jtogether sum to at most 3.I fS0includes a /ETBithat has a 1 in Cj’s position,
then the literal xiappears in clause Cj. Since we have set xiD1when /ETBi2S0,
clause Cjis satisﬁed. If S0includes a /ETB0
ithat has a 1 in that position, then the
literal:xiappears in Cj. Since we have set xiD0when /ETB0
i2S0,c l a u s e Cjis
again satisﬁed. Thus, all clauses of /RSare satisﬁed, which completes the proof.
Exercises
34.5-1
Thesubgraph-isomorphism problem takes two undirected graphs G1andG2,a n d
it asks whether G1is isomorphic to a subgraph of G2. Show that the subgraph-
isomorphism problem is NP-complete.
34.5-2
G i v e na ni n t e g e r m/STXnmatrix Aand an integer m-vector b,t h e0-1 integer-
programming problem asks whether there exists an integer n-vector xwith ele-Problems for Chapter 34 1101
ments in the setf0; 1gsuch that Ax/DC4b. Prove that 0-1 integer programming is
NP-complete. ( Hint: Reduce from 3-CNF-SAT.)
34.5-3
Theinteger linear-programming problem is like the 0-1 integer-programming
problem given in Exercise 34.5-2, except that the values of the vector xmay be
any integers rather than just 0or1. Assuming that the 0-1 integer-programming
problem is NP-hard, show that the integer linear-programming problem is NP-complete.
34.5-4
Show how to solve the subset-sum problem in polynomial time if the target value t
is expressed in unary.
34.5-5
Theset-partition problem takes as input a set Sof numbers. The question is
whether the numbers can be partitioned into two sets Aand
ADS/NULAsuch
thatP
x2AxDP
x2
Ax. Show that the set-partition problem is NP-complete.
34.5-6
Show that the hamiltonian-path problem is NP-complete.
34.5-7
Thelongest-simple-cycle problem is the problem of determining a simple cycle
(no repeated vertices) of maximum length in a graph. Formulate a related decisionproblem, and show that the decision problem is NP-complete.
34.5-8
In the half 3-CNF satisﬁability problem, we are given a 3-CNF formula /RSwithn
variables and mclauses, where mis even. We wish to determine whether there
exists a truth assignment to the variables of /RSsuch that exactly half the clauses
evaluate to 0and exactly half the clauses evaluate to 1. Prove that the half 3-CNF
satisﬁability problem is NP-complete.
Problems
34-1 Independent set
Anindependent set of a graph GD.V; E/ i sas u b s e t V0/DC2Vof vertices such
that each edge in Eis incident on at most one vertex in V0.T h e independent-set
problem is to ﬁnd a maximum-size independent set in G.1102 Chapter 34 NP-Completeness
a.Formulate a related decision problem for the independent-set problem, and
prove that it is NP-complete. ( Hint: Reduce from the clique problem.)
b.Suppose that you are given a “black-box” subroutine to solve the decision prob-
lem you deﬁned in part (a). Give an algorithm to ﬁnd an independent set of max-imum size. The running time of your algorithm should be polynomial in jVj
andjEj, counting queries to the black box as a single step.
Although the independent-set decision problem is NP-complete, certain special
cases are polynomial-time solvable.
c.Give an efﬁcient algorithm to solve the independent-set problem when each ver-
tex in Ghas degree 2. Analyze the running time, and prove that your algorithm
works correctly.
d.Give an efﬁcient algorithm to solve the independent-set problem when Gis
bipartite. Analyze the running time, and prove that your algorithm works cor-rectly. ( Hint: Use the results of Section 26.3.)
34-2 Bonnie and Clyde
Bonnie and Clyde have just robbed a bank. They have a bag of money and want
to divide it up. For each of the following scenarios, either give a polynomial-timealgorithm, or prove that the problem is NP-complete. The input in each case is al i s to ft h e nitems in the bag, along with the value of each.
a.The bag contains ncoins, but only 2different denominations: some coins are
worth xdollars, and some are worth ydollars. Bonnie and Clyde wish to divide
the money exactly evenly.
b.The bag contains ncoins, with an arbitrary number of different denominations,
but each denomination is a nonnegative integer power of 2, i.e., the possible
denominations are 1dollar, 2dollars, 4dollars, etc. Bonnie and Clyde wish to
divide the money exactly evenly.
c.The bag contains nchecks, which are, in an amazing coincidence, made out to
“Bonnie or Clyde.” They wish to divide the checks so that they each get theexact same amount of money.
d.The bag contains nchecks as in part (c), but this time Bonnie and Clyde are
willing to accept a split in which the difference is no larger than 100dollars.Problems for Chapter 34 1103
34-3 Graph coloring
Mapmakers try to use as few colors as possible when coloring countries on a map,as long as no two countries that share a border have the same color. We can modelthis problem with an undirected graph GD.V; E/ in which each vertex repre-
sents a country and vertices whose respective countries share a border are adjacent.Then, a k-coloring is a function cWV!f1 ;2;:::;kgsuch that c.u/¤c./ETB/ for
every edge .u; /ETB/2E. In other words, the numbers 1 ;2;:::;k represent the kcol-
ors, and adjacent vertices must have different colors. The graph-coloring problem
is to determine the minimum number of colors needed to color a given graph.
a.Give an efﬁcient algorithm to determine a 2-coloring of a graph, if one exists.
b.Cast the graph-coloring problem as a decision problem. Show that your deci-
sion problem is solvable in polynomial time if and only if the graph-coloringproblem is solvable in polynomial time.
c.Let the language 3-COLOR be the set of graphs that can be 3-colored. Show
that if 3-COLOR is NP-complete, then your decision problem from part (b) isNP-complete.
To prove that 3-COLOR is NP-complete, we use a reduction from 3-CNF-SAT.
Given a formula /RSofmclauses on nvariables x
1,x2,..., xn, we construct a graph
GD.V; E/ as follows. The set Vconsists of a vertex for each variable, a vertex
for the negation of each variable, 5vertices for each clause, and 3special vertices:
TRUE ,FALSE ,a n d RED. The edges of the graph are of two types: “literal” edges
that are independent of the clauses and “clause” edges that depend on the clauses.The literal edges form a triangle on the special vertices and also form a triangle onx
i,:xi,a n d RED foriD1 ;2;:::;n .
d.Argue that in any 3-coloring cof a graph containing the literal edges, exactly
one of a variable and its negation is colored c.TRUE /and the other is colored
c.FALSE /. Argue that for any truth assignment for /RS, there exists a 3-coloring
of the graph containing just the literal edges.
The widget shown in Figure 34.20 helps to enforce the condition corresponding to
ac l a u s e .x_y_´/. Each clause requires a unique copy of the 5vertices that are
heavily shaded in the ﬁgure; they connect as shown to the literals of the clause andthe special vertex
TRUE .
e.Argue that if each of x,y,a n d ´is colored c.TRUE /orc.FALSE /, then the
widget is 3-colorable if and only if at least one of x,y,o r´is colored c.TRUE /.
f.Complete the proof that 3-COLOR is NP-complete.1104 Chapter 34 NP-Completeness
x
y
zTRUE
Figure 34.20 The widget corresponding to a clause .x_y_´/, used in Problem 34-3.
34-4 Scheduling with proﬁts and deadlines
Suppose that we have one machine and a set of ntasks a1;a2;:::;a n, each of
which requires time on the machine. Each task ajrequires tjtime units on the
machine (its processing time), yields a proﬁt of pj, and has a deadline dj.T h e
machine can process only one task at a time, and task ajmust run without inter-
ruption for tjconsecutive time units. If we complete task ajby its deadline dj,w e
receive a proﬁt pj, but if we complete it after its deadline, we receive no proﬁt. As
an optimization problem, we are given the processing times, proﬁts, and deadlinesfor a set of ntasks, and we wish to ﬁnd a schedule that completes all the tasks and
returns the greatest amount of proﬁt. The processing times, proﬁts, and deadlinesare all nonnegative numbers.
a.State this problem as a decision problem.
b.Show that the decision problem is NP-complete.
c.Give a polynomial-time algorithm for the decision problem, assuming that all
processing times are integers from 1ton.(Hint: Use dynamic programming.)
d.Give a polynomial-time algorithm for the optimization problem, assuming that
all processing times are integers from 1ton.
Chapter notes
The book by Garey and Johnson [129] provides a wonderful guide to NP-complete-
ness, discussing the theory at length and providing a catalogue of many problems
that were known to be NP-complete in 1979. The proof of Theorem 34.13 isadapted from their book, and the list of NP-complete problem domains at the begin-ning of Section 34.5 is drawn from their table of contents. Johnson wrote a seriesNotes for Chapter 34 1105
of 23 columns in the Journal of Algorithms between 1981 and 1992 reporting new
developments in NP-completeness. Hopcroft, Motwani, and Ullman [177], Lewisand Papadimitriou [236], Papadimitriou [270], and Sipser [317] have good treat-ments of NP-completeness in the context of complexity theory. NP-completenessand several reductions also appear in books by Aho, Hopcroft, and Ullman [5];Dasgupta, Papadimitriou, and Vazirani [82]; Johnsonbaugh and Schaefer [193];and Kleinberg and Tardos [208].
The class P was introduced in 1964 by Cobham [72] and, independently, in 1965
by Edmonds [100], who also introduced the class NP and conjectured that P ¤NP.
The notion of NP-completeness was proposed in 1971 by Cook [75], who gavethe ﬁrst NP-completeness proofs for formula satisﬁability and 3-CNF satisﬁabil-ity. Levin [234] independently discovered the notion, giving an NP-completenessproof for a tiling problem. Karp [199] introduced the methodology of reductionsin 1972 and demonstrated the rich variety of NP-complete problems. Karp’s pa-per included the original NP-completeness proofs of the clique, vertex-cover, andhamiltonian-cycle problems. Since then, thousands of problems have been provento be NP-complete by many researchers. In a talk at a meeting celebrating Karp’s60th birthday in 1995, Papadimitriou remarked, “about 6000 papers each year havethe term ‘NP-complete’ on their title, abstract, or list of keywords. This is morethan each of the terms ‘compiler,’ ‘database,’ ‘expert,’ ‘neural network,’ or ‘oper-ating system.’ ”
Recent work in complexity theory has shed light on the complexity of computing
approximate solutions. This work gives a new deﬁnition of NP using “probabilis-
tically checkable proofs.” This new deﬁnition implies that for problems such asclique, vertex cover, the traveling-salesman problem with the triangle inequality,and many others, computing good approximate solutions is NP-hard and hence noeasier than computing optimal solutions. An introduction to this area can be foundin Arora’s thesis [20]; a chapter by Arora and Lund in Hochbaum [172]; a surveyarticle by Arora [21]; a book edited by Mayr, Pr¨ omel, and Steger [246]; and a
survey article by Johnson [191].35 Approximation Algorithms
Many problems of practical signiﬁcance are NP-complete, yet they are too impor-
tant to abandon merely because we don’t know how to ﬁnd an optimal solution inpolynomial time. Even if a problem is NP-complete, there may be hope. We have atleast three ways to get around NP-completeness. First, if the actual inputs are small,an algorithm with exponential running time may be perfectly satisfactory. Second,we may be able to isolate important special cases that we can solve in polynomialtime. Third, we might come up with approaches to ﬁnd near-optimal solutions in
polynomial time (either in the worst case or the expected case). In practice, near-
optimality is often good enough. We call an algorithm that returns near-optimal
solutions an approximation algorithm . This chapter presents polynomial-time ap-
proximation algorithms for several NP-complete problems.
Performance ratios for approximation algorithms
Suppose that we are working on an optimization problem in which each potential
solution has a positive cost, and we wish to ﬁnd a near-optimal solution. Dependingon the problem, we may deﬁne an optimal solution as one with maximum possi-ble cost or one with minimum possible cost; that is, the problem may be either amaximization or a minimization problem.
We say that an algorithm for a problem has an approximation ratio of/SUB.n/ if,
for any input of size n, the cost Cof the solution produced by the algorithm is
within a factor of /SUB.n/ of the cost C
/ETXof an optimal solution:
max/DC2C
C/ETX;C/ETX
C/DC3
/DC4/SUB.n/ : (35.1)
If an algorithm achieves an approximation ratio of /SUB.n/, we call it a /SUB.n/-approx-
imation algorithm . The deﬁnitions of the approximation ratio and of a /SUB.n/-
approximation algorithm apply to both minimization and maximization problems.For a maximization problem, 0<C/DC4C
/ETX, and the ratio C/ETX=Cgives the factor
by which the cost of an optimal solution is larger than the cost of the approximateChapter 35 Approximation Algorithms 1107
solution. Similarly, for a minimization problem, 0<C/ETX/DC4C, and the ratio C=C/ETX
gives the factor by which the cost of the approximate solution is larger than the
cost of an optimal solution. Because we assume that all solutions have positivecost, these ratios are always well deﬁned. The approximation ratio of an approx-imation algorithm is never less than 1,s i n c e C=C
/ETX/DC41implies C/ETX=C/NAK1.
Therefore, a 1-approximation algorithm1produces an optimal solution, and an ap-
proximation algorithm with a large approximation ratio may return a solution thatis much worse than optimal.
For many problems, we have polynomial-time approximation algorithms with
small constant approximation ratios, although for other problems, the best knownpolynomial-time approximation algorithms have approximation ratios that growas functions of the input size n. An example of such a problem is the set-cover
problem presented in Section 35.3.
Some NP-complete problems allow polynomial-time approximation algorithms
that can achieve increasingly better approximation ratios by using more and more
computation time. That is, we can trade computation time for the quality of the
approximation. An example is the subset-sum problem studied in Section 35.5.
This situation is important enough to deserve a name of its own.
Anapproximation scheme for an optimization problem is an approximation al-
gorithm that takes as input not only an instance of the problem, but also a value/SI>0 such that for any ﬁxed /SI, the scheme is a .1C/SI/-approximation algorithm.
We say that an approximation scheme is a polynomial-time approximation scheme
if for any ﬁxed /SI>0 , the scheme runs in time polynomial in the size nof its input
instance.
The running time of a polynomial-time approximation scheme can increase very
rapidly as /SIdecreases. For example, the running time of a polynomial-time ap-
proximation scheme might be O.n
2=/SI/. Ideally, if /SIdecreases by a constant factor,
the running time to achieve the desired approximation should not increase by morethan a constant factor (though not necessarily the same constant factor by which /SI
decreased).
We say that an approximation scheme is a fully polynomial-time approximation
scheme if it is an approximation scheme and its running time is polynomial in
both1=/SIand the size nof the input instance. For example, the scheme might have
a running time of O..1=/SI/
2n3/. With such a scheme, any constant-factor decrease
in/SIcomes with a corresponding constant-factor increase in the running time.
1When the approximation ratio is independent of n, we use the terms “approximation ratio of /SUB”a n d
“/SUB-approximation algorithm,” indicating no dependence on n.1108 Chapter 35 Approximation Algorithms
Chapter outline
The ﬁrst four sections of this chapter present some examples of polynomial-time
approximation algorithms for NP-complete problems, and the ﬁfth section presentsa fully polynomial-time approximation scheme. Section 35.1 begins with a studyof the vertex-cover problem, an NP-complete minimization problem that has anapproximation algorithm with an approximation ratio of 2. Section 35.2 presents
an approximation algorithm with an approximation ratio of 2for the case of the
traveling-salesman problem in which the cost function satisﬁes the triangle in-equality. It also shows that without the triangle inequality, for any constant /SUB/NAK1,
a/SUB-approximation algorithm cannot exist unless P DNP. In Section 35.3, we
show how to use a greedy method as an effective approximation algorithm for theset-covering problem, obtaining a covering whose cost is at worst a logarithmicfactor larger than the optimal cost. Section 35.4 presents two more approximationalgorithms. First we study the optimization version of 3-CNF satisﬁability and
give a simple randomized algorithm that produces a solution with an expected ap-
proximation ratio of 8=7. Then we examine a weighted variant of the vertex-cover
problem and show how to use linear programming to develop a 2-approximation
algorithm. Finally, Section 35.5 presents a fully polynomial-time approximation
scheme for the subset-sum problem.
35.1 The vertex-cover problem
Section 34.5.2 deﬁned the vertex-cover problem and proved it NP-complete. Recallthat a vertex cover of an undirected graph GD.V; E/ i sas u b s e t V
0/DC2Vsuch
that if .u; /ETB/ is an edge of G, then either u2V0or/ETB2V0(or both). The size of a
vertex cover is the number of vertices in it.
Thevertex-cover problem is to ﬁnd a vertex cover of minimum size in a given
undirected graph. We call such a vertex cover an optimal vertex cover . This prob-
lem is the optimization version of an NP-complete decision problem.
Even though we don’t know how to ﬁnd an optimal vertex cover in a graph G
in polynomial time, we can efﬁciently ﬁnd a vertex cover that is near-optimal.The following approximation algorithm takes as input an undirected graph Gand
returns a vertex cover whose size is guaranteed to be no more than twice the sizeof an optimal vertex cover.35.1 The vertex-cover problem 1109
bcd
ae fg
(a)bcd
ae fg
(b)
bcd
ae fg
(c)bcd
ae fg
(d)
bcd
ae fg
(e)bcd
ae fg
(f)
Figure 35.1 The operation of A PPROX -VERTEX -COVER .(a)The input graph G, which has 7
vertices and 8 edges. (b)The edge .b; c/ , shown heavy, is the ﬁrst edge chosen by A PPROX -VERTEX -
COVER . Vertices bandc, shown lightly shaded, are added to the set Ccontaining the vertex cover
being created. Edges .a; b/ ,.c; e/ ,a n d.c; d/ , shown dashed, are removed since they are now covered
by some vertex in C.(c)Edge .e; f / is chosen; vertices eandfare added to C.(d)Edge .d; g/
is chosen; vertices dandgare added to C.(e)The set C, which is the vertex cover produced by
APPROX -VERTEX -COVER , contains the six vertices b;c; d;e; f;g .(f)The optimal vertex cover for
this problem contains only three vertices: b,d,a n d e.
APPROX -VERTEX -COVER .G/
1CD;
2E0DG:E
3while E0¤;
4l e t .u; /ETB/ be an arbitrary edge of E0
5 CDC[fu; /ETBg
6 remove from E0every edge incident on either uor/ETB
7return C
Figure 35.1 illustrates how A PPROX -VERTEX -COVER operates on an example
graph. The variable Ccontains the vertex cover being constructed. Line 1 ini-
tializes Cto the empty set. Line 2 sets E0to be a copy of the edge set G:Eof
the graph. The loop of lines 3–6 repeatedly picks an edge .u; /ETB/ from E0, adds its1110 Chapter 35 Approximation Algorithms
endpoints uand/ETBtoC, and deletes all edges in E0that are covered by either u
or/ETB. Finally, line 7 returns the vertex cover C. The running time of this algorithm
isO.VCE/, using adjacency lists to represent E0.
Theorem 35.1
APPROX -VERTEX -COVER is a polynomial-time 2-approximation algorithm.
Proof We have already shown that A PPROX -VERTEX -COVER runs in polyno-
mial time.
The set Cof vertices that is returned by A PPROX -VERTEX -COVER is a vertex
cover, since the algorithm loops until every edge in G:Ehas been covered by some
vertex in C.
To see that A PPROX -VERTEX -COVER returns a vertex cover that is at most twice
the size of an optimal cover, let Adenote the set of edges that line 4 of A PPROX -
VERTEX -COVER picked. In order to cover the edges in A, any vertex cover—in
particular, an optimal cover C/ETX—must include at least one endpoint of each edge
inA. No two edges in Ashare an endpoint, since once an edge is picked in line 4,
all other edges that are incident on its endpoints are deleted from E0in line 6. Thus,
no two edges in Aare covered by the same vertex from C/ETX, and we have the lower
bound
jC/ETXj/NAKjAj (35.2)
on the size of an optimal vertex cover. Each execution of line 4 picks an edge for
which neither of its endpoints is already in C, yielding an upper bound (an exact
upper bound, in fact) on the size of the vertex cover returned:
jCjD2jAj: (35.3)
Combining equations (35.2) and (35.3), we obtain
jCjD2jAj
/DC42jC/ETXj;
thereby proving the theorem.
Let us reﬂect on this proof. At ﬁrst, you might wonder how we can possibly
prove that the size of the vertex cover returned by A PPROX -VERTEX -COVER is at
most twice the size of an optimal vertex cover, when we do not even know the sizeof an optimal vertex cover. Instead of requiring that we know the exact size of anoptimal vertex cover, we rely on a lower bound on the size. As Exercise 35.1-2 asksyou to show, the set Aof edges that line 4 of A
PPROX -VERTEX -COVER selects is
actually a maximal matching in the graph G.( Amaximal matching is a matching
that is not a proper subset of any other matching.) The size of a maximal matching35.2 The traveling-salesman problem 1111
is, as we argued in the proof of Theorem 35.1, a lower bound on the size of an
optimal vertex cover. The algorithm returns a vertex cover whose size is at mosttwice the size of the maximal matching A. By relating the size of the solution
returned to the lower bound, we obtain our approximation ratio. We will use thismethodology in later sections as well.
Exercises
35.1-1
Give an example of a graph for which A
PPROX -VERTEX -COVER always yields a
suboptimal solution.
35.1-2
Prove that the set of edges picked in line 4 of A PPROX -VERTEX -COVER forms a
maximal matching in the graph G.
35.1-3 ?
Professor B¨ undchen proposes the following heuristic to solve the vertex-cover
problem. Repeatedly select a vertex of highest degree, and remove all of its in-cident edges. Give an example to show that the professor’s heuristic does not have
an approximation ratio of 2.(Hint: Try a bipartite graph with vertices of uniform
degree on the left and vertices of varying degree on the right.)
35.1-4
Give an efﬁcient greedy algorithm that ﬁnds an optimal vertex cover for a tree inlinear time.
35.1-5
From the proof of Theorem 34.12, we know that the vertex-cover problem and theNP-complete clique problem are complementary in the sense that an optimal vertexcover is the complement of a maximum-size clique in the complement graph. Doesthis relationship imply that there is a polynomial-time approximation algorithmwith a constant approximation ratio for the clique problem? Justify your answer.
35.2 The traveling-salesman problem
In the traveling-salesman problem introduced in Section 34.5.4, we are given acomplete undirected graph GD.V; E/ that has a nonnegative integer cost c.u;/ETB/
associated with each edge .u; /ETB/2E, and we must ﬁnd a hamiltonian cycle (a
tour) of Gwith minimum cost. As an extension of our notation, let c.A/ denote
the total cost of the edges in the subset A/DC2E:1112 Chapter 35 Approximation Algorithms
c.A/DX
.u;/ETB/ 2Ac.u;/ETB/ :
In many practical situations, the least costly way to go from a place uto a place w
is to go directly, with no intermediate steps. Put another way, cutting out an inter-
mediate stop never increases the cost. We formalize this notion by saying that thecost function csatisﬁes the triangle inequality if, for all vertices u; /ETB; w2V,
c.u;w//DC4c.u;/ETB/Cc./ETB;w/ :
The triangle inequality seems as though it should naturally hold, and it is au-
tomatically satisﬁed in several applications. For example, if the vertices of thegraph are points in the plane and the cost of traveling between two vertices is theordinary euclidean distance between them, then the triangle inequality is satisﬁed.Furthermore, many cost functions other than euclidean distance satisfy the triangleinequality.
As Exercise 35.2-2 shows, the traveling-salesman problem is NP-complete even
if we require that the cost function satisfy the triangle inequality. Thus, we shouldnot expect to ﬁnd a polynomial-time algorithm for solving this problem exactly.Instead, we look for good approximation algorithms.
In Section 35.2.1, we examine a 2-approximation algorithm for the traveling-
salesman problem with the triangle inequality. In Section 35.2.2, we show that
without the triangle inequality, a polynomial-time approximation algorithm with a
constant approximation ratio does not exist unless P DNP.
35.2.1 The traveling-salesman problem with the triangle inequality
Applying the methodology of the previous section, we shall ﬁrst compute a struc-
ture—a minimum spanning tree—whose weight gives a lower bound on the length
of an optimal traveling-salesman tour. We shall then use the minimum spanning
tree to create a tour whose cost is no more than twice that of the minimum spanning
tree’s weight, as long as the cost function satisﬁes the triangle inequality. The fol-
lowing algorithm implements this approach, calling the minimum-spanning-tree
algorithm MST-P
RIM from Section 23.2 as a subroutine. The parameter Gis a
complete undirected graph, and the cost function csatisﬁes the triangle inequality.
APPROX -TSP-T OUR.G; c/
1 select a vertex r2G:Vto be a “root” vertex
2 compute a minimum spanning tree TforGfrom root r
using MST-P RIM. G ;c;r/
3l e t Hbe a list of vertices, ordered according to when they are ﬁrst visited
in a preorder tree walk of T
4return the hamiltonian cycle H35.2 The traveling-salesman problem 1113
(a)a d
b fe
g
c
h
(b)a d
b fe
g
c
h
(c)
a d
e
c
h
(d)a d
b fe
g
c
h
(e)b f ge
hca
b f gd
Figure 35.2 The operation of A PPROX -TSP-T OUR.(a)A complete undirected graph. Vertices lie
on intersections of integer grid lines. For example, fis one unit to the right and two units up from h.
The cost function between two points is the ordinary euclidean distance. (b)A minimum spanning
treeTof the complete graph, as computed by MST-P RIM.V e r t e x ais the root vertex. Only edges
in the minimum spanning tree are shown. The vertices happen to be labeled in such a way that they
are added to the main tree by MST-P RIM in alphabetical order. (c)Aw a l ko f T, starting at a.A
full walk of the tree visits the vertices in the order a; b;c; b;h; b;a;d; e;f;e; g;e;d; a . A preorder
walk of Tlists a vertex just when it is ﬁrst encountered, as indicated by the dot next to each vertex,
yielding the ordering a; b; c; h; d; e; f; g .(d)A tour obtained by visiting the vertices in the order
given by the preorder walk, which is the tour Hreturned by A PPROX -TSP-T OUR. Its total cost
is approximately 19:074 .(e)An optimal tour H/ETXfor the original complete graph. Its total cost is
approximately 14:715 .
Recall from Section 12.1 that a preorder tree walk recursively visits every vertex
in the tree, listing a vertex when it is ﬁrst encountered, before visiting any of its
children.
Figure 35.2 illustrates the operation of A PPROX -TSP-T OUR. Part (a) of the ﬁg-
ure shows a complete undirected graph, and part (b) shows the minimum spanningtreeTgrown from root vertex aby MST-P
RIM. Part (c) shows how a preorder
walk of Tvisits the vertices, and part (d) displays the corresponding tour, which is
the tour returned by A PPROX -TSP-T OUR. Part (e) displays an optimal tour, which
is about 23% shorter.1114 Chapter 35 Approximation Algorithms
By Exercise 23.2-2, even with a simple implementation of MST-P RIM, the run-
ning time of A PPROX -TSP-T OUR is‚.V2/. We now show that if the cost function
for an instance of the traveling-salesman problem satisﬁes the triangle inequality,then A
PPROX -TSP-T OUR returns a tour whose cost is not more than twice the cost
of an optimal tour.
Theorem 35.2
APPROX -TSP-T OUR is a polynomial-time 2-approximation algorithm for the
traveling-salesman problem with the triangle inequality.
Proof We have already seen that A PPROX -TSP-T OUR runs in polynomial time.
LetH/ETXdenote an optimal tour for the given set of vertices. We obtain a spanning
tree by deleting any edge from a tour, and each edge cost is nonnegative. Therefore,
the weight of the minimum spanning tree Tcomputed in line 2 of A PPROX -TSP-
TOUR provides a lower bound on the cost of an optimal tour:
c.T//DC4c.H/ETX/: (35.4)
Afull walk ofTlists the vertices when they are ﬁrst visited and also whenever
they are returned to after a visit to a subtree. Let us call this full walk W. The full
walk of our example gives the order
a;b;c;b;h;b;a;d;e;f;e;g; e;d; a :Since the full walk traverses every edge of Texactly twice, we have (extending
our deﬁnition of the cost cin the natural manner to handle multisets of edges)
c.W /D2c.T/ : (35.5)
Inequality (35.4) and equation (35.5) imply thatc.W //DC42c.H
/ETX/; (35.6)
and so the cost of Wis within a factor of 2of the cost of an optimal tour.
Unfortunately, the full walk Wis generally not a tour, since it visits some ver-
tices more than once. By the triangle inequality, however, we can delete a visit toany vertex from Wand the cost does not increase. (If we delete a vertex /ETBfrom W
between visits to uandw, the resulting ordering speciﬁes going directly from u
tow.) By repeatedly applying this operation, we can remove from Wall but the
ﬁrst visit to each vertex. In our example, this leaves the ordering
a;b;c;h;d;e;f;g :
This ordering is the same as that obtained by a preorder walk of the tree T.L e t H
be the cycle corresponding to this preorder walk. It is a hamiltonian cycle, since ev-35.2 The traveling-salesman problem 1115
ery vertex is visited exactly once, and in fact it is the cycle computed by A PPROX -
TSP-T OUR.S i n c e His obtained by deleting vertices from the full walk W,w e
have
c.H//DC4c.W / : (35.7)
Combining inequalities (35.6) and (35.7) gives c.H//DC42c.H/ETX/, which completes
the proof.
In spite of the nice approximation ratio provided by Theorem 35.2, A PPROX -
TSP-T OUR is usually not the best practical choice for this problem. There are other
approximation algorithms that typically perform much better in practice. (See thereferences at the end of this chapter.)
35.2.2 The general traveling-salesman problem
If we drop the assumption that the cost function csatisﬁes the triangle inequality,
then we cannot ﬁnd good approximate tours in polynomial time unless P DNP.
Theorem 35.3
If P¤NP, then for any constant /SUB/NAK1, there is no polynomial-time approximation
algorithm with approximation ratio /SUBfor the general traveling-salesman problem.
Proof The proof is by contradiction. Suppose to the contrary that for some num-
ber/SUB/NAK1, there is a polynomial-time approximation algorithm Awith approx-
imation ratio /SUB. Without loss of generality, we assume that /SUBis an integer, by
rounding it up if necessary. We shall then show how to use Ato solve instances
of the hamiltonian-cycle problem (deﬁned in Section 34.2) in polynomial time.Since Theorem 34.13 tells us that the hamiltonian-cycle problem is NP-complete,Theorem 34.4 implies that if we can solve it in polynomial time, then P DNP.
LetGD.V; E/ be an instance of the hamiltonian-cycle problem. We wish to
determine efﬁciently whether Gcontains a hamiltonian cycle by making use of
the hypothesized approximation algorithm A.W e t u r n Ginto an instance of the
traveling-salesman problem as follows. Let G
0D.V; E0/be the complete graph
onV;t h a ti s ,
E0Df.u; /ETB/Wu; /ETB2Vandu¤/ETBg:
Assign an integer cost to each edge in E0as follows:
c.u;/ETB/D(
1 if.u; /ETB/2E;
/SUBjVjC1otherwise :
We can create representations of G0andcfrom a representation of Gin time poly-
nomial injVjandjEj.1116 Chapter 35 Approximation Algorithms
Now, consider the traveling-salesman problem .G0;c/. If the original graph G
has a hamiltonian cycle H, then the cost function cassigns to each edge of Ha
cost of 1,a n ds o .G0;c/contains a tour of cost jVj. On the other hand, if Gdoes
not contain a hamiltonian cycle, then any tour of G0must use some edge not in E.
But any tour that uses an edge not in Ehas a cost of at least
./SUBjVjC1/C.jVj/NUL1/D/SUBjVjCjVj
>/SUBjVj:
Because edges not in Gare so costly, there is a gap of at least /SUBjVjbetween the cost
of a tour that is a hamiltonian cycle in G(costjVj) and the cost of any other tour
(cost at least /SUBjVjCjVj). Therefore, the cost of a tour that is not a hamiltonian
cycle in Gis at least a factor of /SUBC1greater than the cost of a tour that is a
hamiltonian cycle in G.
Now, suppose that we apply the approximation algorithm Ato the traveling-
salesman problem .G0;c/. Because Ais guaranteed to return a tour of cost no
more than /SUBtimes the cost of an optimal tour, if Gcontains a hamiltonian cycle,
thenAmust return it. If Ghas no hamiltonian cycle, then Areturns a tour of cost
more than /SUBjVj. Therefore, we can use Ato solve the hamiltonian-cycle problem
in polynomial time.
The proof of Theorem 35.3 serves as an example of a general technique for
proving that we cannot approximate a problem very well. Suppose that given anNP-hard problem X, we can produce in polynomial time a minimization prob-
lemYsuch that “yes” instances of Xcorrespond to instances of Ywith value at
most k(for some k), but that “no” instances of Xcorrespond to instances of Y
with value greater than /SUBk. Then, we have shown that, unless P DNP, there is no
polynomial-time /SUB-approximation algorithm for problem Y.
Exercises
35.2-1
Suppose that a complete undirected graph GD.V; E/ with at least 3vertices has
a cost function cthat satisﬁes the triangle inequality. Prove that c.u;/ETB//NAK0for all
u; /ETB2V.
35.2-2
Show how in polynomial time we can transform one instance of the traveling-
salesman problem into another instance whose cost function satisﬁes the triangleinequality. The two instances must have the same set of optimal tours. Explainwhy such a polynomial-time transformation does not contradict Theorem 35.3, as-suming that P¤NP.35.3 The set-covering problem 1117
35.2-3
Consider the following closest-point heuristic for building an approximate trav-
eling-salesman tour whose cost function satisﬁes the triangle inequality. Beginwith a trivial cycle consisting of a single arbitrarily chosen vertex. At each step,identify the vertex uthat is not on the cycle but whose distance to any vertex on the
cycle is minimum. Suppose that the vertex on the cycle that is nearest uis vertex /ETB.
Extend the cycle to include uby inserting ujust after /ETB. Repeat until all vertices
are on the cycle. Prove that this heuristic returns a tour whose total cost is not more
than twice the cost of an optimal tour.
35.2-4
In the bottleneck traveling-salesman problem , we wish to ﬁnd the hamiltonian cy-
cle that minimizes the cost of the most costly edge in the cycle. Assuming that thecost function satisﬁes the triangle inequality, show that there exists a polynomial-time approximation algorithm with approximation ratio 3for this problem. ( Hint:
Show recursively that we can visit all the nodes in a bottleneck spanning tree, asdiscussed in Problem 23-3, exactly once by taking a full walk of the tree and skip-ping nodes, but without skipping more than two consecutive intermediate nodes.Show that the costliest edge in a bottleneck spanning tree has a cost that is at mostthe cost of the costliest edge in a bottleneck hamiltonian cycle.)
35.2-5
Suppose that the vertices for an instance of the traveling-salesman problem arepoints in the plane and that the cost c.u;/ETB/ is the euclidean distance between
points uand/ETB. Show that an optimal tour never crosses itself.
35.3 The set-covering problem
The set-covering problem is an optimization problem that models many problems
that require resources to be allocated. Its corresponding decision problem general-izes the NP-complete vertex-cover problem and is therefore also NP-hard. The ap-proximation algorithm developed to handle the vertex-cover problem doesn’t apply
here, however, and so we need to try other approaches. We shall examine a simple
greedy heuristic with a logarithmic approximation ratio. That is, as the size of theinstance gets larger, the size of the approximate solution may grow, relative to thesize of an optimal solution. Because the logarithm function grows rather slowly,however, this approximation algorithm may nonetheless give useful results.1118 Chapter 35 Approximation Algorithms
S3S6
S4 S5S2S1
Figure 35.3 An instance .X;F/of the set-covering problem, where Xconsists of the 12black
points and FDfS1;S2;S3;S4;S5;S6g. A minimum-size set cover is CDfS3;S4;S5g,w i t h
size3. The greedy algorithm produces a cover of size 4by selecting either the sets S1,S4,S5,
andS3or the sets S1,S4,S5,a n d S6,i no r d e r .
An instance .X;F/of the set-covering problem consists of a ﬁnite set Xand
a family Fof subsets of X, such that every element of Xbelongs to at least one
subset in F:
XD[
S2FS:
We say that a subset S2Fcovers its elements. The problem is to ﬁnd a minimum-
size subset C/DC2Fwhose members cover all of X:
XD[
S2CS: (35.8)
We say that any Csatisfying equation (35.8) covers X. Figure 35.3 illustrates the
set-covering problem. The size of Cis the number of sets it contains, rather than
the number of individual elements in these sets, since every subset Cthat covers X
must contain alljXjindividual elements. In Figure 35.3, the minimum set cover
has size 3.
The set-covering problem abstracts many commonly arising combinatorial prob-
lems. As a simple example, suppose that Xrepresents a set of skills that are needed
to solve a problem and that we have a given set of people available to work on theproblem. We wish to form a committee, containing as few people as possible,such that for every requisite skill in X, at least one member of the committee has
that skill. In the decision version of the set-covering problem, we ask whether acovering exists with size at most k,w h e r e kis an additional parameter speciﬁed
in the problem instance. The decision version of the problem is NP-complete, as
Exercise 35.3-2 asks you to show.35.3 The set-covering problem 1119
A greedy approximation algorithm
The greedy method works by picking, at each stage, the set Sthat covers the great-
est number of remaining elements that are uncovered.
GREEDY -SET-COVER .X;F/
1UDX
2CD;
3while U¤;
4 select an S2Fthat maximizesjS\Uj
5 UDU/NULS
6 CDC[fSg
7return C
In the example of Figure 35.3, G REEDY -SET-COVER adds to C, in order, the sets
S1,S4,a n d S5, followed by either S3orS6.
The algorithm works as follows. The set Ucontains, at each stage, the set of
remaining uncovered elements. The set Ccontains the cover being constructed.
Line4is the greedy decision-making step, choosing a subset Sthat covers as many
uncovered elements as possible (breaking ties arbitrarily). After Sis selected,
line 5 removes its elements from U, and line 6 places SintoC. When the algorithm
terminates, the set Ccontains a subfamily of Fthat covers X.
We can easily implement G REEDY -SET-COVER to run in time polynomial in jXj
andjFj. Since the number of iterations of the loop on lines 3–6 is bounded from
above by min .jXj;jFj/, and we can implement the loop body to run in time
O.jXjjFj/, a simple implementation runs in time O.jXjjFjmin.jXj;jFj//.E x -
ercise 35.3-3 asks for a linear-time algorithm.
Analysis
We now show that the greedy algorithm returns a set cover that is not too much
larger than an optimal set cover. For convenience, in this chapter we denote the dth
harmonic number HdDPd
iD11=i(see Section A.1) by H.d/ . As a boundary
condition, we deﬁne H.0/D0.
Theorem 35.4
GREEDY -SET-COVER is a polynomial-time /SUB.n/-approximation algorithm, where
/SUB.n/DH.maxfjSjWS2Fg/:
Proof We have already shown that G REEDY -SET-COVER runs in polynomial
time.1120 Chapter 35 Approximation Algorithms
To show that G REEDY -SET-COVER is a/SUB.n/-approximation algorithm, we as-
sign a cost of 1to each set selected by the algorithm, distribute this cost over
the elements covered for the ﬁrst time, and then use these costs to derive the de-sired relationship between the size of an optimal set cover C
/ETXand the size of the
set cover Creturned by the algorithm. Let Sidenote the ith subset selected by
GREEDY -SET-COVER ; the algorithm incurs a cost of 1when it adds SitoC.W e
spread this cost of selecting Sievenly among the elements covered for the ﬁrst time
bySi.L e t cxdenote the cost allocated to element x, for each x2X. Each element
is assigned a cost only once, when it is covered for the ﬁrst time. If xis covered
for the ﬁrst time by Si,t h e n
cxD1
jSi/NUL.S1[S2[/SOH/SOH/SOH[ Si/NUL1/j:
Each step of the algorithm assigns 1unit of cost, and so
jCjDX
x2Xcx: (35.9)
Each element x2Xis in at least one set in the optimal cover C/ETX, and so we haveX
S2C/ETXX
x2Scx/NAKX
x2Xcx: (35.10)
Combining equation (35.9) and inequality (35.10), we have that
jCj/DC4X
S2C/ETXX
x2Scx: (35.11)
The remainder of the proof rests on the following key inequality, which we shall
prove shortly. For any set Sbelonging to the family F,X
x2Scx/DC4H.jSj/: (35.12)
From inequalities (35.11) and (35.12), it follows that
jCj/DC4X
S2C/ETXH.jSj/
/DC4jC/ETXj/SOHH.maxfjSjWS2Fg/;
thus proving the theorem.
All that remains is to prove inequality (35.12). Consider any set S2Fand any
iD1 ;2;:::;jCj,a n dl e t
uiDjS/NUL.S1[S2[/SOH/SOH/SOH[ Si/j
be the number of elements in Sthat remain uncovered after the algorithm has
selected sets S1;S2;:::;S i.W e d e ﬁ n e u0DjSjto be the number of elements35.3 The set-covering problem 1121
ofS, which are all initially uncovered. Let kbe the least index such that ukD0,
so that every element in Sis covered by at least one of the sets S1;S2;:::;S kand
some element in Sis uncovered by S1[S2[/SOH/SOH/SOH[ Sk/NUL1. Then, ui/NUL1/NAKui,a n d
ui/NUL1/NULuielements of Sare covered for the ﬁrst time by Si,f o riD1 ;2;:::;k .
Thus,
X
x2ScxDkX
iD1.ui/NUL1/NULui//SOH1
jSi/NUL.S1[S2[/SOH/SOH/SOH[ Si/NUL1/j:
Observe that
jSi/NUL.S1[S2[/SOH/SOH/SOH[ Si/NUL1/j/NAKjS/NUL.S1[S2[/SOH/SOH/SOH[ Si/NUL1/j
Dui/NUL1;
because the greedy choice of Siguarantees that Scannot cover more new ele-
ments than Sidoes (otherwise, the algorithm would have chosen Sinstead of Si).
Consequently, we obtain
X
x2Scx/DC4kX
iD1.ui/NUL1/NULui//SOH1
ui/NUL1:
We now bound this quantity as follows:
X
x2Scx/DC4kX
iD1.ui/NUL1/NULui//SOH1
ui/NUL1
DkX
iD1ui/NUL1X
jDuiC11
ui/NUL1
/DC4kX
iD1ui/NUL1X
jDuiC11
j(because j/DC4ui/NUL1)
DkX
iD1 ui/NUL1X
jD11
j/NULuiX
jD11
j!
DkX
iD1.H.u i/NUL1//NULH.u i//
DH.u 0//NULH.u k/ (because the sum telescopes)
DH.u 0//NULH.0/
DH.u 0/ (because H.0/D0)
DH.jSj/;
which completes the proof of inequality (35.12).
1122 Chapter 35 Approximation Algorithms
Corollary 35.5
GREEDY -SET-COVER is a polynomial-time .lnjXjC1/-approximation algorithm.
Proof Use inequality (A.14) and Theorem 35.4.
In some applications, max fjSjWS2Fgis a small constant, and so the solution
returned by G REEDY -SET-COVER is at most a small constant times larger than
optimal. One such application occurs when this heuristic ﬁnds an approximatevertex cover for a graph whose vertices have degree at most 3. In this case, the
solution found by G
REEDY -SET-COVER is not more than H.3/D11=6 times as
large as an optimal solution, a performance guarantee that is slightly better thanthat of A
PPROX -VERTEX -COVER .
Exercises
35.3-1
Consider each of the following words as a set of letters: farid ;dash ;drain ;
heard ;lost ;nose ;shun ;slate ;snare ;threadg. Show which set cover
GREEDY -SET-COVER produces when we break ties in favor of the word that ap-
pears ﬁrst in the dictionary.
35.3-2
Show that the decision version of the set-covering problem is NP-complete byreducing it from the vertex-cover problem.
35.3-3
Show how to implement G
REEDY -SET-COVER in such a way that it runs in time
O/DLEP
S2FjSj/DC1
.
35.3-4
Show that the following weaker form of Theorem 35.4 is trivially true:
jCj/DC4jC/ETXjmaxfjSjWS2Fg:
35.3-5
GREEDY -SET-COVER can return a number of different solutions, depending on
how we break ties in line 4. Give a procedure B AD-SET-COVER -INSTANCE .n/
that returns an n-element instance of the set-covering problem for which, depend-
ing on how we break ties in line 4, G REEDY -SET-COVER can return a number of
different solutions that is exponential in n.35.4 Randomization and linear programming 1123
35.4 Randomization and linear programming
In this section, we study two useful techniques for designing approximation algo-
rithms: randomization and linear programming. We shall give a simple randomizedalgorithm for an optimization version of 3-CNF satisﬁability, and then we shall uselinear programming to help design an approximation algorithm for a weighted ver-
sion of the vertex-cover problem. This section only scratches the surface of these
two powerful techniques. The chapter notes give references for further study ofthese areas.
A randomized approximation algorithm for MAX-3-CNF satisﬁability
Just as some randomized algorithms compute exact solutions, some randomized
algorithms compute approximate solutions. We say that a randomized algorithmfor a problem has an approximation ratio of/SUB.n/ if, for any input of size n,t h e
expected costCof the solution produced by the randomized algorithm is within a
factor of /SUB.n/ of the cost C
/ETXof an optimal solution:
max/DC2C
C/ETX;C/ETX
C/DC3
/DC4/SUB.n/ : (35.13)
We call a randomized algorithm that achieves an approximation ratio of /SUB.n/ a
randomized /SUB.n/-approximation algorithm. In other words, a randomized ap-
proximation algorithm is like a deterministic approximation algorithm, except thatthe approximation ratio is for an expected cost.
A particular instance of 3-CNF satisﬁability, as deﬁned in Section 34.4, may or
may not be satisﬁable. In order to be satisﬁable, there must exist an assignment ofthe variables so that every clause evaluates to 1. If an instance is not satisﬁable, we
may want to compute how “close” to satisﬁable it is, that is, we may wish to ﬁnd an
assignment of the variables that satisﬁes as many clauses as possible. We call the
resulting maximization problem MAX-3-CNF satisﬁability . The input to MAX-3-
CNF satisﬁability is the same as for 3-CNF satisﬁability, and the goal is to return
an assignment of the variables that maximizes the number of clauses evaluating
to1. We now show that randomly setting each variable to 1with probability 1=2
and to 0with probability 1=2yields a randomized 8=7-approximation algorithm.
According to the deﬁnition of 3-CNF satisﬁability from Section 34.4, we require
each clause to consist of exactly three distinct literals. We further assume thatno clause contains both a variable and its negation. (Exercise 35.4-1 asks you toremove this last assumption.)1124 Chapter 35 Approximation Algorithms
Theorem 35.6
Given an instance of MAX-3-CNF satisﬁability with nvariables x1;x2;:::;x n
andmclauses, the randomized algorithm that independently sets each vari-
able to 1with probability 1=2 and to 0with probability 1=2 is a randomized
8=7-approximation algorithm.
Proof Suppose that we have independently set each variable to 1with probabil-
ity1=2and to 0with probability 1=2.F o r iD1 ;2;:::;m , we deﬁne the indicator
random variable
YiDIfclause iis satisﬁedg;
so that YiD1as long as we have set at least one of the literals in the ith clause
to1. Since no literal appears more than once in the same clause, and since we have
assumed that no variable and its negation appear in the same clause, the settings of
the three literals in each clause are independent. A clause is not satisﬁed only if all
three of its literals are set to 0,a n ds oP rfclause iis not satisﬁedgD.1=2/3D1=8.
Thus, we have Prfclause iis satisﬁedgD1/NUL1=8D7=8, and by Lemma 5.1,
we have E ŒYi/c141D7=8.L e t Ybe the number of satisﬁed clauses overall, so that
YDY1CY2C/SOH/SOH/SOHC Ym. Then, we have
EŒY /c141DE"mX
iD1Yi#
DmX
iD1EŒYi/c141 (by linearity of expectation)
DmX
iD17=8
D7m=8 :
Clearly, mis an upper bound on the number of satisﬁed clauses, and hence the
approximation ratio is at most m=.7m=8/D8=7.
Approximating weighted vertex cover using linear programming
In the minimum-weight vertex-cover problem , we are given an undirected graph
GD.V; E/ in which each vertex /ETB2Vhas an associated positive weight w./ETB/ .
For any vertex cover V0/DC2V, we deﬁne the weight of the vertex cover w.V0/DP
/ETB2V0w./ETB/ . The goal is to ﬁnd a vertex cover of minimum weight.
We cannot apply the algorithm used for unweighted vertex cover, nor can we use
a random solution; both methods may return solutions that are far from optimal.We shall, however, compute a lower bound on the weight of the minimum-weight35.4 Randomization and linear programming 1125
vertex cover, by using a linear program. We shall then “round” this solution and
use it to obtain a vertex cover.
Suppose that we associate a variable x./ETB/ with each vertex /ETB2V,a n dl e tu s
require that x./ETB/ equals either 0or1for each /ETB2V. We put /ETBinto the vertex cover
if and only if x./ETB/D1. Then, we can write the constraint that for any edge .u; /ETB/ ,
at least one of uand/ETBmust be in the vertex cover as x.u/Cx./ETB//NAK1.T h i sv i e w
gives rise to the following 0-1 integer program for ﬁnding a minimum-weight
vertex cover:
minimizeX
/ETB2Vw./ETB/ x./ETB/ (35.14)
subject to
x.u/Cx./ETB//NAK1 for each .u; /ETB/2E (35.15)
x./ETB/2f0; 1gfor each /ETB2V: (35.16)
In the special case in which all the weights w./ETB/ are equal to 1, this formu-
lation is the optimization version of the NP-hard vertex-cover problem. Sup-
pose, however, that we remove the constraint that x./ETB/2f0; 1gand replace it
by0/DC4x./ETB//DC41. We then obtain the following linear program, which is known as
thelinear-programming relaxation :
minimizeX
/ETB2Vw./ETB/ x./ETB/ (35.17)
subject to
x.u/Cx./ETB//NAK1for each .u; /ETB/2E (35.18)
x./ETB//DC41for each /ETB2V (35.19)
x./ETB//NAK0for each /ETB2V: (35.20)
Any feasible solution to the 0-1 integer program in lines (35.14)–(35.16) is also
a feasible solution to the linear program in lines (35.17)–(35.20). Therefore, thevalue of an optimal solution to the linear program gives a lower bound on the value
of an optimal solution to the 0-1 integer program, and hence a lower bound on the
optimal weight in the minimum-weight vertex-cover problem.
The following procedure uses the solution to the linear-programming relaxation
to construct an approximate solution to the minimum-weight vertex-cover problem:1126 Chapter 35 Approximation Algorithms
APPROX -MIN-WEIGHT -VC.G; w/
1CD;
2 computeNx, an optimal solution to the linear program in lines (35.17)–(35.20)
3foreach/ETB2V
4 ifNx./ETB//NAK1=2
5 CDC[f/ETBg
6return C
The A PPROX -MIN-WEIGHT -VC procedure works as follows. Line 1 initial-
izes the vertex cover to be empty. Line 2 formulates the linear program inlines (35.17)–(35.20) and then solves this linear program. An optimal solutiongives each vertex /ETBan associated value Nx./ETB/ ,w h e r e 0/DC4Nx./ETB//DC41. We use this
value to guide the choice of which vertices to add to the vertex cover Cin lines 3–5.
IfNx./ETB//NAK1=2,w ea d d /ETBtoC; otherwise we do not. In effect, we are “rounding”
each fractional variable in the solution to the linear program to 0or1in order to
obtain a solution to the 0-1 integer program in lines (35.14)–(35.16). Finally, line 6returns the vertex cover C.
Theorem 35.7
Algorithm A
PPROX -MIN-WEIGHT -VC is a polynomial-time 2-approximation al-
gorithm for the minimum-weight vertex-cover problem.
Proof Because there is a polynomial-time algorithm to solve the linear program
in line 2, and because the forloop of lines 3–5 runs in polynomial time, A PPROX -
MIN-WEIGHT -VC is a polynomial-time algorithm.
Now we show that A PPROX -MIN-WEIGHT -VC is a 2-approximation algo-
rithm. Let C/ETXbe an optimal solution to the minimum-weight vertex-cover prob-
lem, and let ´/ETXbe the value of an optimal solution to the linear program in
lines (35.17)–(35.20). Since an optimal vertex cover is a feasible solution to thelinear program, ´
/ETXmust be a lower bound on w.C/ETX/,t h a ti s ,
´/ETX/DC4w.C/ETX/: (35.21)
Next, we claim that by rounding the fractional values of the variables Nx./ETB/ ,w e
produce a set Cthat is a vertex cover and satisﬁes w.C//DC42´/ETX. To see that Cis
a vertex cover, consider any edge .u; /ETB/2E. By constraint (35.18), we know that
x.u/Cx./ETB//NAK1, which implies that at least one of Nx.u/ andNx./ETB/ is at least 1=2.
Therefore, at least one of uand/ETBis included in the vertex cover, and so every edge
is covered.
Now, we consider the weight of the cover. We have35.4 Randomization and linear programming 1127
´/ETXDX
/ETB2Vw./ETB/Nx./ETB/
/NAKX
/ETB2VWNx./ETB//NAK1=2w./ETB/Nx./ETB/
/NAKX
/ETB2VWNx./ETB//NAK1=2w./ETB//SOH1
2
DX
/ETB2Cw./ETB//SOH1
2
D1
2X
/ETB2Cw./ETB/
D1
2w.C/ : (35.22)
Combining inequalities (35.21) and (35.22) gives
w.C//DC42´/ETX/DC42w.C/ETX/;
and hence A PPROX -MIN-WEIGHT -VC is a 2-approximation algorithm.
Exercises
35.4-1
Show that even if we allow a clause to contain both a variable and its negation, ran-domly setting each variable to 1with probability 1=2and to 0with probability 1=2
still yields a randomized 8=7-approximation algorithm.
35.4-2
TheMAX-CNF satisﬁability problem is like the MAX-3-CNF satisﬁability prob-
lem, except that it does not restrict each clause to have exactly 3literals. Give a
randomized 2-approximation algorithm for the MAX-CNF satisﬁability problem.
35.4-3
In the MAX-CUT problem, we are given an unweighted undirected graph GD
.V; E/ . We deﬁne a cut .S; V/NULS/as in Chapter 23 and the weight of a cut as the
number of edges crossing the cut. The goal is to ﬁnd a cut of maximum weight.Suppose that for each vertex /ETB, we randomly and independently place /ETBinSwith
probability 1=2and in V/NULSwith probability 1=2. Show that this algorithm is a
randomized 2-approximation algorithm.1128 Chapter 35 Approximation Algorithms
35.4-4
Show that the constraints in line (35.19) are redundant in the sense that if we re-move them from the linear program in lines (35.17)–(35.20), any optimal solutionto the resulting linear program must satisfy x./ETB//DC41for each /ETB2V.
35.5 The subset-sum problem
Recall from Section 34.5.5 that an instance of the subset-sum problem is a
pair.S; t/ ,w h e r e Sis a setfx1;x2;:::;x ngof positive integers and tis a posi-
tive integer. This decision problem asks whether there exists a subset of Sthat
adds up exactly to the target value t. As we saw in Section 34.5.5, this problem is
NP-complete.
The optimization problem associated with this decision problem arises in prac-
tical applications. In the optimization problem, we wish to ﬁnd a subset of
fx1;x2;:::;x ngwhose sum is as large as possible but not larger than t.F o r e x -
ample, we may have a truck that can carry no more than tpounds, and ndifferent
boxes to ship, the ith of which weighs xipounds. We wish to ﬁll the truck with as
heavy a load as possible without exceeding the given weight limit.
In this section, we present an exponential-time algorithm that computes the op-
timal value for this optimization problem, and then we show how to modify thealgorithm so that it becomes a fully polynomial-time approximation scheme. (Re-call that a fully polynomial-time approximation scheme has a running time that ispolynomial in 1=/SIas well as in the size of the input.)
An exponential-time exact algorithm
Suppose that we computed, for each subset S
0ofS, the sum of the elements
inS0, and then we selected, among the subsets whose sum does not exceed t,
the one whose sum was closest to t. Clearly this algorithm would return the op-
timal solution, but it could take exponential time. To implement this algorithm,
we could use an iterative procedure that, in iteration i, computes the sums of
all subsets offx1;x2;:::;x ig, using as a starting point the sums of all subsets
offx1;x2;:::;x i/NUL1g. In doing so, we would realize that once a particular subset S0
had a sum exceeding t, there would be no reason to maintain it, since no super-
set of S0could be the optimal solution. We now give an implementation of this
strategy.
The procedure E XACT -SUBSET -SUMtakes an input set SDfx1;x2;:::;x ng
and a target value t; we’ll see its pseudocode in a moment. This procedure it-35.5 The subset-sum problem 1129
eratively computes Li,t h el i s to fs u m so fa l ls u b s e t so f fx1;:::;x igthat do not
exceed t, and then it returns the maximum value in Ln.
IfLis a list of positive integers and xis another positive integer, then we let
LCxdenote the list of integers derived from Lby increasing each element of L
byx. For example, if LDh1; 2; 3; 5; 9i,t h e n LC2Dh3; 4; 5; 7; 11i.W ea l s ou s e
this notation for sets, so that
SCxDfsCxWs2Sg:
We also use an auxiliary procedure M ERGE -LISTS.L; L0/, which returns the
sorted list that is the merge of its two sorted input lists LandL0with duplicate
values removed. Like the M ERGE procedure we used in merge sort (Section 2.3.1),
MERGE -LISTS runs in time O.jLjCjL0j/. We omit the pseudocode for M ERGE -
LISTS.
EXACT -SUBSET -SUM.S; t/
1nDjSj
2L0Dh0i
3foriD1ton
4 LiDMERGE -LISTS.Li/NUL1;Li/NUL1Cxi/
5 remove from Lievery element that is greater than t
6return the largest element in Ln
To see how E XACT -SUBSET -SUMworks, let Pidenote the set of all values
obtained by selecting a (possibly empty) subset of fx1;x2;:::;x igand summing
its members. For example, if SDf1; 4; 5g,t h e n
P1Df0; 1g;
P2Df0; 1; 4; 5g;
P3Df0; 1; 4; 5; 6; 9; 10g:
Given the identityP
iDPi/NUL1[.Pi/NUL1Cxi/; (35.23)
we can prove by induction on i(see Exercise 35.5-1) that the list Liis a sorted list
containing every element of Piwhose value is not more than t. Since the length
ofLican be as much as 2i,EXACT -SUBSET -SUMis an exponential-time algorithm
in general, although it is a polynomial-time algorithm in the special cases in which t
is polynomial injSjor all the numbers in Sare bounded by a polynomial in jSj.
A fully polynomial-time approximation scheme
We can derive a fully polynomial-time approximation scheme for the subset-sum
problem by “trimming” each list Liafter it is created. The idea behind trimming is1130 Chapter 35 Approximation Algorithms
that if two values in Lare close to each other, then since we want just an approxi-
mate solution, we do not need to maintain both of them explicitly. More precisely,we use a trimming parameter ısuch that 0<ı<1 .W h e nw e trim a list Lbyı,
we remove as many elements from Las possible, in such a way that if L
0is the
result of trimming L, then for every element ythat was removed from L, there is
an element ´still in L0that approximates y,t h a ti s ,
y
1Cı/DC4´/DC4y: (35.24)
We can think of such a ´as “representing” yin the new list L0. Each removed
element yis represented by a remaining element ´satisfying inequality (35.24).
For example, if ıD0:1and
LDh10; 11; 12; 15; 20; 21; 22; 23; 24; 29 i;
then we can trim Lto obtain
L0Dh10; 12; 15; 20; 23; 29 i;
where the deleted value 11is represented by 10, the deleted values 21and22
are represented by 20, and the deleted value 24is represented by 23. Because
every element of the trimmed version of the list is also an element of the originalversion of the list, trimming can dramatically decrease the number of elements keptwhile keeping a close (and slightly smaller) representative value in the list for eachdeleted element.
The following procedure trims list LDhy
1;y2;:::;y miin time ‚.m/ ,g i v e n L
andı, and assuming that Lis sorted into monotonically increasing order. The
output of the procedure is a trimmed, sorted list.
TRIM.L; ı/
1l e t mbe the length of L
2L0Dhy1i
3lastDy1
4foriD2tom
5 ifyi>last/SOH.1Cı/ //yi/NAKlastbecause Lis sorted
6 append yionto the end of L0
7 lastDyi
8return L0
The procedure scans the elements of Lin monotonically increasing order. A num-
ber is appended onto the returned list L0only if it is the ﬁrst element of Lor if it
cannot be represented by the most recent number placed into L0.
Given the procedure T RIM, we can construct our approximation scheme as fol-
lows. This procedure takes as input a set SDfx1;x2;:::;x ngofnintegers (in
arbitrary order), a target integer t, and an “approximation parameter” /SI,w h e r e35.5 The subset-sum problem 1131
0</SI<1: (35.25)
It returns a value ´whose value is within a 1C/SIfactor of the optimal solution.
APPROX -SUBSET -SUM. S;t;/SI/
1nDjSj
2L0Dh0i
3foriD1ton
4 LiDMERGE -LISTS.Li/NUL1;Li/NUL1Cxi/
5 LiDTRIM.Li;/SI= 2n /
6 remove from Lievery element that is greater than t
7l e t ´/ETXbe the largest value in Ln
8return ´/ETX
Line 2 initializes the list L0to be the list containing just the element 0.T h e for
loop in lines 3–6 computes Lias a sorted list containing a suitably trimmed ver-
sion of the set Pi, with all elements larger than tremoved. Since we create Li
from Li/NUL1, we must ensure that the repeated trimming doesn’t introduce too much
compounded inaccuracy. In a moment, we shall see that A PPROX -SUBSET -SUM
returns a correct approximation if one exists.
As an example, suppose we have the instance
SDh104; 102; 201; 101i
withtD308and/SID0:40. The trimming parameter ıis/SI=8D0:05.APPROX -
SUBSET -SUMcomputes the following values on the indicated lines:
line 2: L0Dh 0i;
line 4: L1Dh 0; 104i;
line 5: L1Dh 0; 104i;
line 6: L1Dh 0; 104i;
line 4: L2Dh 0; 102; 104; 206i;
line 5: L2Dh 0; 102; 206i;
line 6: L2Dh 0; 102; 206i;
line 4: L3Dh 0; 102; 201; 206; 303; 407 i;
line 5: L3Dh 0; 102; 201; 303; 407 i;
line 6: L3Dh 0; 102; 201; 303i;
line 4: L4Dh 0; 101; 102; 201; 203; 302; 303; 404 i;
line 5: L4Dh 0; 101; 201; 302; 404 i;
line 6: L4Dh 0; 101; 201; 302i:1132 Chapter 35 Approximation Algorithms
The algorithm returns ´/ETXD302as its answer, which is well within /SID40%o f
the optimal answer 307D104C102C101; in fact, it is within 2%.
Theorem 35.8
APPROX -SUBSET -SUMis a fully polynomial-time approximation scheme for the
subset-sum problem.
Proof The operations of trimming Liin line 5 and removing from Lievery ele-
ment that is greater than tmaintain the property that every element of Liis also a
member of Pi. Therefore, the value ´/ETXreturned in line 8 is indeed the sum of some
subset of S.L e t y/ETX2Pndenote an optimal solution to the subset-sum problem.
Then, from line 6, we know that ´/ETX/DC4y/ETX. By inequality (35.1), we need to show
thaty/ETX=´/ETX/DC41C/SI. We must also show that the running time of this algorithm is
polynomial in both 1=/SIand the size of the input.
As Exercise 35.5-2 asks you to show, for every element yinPithat is at most t,
there exists an element ´2Lisuch that
y
.1C/SI=2n/i/DC4´/DC4y: (35.26)
Inequality (35.26) must hold for y/ETX2Pn, and therefore there exists an element
´2Lnsuch that
y/ETX
.1C/SI=2n/n/DC4´/DC4y/ETX;
and thus
y/ETX
´/DC4/DLE
1C/SI
2n/DC1n
: (35.27)
Since there exists an element ´2Lnfulﬁlling inequality (35.27), the inequality
must hold for ´/ETX, which is the largest value in Ln;t h a ti s ,
y/ETX
´/ETX/DC4/DLE
1C/SI
2n/DC1n
: (35.28)
Now, we show that y/ETX=´/ETX/DC41C/SI.W ed os ob ys h o w i n gt h a t .1C/SI=2n/n/DC4
1C/SI. By equation (3.14), we have lim n!1.1C/SI=2n/nDe/SI=2. Exercise 35.5-3
asks you to show that
d
dn/DLE
1C/SI
2n/DC1n
>0: (35.29)
Therefore, the function .1C/SI=2n/nincreases with nas it approaches its limit
ofe/SI=2, and we have35.5 The subset-sum problem 1133
/DLE
1C/SI
2n/DC1n
/DC4e/SI=2
/DC41C/SI=2C./SI=2/2(by inequality (3.13))
/DC41C/SI (by inequality (35.25)) . (35.30)
Combining inequalities (35.28) and (35.30) completes the analysis of the approxi-
mation ratio.
To show that A PPROX -SUBSET -SUMis a fully polynomial-time approximation
scheme, we derive a bound on the length of Li. After trimming, successive ele-
ments ´and´0ofLimust have the relationship ´0=´ > 1C/SI=2n . That is, they must
differ by a factor of at least 1C/SI=2n . Each list, therefore, contains the value 0,
possibly the value 1, and up to/EOT
log1C/SI=2nt˘
additional values. The number of
elements in each list Liis at most
log1C/SI=2ntC2Dlnt
ln.1C/SI=2n/C2
/DC42n.1C/SI=2n/ lnt
/SIC2(by inequality (3.17))
<3nlnt
/SIC2 (by inequality (35.25)) .
This bound is polynomial in the size of the input—which is the number of bits lg t
needed to represent tplus the number of bits needed to represent the set S,w h i c hi s
in turn polynomial in n—and in 1=/SI. Since the running time of A PPROX -SUBSET -
SUMis polynomial in the lengths of the Li, we conclude that A PPROX -SUBSET -
SUMis a fully polynomial-time approximation scheme.
Exercises
35.5-1
Prove equation (35.23). Then show that after executing line 5 of E XACT -SUBSET -
SUM,Liis a sorted list containing every element of Piwhose value is not more
thant.
35.5-2
Using induction on i, prove inequality (35.26).
35.5-3
Prove inequality (35.29).1134 Chapter 35 Approximation Algorithms
35.5-4
How would you modify the approximation scheme presented in this section to ﬁnda good approximation to the smallest value not less than tthat is a sum of some
subset of the given input list?
35.5-5
Modify the A
PPROX -SUBSET -SUMprocedure to also return the subset of Sthat
sums to the value ´/ETX.
Problems
35-1 Bin packing
Suppose that we are given a set of nobjects, where the size siof the ith object
satisﬁes 0<s i<1. We wish to pack all the objects into the minimum number of
unit-size bins. Each bin can hold any subset of the objects whose total size doesnot exceed 1.
a.Prove that the problem of determining the minimum number of bins required is
NP-hard. ( Hint: Reduce from the subset-sum problem.)
Theﬁrst-ﬁt heuristic takes each object in turn and places it into the ﬁrst bin that
can accommodate it. Let SDP
n
iD1si.
b.Argue that the optimal number of bins required is at least dSe.
c.Argue that the ﬁrst-ﬁt heuristic leaves at most one bin less than half full.
d.Prove that the number of bins used by the ﬁrst-ﬁt heuristic is never more
thand2Se.
e.Prove an approximation ratio of 2for the ﬁrst-ﬁt heuristic.
f.Give an efﬁcient implementation of the ﬁrst-ﬁt heuristic, and analyze its running
time.
35-2 Approximating the size of a maximum clique
LetGD.V; E/ be an undirected graph. For any k/NAK1,d e ﬁ n e G.k/to be the undi-
rected graph .V.k/;E.k//,w h e r e V.k/is the set of all ordered k-tuples of vertices
from VandE.k/is deﬁned so that ./ETB1;/ETB2;:::;/ETB k/is adjacent to .w1;w2;:::;w k/
if and only if for iD1 ;2;:::;k , either vertex /ETBiis adjacent to wiinG,o re l s e
/ETBiDwi.Problems for Chapter 35 1135
a.Prove that the size of the maximum clique in G.k/is equal to the kth power of
the size of the maximum clique in G.
b.Argue that if there is an approximation algorithm that has a constant approxi-
mation ratio for ﬁnding a maximum-size clique, then there is a polynomial-timeapproximation scheme for the problem.
35-3 Weighted set-covering problem
Suppose that we generalize the set-covering problem so that each set S
iin the
family Fhas an associated weight wiand the weight of a cover CisP
Si2Cwi.
We wish to determine a minimum-weight cover. (Section 35.3 handles the case inwhich w
iD1for all i.)
Show how to generalize the greedy set-covering heuristic in a natural manner
to provide an approximate solution for any instance of the weighted set-coveringproblem. Show that your heuristic has an approximation ratio of H.d/ ,w h e r e dis
the maximum size of any set S
i.
35-4 Maximum matching
Recall that for an undirected graph G, a matching is a set of edges such that no
two edges in the set are incident on the same vertex. In Section 26.3, we saw howto ﬁnd a maximum matching in a bipartite graph. In this problem, we will look atmatchings in undirected graphs in general (i.e., the graphs are not required to bebipartite).
a.Amaximal matching is a matching that is not a proper subset of any other
matching. Show that a maximal matching need not be a maximum matching by
exhibiting an undirected graph Gand a maximal matching MinGthat is not a
maximum matching. ( Hint: You can ﬁnd such a graph with only four vertices.)
b.Consider an undirected graph GD.V; E/ .G i v e a n O.E/ -time greedy algo-
rithm to ﬁnd a maximal matching in G.
In this problem, we shall concentrate on a polynomial-time approximation algo-
rithm for maximum matching. Whereas the fastest known algorithm for maximum
matching takes superlinear (but polynomial) time, the approximation algorithmhere will run in linear time. You will show that the linear-time greedy algorithmfor maximal matching in part (b) is a 2-approximation algorithm for maximum
matching.
c.Show that the size of a maximum matching in Gis a lower bound on the size
of any vertex cover for G.1136 Chapter 35 Approximation Algorithms
d.Consider a maximal matching MinGD.V; E/ .L e t
TDf/ETB2VWsome edge in Mis incident on /ETBg:
What can you say about the subgraph of Ginduced by the vertices of Gthat
are not in T?
e.Conclude from part (d) that 2jMjis the size of a vertex cover for G.
f.Using parts (c) and (e), prove that the greedy algorithm in part (b) is a 2-approx-
imation algorithm for maximum matching.
35-5 Parallel machine scheduling
In the parallel-machine-scheduling problem ,w ea r eg i v e n njobs, J1;J2;:::;J n,
where each job Jkhas an associated nonnegative processing time of pk.W e a r e
also given midentical machines, M1;M 2;:::;M m. Any job can run on any ma-
chine. A schedule speciﬁes, for each job Jk, the machine on which it runs and
the time period during which it runs. Each job Jkmust run on some machine Mi
forpkconsecutive time units, and during that time period no other job may run
onMi.L e t Ckdenote the completion time of job Jk, that is, the time at which
jobJkcompletes processing. Given a schedule, we deﬁne CmaxDmax 1/DC4j/DC4nCjto
be the makespan of the schedule. The goal is to ﬁnd a schedule whose makespan
is minimum.
For example, suppose that we have two machines M1andM2and that we have
four jobs J1;J2;J3;J4, with p1D2,p2D12,p3D4,a n d p4D5. Then one
possible schedule runs, on machine M1,j o b J1followed by job J2, and on ma-
chine M2, it runs job J4followed by job J3. For this schedule, C1D2,C2D14,
C3D9,C4D5,a n d CmaxD14. An optimal schedule runs J2on machine M1,a n d
it runs jobs J1,J3,a n d J4on machine M2. For this schedule, C1D2,C2D12,
C3D6,C4D11,a n d CmaxD12.
Given a parallel-machine-scheduling problem, we let C/ETX
maxdenote the makespan
of an optimal schedule.
a.Show that the optimal makespan is at least as large as the greatest processing
time, that is,
C/ETX
max/NAKmax
1/DC4k/DC4npk:
b.Show that the optimal makespan is at least as large as the average machine load,
that is,
C/ETX
max/NAK1
mX
1/DC4k/DC4npk:Problems for Chapter 35 1137
Suppose that we use the following greedy algorithm for parallel machine schedul-
ing: whenever a machine is idle, schedule any job that has not yet been scheduled.
c.Write pseudocode to implement this greedy algorithm. What is the running
time of your algorithm?
d.For the schedule returned by the greedy algorithm, show that
Cmax/DC41
mX
1/DC4k/DC4npkCmax
1/DC4k/DC4npk:
Conclude that this algorithm is a polynomial-time 2-approximation algorithm.
35-6 Approximating a maximum spanning tree
LetGD.V; E/ be an undirected graph with distinct edge weights w.u;/ETB/ on each
edge .u; /ETB/2E. For each vertex /ETB2V,l e tm a x ./ETB/Dmax .u;/ETB/ 2Efw.u;/ETB/gbe
the maximum-weight edge incident on that vertex. Let SGDfmax./ETB/W/ETB2Vg
be the set of maximum-weight edges incident on each vertex, and let TGbe the
maximum-weight spanning tree of G, that is, the spanning tree of maximum total
weight. For any subset of edges E0/DC2E,d e ﬁ n e w.E0/DP
.u;/ETB/ 2E0w.u;/ETB/ .
a.Give an example of a graph with at least 4vertices for which SGDTG.
b.Give an example of a graph with at least 4vertices for which SG¤TG.
c.Prove that SG/DC2TGfor any graph G.
d.Prove that w.T G//NAKw.S G/=2for any graph G.
e.Give an O.VCE/-time algorithm to compute a 2-approximation to the maxi-
mum spanning tree.
35-7 An approximation algorithm for the 0-1 knapsack problem
Recall the knapsack problem from Section 16.2. There are nitems, where the ith
item is worth /ETBidollars and weighs wipounds. We are also given a knapsack
that can hold at most Wpounds. Here, we add the further assumptions that each
weight wiis at most Wand that the items are indexed in monotonically decreasing
order of their values: /ETB1/NAK/ETB2/NAK/SOH/SOH/SOH/NAK /ETBn.
In the 0-1 knapsack problem, we wish to ﬁnd a subset of the items whose total
weight is at most Wand whose total value is maximum. The fractional knapsack
problem is like the 0-1 knapsack problem, except that we are allowed to take afraction of each item, rather than being restricted to taking either all or none of1138 Chapter 35 Approximation Algorithms
each item. If we take a fraction xiof item i,w h e r e 0/DC4xi/DC41, we contribute
xiwito the weight of the knapsack and receive value xi/ETBi. Our goal is to develop
a polynomial-time 2-approximation algorithm for the 0-1 knapsack problem.
In order to design a polynomial-time algorithm, we consider restricted instances
of the 0-1 knapsack problem. Given an instance Iof the knapsack problem, we
form restricted instances Ij,f o rjD1 ;2;:::;n , by removing items 1 ;2;:::;j/NUL1
and requiring the solution to include item j(all of item jin both the fractional
and 0-1 knapsack problems). No items are removed in instance I1. For instance Ij,
letPjdenote an optimal solution to the 0-1 problem and Qjdenote an optimal
solution to the fractional problem.
a.Argue that an optimal solution to instance Iof the 0-1 knapsack problem is one
offP1;P2;:::;P ng.
b.Prove that we can ﬁnd an optimal solution Qjto the fractional problem for in-
stance Ijby including item jand then using the greedy algorithm in which
at each step, we take as much as possible of the unchosen item in the set
fjC1; jC2;:::;ngwith maximum value per pound /ETBi=wi.
c.Prove that we can always construct an optimal solution Qjto the fractional
problem for instance Ijthat includes at most one item fractionally. That is, for
all items except possibly one, we either include all of the item or none of theitem in the knapsack.
d.Given an optimal solution Q
jto the fractional problem for instance Ij,f o r m
solution Rjfrom Qjby deleting any fractional items from Qj.L e t /ETB.S/ denote
the total value of items taken in a solution S. Prove that /ETB.R j//NAK/ETB.Q j/=2/NAK
/ETB.P j/=2.
e.Give a polynomial-time algorithm that returns a maximum-value solution from
the setfR1;R2;:::;R ng, and prove that your algorithm is a polynomial-time
2-approximation algorithm for the 0-1 knapsack problem.
Chapter notes
Although methods that do not necessarily compute exact solutions have been
known for thousands of years (for example, methods to approximate the valueof/EM), the notion of an approximation algorithm is much more recent. Hochbaum
[172] credits Garey, Graham, and Ullman [128] and Johnson [190] with formal-izing the concept of a polynomial-time approximation algorithm. The ﬁrst suchalgorithm is often credited to Graham [149].Notes for Chapter 35 1139
Since this early work, thousands of approximation algorithms have been de-
signed for a wide range of problems, and there is a wealth of literature on thisﬁeld. Recent texts by Ausiello et al. [26], Hochbaum [172], and Vazirani [345]deal exclusively with approximation algorithms, as do surveys by Shmoys [315]and Klein and Young [207]. Several other texts, such as Garey and Johnson [129]and Papadimitriou and Steiglitz [271], have signiﬁcant coverage of approximationalgorithms as well. Lawler, Lenstra, Rinnooy Kan, and Shmoys [225] provide anextensive treatment of approximation algorithms for the traveling-salesman prob-
lem.
Papadimitriou and Steiglitz attribute the algorithm A
PPROX -VERTEX -COVER
to F. Gavril and M. Yannakakis. The vertex-cover problem has been studied exten-sively (Hochbaum [172] lists 16 different approximation algorithms for this prob-lem), but all the approximation ratios are at least 2/NULo.1/.
The algorithm A
PPROX -TSP-T OUR appears in a paper by Rosenkrantz, Stearns,
and Lewis [298]. Christoﬁdes improved on this algorithm and gave a 3=2-approx-
imation algorithm for the traveling-salesman problem with the triangle inequality.
Arora [22] and Mitchell [257] have shown that if the points are in the euclidean
plane, there is a polynomial-time approximation scheme. Theorem 35.3 is due to
Sahni and Gonzalez [301].
The analysis of the greedy heuristic for the set-covering problem is modeled
after the proof published by Chv´ atal [68] of a more general result; the basic result
as presented here is due to Johnson [190] and Lov´ asz [238].
The algorithm A PPROX -SUBSET -SUMand its analysis are loosely modeled after
related approximation algorithms for the knapsack and subset-sum problems byIbarra and Kim [187].
Problem 35-7 is a combinatorial version of a more general result on approximat-
ing knapsack-type integer programs by Bienstock and McClosky [45].
The randomized algorithm for MAX-3-CNF satisﬁability is implicit in the work
of Johnson [190]. The weighted vertex-cover algorithm is by Hochbaum [171].Section 35.4 only touches on the power of randomization and linear program-ming in the design of approximation algorithms. A combination of these two ideasyields a technique called “randomized rounding,” which formulates a problem asan integer linear program, solves the linear-programming relaxation, and interpretsthe variables in the solution as probabilities. These probabilities then help guidethe solution of the original problem. This technique was ﬁrst used by Raghavanand Thompson [290], and it has had many subsequent uses. (See Motwani, Naor,
and Raghavan [261] for a survey.) Several other notable recent ideas in the ﬁeld
of approximation algorithms include the primal-dual method (see Goemans andWilliamson [135] for a survey), ﬁnding sparse cuts for use in divide-and-conqueralgorithms [229], and the use of semideﬁnite programming [134].1140 Chapter 35 Approximation Algorithms
As mentioned in the chapter notes for Chapter 34, recent results in probabilisti-
cally checkable proofs have led to lower bounds on the approximability of manyproblems, including several in this chapter. In addition to the references there,the chapter by Arora and Lund [23] contains a good description of the relation-ship between probabilistically checkable proofs and the hardness of approximatingvarious problems.VIII Appendix: Mathematical BackgroundIntroduction
When we analyze algorithms, we often need to draw upon a body of mathematical
tools. Some of these tools are as simple as high-school algebra, but others may be
new to you. In Part I, we saw how to manipulate asymptotic notations and solverecurrences. This appendix comprises a compendium of several other concepts andmethods we use to analyze algorithms. As noted in the introduction to Part I, youmay have seen much of the material in this appendix before having read this book(although the speciﬁc notational conventions we use might occasionally differ fromthose you have seen elsewhere). Hence, you should treat this appendix as referencematerial. As in the rest of this book, however, we have included exercises andproblems, in order for you to improve your skills in these areas.
Appendix A offers methods for evaluating and bounding summations, which
occur frequently in the analysis of algorithms. Many of the formulas here appearin any calculus text, but you will ﬁnd it convenient to have these methods compiledin one place.
Appendix B contains basic deﬁnitions and notations for sets, relations, functions,
graphs, and trees. It also gives some basic properties of these mathematical objects.
Appendix C begins with elementary principles of counting: permutations, com-
binations, and the like. The remainder contains deﬁnitions and properties of basicprobability. Most of the algorithms in this book require no probability for theiranalysis, and thus you can easily omit the latter sections of the chapter on a ﬁrstreading, even without skimming them. Later, when you encounter a probabilisticanalysis that you want to understand better, you will ﬁnd Appendix C well orga-nized for reference purposes.1144 Part VIII Appendix: Mathematical Background
Appendix D deﬁnes matrices, their operations, and some of their basic prop-
erties. You have probably seen most of this material already if you have taken acourse in linear algebra, but you might ﬁnd it helpful to have one place to look forour notation and deﬁnitions.A Summations
When an algorithm contains an iterative control construct such as a while orfor
loop, we can express its running time as the sum of the times spent on each exe-cution of the body of the loop. For example, we found in Section 2.2 that the jth
iteration of insertion sort took time proportional to jin the worst case. By adding
up the time spent on each iteration, we obtained the summation (or series)
nX
jD2j:
When we evaluated this summation, we attained a bound of ‚.n2/on the worst-
case running time of the algorithm. This example illustrates why you should knowhow to manipulate and bound summations.
Section A.1 lists several basic formulas involving summations. Section A.2 of-
fers useful techniques for bounding summations. We present the formulas in Sec-
tion A.1 without proof, though proofs for some of them appear in Section A.2 to
illustrate the methods of that section. You can ﬁnd most of the other proofs in any
calculus text.
A.1 Summation formulas and properties
Given a sequence a1;a2;:::;a nof numbers, where nis a nonnegative integer, we
can write the ﬁnite sum a1Ca2C/SOH/SOH/SOHC anas
nX
kD1ak:
IfnD0, the value of the summation is deﬁned to be 0. The value of a ﬁnite series
is always well deﬁned, and we can add its terms in any order.
Given an inﬁnite sequence a1;a2;:::of numbers, we can write the inﬁnite sum
a1Ca2C/SOH/SOH/SOH as1146 Appendix A Summations
1X
kD1ak;
which we interpret to mean
lim
n!1nX
kD1ak:
If the limit does not exist, the series diverges ; otherwise, it converges . The terms
of a convergent series cannot always be added in any order. We can, however,rearrange the terms of an absolutely convergent series , that is, a seriesP
1
kD1ak
for which the seriesP1
kD1jakjalso converges.
Linearity
For any real number cand any ﬁnite sequences a1;a2;:::;a nandb1;b2;:::;b n,
nX
kD1.ca kCbk/DcnX
kD1akCnX
kD1bk:
The linearity property also applies to inﬁnite convergent series.
We can exploit the linearity property to manipulate summations incorporating
asymptotic notation. For example,
nX
kD1‚.f .k//D‚ nX
kD1f. k/!
:
In this equation, the ‚-notation on the left-hand side applies to the variable k,b u t
on the right-hand side, it applies to n. We can also apply such manipulations to
inﬁnite convergent series.
Arithmetic series
The summation
nX
kD1kD1C2C/SOH/SOH/SOHC n;
is anarithmetic series and has the value
nX
kD1kD1
2n.nC1/ (A.1)
D‚.n2/: (A.2)A.1 Summation formulas and properties 1147
Sums of squares and cubes
We have the following summations of squares and cubes:
nX
kD0k2Dn.nC1/.2nC1/
6; (A.3)
nX
kD0k3Dn2.nC1/2
4: (A.4)
Geometric series
For real x¤1, the summation
nX
kD0xkD1CxCx2C/SOH/SOH/SOHC xn
is ageometric orexponential series and has the value
nX
kD0xkDxnC1/NUL1
x/NUL1: (A.5)
When the summation is inﬁnite and jxj<1, we have the inﬁnite decreasing geo-
metric series
1X
kD0xkD1
1/NULx: (A.6)
Harmonic series
For positive integers n,t h enthharmonic number is
HnD1C1
2C1
3C1
4C/SOH/SOH/SOHC1
n
DnX
kD11
k
DlnnCO.1/ : (A.7)
(We shall prove a related bound in Section A.2.)
Integrating and differentiating series
By integrating or differentiating the formulas above, additional formulas arise. For
example, by differentiating both sides of the inﬁnite geometric series (A.6) andmultiplying by x,w eg e t1148 Appendix A Summations
1X
kD0kxkDx
.1/NULx/2(A.8)
forjxj<1.
Telescoping series
For any sequence a0;a1;:::;a n,
nX
kD1.ak/NULak/NUL1/Dan/NULa0; (A.9)
since each of the terms a1;a2;:::;a n/NUL1is added in exactly once and subtracted out
exactly once. We say that the sum telescopes . Similarly,
n/NUL1X
kD0.ak/NULakC1/Da0/NULan:
As an example of a telescoping sum, consider the series
n/NUL1X
kD11
k.kC1/:
Since we can rewrite each term as
1
k.kC1/D1
k/NUL1
kC1;
we get
n/NUL1X
kD11
k.kC1/Dn/NUL1X
kD1/DC21
k/NUL1
kC1/DC3
D1/NUL1
n:
Products
We can write the ﬁnite product a1a2/SOH/SOH/SOHanas
nY
kD1ak:
IfnD0, the value of the product is deﬁned to be 1. We can convert a formula with
a product to a formula with a summation by using the identity
lg nY
kD1ak!
DnX
kD1lgak:A.2 Bounding summations 1149
Exercises
A.1-1
Find a simple formula forPn
kD1.2k/NUL1/.
A.1-2 ?
Show thatPn
kD11=.2k/NUL1/Dln.p
n/CO.1/ by manipulating the harmonic
series.
A.1-3
Show thatP1
kD0k2xkDx.1Cx/=.1/NULx/3for0<jxj<1.
A.1-4 ?
Show thatP1
kD0.k/NUL1/=2kD0.
A.1-5 ?
Evaluate the sumP1
kD1.2kC1/x2k.
A.1-6
Prove thatPn
kD1O.f k.i//DO/NULPn
kD1fk.i//SOH
by using the linearity property of
summations.
A.1-7
Evaluate the productQn
kD12/SOH4k.
A.1-8 ?
Evaluate the productQn
kD2.1/NUL1=k2/.
A.2 Bounding summations
We have many techniques at our disposal for bounding the summations that de-
scribe the running times of algorithms. Here are some of the most frequently usedmethods.
Mathematical induction
The most basic way to evaluate a series is to use mathematical induction. As an
example, let us prove that the arithmetic seriesP
n
kD1kevaluates to1
2n.nC1/.W e
can easily verify this assertion for nD1. We make the inductive assumption that1150 Appendix A Summations
it holds for n, and we prove that it holds for nC1.W eh a v e
nC1X
kD1kDnX
kD1kC.nC1/
D1
2n.nC1/C.nC1/
D1
2.nC1/.nC2/ :
You don’t always need to guess the exact value of a summation in order to use
mathematical induction. Instead, you can use induction to prove a bound on a sum-
mation. As an example, let us prove that the geometric seriesPn
kD03kisO.3n/.
More speciﬁcally, let us prove thatPn
kD03k/DC4c3nfor some constant c.F o r t h e
initial condition nD0,w eh a v eP0
kD03kD1/DC4c/SOH1as long as c/NAK1. Assuming
that the bound holds for n, let us prove that it holds for nC1.W eh a v e
nC1X
kD03kDnX
kD03kC3nC1
/DC4c3nC3nC1(by the inductive hypothesis)
D/DC21
3C1
c/DC3
c3nC1
/DC4c3nC1
as long as .1=3C1=c//DC41or, equivalently, c/NAK3=2. Thus,Pn
kD03kDO.3n/,
as we wished to show.
We have to be careful when we use asymptotic notation to prove bounds by in-
duction. Consider the following fallacious proof thatPn
kD1kDO.n/ . Certainly,P1
kD1kDO.1/ . Assuming that the bound holds for n, we now prove it for nC1:
nC1X
kD1kDnX
kD1kC.nC1/
DO.n/C.nC1//c143 wrong!!
DO.nC1/ :
The bug in the argument is that the “constant” hidden by the “big-oh” grows with n
and thus is not constant. We have not shown that the same constant works for alln.
Bounding the terms
We can sometimes obtain a good upper bound on a series by bounding each term
of the series, and it often sufﬁces to use the largest term to bound the others. ForA.2 Bounding summations 1151
example, a quick upper bound on the arithmetic series (A.1) is
nX
kD1k/DC4nX
kD1n
Dn2:
In general, for a seriesPn
kD1ak,i fw el e t amaxDmax 1/DC4k/DC4nak,t h e n
nX
kD1ak/DC4n/SOHamax:
The technique of bounding each term in a series by the largest term is a weak
method when the series can in fact be bounded by a geometric series. Given theseriesP
n
kD0ak, suppose that akC1=ak/DC4rfor all k/NAK0,w h e r e 0<r<1 is a
constant. We can bound the sum by an inﬁnite decreasing geometric series, since
ak/DC4a0rk, and thus
nX
kD0ak/DC41X
kD0a0rk
Da01X
kD0rk
Da01
1/NULr:
We can apply this method to bound the summationP1
kD1.k=3k/. In order to
start the summation at kD0, we rewrite it asP1
kD0..kC1/=3kC1/.T h e ﬁ r s t
term ( a0)i s1=3, and the ratio ( r) of consecutive terms is
.kC2/=3kC2
.kC1/=3kC1D1
3/SOHkC2
kC1
/DC42
3
for all k/NAK0. Thus, we have
1X
kD1k
3kD1X
kD0kC1
3kC1
/DC41
3/SOH1
1/NUL2=3
D1:1152 Appendix A Summations
A common bug in applying this method is to show that the ratio of consecu-
tive terms is less than 1and then to assume that the summation is bounded by a
geometric series. An example is the inﬁnite harmonic series, which diverges since
1X
kD11
kD lim
n!1nX
kD11
k
D lim
n!1‚.lgn/
D1 :
The ratio of the .kC1/st and kth terms in this series is k=.kC1/ < 1 , but the series
is not bounded by a decreasing geometric series. To bound a series by a geometricseries, we must show that there is an r<1 ,w h i c hi sa constant , such that the ratio
of all pairs of consecutive terms never exceeds r. In the harmonic series, no such r
exists because the ratio becomes arbitrarily close to 1.
Splitting summations
One way to obtain bounds on a difﬁcult summation is to express the series as the
sum of two or more series by partitioning the range of the index and then to boundeach of the resulting series. For example, suppose we try to ﬁnd a lower boundon the arithmetic seriesP
n
kD1k, which we have already seen has an upper bound
ofn2. We might attempt to bound each term in the summation by the smallest term,
but since that term is 1, we get a lower bound of nfor the summation—far off from
our upper bound of n2.
We can obtain a better lower bound by ﬁrst splitting the summation. Assume for
convenience that nis even. We have
nX
kD1kDn=2X
kD1kCnX
kDn=2C1k
/NAKn=2X
kD10CnX
kDn=2C1.n=2/
D.n=2/2
D/DEL.n2/;
which is an asymptotically tight bound, sincePn
kD1kDO.n2/.
For a summation arising from the analysis of an algorithm, we can often split
the summation and ignore a constant number of the initial terms. Generally, this
technique applies when each term akin a summationPn
kD0akis independent of n.A.2 Bounding summations 1153
Then for any constant k0>0, we can write
nX
kD0akDk0/NUL1X
kD0akCnX
kDk0ak
D‚.1/CnX
kDk0ak;
since the initial terms of the summation are all constant and there are a constant
number of them. We can then use other methods to boundPn
kDk0ak. This tech-
nique applies to inﬁnite summations as well. For example, to ﬁnd an asymptotic
upper bound on
1X
kD0k2
2k;
we observe that the ratio of consecutive terms is
.kC1/2=2kC1
k2=2kD.kC1/2
2k2
/DC48
9
ifk/NAK3. Thus, the summation can be split into
1X
kD0k2
2kD2X
kD0k2
2kC1X
kD3k2
2k
/DC42X
kD0k2
2kC9
81X
kD0/DC28
9/DC3k
DO.1/ ;
since the ﬁrst summation has a constant number of terms and the second summation
is a decreasing geometric series.
The technique of splitting summations can help us determine asymptotic bounds
in much more difﬁcult situations. For example, we can obtain a bound of O.lgn/
on the harmonic series (A.7):
HnDnX
kD11
k:
We do so by splitting the range 1tonintoblgncC1pieces and upper-bounding
the contribution of each piece by 1.F o r iD0; 1; : : : ;blgnc,t h eith piece consists1154 Appendix A Summations
of the terms starting at 1=2iand going up to but not including 1=2iC1. The last
piece might contain terms not in the original harmonic series, and thus we have
nX
kD11
k/DC4blgncX
iD02i/NUL1X
jD01
2iCj
/DC4blgncX
iD02i/NUL1X
jD01
2i
DblgncX
iD01
/DC4lgnC1: (A.10)
Approximation by integrals
When a summation has the formPn
kDmf. k/ ,w h e r e f. k/ is a monotonically in-
creasing function, we can approximate it by integrals:
Zn
m/NUL1f. x/dx/DC4nX
kDmf. k//DC4ZnC1
mf. x/dx: (A.11)
Figure A.1 justiﬁes this approximation. The summation is represented as the area
of the rectangles in the ﬁgure, and the integral is the shaded region under the curve.When f. k/ is a monotonically decreasing function, we can use a similar method
to provide the bounds
Z
nC1
mf. x/dx/DC4nX
kDmf. k//DC4Zn
m/NUL1f. x/dx: (A.12)
The integral approximation (A.12) gives a tight estimate for the nth harmonic
number. For a lower bound, we obtain
nX
kD11
k/NAKZnC1
1dx
x
Dln.nC1/ : (A.13)
For the upper bound, we derive the inequality
nX
kD21
k/DC4Zn
1dx
x
Dlnn;A.2 Bounding summations 1155
n+1 n–1 n–2 m+2 m m–1f (m)
f (m+1)f (m+2)f (n–2)f (n–1)f (n)f(x)
x……n……
(a)m+1
n+1 n–1 n–2 m+2 m m–1f (m)
f (m+1)f (m+2)f (n–2)f (n–1)f (n)f(x)
x……n……
(b)m+1
Figure A.1 Approximation ofPn
kDmf. k/ by integrals. The area of each rectangle is shown
within the rectangle, and the total rectangle area represents the value of the summation. The in-tegral is represented by the shaded area under the curve. By comparing areas in (a),w eg e tR
n
m/NUL1f. x/dx/DC4Pn
kDmf. k/ , and then by shifting the rectangles one unit to the right, we getPn
kDmf. k//DC4RnC1
mf. x/dx in(b).1156 Appendix A Summations
which yields the bound
nX
kD11
k/DC4lnnC1: (A.14)
Exercises
A.2-1
Show thatPn
kD11=k2is bounded above by a constant.
A.2-2
Find an asymptotic upper bound on the summation
blgncX
kD0˙
n=2k/BEL
:
A.2-3
Show that the nth harmonic number is /DEL.lgn/by splitting the summation.
A.2-4
ApproximatePn
kD1k3with an integral.
A.2-5
Why didn’t we use the integral approximation (A.12) directly onPn
kD11=k to
obtain an upper bound on the nth harmonic number?
Problems
A-1 Bounding summations
Give asymptotically tight bounds on the following summations. Assume that r/NAK0
ands/NAK0are constants.
a.nX
kD1kr.
b.nX
kD1lgsk.Notes for Appendix A 1157
c.nX
kD1krlgsk.
Appendix notes
Knuth [209] provides an excellent reference for the material presented here. You
can ﬁnd basic properties of series in any good calculus book, such as Apostol [18]or Thomas et al. [334].B Sets, Etc.
Many chapters of this book touch on the elements of discrete mathematics. This
appendix reviews more completely the notations, deﬁnitions, and elementary prop-erties of sets, relations, functions, graphs, and trees. If you are already well versedin this material, you can probably just skim this chapter.
B.1 Sets
Asetis a collection of distinguishable objects, called its members orelements .I f
an object xis a member of a set S, we write x2S(read “ xi sam e m b e ro f S”
or, more brieﬂy, “ xis in S”). If xis not a member of S, we write x62S.W e
can describe a set by explicitly listing its members as a list inside braces. For
example, we can deﬁne a set Sto contain precisely the numbers 1,2,a n d 3by
writing SDf1; 2; 3g.S i n c e 2is a member of the set S, we can write 22S,a n d
since 4is not a member, we have 4…S. A set cannot contain the same object more
than once,1and its elements are not ordered. Two sets AandBareequal , written
ADB, if they contain the same elements. For example, f1; 2; 3; 1gDf1; 2; 3gD
f3; 2; 1g.
We adopt special notations for frequently encountered sets:
/SI;denotes the empty set , that is, the set containing no members.
/SI Zdenotes the set of integers , that is, the setf:::;/NUL2;/NUL1; 0; 1; 2; : : :g.
/SI Rdenotes the set of real numbers .
/SI Ndenotes the set of natural numbers , that is, the setf0; 1; 2; : : :g.2
1A variation of a set, which can contain the same object more than once, is called a multiset .
2Some authors start the natural numbers with 1instead of 0. The modern trend seems to be to start
with0.B.1 Sets 1159
If all the elements of a set Aare contained in a set B,t h a ti s ,i f x2Aimplies
x2B, then we write A/DC2Band say that Ais asubset ofB. A set Ais a
proper subset ofB, written A/SUBB,i fA/DC2BbutA¤B. (Some authors use the
symbol “/SUB” to denote the ordinary subset relation, rather than the proper-subset
relation.) For any set A,w eh a v e A/DC2A. For two sets AandB,w eh a v e ADB
if and only if A/DC2BandB/DC2A. For any three sets A,B,a n d C,i fA/DC2B
andB/DC2C,t h e n A/DC2C. For any set A,w eh a v e;/DC2A.
We sometimes deﬁne sets in terms of other sets. Given a set A, we can deﬁne a
setB/DC2Aby stating a property that distinguishes the elements of B. For example,
we can deﬁne the set of even integers by fxWx2Zandx=2is an integerg.T h e
colon in this notation is read “such that.” (Some authors use a vertical bar in placeof the colon.)
Given two sets AandB, we can also deﬁne new sets by applying set operations :
/SITheintersection of sets AandBis the set
A\BDfxWx2Aandx2Bg:
/SITheunion of sets AandBis the set
A[BDfxWx2Aorx2Bg:
/SIThedifference between two sets AandBis the set
A/NULBDfxWx2Aandx…Bg:
Set operations obey the following laws:
Empty set laws:
A\; D ; ;
A[; D A:
Idempotency laws:
A\ADA;
A[ADA:
Commutative laws:
A\BDB\A;
A[BDB[A:1160 Appendix B Sets, Etc.
A A A A A
AB B B B B
/NUL/NUL
.B\C/ [[
D DD D
A/NUL.B\C/ . A/NULB/ .A /NULC/C C C C C
Figure B.1 A Venn diagram illustrating the ﬁrst of DeMorgan’s laws (B.2). Each of the sets A,B,
andCis represented as a circle.
Associative laws:
A\.B\C/D.A\B/\C;
A[.B[C/D.A[B/[C:
Distributive laws:
A\.B[C/D.A\B/[.A\C/;
A[.B\C/D.A[B/\.A[C/:(B.1)
Absorption laws:
A\.A[B/DA;
A[.A\B/DA:
DeMorgan’s laws:
A/NUL.B\C/D.A/NULB/[.A/NULC/;
A/NUL.B[C/D.A/NULB/\.A/NULC/:(B.2)
Figure B.1 illustrates the ﬁrst of DeMorgan’s laws, using a Venn diagram : a graph-
ical picture in which sets are represented as regions of the plane.
Often, all the sets under consideration are subsets of some larger set Ucalled the
universe . For example, if we are considering various sets made up only of integers,
the set Zof integers is an appropriate universe. Given a universe U,w ed e ﬁ n et h e
complement of a set Aas
ADU/NULADfxWx2Uandx62Ag. For any set
A/DC2U, we have the following laws:
ADA;
A\
AD; ;
A[
ADU:B.1 Sets 1161
We can rewrite DeMorgan’s laws (B.2) with set complements. For any two sets
B;C/DC2U,w eh a v e
B\CD
B[
C;
B[CD
B\
C:
Two sets AandBaredisjoint if they have no elements in common, that is, if
A\BD;. A collection SDfSigof nonempty sets forms a partition of a set Sif
/SIthe sets are pairwise disjoint ,t h a ti s , Si;Sj2Sandi¤jimply Si\SjD;,
and
/SItheir union is S,t h a ti s ,
SD[
Si2SSi:
In other words, Sforms a partition of Sif each element of Sappears in exactly
oneSi2S.
The number of elements in a set is the cardinality (orsize) of the set, denoted jSj.
Two sets have the same cardinality if their elements can be put into a one-to-onecorrespondence. The cardinality of the empty set is j;jD0. If the cardinality of a
set is a natural number, we say the set is ﬁnite ; otherwise, it is inﬁnite . An inﬁnite
set that can be put into a one-to-one correspondence with the natural numbers Nis
countably inﬁnite ; otherwise, it is uncountable . For example, the integers Zare
countable, but the reals Rare uncountable.
For any two ﬁnite sets AandB, we have the identity
jA[BjDjAjCjBj/NULjA\Bj; (B.3)
from which we can conclude that
jA[Bj/DC4jAjCjBj:
IfAandBare disjoint, thenjA\BjD0and thusjA[BjDjAjCj
Bj.I f
A/DC2B,t h e njAj/DC4jBj.
A ﬁnite set of nelements is sometimes called an n-set.A 1-set is called a
singleton . A subset of kelements of a set is sometimes called a k-subset .
We denote the set of all subsets of a set S, including the empty set and Sitself,
by2S; we call 2Sthepower set ofS. For example, 2fa;bgDf;;fag;fbg;fa;bgg.
The power set of a ﬁnite set Shas cardinality 2jSj(see Exercise B.1-5).
We sometimes care about setlike structures in which the elements are ordered.
Anordered pair of two elements aandbis denoted .a; b/ and is deﬁned formally
as the set .a; b/Dfa;fa;bgg. Thus, the ordered pair .a; b/ isnott h es a m ea st h e
ordered pair .b; a/ .1162 Appendix B Sets, Etc.
TheCartesian product of two sets AandB, denoted A/STXB, is the set of all
ordered pairs such that the ﬁrst element of the pair is an element of Aand the
second is an element of B. More formally,
A/STXBDf.a; b/Wa2Aandb2Bg:
For example,fa;bg/STXfa;b;cgDf.a; a/; .a; b/; .a; c/; .b; a/; .b; b/; .b; c/ g.W h e n
AandBare ﬁnite sets, the cardinality of their Cartesian product is
jA/STXBjDjAj/SOHjBj: (B.4)
The Cartesian product of nsetsA1;A2;:::;A nis the set of n-tuples
A1/STXA2/STX/SOH/SOH/SOH/STX AnDf.a1;a2;:::;a n/Wai2AiforiD1 ;2;:::;ng;
whose cardinality is
jA1/STXA2/STX/SOH/SOH/SOH/STX AnjDjA1j/SOHjA2j/SOH/SOH/SOHjAnj
if all sets are ﬁnite. We denote an n-fold Cartesian product over a single set Aby
the set
AnDA/STXA/STX/SOH/SOH/SOH/STX A;
whose cardinality is jAnjDjAjnifAis ﬁnite. We can also view an n-tuple as a
ﬁnite sequence of length n(see page 1166).
Exercises
B.1-1
Draw Venn diagrams that illustrate the ﬁrst of the distributive laws (B.1).
B.1-2
Prove the generalization of DeMorgan’s laws to any ﬁnite collection of sets:
A1\A2\/SOH/SOH/SOH\ AnD
A1[
A2[/SOH/SOH/SOH[
 An;
A1[A2[/SOH/SOH/SOH[ AnD
A1\
A2\/SOH/SOH/SOH\
 An:B.2 Relations 1163
B.1-3 ?
Prove the generalization of equation (B.3), which is called the principle of inclu-
sion and exclusion :
jA1[A2[/SOH/SOH/SOH[ AnjD
jA1jCjA2jC/SOH/SOH/SOHCjAnj
/NULjA1\A2j/NULjA1\A3j/NUL/SOH/SOH/SOH (all pairs)
CjA1\A2\A3jC/SOH/SOH/SOH (all triples)
:::
C./NUL1/n/NUL1jA1\A2\/SOH/SOH/SOH\ Anj:
B.1-4
Show that the set of odd natural numbers is countable.
B.1-5
Show that for any ﬁnite set S, the power set 2Shas2jSjelements (that is, there
are2jSjdistinct subsets of S).
B.1-6
Give an inductive deﬁnition for an n-tuple by extending the set-theoretic deﬁnition
for an ordered pair.
B.2 Relations
Abinary relation Ron two sets AandBis a subset of the Cartesian product A/STXB.
If.a; b/2R, we sometimes write aRb . When we say that Ris a binary relation
on a set A, we mean that Ris a subset of A/STXA. For example, the “less than”
relation on the natural numbers is the set f.a; b/Wa;b2Nanda<bg.A n n-ary
relation on sets A1;A2;:::;A nis a subset of A1/STXA2/STX/SOH/SOH/SOH/STX An.
A binary relation R/DC2A/STXAisreﬂexive if
aRa
for all a2A. For example, “D”a n d“/DC4” are reﬂexive relations on N,b u t“ <”i s
not. The relation Rissymmetric if
aRb implies bRa
for all a;b2A. For example, “D” is symmetric, but “ <”a n d“/DC4” are not. The
relation Ristransitive if
aRb andbRc imply aRc1164 Appendix B Sets, Etc.
for all a;b;c2A. For example, the relations “ <,” “/DC4,” and “D” are transitive, but
the relation RDf.a; b/Wa;b2NandaDb/NUL1gis not, since 3R4 and4R5
do not imply 3R5 .
A relation that is reﬂexive, symmetric, and transitive is an equivalence relation .
For example, “D” is an equivalence relation on the natural numbers, but “ <” is not.
IfRis an equivalence relation on a set A, then for a2A,t h eequivalence class
ofais the set Œa/c141Dfb2AWaRbg, that is, the set of all elements equivalent to a.
For example, if we deﬁne RDf.a; b/Wa;b2NandaCbis an even numberg,
thenRis an equivalence relation, since aCais even (reﬂexive), aCbis even
implies bCais even (symmetric), and aCbis even and bCcis even imply
aCcis even (transitive). The equivalence class of 4isŒ4/c141Df0;2;4;6;:::g,a n d
the equivalence class of 3isŒ3/c141Df1; 3; 5; 7; : : :g. A basic theorem of equivalence
classes is the following.
Theorem B.1 (An equivalence relation is the same as a partition)
The equivalence classes of any equivalence relation Ron a set Aform a partition
ofA, and any partition of Adetermines an equivalence relation on Afor which the
sets in the partition are the equivalence classes.
Proof For the ﬁrst part of the proof, we must show that the equivalence classes
ofRare nonempty, pairwise-disjoint sets whose union is A. Because Ris reﬂex-
ive,a2Œa/c141, and so the equivalence classes are nonempty; moreover, since every
element a2Abelongs to the equivalence class Œa/c141, the union of the equivalence
classes is A. It remains to show that the equivalence classes are pairwise disjoint,
that is, if two equivalence classes Œa/c141andŒb/c141have an element cin common, then
they are in fact the same set. Suppose that aRc andbRc . By symmetry, cRb ,
and by transitivity, aRb . Thus, for any arbitrary element x2Œa/c141,w eh a v e xRa
and, by transitivity, xRb , and thus Œa/c141/DC2Œb/c141. Similarly, Œb/c141/DC2Œa/c141, and thus
Œa/c141DŒb/c141.
For the second part of the proof, let ADfAigbe a partition of A, and deﬁne
RDf.a; b/Wthere exists isuch that a2Aiandb2Aig. We claim that Ris an
equivalence relation on A. Reﬂexivity holds, since a2Aiimplies aRa . Symme-
try holds, because if aRb ,t h e n aandbare in the same set Ai, and hence bRa .
IfaRb andbRc , then all three elements are in the same set Ai, and thus aRc
and transitivity holds. To see that the sets in the partition are the equivalenceclasses of R, observe that if a2A
i,t h e n x2Œa/c141implies x2Ai,a n d x2Ai
implies x2Œa/c141.
A binary relation Ron a set Aisantisymmetric if
aRb andbRa imply aDb:B.2 Relations 1165
For example, the “/DC4” relation on the natural numbers is antisymmetric, since a/DC4b
andb/DC4aimply aDb. A relation that is reﬂexive, antisymmetric, and transitive
is apartial order , and we call a set on which a partial order is deﬁned a partially
ordered set . For example, the relation “is a descendant of” is a partial order on the
set of all people (if we view individuals as being their own descendants).
In a partially ordered set A, there may be no single “maximum” element asuch
thatbRa for all b2A. Instead, the set may contain several maximal elements a
such that for no b2A,w h e r e b¤a, is it the case that aRb . For example, a
collection of different-sized boxes may contain several maximal boxes that don’t
ﬁt inside any other box, yet it has no single “maximum” box into which any otherbox will ﬁt.
3
A relation Ron a set Ais atotal relation if for all a;b2A,w eh a v e aRb
orbRa (or both), that is, if every pairing of elements of Ais related by R.A
partial order that is also a total relation is a total order orlinear order . For example,
the relation “/DC4” is a total order on the natural numbers, but the “is a descendant
of” relation is not a total order on the set of all people, since there are individualsneither of whom is descended from the other. A total relation that is transitive, butnot necessarily reﬂexive and antisymmetric, is a total preorder .
Exercises
B.2-1
Prove that the subset relation “ /DC2” on all subsets of Zis a partial order but not a
total order.
B.2-2
Show that for any positive integer n, the relation “equivalent modulo n” is an equiv-
alence relation on the integers. (We say that a/DC1b.mod n/if there exists an
integer qsuch that a/NULbDqn.) Into what equivalence classes does this relation
partition the integers?
B.2-3
Give examples of relations that are
a.reﬂexive and symmetric but not transitive,
b.reﬂexive and transitive but not symmetric,
c.symmetric and transitive but not reﬂexive.
3To be precise, in order for the “ﬁt inside” relation to be a partial order, we need to view a box as
ﬁtting inside itself.1166 Appendix B Sets, Etc.
B.2-4
LetSbe a ﬁnite set, and let Rbe an equivalence relation on S/STXS. Show that if
in addition Ris antisymmetric, then the equivalence classes of Swith respect to R
are singletons.
B.2-5
Professor Narcissus claims that if a relation Ris symmetric and transitive, then it is
also reﬂexive. He offers the following proof. By symmetry, aRb implies bRa .
Transitivity, therefore, implies aRa . Is the professor correct?
B.3 Functions
Given two sets AandB,afunction fis a binary relation on AandBsuch that
for all a2A, there exists precisely one b2Bsuch that .a; b/2f. The set Ais
called the domain off, and the set Bis called the codomain off. We sometimes
write fWA!B;a n di f .a; b/2f, we write bDf. a / ,s i n c e bis uniquely
determined by the choice of a.
Intuitively, the function fassigns an element of Bto each element of A.N o
element of Ais assigned two different elements of B, but the same element of B
can be assigned to two different elements of A. For example, the binary relation
fDf.a; b/Wa;b2NandbDamod2g
is a function fWN!f0; 1g, since for each natural number a, there is exactly one
value binf0; 1gsuch that bDamod2. For this example, 0Df. 0 / ,1Df. 1 / ,
0Df. 2 / , etc. In contrast, the binary relation
gDf.a; b/Wa;b2NandaCbis eveng
is not a function, since .1; 3/ and.1; 5/ are both in g, and thus for the choice aD1,
there is not precisely one bsuch that .a; b/2g.
Given a function fWA!B,i fbDf. a / , we say that ais theargument off
and that bis thevalue offata. We can deﬁne a function by stating its value for
every element of its domain. For example, we might deﬁne f .n/D2nforn2N,
which means fDf.n; 2n/Wn2Ng. Two functions fandgareequal if they
have the same domain and codomain and if, for all ain the domain, f. a /Dg.a/ .
Aﬁnite sequence of length nis a function fwhose domain is the set of n
integersf0; 1; : : : ; n/NUL1g. We often denote a ﬁnite sequence by listing its values:
hf. 0 / ;f. 1 / ;:::;f. n /NUL1/i.A ninﬁnite sequence is a function whose domain is
the set Nof natural numbers. For example, the Fibonacci sequence, deﬁned by
recurrence (3.22), is the inﬁnite sequence h0; 1; 1; 2; 3; 5; 8; 13; 21; : : : i.B.3 Functions 1167
When the domain of a function fis a Cartesian product, we often omit the extra
parentheses surrounding the argument of f. For example, if we had a function
fWA1/STXA2/STX/SOH/SOH/SOH/STX An!B, we would write bDf. a 1;a2;:::;a n/instead
ofbDf ..a 1;a2;:::;a n//. We also call each aianargument to the function f,
though technically the (single) argument to fis the n-tuple .a1;a2;:::;a n/.
IffWA!Bis a function and bDf. a / , then we sometimes say that bis the
image ofaunder f. The image of a set A0/DC2Aunder fis deﬁned by
f. A0/Dfb2BWbDf. a / for some a2A0g:
Therange offis the image of its domain, that is, f. A / . For example, the range
of the function fWN! Ndeﬁned by f .n/D2nisf.N/DfmWmD2nfor
some n2Ng, in other words, the set of nonnegative even integers.
A function is a surjection if its range is its codomain. For example, the function
f .n/Dbn=2cis a surjective function from NtoN, since every element in N
appears as the value of ffor some argument. In contrast, the function f .n/D2n
is not a surjective function from NtoN, since no argument to fcan produce 3as a
value. The function f .n/D2nis, however, a surjective function from the natural
numbers to the even numbers. A surjection fWA!Bis sometimes described as
mapping AontoB. When we say that fis onto, we mean that it is surjective.
A function fWA!Bis aninjection if distinct arguments to fproduce
distinct values, that is, if a¤a0implies f. a /¤f. a0/. For example, the function
f .n/D2nis an injective function from NtoN, since each even number bis the
image under fof at most one element of the domain, namely b=2. The function
f .n/Dbn=2cis not injective, since the value 1is produced by two arguments: 2
and3. An injection is sometimes called a one-to-one function.
A function fWA!Bis abijection if it is injective and surjective. For example,
the function f .n/D./NUL1/ndn=2eis a bijection from NtoZ:
0! 0;
1!/NUL 1;
2! 1;
3!/NUL 2;
4! 2;
:::
The function is injective, since no element of Zis the image of more than one
element of N. It is surjective, since every element of Zappears as the image of
some element of N. Hence, the function is bijective. A bijection is sometimes
called a one-to-one correspondence , since it pairs elements in the domain and
codomain. A bijection from a set Ato itself is sometimes called a permutation .
When a function fis bijective, we deﬁne its inverse f/NUL1as
f/NUL1.b/Daif and only if f. a /Db:1168 Appendix B Sets, Etc.
For example, the inverse of the function f .n/D./NUL1/ndn=2eis
f/NUL1.m/D(
2m ifm/NAK0;
/NUL2m/NUL1ifm<0:
Exercises
B.3-1
LetAandBbe ﬁnite sets, and let fWA!Bbe a function. Show that
a.iffis injective, thenjAj/DC4jBj;
b.iffis surjective, thenjAj/NAKjBj.
B.3-2
Is the function f. x/DxC1bijective when the domain and the codomain are N?
Is it bijective when the domain and the codomain are Z?
B.3-3
Give a natural deﬁnition for the inverse of a binary relation such that if a relation
is in fact a bijective function, its relational inverse is its functional inverse.
B.3-4 ?
Give a bijection from ZtoZ/STXZ.
B.4 Graphs
This section presents two kinds of graphs: directed and undirected. Certain def-
initions in the literature differ from those given here, but for the most part, the
differences are slight. Section 22.1 shows how we can represent graphs in com-puter memory.
Adirected graph (ordigraph )Gi sap a i r .V; E/ ,w h e r e Vis a ﬁnite set and E
is a binary relation on V. The set Vis called the vertex set ofG, and its elements
are called vertices (singular: vertex ). The set Eis called the edge set ofG, and its
elements are called edges . Figure B.2(a) is a pictorial representation of a directed
graph on the vertex set f1; 2; 3; 4; 5; 6g. Vertices are represented by circles in the
ﬁgure, and edges are represented by arrows. Note that self-loops —edges from a
vertex to itself—are possible.
In an undirected graph GD.V; E/ , the edge set Econsists of unordered
pairs of vertices, rather than ordered pairs. That is, an edge is a set fu; /ETBg,w h e r eB.4 Graphs 1169
12 3
45 6
(a)12 345 6
(b)12 3
6
(c)
Figure B.2 Directed and undirected graphs. (a)A directed graph GD.V; E/ ,w h e r e VD
f1; 2; 3; 4; 5; 6gandEDf.1; 2/; .2; 2/; .2; 4/; .2; 5/; .4; 1/; .4; 5/; .5; 4/; .6; 3/ g. The edge .2; 2/
is a self-loop. (b)An undirected graph GD.V; E/ ,w h e r e VDf1; 2; 3; 4; 5; 6gandED
f.1; 2/; .1; 5/; .2; 5/; .3; 6/ g.T h e v e r t e x 4is isolated. (c)The subgraph of the graph in part (a)
induced by the vertex set f1; 2; 3; 6g.
u; /ETB2Vandu¤/ETB. By convention, we use the notation .u; /ETB/ for an edge, rather
than the set notation fu; /ETBg, and we consider .u; /ETB/ and./ETB; u/ to be the same edge.
In an undirected graph, self-loops are forbidden, and so every edge consists of twodistinct vertices. Figure B.2(b) is a pictorial representation of an undirected graphon the vertex setf1; 2; 3; 4; 5; 6g.
Many deﬁnitions for directed and undirected graphs are the same, although cer-
tain terms have slightly different meanings in the two contexts. If .u; /ETB/ is an edge
in a directed graph GD.V; E/ , we say that .u; /ETB/ isincident from orleaves
vertex uand is incident to orenters vertex /ETB. For example, the edges leaving ver-
tex2in Figure B.2(a) are .2; 2/ ,.2; 4/ ,a n d .2; 5/ . The edges entering vertex 2are
.1; 2/ and.2; 2/ .I f.u; /ETB/ is an edge in an undirected graph GD.V; E/ , we say
that.u; /ETB/ isincident on vertices uand/ETB. In Figure B.2(b), the edges incident on
vertex 2are.1; 2/ and.2; 5/ .
If.u; /ETB/ is an edge in a graph GD.V; E/ , we say that vertex /ETBisadjacent to
vertex u. When the graph is undirected, the adjacency relation is symmetric. When
the graph is directed, the adjacency relation is not necessarily symmetric. If /ETBis
adjacent to uin a directed graph, we sometimes write u!/ETB. In parts (a) and (b)
of Figure B.2, vertex 2is adjacent to vertex 1, since the edge .1; 2/ belongs to both
graphs. Vertex 1isnotadjacent to vertex 2in Figure B.2(a), since the edge .2; 1/
does not belong to the graph.
Thedegree of a vertex in an undirected graph is the number of edges incident on
it. For example, vertex 2in Figure B.2(b) has degree 2. A vertex whose degree is 0,
such as vertex 4in Figure B.2(b), is isolated . In a directed graph, the out-degree
of a vertex is the number of edges leaving it, and the in-degree o fav e r t e xi st h e
number of edges entering it. The degree of a vertex in a directed graph is its in-1170 Appendix B Sets, Etc.
degree plus its out-degree. Vertex 2in Figure B.2(a) has in-degree 2, out-degree 3,
and degree 5.
Apath oflength kfrom a vertex uto a vertex u0in a graph GD.V; E/
is a sequenceh/ETB0;/ETB1;/ETB2; :::;/ETB kiof vertices such that uD/ETB0,u0D/ETBk,a n d
./ETBi/NUL1;/ETBi/2EforiD1 ;2;:::;k . The length of the path is the number of
edges in the path. The path contains the vertices /ETB0;/ETB1;:::;/ETB kand the edges
./ETB0;/ETB1/; ./ETB 1;/ETB2/ ;:::;. /ETB k/NUL1;/ETBk/.( T h e r ei sa l w a y sa 0-length path from utou.) If
there is a path pfrom utou0, we say that u0isreachable from uviap,w h i c hw e
sometimes write as up;u0ifGis directed. A path is simple4if all vertices in the
path are distinct. In Figure B.2(a), the path h1; 2; 5; 4iis a simple path of length 3.
The pathh2;5;4;5iis not simple.
Asubpath of path pDh/ETB0;/ETB1;:::;/ETB kiis a contiguous subsequence of its ver-
tices. That is, for any 0/DC4i/DC4j/DC4k, the subsequence of vertices h/ETBi;/ETBiC1;:::;/ETB ji
is a subpath of p.
In a directed graph, a path h/ETB0;/ETB1;:::;/ETB kiforms a cycle if/ETB0D/ETBkand the
path contains at least one edge. The cycle is simple if, in addition, /ETB1;/ETB2;:::;/ETB k
are distinct. A self-loop is a cycle of length 1.T w op a t h sh/ETB0;/ETB1;/ETB2;:::;/ETB k/NUL1;/ETB0i
andh/ETB0
0;/ETB0
1;/ETB0
2;:::;/ETB0
k/NUL1;/ETB0
0iform the same cycle if there exists an integer jsuch
that/ETB0
iD/ETB.iCj/modkforiD0; 1; : : : ; k/NUL1. In Figure B.2(a), the path h1;2; 4;1i
forms the same cycle as the paths h2;4; 1; 2iandh4;1; 2;4i. This cycle is simple,
but the cycleh1; 2; 4; 5; 4; 1iis not. The cycleh2; 2iformed by the edge .2; 2/ is
a self-loop. A directed graph with no self-loops is simple . In an undirected graph,
ap a t hh/ETB0;/ETB1;:::;/ETB kiforms a cycle ifk/NAK3and/ETB0D/ETBk;t h ec y c l ei s simple if
/ETB1;/ETB2;:::;/ETB kare distinct. For example, in Figure B.2(b), the path h1; 2; 5; 1iis a
simple cycle. A graph with no cycles is acyclic .
An undirected graph is connected if every vertex is reachable from all other
vertices. The connected components of a graph are the equivalence classes of
vertices under the “is reachable from” relation. The graph in Figure B.2(b) hasthree connected components: f1; 2; 5g,f3; 6g,a n df4g. Every vertex inf1; 2; 5gis
reachable from every other vertex in f1; 2; 5g. An undirected graph is connected
if it has exactly one connected component. The edges of a connected componentare those that are incident on only the vertices of the component; in other words,edge .u; /ETB/ is an edge of a connected component only if both uand/ETBare vertices
of the component.
A directed graph is strongly connected if every two vertices are reachable from
each other. The strongly connected components of a directed graph are the equiv-
4Some authors refer to what we call a path as a “walk” and to what we call a simple path as just a
“path.” We use the terms “path” and “simple path” throughout this book in a manner consistent withtheir deﬁnitions.B.4 Graphs 1171
12
3
4 56
uv wxyz
(a)12
3
45
uv wxy
(b)G
G′
Figure B.3 (a) A pair of isomorphic graphs. The vertices of the top graph are mapped to the
vertices of the bottom graph by f. 1 /Du; f .2/D/ETB;f.3/Dw;f.4/Dx;f.5/Dy;f.6/D´.
(b)Two graphs that are not isomorphic, since the top graph has a vertex of degree 4 and the bottom
graph does not.
alence classes of vertices under the “are mutually reachable” relation. A directed
graph is strongly connected if it has only one strongly connected component. The
graph in Figure B.2(a) has three strongly connected components: f1;2;4;5g,f3g,
andf6g. All pairs of vertices in f1;2;4;5gare mutually reachable. The ver-
ticesf3; 6gdo not form a strongly connected component, since vertex 6cannot
be reached from vertex 3.
Two graphs GD.V; E/ andG0D.V0;E0/areisomorphic if there exists a
bijection fWV!V0such that .u; /ETB/2Eif and only if .f .u/; f ./ETB//2E0.
In other words, we can relabel the vertices of Gto be vertices of G0, maintain-
ing the corresponding edges in GandG0. Figure B.3(a) shows a pair of iso-
morphic graphs GandG0with respective vertex sets VDf1; 2; 3; 4; 5; 6gand
V0Dfu; /ETB; w; x; y; ´g. The mapping from VtoV0given by f. 1 /Du; f .2/D/ETB;
f. 3 /Dw;f.4/Dx;f.5/Dy;f.6/D´provides the required bijective func-
tion. The graphs in Figure B.3(b) are not isomorphic. Although both graphs have5vertices and 7edges, the top graph has a vertex of degree 4and the bottom graph
does not.
We say that a graph G
0D.V0;E0/is asubgraph ofGD.V; E/ ifV0/DC2V
andE0/DC2E. Given a set V0/DC2V, the subgraph of Ginduced byV0is the graph
G0D.V0;E0/,w h e r e
E0Df.u; /ETB/2EWu; /ETB2V0g:1172 Appendix B Sets, Etc.
The subgraph induced by the vertex set f1; 2; 3; 6gin Figure B.2(a) appears in
Figure B.2(c) and has the edge set f.1; 2/; .2; 2/; .6; 3/g.
Given an undirected graph GD.V; E/ ,t h edirected version ofGis the directed
graph G0D.V; E0/,w h e r e .u; /ETB/2E0if and only if .u; /ETB/2E.T h a t i s , w e
replace each undirected edge .u; /ETB/ inGby the two directed edges .u; /ETB/ and./ETB; u/
in the directed version. Given a directed graph GD.V; E/ ,t h eundirected version
ofGis the undirected graph G0D.V; E0/,w h e r e .u; /ETB/2E0if and only if u¤/ETB
and.u; /ETB/2E. That is, the undirected version contains the edges of G“with
their directions removed” and with self-loops eliminated. (Since .u; /ETB/ and./ETB; u/
are the same edge in an undirected graph, the undirected version of a directedgraph contains it only once, even if the directed graph contains both edges .u; /ETB/
and./ETB; u/ .) In a directed graph GD.V; E/ ,aneighbor of a vertex uis any vertex
that is adjacent to uin the undirected version of G.T h a ti s , /ETBis a neighbor of uif
u¤/ETBand either .u; /ETB/2Eor./ETB; u/2E. In an undirected graph, uand/ETBare
neighbors if they are adjacent.
Several kinds of graphs have special names. A complete graph is an undirected
graph in which every pair of vertices is adjacent. A bipartite graph is an undirected
graph GD.V; E/ in which Vcan be partitioned into two sets V
1andV2such that
.u; /ETB/2Eimplies either u2V1and/ETB2V2oru2V2and/ETB2V1. That is, all
edges go between the two sets V1andV2. An acyclic, undirected graph is a forest ,
and a connected, acyclic, undirected graph is a (free) tree (see Section B.5). We
often take the ﬁrst letters of “directed acyclic graph” and call such a graph a dag.
There are two variants of graphs that you may occasionally encounter. A multi-
graph is like an undirected graph, but it can have both multiple edges between ver-
tices and self-loops. A hypergraph is like an undirected graph, but each hyperedge ,
rather than connecting two vertices, connects an arbitrary subset of vertices. Manyalgorithms written for ordinary directed and undirected graphs can be adapted torun on these graphlike structures.
Thecontraction of an undirected graph GD.V; E/ by an edge eD.u; /ETB/ is a
graph G
0D.V0;E0/,w h e r e V0DV/NULfu; /ETBg[fxgandxis a new vertex. The set
of edges E0is formed from Eby deleting the edge .u; /ETB/ and, for each vertex w
incident on uor/ETB, deleting whichever of .u; w/ and./ETB; w/ is inEand adding the
new edge .x; w/ . In effect, uand/ETBare “contracted” into a single vertex.
Exercises
B.4-1
Attendees of a faculty party shake hands to greet each other, and each professorremembers how many times he or she shook hands. At the end of the party, thedepartment head adds up the number of times that each professor shook hands.B.5 Trees 1173
Show that the result is even by proving the handshaking lemma :i fGD.V; E/ is
an undirected graph, then
X
/ETB2Vdegree ./ETB/D2jEj:
B.4-2
Show that if a directed or undirected graph contains a path between two vertices u
and/ETB, then it contains a simple path between uand/ETB. Show that if a directed graph
contains a cycle, then it contains a simple cycle.
B.4-3
Show that any connected, undirected graph GD.V; E/ satisﬁesjEj/NAKjVj/NUL1.
B.4-4
Verify that in an undirected graph, the “is reachable from” relation is an equiv-alence relation on the vertices of the graph. Which of the three properties of anequivalence relation hold in general for the “is reachable from” relation on thevertices of a directed graph?
B.4-5
What is the undirected version of the directed graph in Figure B.2(a)? What is thedirected version of the undirected graph in Figure B.2(b)?
B.4-6 ?
Show that we can represent a hypergraph by a bipartite graph if we let incidence inthe hypergraph correspond to adjacency in the bipartite graph. ( Hint: Let one set
of vertices in the bipartite graph correspond to vertices of the hypergraph, and let
the other set of vertices of the bipartite graph correspond to hyperedges.)
B.5 Trees
As with graphs, there are many related, but slightly different, notions of trees. Thissection presents deﬁnitions and mathematical properties of several kinds of trees.Sections 10.4 and 22.1 describe how we can represent trees in computer memory.
B.5.1 Free trees
As deﬁned in Section B.4, a free tree is a connected, acyclic, undirected graph. We
often omit the adjective “free” when we say that a graph is a tree. If an undirectedgraph is acyclic but possibly disconnected, it is a forest . Many algorithms that work1174 Appendix B Sets, Etc.
(a) (b) (c)
Figure B.4 (a) A free tree. (b)Af o r e s t . (c)A graph that contains a cycle and is therefore neither
at r e en o raf o r e s t .
for trees also work for forests. Figure B.4(a) shows a free tree, and Figure B.4(b)
shows a forest. The forest in Figure B.4(b) is not a tree because it is not connected.
The graph in Figure B.4(c) is connected but neither a tree nor a forest, because itcontains a cycle.
The following theorem captures many important facts about free trees.
Theorem B.2 (Properties of free trees)
LetGD.V; E/ be an undirected graph. The following statements are equivalent.
1.Gis a free tree.
2. Any two vertices in Gare connected by a unique simple path.
3.Gis connected, but if any edge is removed from E, the resulting graph is dis-
connected.
4.Gis connected, andjEjDjVj/NUL1.
5.Gis acyclic, andjEjDjVj/NUL1.
6.Gis acyclic, but if any edge is added to E, the resulting graph contains a cycle.
Proof (1))(2): Since a tree is connected, any two vertices in Gare connected
by at least one simple path. Suppose, for the sake of contradiction, that vertices u
and/ETBare connected by two distinct simple paths p
1andp2, as shown in Figure B.5.
Letwbe the vertex at which the paths ﬁrst diverge; that is, wis the ﬁrst vertex
on both p1andp2whose successor on p1isxand whose successor on p2isy,
where x¤y.L e t ´be the ﬁrst vertex at which the paths reconverge; that is, ´is
the ﬁrst vertex following wonp1that is also on p2.L e t p0be the subpath of p1
from wthrough xto´,a n dl e t p00be the subpath of p2from wthrough yto´.
Paths p0andp00share no vertices except their endpoints. Thus, the path obtained by
concatenating p0and the reverse of p00is a cycle, which contradicts our assumptionB.5 Trees 1175
uw
zvx
yp′
p′′
Figure B.5 A step in the proof of Theorem B.2: if (1) Gis a free tree, then (2) any two vertices
inGare connected by a unique simple path. Assume for the sake of contradiction that vertices u
and/ETBare connected by two distinct simple paths p1andp2. These paths ﬁrst diverge at vertex w,
and they ﬁrst reconverge at vertex ´. The path p0concatenated with the reverse of the path p00forms
a cycle, which yields the contradiction.
thatGis a tree. Thus, if Gis a tree, there can be at most one simple path between
two vertices.
(2))(3): If any two vertices in Gare connected by a unique simple path,
thenGis connected. Let .u; /ETB/ be any edge in E. This edge is a path from uto/ETB,
and so it must be the unique path from uto/ETB. If we remove .u; /ETB/ from G,t h e r e
is no path from uto/ETB, and hence its removal disconnects G.
(3))(4): By assumption, the graph Gis connected, and by Exercise B.4-3, we
havejEj/NAKjVj/NUL1. We shall provejEj/DC4jVj/NUL1by induction. A connected
graph with nD1ornD2vertices has n/NUL1edges. Suppose that Ghasn/NAK3
vertices and that all graphs satisfying (3) with fewer than nvertices also satisfy
jEj/DC4jVj/NUL1. Removing an arbitrary edge from Gseparates the graph into k/NAK2
connected components (actually kD2). Each component satisﬁes (3), or else G
would not satisfy (3). If we view each connected component Vi, with edge set Ei,
as its own free tree, then because each component has fewer than jVjvertices, by
the inductive hypothesis we have jEij/DC4jVij/NUL1. Thus, the number of edges in all
components combined is at most jVj/NULk/DC4jVj/NUL2. Adding in the removed edge
yieldsjEj/DC4jVj/NUL1.
(4))(5): Suppose that Gis connected and that jEjDjVj/NUL1. We must show
thatGis acyclic. Suppose that Ghas a cycle containing kvertices /ETB1;/ETB2;:::;/ETB k,
and without loss of generality assume that this cycle is simple. Let GkD.Vk;Ek/
be the subgraph of Gconsisting of the cycle. Note that jVkjDjEkjDk.
Ifk<jVj, there must be a vertex /ETBkC12V/NULVkthat is adjacent to some ver-
tex/ETBi2Vk,s i n c e Gis connected. Deﬁne GkC1D.VkC1;EkC1/to be the sub-
graph of GwithVkC1DVk[f/ETBkC1gandEkC1DEk[f./ETBi;/ETBkC1/g. Note that
jVkC1jDjEkC1jDkC1.I fkC1<jVj, we can continue, deﬁning GkC2in
the same manner, and so forth, until we obtain GnD.Vn;En/,w h e r e nDjVj,1176 Appendix B Sets, Etc.
VnDV,a n djEnjDjVnjDjVj.S i n c e Gnis a subgraph of G,w eh a v e En/DC2E,
and hencejEj/NAKjVj, which contradicts the assumption that jEjDjVj/NUL1. Thus,
Gis acyclic.
(5))(6): Suppose that Gis acyclic and that jEjDjVj/NUL1.L e t kbe the
number of connected components of G. Each connected component is a free tree
by deﬁnition, and since (1) implies (5), the sum of all edges in all connected com-ponents of GisjVj/NULk. Consequently, we must have kD1,a n d Gis in fact a
tree. Since (1) implies (2), any two vertices in Gare connected by a unique simple
path. Thus, adding any edge to Gcreates a cycle.
(6))(1): Suppose that Gis acyclic but that adding any edge to Ecreates a
cycle. We must show that Gis connected. Let uand/ETBbe arbitrary vertices in G.
Ifuand/ETBare not already adjacent, adding the edge .u; /ETB/ creates a cycle in which
all edges but .u; /ETB/ belong to G. Thus, the cycle minus edge .u; /ETB/ must contain a
path from uto/ETB, and since u
and/ETBwere chosen arbitrarily, Gis connected.
B.5.2 Rooted and ordered trees
Arooted tree is a free tree in which one of the vertices is distinguished from the
others. We call the distinguished vertex the root of the tree. We often refer to a
vertex of a rooted tree as a node5of the tree. Figure B.6(a) shows a rooted tree on
a set of 12nodes with root 7.
Consider a node xin a rooted tree Twith root r. We call any node yon the
unique simple path from rtoxanancestor ofx.I fyis an ancestor of x,t h e n xis
adescendant ofy. (Every node is both an ancestor and a descendant of itself.) If y
is an ancestor of xandx¤y,t h e n yis aproper ancestor ofxandxis aproper
descendant ofy.T h esubtree rooted at xis the tree induced by descendants of x,
rooted at x. For example, the subtree rooted at node 8in Figure B.6(a) contains
nodes 8,6,5,a n d 9.
If the last edge on the simple path from the root rof a tree Tto a node xis.y; x/ ,
thenyis theparent ofx,a n d xis achild ofy. The root is the only node in Twith
no parent. If two nodes have the same parent, they are siblings . A node with no
children is a leaf orexternal node . A nonleaf node is an internal node .
5The term “node” is often used in the graph theory literature as a synonym for “vertex.” We reserve
the term “node” to mean a vertex of a rooted tree.B.5 Trees 1177
9658
11231 07
11 24
height = 4depth 0
depth 1depth 2depth 3depth 4
(a)
9658 1231 07
11 24
(b)1
Figure B.6 Rooted and ordered trees. (a)A rooted tree with height 4. The tree is drawn in a
standard way: the root (node 7) is at the top, its children (nodes with depth 1) are beneath it, their
children (nodes with depth 2) are beneath them, and so forth. If the tree is ordered, the relative left-
to-right order of the children of a node matters; otherwise it doesn’t. (b)Another rooted tree. As a
rooted tree, it is identical to the tree in (a), but as an ordered tree it is different, since the children of
node 3appear in a different order.
The number of children of a node xin a rooted tree Tequals the degree ofx.6
The length of the simple path from the root rto a node xis the depth ofxinT.
Alevel of a tree consists of all nodes at the same depth. The height of a node in a
tree is the number of edges on the longest simple downward path from the node toa leaf, and the height of a tree is the height of its root. The height of a tree is alsoequal to the largest depth of any node in the tree.
Anordered tree is a rooted tree in which the children of each node are ordered.
That is, if a node has kc h i l d r e n , t h e n t h e r e i s a ﬁ r s t c h i l d , a s e c o n d c h i l d , ...,
and a kth child. The two trees in Figure B.6 are different when considered to be
ordered trees, but the same when considered to be just rooted trees.
B.5.3 Binary and positional trees
We deﬁne binary trees recursively. A binary tree Tis a structure deﬁned on a ﬁnite
set of nodes that either
/SIcontains no nodes, or
6Notice that the degree of a node depends on whether we consider Tto be a rooted tree or a free tree.
The degree of a vertex in a free tree is, as in any undirected graph, the number of adjacent vertices.
In a rooted tree, however, the degree is the number of children—the parent of a node does not count
toward its degree.1178 Appendix B Sets, Etc.
3
2
41
67
5
(a)3
2
41
67
5
(b)3
2
41
67
5
(c)
Figure B.7 Binary trees. (a)A binary tree drawn in a standard way. The left child of a node is
drawn beneath the node and to the left. The right child is drawn beneath and to the right. (b)Ab i n a r y
tree different from the one in (a). In (a), the left child of node 7is5and the right child is absent.
In (b), the left child of node 7is absent and the right child is 5. As ordered trees, these trees are
the same, but as binary trees, they are distinct. (c)The binary tree in (a) represented by the internal
nodes of a full binary tree: an ordered tree in which each internal node has degree 2. The leaves in
the tree are shown as squares.
/SIis composed of three disjoint sets of nodes: a root node, a binary tree called its
left subtree , and a binary tree called its right subtree .
The binary tree that contains no nodes is called the empty tree ornull tree ,s o m e -
times denoted NIL. If the left subtree is nonempty, its root is called the left child of
the root of the entire tree. Likewise, the root of a nonnull right subtree is the right
child of the root of the entire tree. If a subtree is the null tree NIL, we say that the
child is absent ormissing . Figure B.7(a) shows a binary tree.
A binary tree is not simply an ordered tree in which each node has degree at
most 2. For example, in a binary tree, if a node has just one child, the position
of the child—whether it is the left child or the right child —matters. In an or-
dered tree, there is no distinguishing a sole child as being either left or right. Fig-ure B.7(b) shows a binary tree that differs from the tree in Figure B.7(a) because ofthe position of one node. Considered as ordered trees, however, the two trees areidentical.
We can represent the positioning information in a binary tree by the internal
nodes of an ordered tree, as shown in Figure B.7(c). The idea is to replace eachmissing child in the binary tree with a node having no children. These leaf nodesare drawn as squares in the ﬁgure. The tree that results is a full binary tree : each
node is either a leaf or has degree exactly 2. There are no degree- 1nodes. Conse-
quently, the order of the children of a node preserves the position information.
We can extend the positioning information that distinguishes binary trees from
ordered trees to trees with more than 2children per node. In a positional tree ,t h eB.5 Trees 1179
height = 3depth 0
depth 1depth 2depth 3
Figure B.8 A complete binary tree of height 3with8leaves and 7internal nodes.
children of a node are labeled with distinct positive integers. The ith child of a
node is absent if no child is labeled with integer i.Ak-ary tree is a positional tree
in which for every node, all children with labels greater than kare missing. Thus,
a binary tree is a k-ary tree with kD2.
Acomplete k-ary tree is ak-ary tree in which all leaves have the same depth
and all internal nodes have degree k. Figure B.8 shows a complete binary tree of
height 3. How many leaves does a complete k-ary tree of height hhave? The root
haskchildren at depth 1, each of which has kchildren at depth 2, etc. Thus, the
number of leaves at depth hiskh. Consequently, the height of a complete k-ary
tree with nleaves is logkn. The number of internal nodes of a complete k-ary tree
of height his
1CkCk2C/SOH/SOH/SOHC kh/NUL1Dh/NUL1X
iD0ki
Dkh/NUL1
k/NUL1
by equation (A.5). Thus, a complete binary tree has 2h/NUL1internal nodes.
Exercises
B.5-1
Draw all the free trees composed of the three vertices x,y,a n d ´. Draw all the
rooted trees with nodes x,y,a n d ´withxas the root. Draw all the ordered trees
with nodes x,y,a n d ´withxas the root. Draw all the binary trees with nodes x,
y,a n d ´withxas the root.1180 Appendix B Sets, Etc.
B.5-2
LetGD.V; E/ be a directed acyclic graph in which there is a vertex /ETB02V
such that there exists a unique path from /ETB0to every vertex /ETB2V. Prove that the
undirected version of Gforms a tree.
B.5-3
Show by induction that the number of degree- 2nodes in any nonempty binary tree
is1fewer than the number of leaves. Conclude that the number of internal nodes
in a full binary tree is 1fewer than the number of leaves.
B.5-4
Use induction to show that a nonempty binary tree with nnodes has height at
leastblgnc.
B.5-5 ?
Theinternal path length of a full binary tree is the sum, taken over all internal
nodes of the tree, of the depth of each node. Likewise, the external path length is
the sum, taken over all leaves of the tree, of the depth of each leaf. Consider a fullbinary tree with ninternal nodes, internal path length i, and external path length e.
Prove that eDiC2n.
B.5-6 ?
Let us associate a “weight” w.x/D2
/NULdwith each leaf xof depth din a binary
treeT,a n dl e t Lbe the set of leaves of T. Prove thatP
x2Lw.x//DC41. (This is
known as the Kraft inequality .)
B.5-7 ?
Show that if L/NAK2, then every binary tree with Lleaves contains a subtree having
between L=3 and2L=3 leaves, inclusive.
Problems
B-1 Graph coloring
Given an undirected graph GD.V; E/ ,ak-coloring ofGis a function cWV!
f0; 1; : : : ; k/NUL1gsuch that c.u/¤c./ETB/ for every edge .u; /ETB/2E. In other words,
the numbers 0; 1; : : : ; k/NUL1represent the kcolors, and adjacent vertices must have
different colors.
a.Show that any tree is 2-colorable.Problems for Appendix B 1181
b.Show that the following are equivalent:
1.Gis bipartite.
2.Gis2-colorable.
3.Ghas no cycles of odd length.
c.Letdbe the maximum degree of any vertex in a graph G. Prove that we can
color GwithdC1colors.
d.Show that if GhasO.jVj/edges, then we can color GwithO.p
jVj/colors.
B-2 Friendly graphs
Reword each of the following statements as a theorem about undirected graphs,and then prove it. Assume that friendship is symmetric but not reﬂexive.
a.Any group of at least two people contains at least two people with the same
number of friends in the group.
b.Every group of six people contains either at least three mutual friends or at least
three mutual strangers.
c.Any group of people can be partitioned into two subgroups such that at least
half the friends of each person belong to the subgroup of which that person isnotam e m b e r .
d.If everyone in a group is the friend of at least half the people in the group, then
the group can be seated around a table in such a way that everyone is seatedbetween two friends.
B-3 Bisecting trees
Many divide-and-conquer algorithms that operate on graphs require that the graphbe bisected into two nearly equal-sized subgraphs, which are induced by a partitionof the vertices. This problem investigates bisections of trees formed by removing asmall number of edges. We require that whenever two vertices end up in the samesubtree after removing edges, then they must be in the same partition.
a.Show that we can partition the vertices of any n-vertex binary tree into two
setsAandB, such thatjAj/DC43n=4 andjBj/DC43n=4 ,b yr e m o v i n gas i n g l e
edge.
b.Show that the constant 3=4in part (a) is optimal in the worst case by giving
an example of a simple binary tree whose most evenly balanced partition uponremoval of a single edge has jAjD3n=4 .1182 Appendix B Sets, Etc.
c.Show that by removing at most O.lgn/edges, we can partition the vertices
of any n-vertex binary tree into two sets AandBsuch thatjAjDbn=2c
andjBjDdn=2e.
Appendix notes
G. Boole pioneered the development of symbolic logic, and he introduced many of
the basic set notations in a book published in 1854. Modern set theory was createdby G. Cantor during the period 1874–1895. Cantor focused primarily on sets ofinﬁnite cardinality. The term “function” is attributed to G. W. Leibniz, who used itto refer to several kinds of mathematical formulas. His limited deﬁnition has beengeneralized many times. Graph theory originated in 1736, when L. Euler provedthat it was impossible to cross each of the seven bridges in the city of K¨ onigsberg
exactly once and return to the starting point.
The book by Harary [160] provides a useful compendium of many deﬁnitions
and results from graph theory.C Counting and Probability
This appendix reviews elementary combinatorics and probability theory. If you
have a good background in these areas, you may want to skim the beginning of thisappendix lightly and concentrate on the later sections. Most of this book’s chaptersdo not require probability, but for some chapters it is essential.
Section C.1 reviews elementary results in counting theory, including standard
formulas for counting permutations and combinations. The axioms of probabilityand basic facts concerning probability distributions form Section C.2. Randomvariables are introduced in Section C.3, along with the properties of expectationand variance. Section C.4 investigates the geometric and binomial distributionsthat arise from studying Bernoulli trials. The study of the binomial distributioncontinues in Section C.5, an advanced discussion of the “tails” of the distribution.
C.1 Counting
Counting theory tries to answer the question “How many?” without actually enu-merating all the choices. For example, we might ask, “How many different n-bit
numbers are there?” or “How many orderings of ndistinct elements are there?” In
this section, we review the elements of counting theory. Since some of the materialassumes a basic understanding of sets, you might wish to start by reviewing thematerial in Section B.1.
Rules of sum and product
We can sometimes express a set of items that we wish to count as a union of disjoint
sets or as a Cartesian product of sets.
Therule of sum says that the number of ways to choose one element from one
of two disjoint sets is the sum of the cardinalities of the sets. That is, if AandB
are two ﬁnite sets with no members in common, then jA[BjDjAjCjBj,w h i c h1184 Appendix C Counting and Probability
follows from equation (B.3). For example, each position on a car’s license plate
is a letter or a digit. The number of possibilities for each position is therefore26C10D36, since there are 26 choices if it is a letter and 10 choices if it is a
digit.
Therule of product says that the number of ways to choose an ordered pair is the
number of ways to choose the ﬁrst element times the number of ways to choose thesecond element. That is, if AandBare two ﬁnite sets, then jA/STXBjDjAj/SOHjBj,
which is simply equation (B.4). For example, if an ice-cream parlor offers 28
ﬂavors of ice cream and 4 toppings, the number of possible sundaes with one scoop
of ice cream and one topping is 28/SOH4D112.
Strings
Astring over a ﬁnite set Sis a sequence of elements of S. For example, there are 8
binary strings of length 3:
000; 001; 010; 011; 100; 101; 110; 111 :
We sometimes call a string of length kak-string .Asubstring s
0of a string s
is an ordered sequence of consecutive elements of s.Ak-substring of a string
is a substring of length k. For example, 010is a3-substring of 01101001 (the
3-substring that begins in position 4), but 111is not a substring of 01101001 .
We can view a k-string over a set Sas an element of the Cartesian product Sk
ofk-tuples; thus, there are jSjkstrings of length k. For example, the number of
binary k-strings is 2k. Intuitively, to construct a k-string over an n-set, we have n
ways to pick the ﬁrst element; for each of these choices, we have nways to pick the
second element; and so forth ktimes. This construction leads to the k-fold product
n/SOHn/SOH/SOH/SOHnDnkas the number of k-strings.
Permutations
Apermutation of a ﬁnite set Sis an ordered sequence of all the elements of S,
with each element appearing exactly once. For example, if SDfa;b;cg,t h e n S
has6permutations:
abc;acb;bac;bca;cab;cba :
There are nŠpermutations of a set of nelements, since we can choose the ﬁrst
element of the sequence in nways, the second in n/NUL1ways, the third in n/NUL2
ways, and so on.
Ak-permutation ofSis an ordered sequence of kelements of S, with no ele-
ment appearing more than once in the sequence. (Thus, an ordinary permutation isann-permutation of an n-set.) The twelve 2-permutations of the set fa;b;c;dgareC.1 Counting 1185
ab;ac;ad;ba;bc;bd;ca;cb;cd;da;db;dc :
The number of k-permutations of an n-set is
n.n/NUL1/.n/NUL2//SOH/SOH/SOH.n/NULkC1/DnŠ
.n/NULk/Š; (C.1)
since we have nways to choose the ﬁrst element, n/NUL1ways to choose the second
element, and so on, until we have selected kelements, the last being a selection
from the remaining n/NULkC1elements.
Combinations
Ak-combination of an n-setSis simply a k-subset of S. For example, the 4-set
fa;b;c;dghas six 2-combinations:
ab;ac;ad;bc;bd;cd :
(Here we use the shorthand of denoting the 2-subsetfa;bgbyab, and so on.)
We can construct a k-combination of an n-set by choosing kdistinct (different)
elements from the n-set. The order in which we select the elements does not matter.
We can express the number of k-combinations of an n-set in terms of the number
ofk-permutations of an n-set. Every k-combination has exactly kŠpermutations
of its elements, each of which is a distinct k-permutation of the n-set. Thus, the
number of k-combinations of an n-set is the number of k-permutations divided
bykŠ; from equation (C.1), this quantity is
nŠ
kŠ.n/NULk/Š: (C.2)
ForkD0, this formula tells us that the number of ways to choose 0elements from
ann-set is 1(not0), since 0ŠD1.
Binomial coefﬁcients
The notation/NULn
k/SOH
(read “ nchoose k”) denotes the number of k-combinations of
ann-set. From equation (C.2), we have
 
n
k!
DnŠ
kŠ.n/NULk/Š:
This formula is symmetric in kandn/NULk:
 
n
k!
D 
n
n/NULk!
: (C.3)1186 Appendix C Counting and Probability
These numbers are also known as binomial coefﬁcients , due to their appearance in
thebinomial expansion :
.xCy/nDnX
kD0 
n
k!
xkyn/NULk: (C.4)
A special case of the binomial expansion occurs when xDyD1:
2nDnX
kD0 
n
k!
:
This formula corresponds to counting the 2nbinary n-strings by the number of 1s
they contain:/NULn
k/SOH
binary n-strings contain exactly k1s, since we have/NULn
k/SOH
ways to
choose kout of the npositions in which to place the 1s.
Many identities involve binomial coefﬁcients. The exercises at the end of this
section give you the opportunity to prove a few.
Binomial bounds
We sometimes need to bound the size of a binomial coefﬁcient. For 1/DC4k/DC4n,
we have the lower bound
 
n
k!
Dn.n/NUL1//SOH/SOH/SOH.n/NULkC1/
k.k/NUL1//SOH/SOH/SOH1
D/DLEn
k/DC1/DC2n/NUL1
k/NUL1/DC3
/SOH/SOH/SOH/DC2n/NULkC1
1/DC3
/NAK/DLEn
k/DC1k
:
Taking advantage of the inequality kŠ/NAK.k=e/kderived from Stirling’s approxi-
mation (3.18), we obtain the upper bounds
 
n
k!
Dn.n/NUL1//SOH/SOH/SOH.n/NULkC1/
k.k/NUL1//SOH/SOH/SOH1
/DC4nk
kŠ
/DC4/DLEen
k/DC1k
: (C.5)
For all integers ksuch that 0/DC4k/DC4n, we can use induction (see Exercise C.1-12)
to prove the boundC.1 Counting 1187
 
n
k!
/DC4nn
kk.n/NULk/n/NULk; (C.6)
where for convenience we assume that 00D1.F o r kD/NAKn,w h e r e 0/DC4/NAK/DC41,w e
can rewrite this bound as
 
n
/NAKn!
/DC4nn
./NAKn//NAKn..1/NUL/NAK/n/.1/NUL/NAK/n
D /DC21
/NAK/DC3/NAK/DC21
1/NUL/NAK/DC31/NUL/NAK!n
D2nH. /NAK /;
whereH./NAK/D/NUL/NAKlg/NAK/NUL.1/NUL/NAK/lg.1/NUL/NAK/ (C.7)
is the (binary) entropy function and where, for convenience, we assume that
0lg0D0,s ot h a t H.0/DH.1/D0.
Exercises
C.1-1
How many k-substrings does an n-string have? (Consider identical k-substrings at
different positions to be different.) How many substrings does an n-string have in
total?
C.1-2
Ann-input, m-output boolean function is a function from f
TRUE ;FALSEgnto
fTRUE ;FALSEgm.H o wm a n y n-input, 1-output boolean functions are there? How
many n-input, m-output boolean functions are there?
C.1-3
In how many ways can nprofessors sit around a circular conference table? Con-
sider two seatings to be the same if one can be rotated to form the other.
C.1-4
In how many ways can we choose three distinct numbers from the set f1 ;2;:::;9 9g
so that their sum is even?1188 Appendix C Counting and Probability
C.1-5
Prove the identity
 
n
k!
Dn
k 
n/NUL1
k/NUL1!
(C.8)
for0<k/DC4n.
C.1-6
Prove the identity
 
n
k!
Dn
n/NULk 
n/NUL1
k!
for0/DC4k<n .
C.1-7
To choose kobjects from n, you can make one of the objects distinguished and
consider whether the distinguished object is chosen. Use this approach to provethat 
n
k!
D 
n/NUL1
k!
C 
n/NUL1
k/NUL1!
:
C.1-8
Using the result of Exercise C.1-7, make a table for nD0; 1; : : : ; 6 and0/DC4k/DC4n
of the binomial coefﬁcients/NUL
n
k/SOH
with/NUL0
0/SOH
at the top,/NUL1
0/SOH
and/NUL1
1/SOH
on the next line, and
so forth. Such a table of binomial coefﬁcients is called Pascal’s triangle .
C.1-9
Prove that
nX
iD1iD 
nC1
2!
:
C.1-10
Show that for any integers n/NAK0and0/DC4k/DC4n, the expression/NULn
k/SOH
achieves its
maximum value when kDbn=2corkDdn=2e.
C.1-11 ?
Argue that for any integers n/NAK0,j/NAK0,k/NAK0,a n d jCk/DC4n,
 
n
jCk!
/DC4 
n
j! 
n/NULj
k!
: (C.9)C.2 Probability 1189
Provide both an algebraic proof and an argument based on a method for choosing
jCkitems out of n. Give an example in which equality does not hold.
C.1-12 ?
Use induction on all integers ksuch that 0/DC4k/DC4n=2to prove inequality (C.6),
and use equation (C.3) to extend it to all integers ksuch that 0/DC4k/DC4n.
C.1-13 ?
Use Stirling’s approximation to prove that
 
2n
n!
D22n
p
/EMn.1CO.1=n// : (C.10)
C.1-14 ?
By differentiating the entropy function H./NAK/ , show that it achieves its maximum
value at /NAKD1=2.W h a ti s H.1=2/ ?
C.1-15 ?
Show that for any integer n/NAK0,
nX
kD0 
n
k!
kDn2n/NUL1: (C.11)
C.2 Probability
Probability is an essential tool for the design and analysis of probabilistic and ran-
domized algorithms. This section reviews basic probability theory.
We deﬁne probability in terms of a sample space S, which is a set whose ele-
ments are called elementary events . We can think of each elementary event as a
possible outcome of an experiment. For the experiment of ﬂipping two distinguish-able coins, with each individual ﬂip resulting in a head (
H)o rat a i l( T), we can view
the sample space as consisting of the set of all possible 2-strings overfH;Tg:
SDfHH;HT;TH;TTg:1190 Appendix C Counting and Probability
Anevent i sas u b s e t1of the sample space S. For example, in the experiment of
ﬂipping two coins, the event of obtaining one head and one tail is fHT;THg.T h e
event Sis called the certain event , and the event;is called the null event . We say
that two events AandBaremutually exclusive ifA\BD;. We sometimes treat
an elementary event s2Sas the eventfsg. By deﬁnition, all elementary events
are mutually exclusive.
Axioms of probability
Aprobability distribution Prfgon a sample space Sis a mapping from events of S
to real numbers satisfying the following probability axioms :
1. PrfAg/NAK0for any event A.
2. PrfSgD1.
3. PrfA[BgDPrfAgCPrfBgfor any two mutually exclusive events A
andB. More generally, for any (ﬁnite or countably inﬁnite) sequence of events
A1;A2;:::that are pairwise mutually exclusive,
Pr([
iAi)
DX
iPrfAig:
We call PrfAgtheprobability of the event A. We note here that axiom 2 is a
normalization requirement: there is really nothing fundamental about choosing 1
as the probability of the certain event, except that it is natural and convenient.
Several results follow immediately from these axioms and basic set theory (see
Section B.1). The null event ;has probability Prf;gD0.I fA/DC2B,t h e n
PrfAg/DC4PrfBg.U s i n g
 Ato denote the event S/NULA(thecomplement ofA),
we have Pr˚
A/TAB
D1/NULPrfAg. For any two events AandB,
PrfA[BgDPrfAgCPrfBg/NULPrfA\Bg (C.12)
/DC4PrfAgCPrfBg: (C.13)
1For a general probability distribution, there may be some subsets of the sample space Sthat are not
considered to be events. This situation usually arises when the sample space is uncountably inﬁnite.The main requirement for what subsets are events is that the set of events of a sample space be closed
under the operations of taking the complement of an event, forming the union of a ﬁnite or countable
number of events, and taking the intersection of a ﬁnite or countable number of events. Most of
the probability distributions we shall see are over ﬁnite or countable sample spaces, and we shallgenerally consider all subsets of a sample space to be events. A notable exception is the continuous
uniform probability distribution, which we shall see shortly.C.2 Probability 1191
In our coin-ﬂipping example, suppose that each of the four elementary events
has probability 1=4. Then the probability of getting at least one head is
PrfHH;HT;THgDPrfHHgCPrfHTgCPrfTHg
D3=4 :
Alternatively, since the probability of getting strictly less than one head is
PrfTTgD1=4, the probability of getting at least one head is 1/NUL1=4D3=4.
Discrete probability distributions
A probability distribution is discrete if it is deﬁned over a ﬁnite or countably inﬁnite
sample space. Let Sbe the sample space. Then for any event A,
PrfAgDX
s2APrfsg;
since elementary events, speciﬁcally those in A, are mutually exclusive. If Sis
ﬁnite and every elementary event s2Shas probability
PrfsgD1=jSj;
then we have the uniform probability distribution onS. In such a case the experi-
ment is often described as “picking an element of Sat random.”
As an example, consider the process of ﬂipping a fair coin , one for which the
probability of obtaining a head is the same as the probability of obtaining a tail, thatis,1=2. If we ﬂip the coin ntimes, we have the uniform probability distribution
deﬁned on the sample space SDf
H;Tgn, a set of size 2n. We can represent each
elementary event in Sas a string of length noverfH;Tg, each string occurring with
probability 1=2n.T h ee v e n t
ADfexactly kheads and exactly n/NULktails occurg
is a subset of Sof sizejAjD/NULn
k/SOH
,s i n c e/NULn
k/SOH
strings of length noverfH;Tgcontain
exactly kH’s. The probability of event Ais thus PrfAgD/NULn
k/SOH
=2n.
Continuous uniform probability distribution
The continuous uniform probability distribution is an example of a probability
distribution in which not all subsets of the sample space are considered to beevents. The continuous uniform probability distribution is deﬁned over a closedinterval Œa; b/c141 of the reals, where a<b . Our intuition is that each point in the in-
terval Œa; b/c141 should be “equally likely.” There are an uncountable number of points,
however, so if we give all points the same ﬁnite, positive probability, we cannot si-
multaneously satisfy axioms 2 and 3. For this reason, we would like to associate a1192 Appendix C Counting and Probability
probability only with some of the subsets of S, in such a way that the axioms are
satisﬁed for these events.
For any closed interval Œc; d/c141 ,w h e r e a/DC4c/DC4d/DC4b,t h econtinuous uniform
probability distribution deﬁnes the probability of the event Œc; d/c141 to be
PrfŒc; d/c141gDd/NULc
b/NULa:
Note that for any point xDŒx; x/c141 , the probability of xis0. If we remove
the endpoints of an interval Œc; d/c141 , we obtain the open interval .c; d/ .S i n c e
Œc; d/c141DŒc; c/c141[.c; d/[Œd; d/c141 , axiom 3 gives us Pr fŒc; d/c141gDPrf.c; d/g.G e n -
erally, the set of events for the continuous uniform probability distribution containsany subset of the sample space Œa; b/c141 that can be obtained by a ﬁnite or countable
union of open and closed intervals, as well as certain more complicated sets.
Conditional probability and independence
Sometimes we have some prior partial knowledge about the outcome of an exper-
iment. For example, suppose that a friend has ﬂipped two fair coins and has toldyou that at least one of the coins showed a head. What is the probability that bothcoins are heads? The information given eliminates the possibility of two tails. Thethree remaining elementary events are equally likely, so we infer that each occurswith probability 1=3. Since only one of these elementary events shows two heads,
the answer to our question is 1=3.
Conditional probability formalizes the notion of having prior partial knowledge
of the outcome of an experiment. The conditional probability of an event Agiven
that another event Boccurs is deﬁned to be
PrfAjBgDPrfA\Bg
PrfBg(C.14)
whenever PrfBg¤0. (We read “PrfAjBg” as “the probability of Agiven B.”)
Intuitively, since we are given that event Boccurs, the event that Aalso occurs
isA\B.T h a t i s , A\Bis the set of outcomes in which both AandBoccur.
Because the outcome is one of the elementary events in B, we normalize the prob-
abilities of all the elementary events in Bby dividing them by Pr fBg, so that they
sum to 1. The conditional probability of Agiven Bis, therefore, the ratio of the
probability of event A\Bto the probability of event B. In the example above, A
is the event that both coins are heads, and Bis the event that at least one coin is a
head. Thus, PrfAjBgD.1=4/=.3=4/D1=3.
Two events are independent if
PrfA\BgDPrfAgPrfBg; (C.15)
which is equivalent, if Pr fBg¤0, to the conditionC.2 Probability 1193
PrfAjBgDPrfAg:
For example, suppose that we ﬂip two fair coins and that the outcomes are inde-
pendent. Then the probability of two heads is .1=2/.1=2/D1=4. Now suppose
that one event is that the ﬁrst coin comes up heads and the other event is that thecoins come up differently. Each of these events occurs with probability 1=2,a n d
the probability that both events occur is 1=4; thus, according to the deﬁnition of
independence, the events are independent—even though you might think that bothevents depend on the ﬁrst coin. Finally, suppose that the coins are welded to-gether so that they both fall heads or both fall tails and that the two possibilities areequally likely. Then the probability that each coin comes up heads is 1=2,b u tt h e
probability that they both come up heads is 1=2¤.1=2/.1=2/ . Consequently, the
event that one comes up heads and the event that the other comes up heads are notindependent.
A collection A
1;A2;:::;A nof events is said to be pairwise independent if
PrfAi\AjgDPrfAigPrfAjg
for all 1/DC4i<j/DC4n. We say that the events of the collection are (mutually)
independent if every k-subset Ai1;Ai2;:::;A ikof the collection, where 2/DC4k/DC4n
and1/DC4i1<i2</SOH/SOH/SOH<ik/DC4n, satisﬁes
PrfAi1\Ai2\/SOH/SOH/SOH\ AikgDPrfAi1gPrfAi2g/SOH/SOH/SOHPrfAikg:
For example, suppose we ﬂip two fair coins. Let A1be the event that the ﬁrst coin
is heads, let A2be the event that the second coin is heads, and let A3be the event
that the two coins are different. We have
PrfA1gD1=2 ;
PrfA2gD1=2 ;
PrfA3gD1=2 ;
PrfA1\A2gD1=4 ;
PrfA1\A3gD1=4 ;
PrfA2\A3gD1=4 ;
PrfA1\A2\A3gD0:
Since for 1/DC4i<j/DC43,w eh a v eP rfAi\AjgDPrfAigPrfAjgD1=4,t h e
events A1,A2,a n d A3are pairwise independent. The events are not mutually inde-
pendent, however, because Pr fA1\A2\A3gD0and PrfA1gPrfA2gPrfA3gD
1=8¤0.1194 Appendix C Coun ting and Probability
Bayes’s theorem
From the deﬁnition of conditional probability (C.14) and the commutative law
A\BDB\A, it follows that for two events AandB, each with nonzero
probability,
PrfA\BgDPrfBgPrfAjBg (C.16)
DPrfAgPrfBjAg:
Solving for PrfAjBg, we obtain
PrfAjBgDPrfAgPrfBjAg
PrfBg; (C.17)
which is known as Bayes’s theorem . The denominator Pr fBgis a normalizing
constant, which we can reformulate as follows. Since BD.B\A/[.B\
A/,
and since B\AandB\
Aare mutually exclusive events,
PrfBgDPrfB\AgCPr˚
B\
A/TAB
DPrfAgPrfBjAgCPr˚
A/TAB
Pr˚
Bj
A/TAB
:
Substituting into equation (C.17), we obtain an equivalent form of Bayes’s theo-
rem:
PrfAjBgDPrfAgPrfBjAg
PrfAgPrfBjAgCPr˚
A/TAB
Pr˚
Bj
A/TAB: (C.18)
Bayes’s theorem can simplify the computing of conditional probabilities. For
example, suppose that we have a fair coin and a biased coin that always comes upheads. We run an experiment consisting of three independent events: we chooseone of the two coins at random, we ﬂip that coin once, and then we ﬂip it again.Suppose that the coin we have chosen comes up heads both times. What is the
probability that it is biased?
We solve this problem using Bayes’s theorem. Let Abe the event that we choose
the biased coin, and let Bbe the event that the chosen coin comes up heads both
times. We wish to determine Pr fAjBg.W eh a v eP rfAgD1=2,P rfBjAgD1,
Pr˚
A/TAB
D1=2,a n dP r˚
Bj
A/TAB
D1=4; hence,
PrfAjBgD.1=2//SOH1
.1=2//SOH1C.1=2//SOH.1=4/
D4=5 :
Exercises
C.2-1
Professor Guildenstern ﬂips a fair
?coin twice. What is the probability that Professor Rosencrantz obtains more headsProfessor Rosencrantz ﬂips a fair coin once.
than Professor GuildensternC.2 Probability 1195
C.2-2
Prove Boole’s inequality : For any ﬁnite or countably inﬁnite sequence of events
A1;A2;:::,
PrfA1[A2[/SOH/SOH/SOHg/DC4PrfA1gCPrfA2gC/SOH/SOH/SOH : (C.19)
C.2-3
Suppose we shufﬂe a deck of 10 cards, each bearing a distinct number from 1 to 10,to mix the cards thoroughly. We then remove three cards, one at a time, from thedeck. What is the probability that we select the three cards in sorted (increasing)order?
C.2-4
Prove that
PrfAjBgCPr˚
AjB/TAB
D1:
C.2-5
Prove that for any collection of events A1;A2;:::;A n,
PrfA1\A2\/SOH/SOH/SOH\ AngDPrfA1g/SOHPrfA2jA1g/SOHPrfA3jA1\A2g/SOH/SOH/SOH
PrfAnjA1\A2\/SOH/SOH/SOH\ An/NUL1g:
C.2-6 ?
Describe a procedure that takes as input two integers aandbsuch that 0<a<b
and, using fair coin ﬂips, produces as output heads with probability a=b and tails
with probability .b/NULa/=b . Give a bound on the expected number of coin ﬂips,
which should be O.1/ .(Hint: Represent a=bin binary.)
C.2-7 ?
Show how to construct a set of nevents that are pairwise independent but such that
no subset of k>2 of them is mutually independent.
C.2-8 ?
Two events AandBareconditionally independent ,g i v e n C,i f
PrfA\BjCgDPrfAjCg/SOHPrfBjCg:
Give a simple but nontrivial example of two events that are not independent but are
conditionally independent given a third event.
C.2-9 ?
You are a contestant in a game show in which a prize is hidden behind one ofthree curtains. You will win the prize if you select the correct curtain. After you1196 Appendix C Counting and Probability
have picked one curtain but before the curtain is lifted, the emcee lifts one of the
other curtains, knowing that it will reveal an empty stage, and asks if you wouldlike to switch from your current selection to the remaining curtain. How wouldyour chances change if you switch? (This question is the celebrated Monty Hall
problem , named after a game-show host who often presented contestants with just
this dilemma.)
C.2-10 ?
A prison warden has randomly picked one prisoner among three to go free. The
other two will be executed. The guard knows which one will go free but is forbid-
den to give any prisoner information regarding his status. Let us call the prisonersX,Y,a n d Z. Prisoner Xasks the guard privately which of YorZwill be exe-
cuted, arguing that since he already knows that at least one of them must die, theguard won’t be revealing any information about his own status. The guard tells X
thatYis to be executed. Prisoner Xfeels happier now, since he ﬁgures that either
he or prisoner Zwill go free, which means that his probability of going free is
now1=2. Is he right, or are his chances still 1=3? Explain.
C.3 Discrete random variables
A(discrete) random variable Xis a function from a ﬁnite or countably inﬁnite
sample space Sto the real numbers. It associates a real number with each possible
outcome of an experiment, which allows us to work with the probability distribu-tion induced on the resulting set of numbers. Random variables can also be deﬁnedfor uncountably inﬁnite sample spaces, but they raise technical issues that are un-necessary to address for our purposes. Henceforth, we shall assume that randomvariables are discrete.
For a random variable Xand a real number x, we deﬁne the event XDxto be
fs2SWX.s/Dxg; thus,
PrfXDxgDX
s2SWX.s/ DxPrfsg:
The function
f. x/DPrfXDxg
is theprobability density function of the random variable X. From the probability
axioms, PrfXDxg/NAK0andP
xPrfXDxgD1.
As an example, consider the experiment of rolling a pair of ordinary, 6-sided
dice. There are 36possible elementary events in the sample space. We assumeC.3 Discrete random variables 1197
that the probability distribution is uniform, so that each elementary event s2Sis
equally likely: PrfsgD1=36 . Deﬁne the random variable Xto be the maximum of
the two values showing on the dice. We have Pr fXD3gD5=36 ,s i n c e Xassigns
a value of 3to 5 of the 36 possible elementary events, namely, .1; 3/ ,.2; 3/ ,.3; 3/ ,
.3; 2/ ,a n d .3; 1/ .
We often deﬁne several random variables on the same sample space. If XandY
are random variables, the function
f. x;y/DPrfXDxandYDyg
is thejoint probability density function ofXandY. For a ﬁxed value y,
PrfYDygDX
xPrfXDxandYDyg;
and similarly, for a ﬁxed value x,
PrfXDxgDX
yPrfXDxandYDyg:
Using the deﬁnition (C.14) of conditional probability, we have
PrfXDxjYDygDPrfXDxandYDyg
PrfYDyg:
We deﬁne two random variables XandYto beindependent if for all xandy,t h e
events XDxandYDyare independent or, equivalently, if for all xandy,w e
have PrfXDxandYDygDPrfXDxgPrfYDyg.
Given a set of random variables deﬁned over the same sample space, we can
deﬁne new random variables as sums, products, or other functions of the originalvariables.
Expected value of a random variable
The simplest and most useful summary of the distribution of a random variable is
the “average” of the values it takes on. The expected value (or, synonymously,
expectation ormean ) of a discrete random variable Xis
EŒX/c141DX
xx/SOHPrfXDxg; (C.20)
which is well deﬁned if the sum is ﬁnite or converges absolutely. Sometimes the
expectation of Xis denoted by /SYNXor, when the random variable is apparent from
context, simply by /SYN.
Consider a game in which you ﬂip two fair coins. You earn $ 3for each head but
lose $ 2for each tail. The expected value of the random variable Xrepresenting1198 Appendix C Counting and Probability
your earnings is
EŒX/c141D6/SOHPrf2H’sgC1/SOHPrf1H,1Tg/NUL4/SOHPrf2T’sg
D6.1=4/C1.1=2//NUL4.1=4/
D1:
The expectation of the sum of two random variables is the sum of their expecta-
tions, that is,
EŒXCY/c141DEŒX/c141CEŒY /c141 ; (C.21)
whenever E ŒX/c141and E ŒY /c141are deﬁned. We call this property linearity of expecta-
tion, and it holds even if XandYare not independent. It also extends to ﬁnite and
absolutely convergent summations of expectations. Linearity of expectation is the
key property that enables us to perform probabilistic analyses by using indicator
random variables (see Section 5.2).
IfXis any random variable, any function g.x/ deﬁnes a new random vari-
ableg.X/ . If the expectation of g.X/ is deﬁned, then
EŒg.X//c141DX
xg.x//SOHPrfXDxg:
Letting g.x/Dax, we have for any constant a,
EŒaX/c141DaEŒX/c141 : (C.22)
Consequently, expectations are linear: for any two random variables XandYand
any constant a,
EŒaXCY/c141DaEŒX/c141CEŒY /c141 : (C.23)
When two random variables XandYare independent and each has a deﬁned
expectation,
EŒXY /c141DX
xX
yxy/SOHPrfXDxandYDyg
DX
xX
yxy/SOHPrfXDxgPrfYDyg
D X
xx/SOHPrfXDxg! X
yy/SOHPrfYDyg!
DEŒX/c141EŒY /c141 :
In general, when nrandom variables X1;X2;:::;X nare mutually independent,
EŒX1X2/SOH/SOH/SOHXn/c141DEŒX1/c141EŒX2/c141/SOH/SOH/SOHEŒXn/c141: (C.24)C.3 Discrete random variables 1199
When a random variable Xtakes on values from the set of natural numbers
NDf0; 1; 2; : : :g, we have a nice formula for its expectation:
EŒX/c141D1X
iD0i/SOHPrfXDig
D1X
iD0i.PrfX/NAKig/NULPrfX/NAKiC1g/
D1X
iD1PrfX/NAKig; (C.25)
since each term Pr fX/NAKigis added in itimes and subtracted out i/NUL1times
(except PrfX/NAK0g, which is added in 0times and not subtracted out at all).
When we apply a convex function f. x/ to a random variable X,Jensen’s in-
equality gives us
EŒf .X//c141/NAKf.EŒX/c141/ ; (C.26)
provided that the expectations exist and are ﬁnite. (A function f. x/ isconvex
if for all xandyand for all 0/DC4/NAK/DC41,w eh a v e f. /NAK xC.1/NUL/NAK/y//DC4
/NAKf .x /C.1/NUL/NAK/f .y/ .)
Variance and standard deviation
The expected value of a random variable does not tell us how “spread out” the
variable’s values are. For example, if we have random variables XandYfor which
PrfXD1=4gDPrfXD3=4gD1=2and PrfYD0gDPrfYD1gD1=2,
then both E ŒX/c141and E ŒY /c141are1=2, yet the actual values taken on by Yare farther
from the mean than the actual values taken on by X.
The notion of variance mathematically expresses how far from the mean a ran-
dom variable’s values are likely to be. The variance of a random variable Xwith
mean E ŒX/c141is
VarŒX/c141DE/STX
.X/NULEŒX/c141/2/ETX
DE/STX
X2/NUL2XEŒX/c141CE2ŒX/c141/ETX
DE/STX
X2/ETX
/NUL2EŒXEŒX/c141/c141CE2ŒX/c141
DE/STX
X2/ETX
/NUL2E2ŒX/c141CE2ŒX/c141
DE/STX
X2/ETX
/NULE2ŒX/c141 : (C.27)
To justify the equality E ŒE2ŒX/c141/c141DE2ŒX/c141, note that because E ŒX/c141i sar e a ln u m -
ber and not a random variable, so is E2ŒX/c141. The equality E ŒXEŒX/c141/c141DE2ŒX/c1411200 Appendix C Counting and Probability
follows from equation (C.22), with aDEŒX/c141. Rewriting equation (C.27) yields
an expression for the expectation of the square of a random variable:
E/STX
X2/ETX
DVarŒX/c141CE2ŒX/c141 : (C.28)
The variance of a random variable Xand the variance of aXare related (see
Exercise C.3-10):
VarŒaX/c141Da2VarŒX/c141 :
When XandYare independent random variables,
VarŒXCY/c141DVarŒX/c141CVarŒY /c141 :
In general, if nrandom variables X1;X2;:::;X nare pairwise independent, then
Var"nX
iD1Xi#
DnX
iD1VarŒXi/c141: (C.29)
Thestandard deviation of a random variable Xis the nonnegative square root
of the variance of X. The standard deviation of a random variable Xis sometimes
denoted /ESCXor simply /ESCwhen the random variable Xis understood from context.
With this notation, the variance of Xis denoted /ESC2.
Exercises
C.3-1
Suppose we roll two ordinary, 6-sided dice. What is the expectation of the sum
of the two values showing? What is the expectation of the maximum of the two
values showing?
C.3-2
An array AŒ1 : : n/c141 contains ndistinct numbers that are randomly ordered, with each
permutation of the nnumbers being equally likely. What is the expectation of the
index of the maximum element in the array? What is the expectation of the index
of the minimum element in the array?
C.3-3
A carnival game consists of three dice in a cage. A player can bet a dollar on anyof the numbers 1through 6. The cage is shaken, and the payoff is as follows. If the
player’s number doesn’t appear on any of the dice, he loses his dollar. Otherwise,if his number appears on exactly kof the three dice, for kD1; 2; 3 , he keeps his
dollar and wins kmore dollars. What is his expected gain from playing the carnival
game once?C.4 The geometric and binomial distributions 1201
C.3-4
Argue that if XandYare nonnegative random variables, then
EŒmax.X; Y //c141/DC4EŒX/c141CEŒY /c141 :
C.3-5 ?
LetXandYbe independent random variables. Prove that f. X/ andg.Y / are
independent for any choice of functions fandg.
C.3-6 ?
LetXbe a nonnegative random variable, and suppose that E ŒX/c141is well deﬁned.
Prove Markov’s inequality :
PrfX/NAKtg/DC4EŒX/c141 =t (C.30)
for all t>0 .
C.3-7 ?
LetSbe a sample space, and let XandX0be random variables such that
X.s//NAKX0.s/for all s2S. Prove that for any real constant t,
PrfX/NAKtg/NAKPrfX0/NAKtg:
C.3-8
Which is larger: the expectation of the square of a random variable, or the squareof its expectation?
C.3-9
Show that for any random variable Xthat takes on only the values 0and1,w eh a v e
VarŒX/c141DEŒX/c141EŒ1/NULX/c141.
C.3-10
Prove that Var ŒaX/c141Da
2VarŒX/c141from the deﬁnition (C.27) of variance.
C.4 The geometric and binomial distributions
We can think of a coin ﬂip as an instance of a Bernoulli trial , which is an experi-
ment with only two possible outcomes: success , which occurs with probability p,
andfailure , which occurs with probability qD1/NULp. When we speak of Bernoulli
trials collectively, we mean that the trials are mutually independent and, unless we
speciﬁcally say otherwise, that each has the same probability pfor success. Two1202 Appendix C Counting and Probability
0.050.100.150.200.25
123456789 1 0 1 1 1 2 1 3 1 4 1 50.300.35
k/DC22
3/DC3k/NUL1/DC21
3/DC3
Figure C.1 A geometric distribution with probability pD1=3 of success and a probability
qD1/NULpof failure. The expectation of the distribution is 1=pD3.
important distributions arise from Bernoulli trials: the geometric distribution and
the binomial distribution.
The geometric distribution
Suppose we have a sequence of Bernoulli trials, each with a probability pof suc-
cess and a probability qD1/NULpof failure. How many trials occur before we obtain
a success? Let us deﬁne the random variable Xbe the number of trials needed to
obtain a success. Then Xhas values in the range f1 ;2;:::g,a n df o r k/NAK1,
PrfXDkgDqk/NUL1p; (C.31)
since we have k/NUL1failures before the one success. A probability distribution sat-
isfying equation (C.31) is said to be a geometric distribution . Figure C.1 illustrates
such a distribution.C.4 The geometric and binomial distributions 1203
Assuming that q<1 , we can calculate the expectation of a geometric distribu-
tion using identity (A.8):
EŒX/c141D1X
kD1kqk/NUL1p
Dp
q1X
kD0kqk
Dp
q/SOHq
.1/NULq/2
Dp
q/SOHq
p2
D1=p : (C.32)
Thus, on average, it takes 1=p trials before we obtain a success, an intuitive result.
The variance, which can be calculated similarly, but using Exercise A.1-3, is
VarŒX/c141Dq=p2: (C.33)
As an example, suppose we repeatedly roll two dice until we obtain either a
seven or an eleven. Of the 36 possible outcomes, 6 yield a seven and 2 yield aneleven. Thus, the probability of success is pD8=36D2=9, and we must roll
1=pD9=2D4:5times on average to obtain a seven or eleven.
The binomial distribution
How many successes occur during nBernoulli trials, where a success occurs with
probability pand a failure with probability qD1/NULp? Deﬁne the random vari-
ableXto be the number of successes in ntrials. Then Xhas values in the range
f0; 1; : : : ; ng,a n df o r kD0; 1; : : : ; n ,
PrfXDkgD 
n
k!
p
kqn/NULk; (C.34)
since there are/NULn
k/SOH
ways to pick which kof the ntrials are successes, and the
probability that each occurs is pkqn/NULk. A probability distribution satisfying equa-
tion (C.34) is said to be a binomial distribution . For convenience, we deﬁne the
family of binomial distributions using the notation
b.kIn;p/D 
n
k!
pk.1/NULp/n/NULk: (C.35)
Figure C.2 illustrates a binomial distribution. The name “binomial” comes from the
right-hand side of equation (C.34) being the kth term of the expansion of .pCq/n.
Consequently, since pCqD1,1204 Appendix C Counting and Probability
0.050.100.150.200.25
k0123456789 1 0 1 1 1 2 1 3 1 4 1 5b(k; 15, 1/3)
Figure C.2 The binomial distribution b.kI15; 1=3/ resulting from nD15Bernoulli trials, each
with probability pD1=3of success. The expectation of the distribution is npD5.
nX
kD0b.kIn;p/D1; (C.36)
as axiom 2 of the probability axioms requires.
We can compute the expectation of a random variable having a binomial distri-
bution from equations (C.8) and (C.36). Let Xbe a random variable that follows
the binomial distribution b.kIn;p/ ,a n dl e t qD1/NULp. By the deﬁnition of expec-
tation, we have
EŒX/c141DnX
kD0k/SOHPrfXDkg
DnX
kD0k/SOHb.kIn;p/
DnX
kD1k 
n
k!
pkqn/NULk
DnpnX
kD1 
n/NUL1
k/NUL1!
pk/NUL1qn/NULk(by equation (C.8))
Dnpn/NUL1X
kD0 
n/NUL1
k!
pkq.n/NUL1//NULkC.4 The geometric and binomial distributions 1205
Dnpn/NUL1X
kD0b.kIn/NUL1; p/
Dnp (by equation (C.36)) . (C.37)
By using the linearity of expectation, we can obtain the same result with sub-
stantially less algebra. Let Xibe the random variable describing the number of
successes in the ith trial. Then E ŒXi/c141Dp/SOH1Cq/SOH0Dp, and by linearity of
expectation (equation (C.21)), the expected number of successes for ntrials is
EŒX/c141DE"nX
iD1Xi#
DnX
iD1EŒXi/c141
DnX
iD1p
Dnp : (C.38)
We can use the same approach to calculate the variance of the distribution. Using
equation (C.27), we have Var ŒXi/c141DEŒX2
i/c141/NULE2ŒXi/c141.S i n c e Xionly takes on the
values 0and1,w eh a v e X2
iDXi, which implies E ŒX2
i/c141DEŒXi/c141Dp. Hence,
VarŒXi/c141Dp/NULp2Dp.1/NULp/Dpq : (C.39)
To compute the variance of X, we take advantage of the independence of the n
trials; thus, by equation (C.29),
VarŒX/c141DVar"nX
iD1Xi#
DnX
iD1VarŒXi/c141
DnX
iD1pq
Dnpq : (C.40)
As Figure C.2 shows, the binomial distribution b.kIn;p/ increases with kuntil
it reaches the mean np, and then it decreases. We can prove that the distribution
always behaves in this manner by looking at the ratio of successive terms:1206 Appendix C Counting and Probability
b.kIn;p/
b.k/NUL1In;p/D/NULn
k/SOH
pkqn/NULk
/NULn
k/NUL1/SOH
pk/NUL1qn/NULkC1
DnŠ.k/NUL1/Š.n/NULkC1/Šp
kŠ.n/NULk/ŠnŠq
D.n/NULkC1/p
kq(C.41)
D1C.nC1/p/NULk
kq:
This ratio is greater than 1precisely when .nC1/p/NULkis positive. Conse-
quently, b.kIn;p/ > b.k/NUL1In;p/ fork<. nC1/p(the distribution increases),
andb.kIn;p/ < b.k/NUL1In;p/ fork>. nC1/p (the distribution decreases).
IfkD.nC1/pis an integer, then b.kIn;p/Db.k/NUL1In;p/ , and so the distri-
bution then has two maxima: at kD.nC1/pand at k/NUL1D.nC1/p/NUL1Dnp/NULq.
Otherwise, it attains a maximum at the unique integer kthat lies in the range
np/NULq<k<. nC1/p.
The following lemma provides an upper bound on the binomial distribution.
Lemma C.1
Letn/NAK0,l e t0<p<1 ,l e tqD1/NULp,a n dl e t 0/DC4k/DC4n.T h e n
b.kIn;p//DC4/DLEnp
k/DC1k/DLEnq
n/NULk/DC1n/NULk
:
Proof Using equation (C.6), we have
b.kIn;p/D 
n
k!
pkqn/NULk
/DC4/DLEn
k/DC1k/DLEn
n/NULk/DC1n/NULk
pkqn/NULk
D/DLEnp
k/DC1k/DLEnq
n/NULk/DC1n/NULk
:
Exercises
C.4-1
Verify axiom 2 of the probability axioms for the geometric distribution.
C.4-2
How many times on average must we ﬂip 6 fair coins before we obtain 3 headsand 3 tails?C.4 The geometric and binomial distributions 1207
C.4-3
Show that b.kIn;p/Db.n/NULkIn;q/ ,w h e r e qD1/NULp.
C.4-4
Show that value of the maximum of the binomial distribution b.kIn;p/ is approx-
imately 1=p
2/EMnpq ,w h e r e qD1/NULp.
C.4-5 ?
Show that the probability of no successes in nBernoulli trials, each with probability
pD1=n, is approximately 1=e. Show that the probability of exactly one success
is also approximately 1=e.
C.4-6 ?
Professor Rosencrantz ﬂips a fair coin ntimes, and so does Professor Guildenstern.
Show that the probability that they get the same number of heads is/NUL2n
n/SOH
=4n.(Hint:
For Professor Rosencrantz, call a head a success; for Professor Guildenstern, calla tail a success.) Use your argument to verify the identity
nX
kD0 
n
k!2
D 
2n
n!
:
C.4-7 ?
Show that for 0/DC4k/DC4n,
b.kIn; 1=2//DC42nH. k=n / /NULn;
where H.x/ is the entropy function (C.7).
C.4-8 ?
Consider nBernoulli trials, where for iD1 ;2;:::;n ,t h e ith trial has probabil-
itypiof success, and let Xbe the random variable denoting the total number of
successes. Let p/NAKpifor all iD1 ;2;:::;n . Prove that for 1/DC4k/DC4n,
PrfX<kg/NAKk/NUL1X
iD0b.iIn;p/ :
C.4-9 ?
LetXbe the random variable for the total number of successes in a set Aofn
Bernoulli trials, where the ith trial has a probability piof success, and let X0
be the random variable for the total number of successes in a second set A0ofn
Bernoulli trials, where the ith trial has a probability p0
i/NAKpiof success. Prove that
for0/DC4k/DC4n,1208 Appendix C Counting and Probability
PrfX0/NAKkg/NAKPrfX/NAKkg:
(Hint: Show how to obtain the Bernoulli trials in A0by an experiment involving
the trials of A, and use the result of Exercise C.3-7.)
?C.5 The tails of the binomial distribution
The probability of having at least, or at most, ksuccesses in nBernoulli trials,
each with probability pof success, is often of more interest than the probability of
having exactly ksuccesses. In this section, we investigate the tails of the binomial
distribution: the two regions of the distribution b.kIn;p/ that are far from the
mean np. We shall prove several important bounds on (the sum of all terms in) a
tail.
We ﬁrst provide a bound on the right tail of the distribution b.kIn;p/ . We can
determine bounds on the left tail by inverting the roles of successes and failures.
Theorem C.2
Consider a sequence of nBernoulli trials, where success occurs with probability p.
LetXbe the random variable denoting the total number of successes. Then for
0/DC4k/DC4n, the probability of at least ksuccesses is
PrfX/NAKkgDnX
iDkb.iIn;p/
/DC4 
n
k!
pk:
Proof ForS/DC2f1 ;2;:::;ng,w el e t ASdenote the event that the ith trial is a
success for every i2S. Clearly PrfASgDpkifjSjDk.W eh a v e
PrfX/NAKkgDPrfthere exists S/DC2f1 ;2;:::;ngWjSjDkandASg
DPr/SUB[
S/DC2f1;2;:::;n gWjSjDkAS/ESC
/DC4X
S/DC2f1;2;:::;n gWjSjDkPrfASg (by inequality (C.19))
D 
n
k!
pk:
C.5 The tails of the binomial distribution 1209
The following corollary restates the theorem for the left tail of the binomial
distribution. In general, we shall leave it to you to adapt the proofs from one tail tothe other.
Corollary C.3
Consider a sequence of nBernoulli trials, where success occurs with probabil-
ityp.I fXis the random variable denoting the total number of successes, then for
0/DC4k/DC4n, the probability of at most ksuccesses is
PrfX/DC4kgD kX
iD0b.iIn;p/
/DC4 
n
n/NULk!
.1/NULp/n/NULk
D 
n
k!
.1/NULp/n/NULk:
Our next bound concerns the left tail of the binomial distribution. Its corollary
shows that, far from the mean, the left tail diminishes exponentially.
Theorem C.4
Consider a sequence of nBernoulli trials, where success occurs with probability p
and failure with probability qD1/NULp.L e t Xbe the random variable denoting the
total number of successes. Then for 0<k<n p , the probability of fewer than k
successes is
PrfX<kgDk/NUL1X
iD0b.iIn;p/
<kq
np/NULkb.kIn;p/ :
Proof We bound the seriesPk/NUL1
iD0b.iIn;p/ by a geometric series using the tech-
nique from Section A.2, page 1151. For iD1 ;2;:::;k , we have from equa-
tion (C.41),
b.i/NUL1In;p/
b.iIn;p/Diq
.n/NULiC1/p
<iq
.n/NULi/p
/DC4kq
.n/NULk/p:1210 Appendix C Counting and Probability
If we let
xDkq
.n/NULk/p
<kq
.n/NULnp/p
Dkq
nqp
Dk
np
<1 ;
it follows that
b.i/NUL1In;p/ < x b.iIn;p/
for0<i/DC4k. Iteratively applying this inequality k/NULitimes, we obtain
b.iIn;p/ < xk/NULib.kIn;p/
for0/DC4i<k , and hence
k/NUL1X
iD0b.iIn;p/ <k/NUL1X
iD0xk/NULib.kIn;p/
<b . kIn;p/1X
iD0xi
Dx
1/NULxb.kIn;p/
Dkq
np/NULkb.kIn;p/ :
Corollary C.5
Consider a sequence of nBernoulli trials, where success occurs with probability p
and failure with probability qD1/NULp. Then for 0<k/DC4np=2 , the probability of
fewer than ksuccesses is less than one half of the probability of fewer than kC1
successes.
Proof Because k/DC4np=2 ,w eh a v e
kq
np/NULk/DC4.np=2/q
np/NUL.np=2/C.5 The tails of the binomial distribution 1211
D.np=2/q
np=2
/DC41; (C.42)
since q/DC41. Letting Xbe the random variable denoting the number of successes,
Theorem C.4 and inequality (C.42) imply that the probability of fewer than ksuc-
cesses is
PrfX<kgDk/NUL1X
iD0b.iIn;p/ < b.kIn;p/ :
Thus we have
PrfX<kg
PrfX<kC1gDPk/NUL1
iD0b.iIn;p/
Pk
iD0b.iIn;p/
DPk/NUL1
iD0b.iIn;p/
Pk/NUL1
iD0b.iIn;p/Cb.kIn;p/
<1 = 2 ;
sincePk/NUL1
iD0b.iIn;p/ < b.kIn;p/ .
Bounds on the right tail follow similarly. Exercise C.5-2 asks you to prove them.
Corollary C.6
Consider a sequence of nBernoulli trials, where success occurs with probability p.
LetXbe the random variable denoting the total number of successes. Then for
n p<k<n , the probability of more than ksuccesses is
PrfX>kgDnX
iDkC1b.iIn;p/
<.n/NULk/p
k/NULnpb.kIn;p/ :
Corollary C.7
Consider a sequence of nBernoulli trials, where success occurs with probability p
and failure with probability qD1/NULp. Then for .npCn/=2 < k < n ,t h e
probability of more than ksuccesses is less than one half of the probability of
more than k/NUL1successes.
The next theorem considers nBernoulli trials, each with a probability piof
success, for iD1 ;2;:::;n . As the subsequent corollary shows, we can use the1212 Appendix C Counting and Probability
theorem to provide a bound on the right tail of the binomial distribution by setting
piDpfor each trial.
Theorem C.8
Consider a sequence of nBernoulli trials, where in the ith trial, for iD1 ;2;:::;n ,
success occurs with probability piand failure occurs with probability qiD1/NULpi.
LetXbe the random variable describing the total number of successes, and let
/SYNDEŒX/c141. Then for r>/SYN ,
PrfX/NUL/SYN/NAKrg/DC4/DLE/SYNe
r/DC1r
:
Proof Since for any ˛>0 , the function e˛xis strictly increasing in x,
PrfX/NUL/SYN/NAKrgDPr˚
e˛.X/NUL/SYN//NAKe˛r/TAB
; (C.43)
where we will determine ˛later. Using Markov’s inequality (C.30), we obtain
Pr˚
e˛.X/NUL/SYN//NAKe˛r/TAB
/DC4E/STX
e˛.X/NUL/SYN//ETX
e/NUL˛r: (C.44)
The bulk of the proof consists of bounding E/STX
e˛.X/NUL/SYN//ETX
and substituting a suit-
able value for ˛in inequality (C.44). First, we evaluate E/STX
e˛.X/NUL/SYN//ETX
.U s i n g t h e
technique of indicator random variables (see Section 5.2), let XiDIftheith
Bernoulli trial is a success gforiD1 ;2;:::;n ;t h a ti s , Xiis the random vari-
able that is 1if the ith Bernoulli trial is a success and 0if it is a failure. Thus,
XDnX
iD1Xi;
and by linearity of expectation,
/SYNDEŒX/c141DE"nX
iD1Xi#
DnX
iD1EŒXi/c141DnX
iD1pi;
which implies
X/NUL/SYNDnX
iD1.Xi/NULpi/:
To evaluate E/STX
e˛.X/NUL/SYN//ETX
, we substitute for X/NUL/SYN, obtaining
E/STX
e˛.X/NUL/SYN//ETX
DE/STX
e˛Pn
iD1.Xi/NULpi//ETX
DE"nY
iD1e˛.X i/NULpi/#
DnY
iD1E/STX
e˛.X i/NULpi//ETX
;C.5 The tails of the binomial distribution 1213
which follows from (C.24), since the mutual independence of the random vari-
ables Xiimplies the mutual independence of the random variables e˛.X i/NULpi/(see
Exercise C.3-5). By the deﬁnition of expectation,
E/STX
e˛.X i/NULpi//ETX
De˛.1/NULpi/piCe˛.0/NULpi/qi
Dpie˛qiCqie/NUL˛pi
/DC4pie˛C1 (C.45)
/DC4exp.pie˛/;
where exp .x/denotes the exponential function: exp .x/Dex. (Inequality (C.45)
follows from the inequalities ˛>0 ,qi/DC41,e˛qi/DC4e˛,a n d e/NUL˛pi/DC41, and the last
line follows from inequality (3.12).) Consequently,
E/STX
e˛.X/NUL/SYN//ETX
DnY
iD1E/STX
e˛.X i/NULpi//ETX
/DC4nY
iD1exp.pie˛/
Dexp nX
iD1pie˛!
Dexp./SYNe˛/; (C.46)
since /SYNDPn
iD1pi. Therefore, from equation (C.43) and inequalities (C.44)
and (C.46), it follows that
PrfX/NUL/SYN/NAKrg/DC4exp./SYNe˛/NUL˛r/ : (C.47)
Choosing ˛Dln.r=/SYN/ (see Exercise C.5-7), we obtain
PrfX/NUL/SYN/NAKrg/DC4exp./SYNeln.r=/SYN//NULrln.r=/SYN//
Dexp.r/NULrln.r=/SYN//
Der
.r=/SYN/r
D/DLE/SYNe
r/DC1r
:
When applied to Bernoulli trials in which each trial has the same probability of
success, Theorem C.8 yields the following corollary bounding the right tail of abinomial distribution.1214 Appendix C Counting and Probability
Corollary C.9
Consider a sequence of nBernoulli trials, where in each trial success occurs with
probability pand failure occurs with probability qD1/NULp. Then for r>n p ,
PrfX/NULnp/NAKrgDnX
kDdnpCreb.kIn;p/
/DC4/DLEnpe
r/DC1r
:
Proof By equation (C.37), we have /SYNDEŒX/c141Dnp.
Exercises
C.5-1 ?
Which is less likely: obtaining no heads when you ﬂip a fair coin ntimes, or
obtaining fewer than nheads when you ﬂip the coin 4ntimes?
C.5-2 ?
Prove Corollaries C.6 and C.7.
C.5-3 ?
Show that
k/NUL1X
iD0 
n
i!
ai<. aC1/n k
na/NULk.aC1/b.kIn;a=.aC1//
for all a>0 and all ksuch that 0<k<n a = . aC1/.
C.5-4 ?
Prove that if 0<k<n p ,w h e r e 0<p<1 andqD1/NULp,t h e n
k/NUL1X
iD0piqn/NULi<kq
np/NULk/DLEnp
k/DC1k/DLEnq
n/NULk/DC1n/NULk
:
C.5-5 ?
Show that the conditions of Theorem C.8 imply that
Prf/SYN/NULX/NAKrg/DC4/DC2.n/NUL/SYN/e
r/DC3r
:
Similarly, show that the conditions of Corollary C.9 imply that
Prfnp/NULX/NAKrg/DC4/DLEnqe
r/DC1r
:Problems for Appendix C 1215
C.5-6 ?
Consider a sequence of nBernoulli trials, where in the ith trial, for iD1 ;2;:::;n ,
success occurs with probability piand failure occurs with probability qiD1/NULpi.
LetXbe the random variable describing the total number of successes, and let
/SYNDEŒX/c141. Show that for r/NAK0,
PrfX/NUL/SYN/NAKrg/DC4e/NULr2=2n:
(Hint: Prove that pie˛qiCqie/NUL˛pi/DC4e˛2=2. Then follow the outline of the proof
of Theorem C.8, using this inequality in place of inequality (C.45).)
C.5-7 ?
Show that choosing ˛Dln.r=/SYN/ minimizes the right-hand side of inequal-
ity (C.47).
Problems
C-1 Balls and binsIn this problem, we investigate the effect of various assumptions on the number ofways of placing nballs into bdistinct bins.
a.Suppose that the nballs are distinct and that their order within a bin does not
matter. Argue that the number of ways of placing the balls in the bins is b
n.
b.Suppose that the balls are distinct and that the balls in each bin are ordered.
Prove that there are exactly .bCn/NUL1/Š=.b/NUL1/Šways to place the balls in the
bins. ( Hint: Consider the number of ways of arranging ndistinct balls and b/NUL1
indistinguishable sticks in a row.)
c.Suppose that the balls are identical, and hence their order within a bin does not
matter. Show that the number of ways of placing the balls in the bins is/NULbCn/NUL1
n/SOH
.
(Hint: Of the arrangements in part (b), how many are repeated if the balls are
made identical?)
d.Suppose that the balls are identical and that no bin may contain more than one
ball, so that n/DC4b. Show that the number of ways of placing the balls is/NULb
n/SOH
.
e.Suppose that the balls are identical and that no bin may be left empty. Assuming
thatn/NAKb, show that the number of ways of placing the balls is/NULn/NUL1
b/NUL1/SOH
.1216 Appendix C Counting and Probability
Appendix notes
The ﬁrst general methods for solving probability problems were discussed in a
famous correspondence between B. Pascal and P. de Fermat, which began in 1654,and in a book by C. Huygens in 1657. Rigorous probability theory began with thework of J. Bernoulli in 1713 and A. De Moivre in 1730. Further developments of
the theory were provided by P.-S. Laplace, S.-D. Poisson, and C. F. Gauss.
Sums of random variables were originally studied by P. L. Chebyshev and A. A.
Markov. A. N. Kolmogorov axiomatized probability theory in 1933. Chernoff [66]and Hoeffding [173] provided bounds on the tails of distributions. Seminal workin random combinatorial structures was done by P. Erd¨ os.
Knuth [209] and Liu [237] are good references for elementary combinatorics
and counting. Standard textbooks such as Billingsley [46], Chung [67], Drake [95],Feller [104], and Rozanov [300] offer comprehensive introductions to probability.D Matrices
Matrices arise in numerous applications, including, but by no means limited to,
scientiﬁc computing. If you have seen matrices before, much of the material in thisappendix will be familiar to you, but some of it might be new. Section D.1 coversbasic matrix deﬁnitions and operations, and Section D.2 presents some basic matrixproperties.
D.1 Matrices and matrix operations
In this section, we review some basic concepts of matrix theory and some funda-mental properties of matrices.
Matrices and vectors
Amatrix is a rectangular array of numbers. For example,
AD/DC2a
11a12a13
a21a22a23/DC3
D/DC2123
456/DC3
(D.1)
is a2/STX3matrix AD.aij/, where for iD1; 2andjD1; 2; 3 , we denote the
element of the matrix in row iand column jbyaij. We use uppercase letters
to denote matrices and corresponding subscripted lowercase letters to denote their
elements. We denote the set of all m/STXnmatrices with real-valued entries by Rm/STXn
and, in general, the set of m/STXnmatrices with entries drawn from a set SbySm/STXn.
Thetranspose of a matrix Ais the matrix ATobtained by exchanging the rows
and columns of A. For the matrix Aof equation (D.1),1218 Appendix D Matrices
ATD/NUL
14
2536/SOH
:
Avector is a one-dimensional array of numbers. For example,
xD/NUL
2
35/SOH
is a vector of size 3. We sometimes call a vector of length nann-vector .W e
use lowercase letters to denote vectors, and we denote the ith element of a size- n
vector xbyxi,f o riD1 ;2;:::;n . We take the standard form of a vector to be
as acolumn vector equivalent to an n/STX1matrix; the corresponding row vector is
obtained by taking the transpose:
xTD.235 /:
Theunit vector eiis the vector whose ith element is 1and all of whose other
elements are 0. Usually, the size of a unit vector is clear from the context.
Azero matrix is a matrix all of whose entries are 0. Such a matrix is often
denoted 0, since the ambiguity between the number 0and a matrix of 0si su s u a l l y
easily resolved from context. If a matrix of 0s is intended, then the size of the
matrix also needs to be derived from the context.
Square matrices
Square n/STXnmatrices arise frequently. Several special cases of square matrices
are of particular interest:
1. Adiagonal matrix hasaijD0whenever i¤j. Because all of the off-diagonal
elements are zero, we can specify the matrix by listing the elements along thediagonal:
diag.a
11;a22;:::;a nn/D˙
a11 0 ::: 0
0a 22::: 0
::::::::::::
0 0 ::: a
nn/BEL
:
2. The n/STXnidentity matrix Inis a diagonal matrix with 1s along the diagonal:
InDdiag. 1 ;1 ;:::;1 /
D˙
1 0 ::: 0
0 1 ::: 0
::::::::::::
0 0 ::: 1/BEL
:D.1 Matrices and matrix operations 1219
When Iappears without a subscript, we derive its size from the context. The ith
column of an identity matrix is the unit vector ei.
3. Atridiagonal matrix Tis one for which tijD0ifji/NULjj>1. Nonzero entries
appear only on the main diagonal, immediately above the main diagonal ( ti;iC1
foriD1 ;2;:::;n/NUL1), or immediately below the main diagonal ( tiC1;ifor
iD1 ;2;:::;n/NUL1):
TD/EOT
t11t120 0 ::: 0 0 0
t21t22t230 ::: 0 0 0
0t 32t33t34::: 0 0 0
::::::::::::::::::::::::
0000 : : : t
n/NUL2;n/NUL2tn/NUL2;n/NUL1 0
0000 : : : t n/NUL1;n/NUL2tn/NUL1;n/NUL1tn/NUL1;n
0000 : : : 0 t n;n/NUL1 tnn˘
:
4. An upper-triangular matrix Uis one for which uijD0ifi>j . All entries
below the diagonal are zero:
UD˙
u11u12::: u 1n
0u 22::: u 2n
::::::::::::
0 0 ::: u nn/BEL
:
An upper-triangular matrix is unit upper-triangular if it has all 1s along the
diagonal.
5. Alower-triangular matrix Lis one for which lijD0ifi<j . All entries
above the diagonal are zero:
LD˙
l110 ::: 0
l21l22::: 0
::::::::::::
ln1ln2::: l nn/BEL
:
A lower-triangular matrix is unit lower-triangular if it has all 1s along the
diagonal.1220 Appendix D Matrices
6. A permutation matrix Phas exactly one 1in each row or column, and 0s
elsewhere. An example of a permutation matrix is
PDˇ
01000
00010100000000100100/CR
:
Such a matrix is called a permutation matrix because multiplying a vector x
by a permutation matrix has the effect of permuting (rearranging) the elementsofx. Exercise D.1-4 explores additional properties of permutation matrices.
7. Asymmetric matrix Asatisﬁes the condition ADA
T. For example,/NUL
123
264345/SOH
is a symmetric matrix.
Basic matrix operations
The elements of a matrix or vector are numbers from a number system, such as
the real numbers, the complex numbers, or integers modulo a prime. The numbersystem deﬁnes how to add and multiply numbers. We can extend these deﬁnitionsto encompass addition and multiplication of matrices.
We deﬁne matrix addition as follows. If AD.a
ij/andBD.bij/arem/STXn
matrices, then their matrix sum CD.cij/DACBis the m/STXnmatrix deﬁned by
cijDaijCbij
foriD1 ;2;:::;m andjD1 ;2;:::;n . That is, matrix addition is performed
componentwise. A zero matrix is the identity for matrix addition:
AC0DAD0CA:
If/NAKis a number and AD.aij/is a matrix, then /NAKAD./NAKa ij/is the scalar
multiple ofAobtained by multiplying each of its elements by /NAK. As a special case,
we deﬁne the negative of a matrix AD.aij/to be/NUL1/SOHAD/NULA, so that the ijth
entry of/NULAis/NULaij. Thus,
AC./NULA/D0D./NULA/CA:D.1 Matrices and matrix operations 1221
We use the negative of a matrix to deﬁne matrix subtraction :A/NULBDAC./NULB/.
We deﬁne matrix multiplication as follows. We start with two matrices AandB
that are compatible in the sense that the number of columns of Aequals the number
of rows of B. (In general, an expression containing a matrix product ABis always
assumed to imply that matrices AandBare compatible.) If AD.aik/is anm/STXn
matrix and BD.bkj/is ann/STXpmatrix, then their matrix product CDABis the
m/STXpmatrix CD.cij/,w h e r e
cijDnX
kD1aikbkj (D.2)
foriD1 ;2;:::;m andjD1 ;2;:::;p . The procedure S QUARE -MATRIX -
MULTIPLY in Section 4.2 implements matrix multiplication in the straightfor-
ward manner based on equation (D.2), assuming that the matrices are square:mDnDp. To multiply n/STXnmatrices, S
QUARE -MATRIX -MULTIPLY per-
forms n3multiplications and n2.n/NUL1/additions, and so its running time is ‚.n3/.
Matrices have many (but not all) of the algebraic properties typical of numbers.
Identity matrices are identities for matrix multiplication:
ImADAInDA
for any m/STXnmatrix A. Multiplying by a zero matrix gives a zero matrix:
A0D0:
Matrix multiplication is associative:
A.BC /D.AB/C
for compatible matrices A,B,a n d C. Matrix multiplication distributes over addi-
tion:
A.BCC/DABCAC ;
.BCC/ DDBDCCD :
Forn>1 , multiplication of n/STXnmatrices is not commutative. For example, if
AD/DC201
00/DC3
andBD/DC200
10/DC3
,t h e n
ABD/DC210
00/DC3
and
BAD/DC200
01/DC3
:1222 Appendix D Matrices
We deﬁne matrix-vector products or vector-vector products as if the vector were
the equivalent n/STX1matrix (or a 1/STXnmatrix, in the case of a row vector). Thus,
ifAis an m/STXnmatrix and xis an n-vector, then Axis an m-vector. If xandy
aren-vectors, then
xTyDnX
iD1xiyi
is a number (actually a 1/STX1matrix) called the inner product ofxandy.T h em a -
trixxyTis ann/STXnmatrix Zcalled the outer product ofxandy, with ´ijDxiyj.
The(euclidean) normkxkof an n-vector xis deﬁned by
kxkD.x2
1Cx2
2C/SOH/SOH/SOHC x2
n/1=2
D.xTx/1=2:
Thus, the norm of xis its length in n-dimensional euclidean space.
Exercises
D.1-1
Show that if AandBare symmetric n/STXnmatrices, then so are ACBandA/NULB.
D.1-2
Prove that .AB/TDBTATand that ATAis always a symmetric matrix.
D.1-3
Prove that the product of two lower-triangular matrices is lower-triangular.
D.1-4
Prove that if Pis an n/STXnpermutation matrix and Ais an n/STXnmatrix, then the
matrix product PAisAwith its rows permuted, and the matrix product APisA
with its columns permuted. Prove that the product of two permutation matrices isa permutation matrix.
D.2 Basic matrix properties
In this section, we deﬁne some basic properties pertaining to matrices: inverses,linear dependence and independence, rank, and determinants. We also deﬁne theclass of positive-deﬁnite matrices.D.2 Basic matrix properties 1223
Matrix inverses, ranks, and determinants
We deﬁne the inverse of an n/STXnmatrix Ato be the n/STXnmatrix, denoted A/NUL1(if
it exists), such that AA/NUL1DInDA/NUL1A. For example,
/DC211
10/DC3/NUL1
D/DC201
1/NUL1/DC3
:
Many nonzero n/STXnmatrices do not have inverses. A matrix without an inverse is
called noninvertible ,o rsingular . An example of a nonzero singular matrix is
/DC210
10/DC3
:
If a matrix has an inverse, it is called invertible ,o rnonsingular . Matrix inverses,
when they exist, are unique. (See Exercise D.2-1.) If AandBare nonsingular
n/STXnmatrices, then
.BA//NUL1DA/NUL1B/NUL1:
The inverse operation commutes with the transpose operation:
.A/NUL1/TD.AT//NUL1:
The vectors x1;x2;:::;x narelinearly dependent if there exist coefﬁcients
c1;c2;:::;c n, not all of which are zero, such that c1x1Cc2x2C/SOH/SOH/SOHC cnxnD0.
The row vectors x1D.123 /,x2D.264 /,a n d x3D.41 19 /are
linearly dependent, for example, since 2x1C3x2/NUL2x3D0. If vectors are not
linearly dependent, they are linearly independent . For example, the columns of an
identity matrix are linearly independent.
Thecolumn rank of a nonzero m/STXnmatrix Ais the size of the largest set
of linearly independent columns of A. Similarly, the row rank ofAis the size
of the largest set of linearly independent rows of A. A fundamental property of
any matrix Ais that its row rank always equals its column rank, so that we can
simply refer to the rank ofA. The rank of an m/STXnmatrix is an integer between 0
and min .m; n/ , inclusive. (The rank of a zero matrix is 0,a n dt h er a n ko fa n n/STXn
identity matrix is n.) An alternate, but equivalent and often more useful, deﬁnition
is that the rank of a nonzero m/STXnmatrix Ais the smallest number rsuch that
there exist matrices BandCof respective sizes m/STXrandr/STXnsuch that
ADBC :
A square n/STXnmatrix has full rank if its rank is n.A n m/STXnmatrix has full
column rank if its rank is n. The following theorem gives a fundamental property
of ranks.1224 Appendix D Matrices
Theorem D.1
A square matrix has full rank if and only if it is nonsingular.
Anull vector for a matrix Ais a nonzero vector xsuch that AxD0.T h e
following theorem (whose proof is left as Exercise D.2-7) and its corollary relate
the notions of column rank and singularity to null vectors.
Theorem D.2
A matrix Ahas full column rank if and only if it does not have a null vector.
Corollary D.3
A square matrix Ais singular if and only if it has a null vector.
Theijthminor of an n/STXnmatrix A,f o rn>1 ,i st h e .n/NUL1//STX.n/NUL1/matrix AŒij /c141
obtained by deleting the ith row and jth column of A.W ed e ﬁ n et h e determinant
of an n/STXnmatrix Arecursively in terms of its minors by
det.A/D‚
a11 ifnD1;
nX
jD1./NUL1/1Cja1jdet.AŒ1j /c141/ifn>1:
The term ./NUL1/iCjdet.AŒij /c141/is known as the cofactor of the element aij.
The following theorems, whose proofs are omitted here, express fundamental
properties of the determinant.
Theorem D.4 (Determinant properties)
The determinant of a square matrix Ahas the following properties:
/SIIf any row or any column of Ais zero, then det .A/D0.
/SIThe determinant of Ais multiplied by /NAKif the entries of any one row (or any
one column) of Aare all multiplied by /NAK.
/SIThe determinant of Ais unchanged if the entries in one row (respectively, col-
umn) are added to those in another row (respectively, column).
/SIThe determinant of Aequals the determinant of AT.
/SIThe determinant of Ais multiplied by/NUL1if any two rows (or any two columns)
are exchanged.
Also, for any square matrices AandB,w eh a v ed e t .AB/Ddet.A/det.B/.
D.2 Basic matrix properties 1225
Theorem D.5
Ann/STXnmatrix Ais singular if and only if det .A/D0.
Positive-deﬁnite matrices
Positive-deﬁnite matrices play an important role in many applications. An n/STXn
matrix Aispositive-deﬁnite ifxTAx > 0 for all n-vectors x¤0.F o r
example, the identity matrix is positive-deﬁnite, since for any nonzero vectorxD.x
1x2/SOH/SOH/SOHxn/T,
xTInxDxTx
DnX
iD1x2
i
>0 :
Matrices that arise in applications are often positive-deﬁnite due to the following
theorem.
Theorem D.6
For any matrix Awith full column rank, the matrix ATAis positive-deﬁnite.
Proof We must show that xT.ATA/x > 0 for any nonzero vector x.F o r a n y
vector x,
xT.ATA/xD.Ax/T.Ax/ (by Exercise D.1-2)
DkAxk2:
Note thatkAxk2is just the sum of the squares of the elements of the vector Ax.
Therefore,kAxk2/NAK0.I fkAxk2D0, every element of Axis0, which is to say
AxD0.S i n c e Ahas full column rank, AxD0implies xD0, by Theorem D.2.
Hence, ATAis positive-deﬁnite.
Section 28.3 explores other properties of positive-deﬁnite matrices.
Exercises
D.2-1
Prove that matrix inverses are unique, that is, if BandCare inverses of A,t h e n
BDC.
D.2-2
Prove that the determinant of a lower-triangular or upper-triangular matrix is equalto the product of its diagonal elements. Prove that the inverse of a lower-triangularmatrix, if it exists, is lower-triangular.1226 Appendix D Matrices
D.2-3
Prove that if Pis a permutation matrix, then Pis invertible, its inverse is PT,
andPTis a permutation matrix.
D.2-4
LetAandBben/STXnmatrices such that ABDI. Prove that if A0is obtained
from Aby adding row jinto row i, then subtracting column ifrom column jofB
yields the inverse B0ofA0.
D.2-5
LetAbe a nonsingular n/STXnmatrix with complex entries. Show that every entry
ofA/NUL1is real if and only if every entry of Ais real.
D.2-6
Show that if Ais a nonsingular, symmetric, n/STXnmatrix, then A/NUL1is symmetric.
Show that if Bis an arbitrary m/STXnmatrix, then the m/STXmmatrix given by the
product BABTis symmetric.
D.2-7
Prove Theorem D.2. That is, show that a matrix Ahas full column rank if and only
ifAxD0implies xD0.(Hint: Express the linear dependence of one column on
the others as a matrix-vector equation.)
D.2-8
Prove that for any two compatible matrices AandB,
rank.AB//DC4min.rank.A/; rank.B// ;
where equality holds if either AorBis a nonsingular square matrix. ( Hint: Use
the alternate deﬁnition of the rank of a matrix.)
Problems
D-1 Vandermonde matrixGiven numbers x
0;x1;:::;x n/NUL1, prove that the determinant of the Vandermonde
matrix
V.x 0;x1;:::;x n/NUL1/D˙1x 0 x2
0/SOH/SOH/SOHxn/NUL1
0
1x 1 x2
1/SOH/SOH/SOHxn/NUL1
1:::::::::::::::
1x
n/NUL1x2
n/NUL1/SOH/SOH/SOHxn/NUL1
n/NUL1/BELProblems for Appendix D 1227
is
det.V.x 0;x1;:::;x n/NUL1//DY
0/DC4j< k /DC4n/NUL1.xk/NULxj/:
(Hint: Multiply column iby/NULx0and add it to column iC1foriDn/NUL1;
n/NUL2;:::;1 , and then use induction.)
D-2 Permutations deﬁned by matrix-vector multiplication over GF .2/
One class of permutations of the integers in the set SnDf0; 1; 2; : : : ; 2n/NUL1gis
deﬁned by matrix multiplication over GF.2/. For each integer xinSn, we view its
binary representation as an n-bit vector /NUL
x0
x1
x2
:::
xn/NUL1/SOH
;
where xDPn/NUL1
iD0xi2i.I fAis an n/STXnmatrix in which each entry is either 0
or1, then we can deﬁne a permutation mapping each value xinSnto the number
whose binary representation is the matrix-vector product Ax. Here, we perform
all arithmetic over GF.2/: all values are either 0or1, and with one exception the
usual rules of addition and multiplication apply. The exception is that 1C1D0.
You can think of arithmetic over GF.2/as being just like regular integer arithmetic,
except that you use only the least signiﬁcant bit.
As an example, for S2Df0; 1; 2; 3g, the matrix
AD/DC210
11/DC3
deﬁnes the following permutation /EMA:/EMA.0/D0,/EMA.1/D3,/EMA.2/D2,
/EMA.3/D1. To see why /EMA.3/D1, observe that, working in GF.2/,
/EMA.3/D/DC210
11/DC3/DC21
1/DC3
D/DC21/SOH1C0/SOH1
1/SOH1C1/SOH1/DC3
D/DC21
0/DC3
;
which is the binary representation of 1.1228 Appendix D Matrices
For the remainder of this problem, we work over GF.2/, and all matrix and
vector entries are 0or1. We deﬁne the rank of a 0-1 matrix (a matrix for which
each entry is either 0or1) over GF.2/the same as for a regular matrix, but with all
arithmetic that determines linear independence performed over GF.2/.W e d e ﬁ n e
therange of an n/STXn0-1 matrix Aby
R.A/DfyWyDAxfor some x2Sng;
so that R.A/ is the set of numbers in Snthat we can produce by multiplying each
value xinSnbyA.
a.Ifris the rank of matrix A, prove thatjR.A/jD2r. Conclude that Adeﬁnes a
permutation on Snonly if Ahas full rank.
For a given n/STXnmatrix Aand a given value y2R.A/ ,w ed e ﬁ n et h e preimage
ofyby
P. A ;y/DfxWAxDyg;
so that P. A ;y/ is the set of values in Snthat map to ywhen multiplied by A.
b.Ifris the rank of n/STXnmatrix Aandy2R.A/ , prove thatjP. A ;y/jD2n/NULr.
Let0/DC4m/DC4n, and suppose we partition the set Sninto blocks of consec-
utive numbers, where the ith block consists of the 2mnumbers i2m;i2mC1;
i2mC2;:::;. iC1/2m/NUL1. For any subset S/DC2Sn,d e ﬁ n e B.S;m/ to be the
set of size- 2mblocks of Sncontaining some element of S. As an example, when
nD3,mD1,a n d SDf1; 4; 5g,t h e n B.S;m/ consists of blocks 0(since 1is in
the0th block) and 2(since both 4and5are in block 2).
c.Letrbe the rank of the lower left .n/NULm//STXmsubmatrix of A,t h a ti s ,t h e
matrix formed by taking the intersection of the bottom n/NULmrows and the
leftmost mcolumns of A.L e t Sbe any size- 2mblock of Sn,a n dl e t S0D
fyWyDAxfor some x2Sg. Prove thatjB.S0;m /jD2rand that for each
block in B.S0;m /, exactly 2m/NULrnumbers in Smap to that block.
Because multiplying the zero vector by any matrix yields a zero vector, the set
of permutations of Sndeﬁned by multiplying by n/STXn0-1 matrices with full rank
over GF.2/cannot include all permutations of Sn. Let us extend the class of per-
mutations deﬁned by matrix-vector multiplication to include an additive term, sothatx2S
nmaps to AxCc,w h e r e cis an n-bit vector and addition is performed
over GF.2/. For example, when
AD/DC210
11/DC3Notes for Appendix D 1229
and
cD/DC20
1/DC3
;
we get the following permutation /EMA;c:/EMA;c.0/D2,/EMA;c.1/D1,/EMA;c.2/D0,
/EMA;c.3/D3. We call any permutation that maps x2SntoAxCc,f o rs o m e n/STXn
0-1 matrix Awith full rank and some n-bit vector c,alinear permutation .
d.Use a counting argument to show that the number of linear permutations of Sn
is much less than the number of permutations of Sn.
e.Give an example of a value of nand a permutation of Snthat cannot be achieved
by any linear permutation. ( Hint: For a given permutation, think about how
multiplying a matrix by a unit vector relates to the columns of the matrix.)
Appendix notes
Linear-algebra textbooks provide plenty of background information on matrices.The books by Strang [323, 324] are particularly good.Bibliography
[1] Milton Abramowitz and Irene A. Stegun, editors. Handbook of Mathematical Functions .
Dover, 1965.
[2] G. M. Adel’son-Vel’ski˘ ı and E. M. Landis. An algorithm for the organization of information.
Soviet Mathematics Doklady , 3(5):1259–1263, 1962.
[3] Alok Aggarwal and Jeffrey Scott Vitter. The i nput/output complexity of sorting and related
problems. Communications of the ACM , 31(9):1116–1127, 1988.
[4] Manindra Agrawal, Neeraj Kayal, and Nitin Saxena. PRIMES is in P. Annals of Mathe-
matics , 160(2):781–793, 2004.
[5] Alfred V . Aho, John E. Hopcroft, and Jeffrey D. Ullman. The Design and Analysis of
Computer Algorithms . Addison-Wesley, 1974.
[6] Alfred V . Aho, John E. Hopcroft, and Jeffrey D. Ullman. Data Structures and Algorithms .
Addison-Wesley, 1983.
[7] Ravindra K. Ahuja, Thomas L. Magnanti, and James B. Orlin. Network Flows: Theory,
Algorithms, and Applications . Prentice Hall, 1993.
[8] Ravindra K. Ahuja, Kurt Mehlhorn, James B. Or lin, and Robert E. Tarjan. Faster algorithms
for the shortest path problem. Journal of the ACM , 37:213–223, 1990.
[9] Ravindra K. Ahuja and James B. Orlin. A fast and simple algorithm for the maximum ﬂow
problem. Operations Research , 37(5):748–759, 1989.
[10] Ravindra K. Ahuja, James B. Orlin, and Robert E. Tarjan. Improved time bounds for the
maximum ﬂow problem. SIAM Journal on Computing , 18(5):939–954, 1989.
[11] Mikl´ os Ajtai, Nimrod Megiddo, and Orli Waarts . Improved algorithms and analysis for
secretary problems and generalizations. In Proceedings of the 36th Annual Symposium on
Foundations of Computer Science , pages 473–482, 1995.
[12] Selim G. Akl. The Design and Analysis of Parallel Algorithms . Prentice Hall, 1989.
[13] Mohamad Akra and Louay Bazzi. On the solution of linear recurrence equations. Compu-
tational Optimization and Applications , 10(2):195–210, 1998.
[14] Noga Alon. Generating pseudo-random permutations and maximum ﬂow algorithms. In-
formation Processing Letters , 35:201–204, 1990.1232 Bibliography
[15] Arne Andersson. Balanced search trees made simple. In Proceedings of the Third Workshop
on Algorithms and Data Structures , volume 709 of Lecture Notes in Computer Science ,
pages 60–71. Springer, 1993.
[16] Arne Andersson. Faster deterministic sorting and searching in linear space. In Proceedings
of the 37th Annual Symposium on Foundations of Computer Science , pages 135–141, 1996.
[17] Arne Andersson, Torben Hagerup, Stefan Nilsson, and Rajeev Raman. Sorting in linear
time? Journal of Computer and System Sciences , 57:74–93, 1998.
[18] Tom M. Apostol. Calculus , volume 1. Blaisdell Publishing Company, second edition, 1967.
[19] Nimar S. Arora, Robert D. Blumofe, and C. Greg Plaxton. Thread scheduling for multipro-
grammed multiprocessors. In Proceedings of the 10th Annual ACM Symposium on Parallel
Algorithms and Architectures , pages 119–129, 1998.
[20] Sanjeev Arora. Probabilistic checking of proofs and the hardness of approximation prob-
lems. PhD thesis, University of California, Berkeley, 1994.
[21] Sanjeev Arora. The approximability of NP-hard problems. In Proceedings of the 30th
Annual ACM Symposium on Theory of Computing , pages 337–348, 1998.
[22] Sanjeev Arora. Polynomial time approximation schemes for euclidean traveling salesman
and other geometric problems. Journal of the ACM , 45(5):753–782, 1998.
[23] Sanjeev Arora and Carsten Lund. Hardness of approximations. In Dorit S. Hochbaum,
editor, Approximation Algorithms for NP-Hard Problems , pages 399–446. PWS Publishing
Company, 1997.
[24] Javed A. Aslam. A simple bound on the expected height of a randomly built binary search
tree. Technical Report TR2001-387, Dartmouth College Department of Computer Science,
2001.
[25] Mikhail J. Atallah, editor. Algorithms and Theory of Computation Handbook . CRC Press,
1999.
[26] G. Ausiello, P. Crescenzi, G. Gambosi, V . Kann, A. Marchetti-Spaccamela, and M. Protasi.
Complexity and Approximation: Combinatorial Optimization Problems and Their Approx-imability Properties . Springer, 1999.
[27] Shai Avidan and Ariel Shamir. Seam carving for content-aware image resizing. ACM Trans-
actions on Graphics , 26(3), article 10, 2007.
[28] Sara Baase and Alan Van Gelder. Computer Algorithms: Introduction to Design and Anal-
ysis. Addison-Wesley, third edition, 2000.
[29] Eric Bach. Private communication, 1989.
[30] Eric Bach. Number-theoretic algorithms. In Annual Review of Computer Science , volume 4,
pages 119–172. Annual Reviews, Inc., 1990.
[31] Eric Bach and Jeffrey Shallit. Algorithmic Number Theory—Volume I: Efﬁcient Algorithms .
The MIT Press, 1996.
[32] David H. Bailey, King Lee, and Horst D. Simon. Using Strassen’s algorithm to accelerate
the solution of linear systems. The Journal of Supercomputing , 4(4):357–371, 1990.Bibliography 1233
[33] Surender Baswana, Ramesh Hariharan, and Sandeep Sen. Improved decremental algo-
rithms for maintaining transitive closure and all-pairs shortest paths. Journal of Algorithms ,
62(2):74–92, 2007.
[34] R. Bayer. Symmetric binary B-trees: Data structure and maintenance algorithms. Acta
Informatica , 1(4):290–306, 1972.
[35] R. Bayer and E. M. McCreight. Organization and maintenance of large ordered indexes.
Acta Informatica , 1(3):173–189, 1972.
[36] Pierre Beauchemin, Gilles Brassard, Claude Cr´ epeau, Claude Goutier, and Carl Pomerance.
The generation of random numbers that are probably prime. Journal of Cryptology , 1(1):53–
64, 1988.
[37] Richard Bellman. Dynamic Programming . Princeton University Press, 1957.
[38] Richard Bellman. On a routing problem. Quarterly of Applied Mathematics , 16(1):87–90,
1958.
[39] Michael Ben-Or. Lower bounds for algebraic computation trees. In Proceedings of the
Fifteenth Annual ACM Symposium on Theory of Computing , pages 80–86, 1983.
[40] Michael A. Bender, Erik D. Demaine, and Martin Farach-Colton. Cache-oblivious B-trees.
InProceedings of the 41st Annual Symposium on Foundations of Computer Science , pages
399–409, 2000.
[41] Samuel W. Bent and John W. John. Finding the median requires 2ncomparisons. In Pro-
ceedings of the Seventeenth Annual ACM Symposium on Theory of Computing , pages 213–
216, 1985.
[42] Jon L. Bentley. Writing Efﬁcient Programs . Prentice Hall, 1982.
[43] Jon L. Bentley. Programming Pearls . Addison-Wesley, 1986.
[44] Jon L. Bentley, Dorothea Haken, and James B. Saxe. A general method for solving divide-
and-conquer recurrences. SIGACT News , 12(3):36–44, 1980.
[45] Daniel Bienstock and Benjamin McClosky. Tightening simplex mixed-integer sets with
guaranteed bounds. Optimization Online , July 2008.
[46] Patrick Billingsley. Probability and Measure . John Wiley & Sons, second edition, 1986.
[47] Guy E. Blelloch. Scan Primitives and Parallel Vector Models . PhD thesis, Department of
Electrical Engineering and Computer Science, MIT, 1989. Available as MIT Laboratory for
Computer Science Technical Report MIT/LCS/TR-463.
[48] Guy E. Blelloch. Programming parallel algorithms. Communications of the ACM ,
39(3):85–97, 1996.
[49] Guy E. Blelloch, Phillip B. Gibbons, and Yossi Matias. Provably efﬁcient scheduling for
languages with ﬁne-grained parallelism. In Proceedings of the 7th Annual ACM Symposium
on Parallel Algorithms and Architectures , pages 1–12, 1995.
[50] Manuel Blum, Robert W. Floyd, Vaughan Pratt, Ronald L. Rivest, and Robert E. Tarjan.
Time bounds for selection. Journal of Computer and System Sciences , 7(4):448–461, 1973.
[51] Robert D. Blumofe, Christopher F. Joerg, Bradley C. Kuszmaul, Charles E. Leiserson,
Keith H. Randall, and Yuli Zhou. Cilk: An efﬁcient multithreaded runtime system. Journal
of Parallel and Distributed Computing , 37(1):55–69, 1996.1234 Bibliography
[52] Robert D. Blumofe and Charles E. Leiserson. Scheduling multithreaded computations by
work stealing. Journal of the ACM , 46(5):720–748, 1999.
[53] B´ ela Bollob´ as.Random Graphs . Academic Press, 1985.
[54] Gilles Brassard and Paul Bratley. Fundamentals of Algorithmics . Prentice Hall, 1996.
[55] Richard P. Brent. The parallel evaluation of general arithmetic expressions. Journal of the
ACM , 21(2):201–206, 1974.
[56] Richard P. Brent. An improved Monte Carlo factorization algorithm. BIT, 20(2):176–184,
1980.
[57] J. P. Buhler, H. W. Lenstra, Jr., and Carl Pomerance. Factoring integers with the number
ﬁeld sieve. In A. K. Lenstra and H. W. Lenstra, Jr., editors, The Development of the Number
Field Sieve , volume 1554 of Lecture Notes in Mathematics , pages 50–94. Springer, 1993.
[58] J. Lawrence Carter and Mark N. Wegman. Universal classes of hash functions. Journal of
Computer and System Sciences , 18(2):143–154, 1979.
[59] Barbara Chapman, Gabriele Jost, and Ruud van der Pas. Using OpenMP: Portable Shared
Memory Parallel Programming . The MIT Press, 2007.
[60] Bernard Chazelle. A minimum spanning tree algorithm with inverse-Ackermann type com-
plexity. Journal of the ACM , 47(6):1028–1047, 2000.
[61] Joseph Cheriyan and Torben Hager up. A randomized maximum-ﬂow algorithm. SIAM
Journal on Computing , 24(2):203–226, 1995.
[62] Joseph Cheriyan and S. N. Maheshwari. Analysis of preﬂow push algorithms for maximum
network ﬂow. SIAM Journal on Computing , 18(6):1057–1086, 1989.
[63] Boris V . Cherkassky and Andrew V . Goldberg. On implementing the push-relabel method
for the maximum ﬂow problem. Algorithmica , 19(4):390–410, 1997.
[64] Boris V . Cherkassky, Andrew V . Goldberg, and Tomasz Radzik. Shortest paths algorithms:
Theory and experimental evaluation. Mathematical Programming , 73(2):129–174, 1996.
[65] Boris V . Cherkassky, Andrew V . Goldberg, and Craig Silverstein. Buckets, heaps, lists and
monotone priority queues. SIAM Journal on Computing , 28(4):1326–1346, 1999.
[66] H. Chernoff. A measure of asymptotic efﬁciency for tests of a hypothesis based on the sum
of observations. Annals of Mathematical Statistics , 23(4):493–507, 1952.
[67] Kai Lai Chung. Elementary Probability Theory with Stochastic Processes . Springer, 1974.
[68] V . Chv´ atal. A greedy heuristic for the set-covering problem. Mathematics of Operations
Research , 4(3):233–235, 1979.
[69] V . Chv´ atal. Linear Programming . W. H. Freeman and Company, 1983.
[70] V . Chv´ atal, D. A. Klarner, and D. E. Knuth. Selected combinatorial research problems.
Technical Report STAN-CS-72-292, Computer Science Department, Stanford University,
1972.
[71] Cilk Arts, Inc., Burlington, Massachusetts. Cilk++ Programmer’s Guide , 2008. Available
at http://www.cilk.com/archive/docs/cilk1guide.Bibliography 1235
[72] Alan Cobham. The intrinsic computational difﬁculty of functions. In Proceedings of the
1964 Congress for Logic, Methodology, and the Philosophy of Science , pages 24–30. North-
Holland, 1964.
[73] H. Cohen and H. W. Lenstra, Jr. Primality testing and Jacobi sums. Mathematics of Com-
putation , 42(165):297–330, 1984.
[74] D. Comer. The ubiquitous B-tree. ACM Computing Surveys , 11(2):121–137, 1979.
[75] Stephen Cook. The complexity of theorem proving procedures. In Proceedings of the Third
Annual ACM Symposium on Theory of Computing , pages 151–158, 1971.
[76] James W. Cooley and John W. Tukey. An algorithm for the machine calculation of complex
Fourier series. Mathematics of Computation , 19(90):297–301, 1965.
[77] Don Coppersmith. Modiﬁcations to the number ﬁeld sieve. Journal of Cryptology ,
6(3):169–180, 1993.
[78] Don Coppersmith and Shmuel Winograd. Matrix multiplication via arithmetic progression.
Journal of Symbolic Computation , 9(3):251–280, 1990.
[79] Thomas H. Cormen, Thomas Sundquist, and Leonard F. Wisniewski. Asymptotically tight
bounds for performing BMMC permutations on parallel disk systems. SIAM Journal on
Computing , 28(1):105–136, 1998.
[80] Don Dailey and Charles E. Leiserson. Using Cilk to write multiprocessor chess programs.
In H. J. van den Herik and B. Monien, editors, Advances in Computer Games , volume 9,
pages 25–52. University of Maastricht, Netherlands, 2001.
[81] Paolo D’Alberto and Alexandru Nicolau. Adaptive Strassen’s matrix multiplication. In
Proceedings of the 21st Annual International Conference on Supercomputing , pages 284–
292, June 2007.
[82] Sanjoy Dasgupta, Christos Papadimitriou, and Umesh Vazirani. Algorithms . McGraw-Hill,
2008.
[83] Roman Dementiev, Lutz Kettner, Jens Mehnert, and Peter Sanders. Engineering a sorted list
data structure for 32 bit keys. In Proceedings of the Sixth Workshop on Algorithm Engineer-
ing and Experiments and the First Workshop on Analytic Algorithmics and Combinatorics ,
pages 142–151, January 2004.
[84] Camil Demetrescu and Giuseppe F. Italiano. Fully dynamic all pairs shortest paths with real
edge weights. Journal of Computer and System Sciences , 72(5):813–837, 2006.
[85] Eric V . Denardo and Bennett L. Fox. Shortest-route methods: 1. Reaching, pruning, and
buckets. Operations Research , 27(1):161–186, 1979.
[86] Martin Dietzfelbinger, Anna Karlin, Kurt Mehlhorn, Friedhelm Meyer auf der Heide, Hans
Rohnert, and Robert E. Tarjan. Dynamic perfect hashing: Upper and lower bounds. SIAM
Journal on Computing , 23(4):738–761, 1994.
[87] Whitﬁeld Difﬁe and Martin E. Hellman. New directions in cryptography. IEEE Transac-
tions on Information Theory , IT-22(6):644–654, 1976.
[88] E. W. Dijkstra. A note on two problems in connexion with graphs. Numerische Mathematik ,
1(1):269–271, 1959.1236 Bibliography
[89] E. A. Dinic. Algorithm for solution of a problem of maximum ﬂow in a network with power
estimation. Soviet Mathematics Doklady , 11(5):1277–1280, 1970.
[90] Brandon Dixon, Monika Rauch, and Robert E. Tarjan. Veriﬁcation and sensitivity analysis
of minimum spanning trees in linear time. SIAM Journal on Computing , 21(6):1184–1192,
1992.
[91] John D. Dixon. Fact orization and primality tests. The American Mathematical Monthly ,
91(6):333–352, 1984.
[92] Dorit Dor, Johan H˚ astad, Staffan Ulfberg, and Uri Zwick. On lower bounds for selecting
the median. SIAM Journal on Discrete Mathematics , 14(3):299–311, 2001.
[93] Dorit Dor and Uri Zwick. Selecting the median. SIAM Journal on Computing , 28(5):1722–
1758, 1999.
[94] Dorit Dor and Uri Zwick. Median selection requires .2C/SI/ncomparisons. SIAM Journal
on Discrete Mathematics , 14(3):312–325, 2001.
[95] Alvin W. Drake. Fundamentals of App lied Probability Theory . McGraw-Hill, 1967.
[96] James R. Driscoll, Harold N. Gabow, Ruth Shrairman, and Robert E. Tarjan. Relaxed heaps:
An alternative to Fibonacci heaps with applications to parallel computation. Communica-
tions of the ACM , 31(11):1343–1354, 1988.
[97] James R. Driscoll, Neil Sarnak, Daniel D. Sl eator, and Robert E. Tarjan. Making data
structures persistent. Journal of Computer and System Sciences , 38(1):86–124, 1989.
[98] Derek L. Eager, John Zahorjan, and Edward D. Lazowska. Speedup versus efﬁciency in
parallel systems. IEEE Transactions on Computers , 38(3):408–423, 1989.
[99] Herbert Edelsbrunner. Algorithms in Combinatorial Geometry , volume 10 of EATCS Mono-
graphs on Theoretical Computer Science . Springer, 1987.
[100] Jack Edmonds. Paths, trees, and ﬂowers. Canadian Journal of Mathematics , 17:449–467,
1965.
[101] Jack Edmonds. Matroids and the greedy algorithm. Mathematical Programming , 1(1):127–
136, 1971.
[102] Jack Edmonds and Richard M. Karp. Theore tical improvements in the algorithmic efﬁ-
ciency for network ﬂow problems. Journal of the ACM , 19(2):248–264, 1972.
[103] Shimon Even. Graph Algorithms . Computer Science Press, 1979.
[104] William Feller. An Introduction to Probability Theory and Its Applications . John Wiley &
Sons, third edition, 1968.
[105] Robert W. Floyd. Algorithm 97 (SHORTEST PATH). Communications of the ACM ,
5(6):345, 1962.
[106] Robert W. Floyd. Algorithm 245 (TREESORT). Communications of the ACM , 7(12):701,
1964.
[107] Robert W. Floyd. Permuting information in idealized two-level storage. In Raymond E.
Miller and James W. Thatcher, editors, Complexity of Computer Computations , pages 105–
109. Plenum Press, 1972.Bibliography 1237
[108] Robert W. Floyd and Ronald L. Rivest. Expected time bounds for selection. Communica-
tions of the ACM , 18(3):165–172, 1975.
[109] Lestor R. Ford, Jr. and D. R. Fulkerson. Flows in Networks . Princeton University Press,
1962.
[110] Lestor R. Ford, Jr. and Selmer M. Johnson. A tournament problem. The American Mathe-
matical Monthly , 66(5):387–389, 1959.
[111] Michael L. Fredman. New bounds on the complexity of the shortest path problem. SIAM
Journal on Computing , 5(1):83–89, 1976.
[112] Michael L. Fredman, J´ anos Koml´ os, and Endre Szemer´ edi. Storing a sparse table with O.1/
worst case access time. Journal of the ACM , 31(3):538–544, 1984.
[113] Michael L. Fredman and Michael E. Saks. The cell probe complexity of dynamic data struc-
tures. In Proceedings of the Twenty First Annual ACM Symposium on Theory of Computing ,
pages 345–354, 1989.
[114] Michael L. Fredman and Robert E. Tarjan. Fibonacci heaps and their uses in improved
network optimization algorithms. Journal of the ACM , 34(3):596–615, 1987.
[115] Michael L. Fredman and Dan E. Willard. Surpassing the information theoretic bound with
fusion trees. Journal of Computer and System Sciences , 47(3):424–436, 1993.
[116] Michael L. Fredman and Dan E. Willard. Trans-dichotomous algorithms for minimum span-
ning trees and shortest paths. Journal of Computer and System Sciences , 48(3):533–551,
1994.
[117] Matteo Frigo and Steven G. Johnson. The design and implementation of FFTW3. Proceed-
ings of the IEEE , 93(2):216–231, 2005.
[118] Matteo Frigo, Charles E. Leiserson, and Keith H. Randall. The implementation of the Cilk-5
multithreaded language. In Proceedings of the 1998 ACM SIGPLAN Conference on Pro-
gramming Language Design and Implementation , pages 212–223, 1998.
[119] Harold N. Gabow. Path-based depth-ﬁrst search for strong and biconnected components.
Information Processing Letters , 74(3–4):107–114, 2000.
[120] Harold N. Gabow, Z. Galil, T. Spencer, and Robert E. Tarjan. Efﬁcient algorithms for ﬁnd-
ing minimum spanning trees in undirected and directed graphs. Combinatorica , 6(2):109–
122, 1986.
[121] Harold N. Gabow and Robert E. Tarjan. A linear-time algorithm for a special case of disjoint
set union. Journal of Computer and System Sciences , 30(2):209–221, 1985.
[122] Harold N. Gabow and Robert E. Tarjan. Faster scaling algorithms for network problems.
SIAM Journal on Computing , 18(5):1013–1036, 1989.
[123] Zvi Galil and Oded Margalit. All pairs shortest distances for graphs with small integer
length edges. Information and Computation , 134(2):103–139, 1997.
[124] Zvi Galil and Oded Margalit. All pairs shortest paths for graphs with small integer length
edges. Journal of Computer and System Sciences , 54(2):243–254, 1997.
[125] Zvi Galil and Kunsoo Park. Dynamic programming with convexity, concavity and sparsity.
Theoretical Computer Science , 92(1):49–76, 1992.1238 Bibliography
[126] Zvi Galil and Joel Seiferas. Time-space-optimal string matching. Journal of Computer and
System Sciences , 26(3):280–294, 1983.
[127] Igal Galperin and Ronald L. Rivest. Scapegoat trees. In Proceedings of the 4th ACM-SIAM
Symposium on Discrete Algorithms , pages 165–174, 1993.
[128] Michael R. Garey, R. L. Graham, and J. D. Ullman. Worst-case analyis of memory al-
location algorithms. In Proceedings of the Fourth Annual ACM Symposium on Theory of
Computing , pages 143–150, 1972.
[129] Michael R. Garey and David S. Johnson. Computers and Intractability: A Guide to the
Theory of NP-Completeness . W. H. Freeman, 1979.
[130] Saul Gass. Linear Programming: Methods and Applications . International Thomson Pub-
lishing, fourth edition, 1975.
[131] F˘ anic˘a Gavril. Algorithms for minimum coloring, maximum clique, minimum covering by
cliques, and maximum independent set of a chordal graph. SIAM Journal on Computing ,
1(2):180–187, 1972.
[132] Alan George and Joseph W-H Liu. Computer Solution of Large Sparse Positive Deﬁnite
Systems . Prentice Hall, 1981.
[133] E. N. Gilbert and E. F. Moore. Variable-length binary encodings. Bell System Technical
Journal , 38(4):933–967, 1959.
[134] Michel X. Goemans and David P. Williamson. Improved approximation algorithms for
maximum cut and satisﬁability problems using semideﬁnite programming. Journal of the
ACM , 42(6):1115–1145, 1995.
[135] Michel X. Goemans and David P. Williamson. The primal-dual method for approximation
algorithms and its application to network design problems. In Dorit S. Hochbaum, editor,
Approximation Algorithms for NP-Hard Problems , pages 144–191. PWS Publishing Com-
pany, 1997.
[136] Andrew V . Goldberg. Efﬁcient Graph Algorithms for Sequential and Parallel Computers .
PhD thesis, Department of Electrical Engineering and Computer Science, MIT, 1987.
[137] Andrew V . Goldberg. Scaling algorithms for the shortest paths problem. SIAM Journal on
Computing , 24(3):494–504, 1995.
[138] Andrew V . Goldberg and Satish Rao. Beyond the ﬂow decomposition barrier. Journal of
the ACM , 45(5):783–797, 1998.
[139] Andrew V . Goldberg, ´Eva Tardos, and Robert E. Tarjan. Network ﬂow algorithms. In Bern-
hard Korte, L´ aszl´oL o v ´ asz, Hans J¨ urgen Pr¨ omel, and Alexander Schrijver, editors, Paths,
Flows, and VLSI-Layout , pages 101–164. Springer, 1990.
[140] Andrew V . Goldberg and Robert E. Tarjan. A new approach to the maximum ﬂow problem.
Journal of the ACM , 35(4):921–940, 1988.
[141] D. Goldfarb and M. J. Todd. Linear programming. In G. L. Nemhauser, A. H. G. Rinnooy-
Kan, and M. J. Todd, editors, Handbook in Operations Research and Management Science,
Vol. 1, Optimization , pages 73–170. Elsevier Science Publishers, 1989.
[142] Shaﬁ Goldwasser and Silvio Micali. Probabilistic encryption. Journal of Computer and
System Sciences , 28(2):270–299, 1984.Bibliography 1239
[143] Shaﬁ Goldwasser, Silvio Micali, and Ronald L. Rivest. A digital signature scheme secure
against adaptive chosen-message attacks. SIAM Journal on Computing , 17(2):281–308,
1988.
[144] Gene H. Golub and Charles F. Van Loan. Matrix Computations . The Johns Hopkins Uni-
versity Press, third edition, 1996.
[145] G. H. Gonnet. Handbook of Algorithms and Data Structures . Addison-Wesley, 1984.
[146] Rafael C. Gonzalez and Richard E. Woods. Digital Image Processing . Addison-Wesley,
1992.
[147] Michael T. Goodrich and Roberto Tamassia. Data Structures and Algorithms in Java . John
Wiley & Sons, 1998.
[148] Michael T. Goodrich and Roberto Tamassia. Algorithm Design: Foundations, Analysis, and
Internet Examples . John Wiley & Sons, 2001.
[149] Ronald L. Graham. Bounds for certain multiprocessor anomalies. Bell System Technical
Journal , 45(9):1563–1581, 1966.
[150] Ronald L. Graham. An efﬁcient algorithm for determining the convex hull of a ﬁnite planar
set.Information Processing Letters , 1(4):132–133, 1972.
[151] Ronald L. Graham and Pavol Hell. On the history of the minimum spanning tree problem.
Annals of the History of Computing , 7(1):43–57, 1985.
[152] Ronald L. Graham, Donald E. Knuth, and Oren Patashnik. Concrete Mathematics .
Addison-Wesley, second edition, 1994.
[153] David Gries. The Science of Programming . Springer, 1981.
[154] M. Gr¨ otschel, L´ aszl´oL o v ´ asz, and Alexander Schrijver. Geometric Algorithms and Combi-
natorial Optimization . Springer, 1988.
[155] Leo J. Guibas and Robert Sedgewick. A dichromatic framework for balanced trees. In
Proceedings of the 19th Annual Symposium on Foundations of Computer Science , pages
8–21, 1978.
[156] Dan Gusﬁeld. Algorithms on Strings, Trees, and Sequences: Computer Science and Com-
putational Biology . Cambridge University Press, 1997.
[157] H. Halberstam and R. E. Ingram, editors. The Mathematical Papers of Sir William Rowan
Hamilton , volume III (Algebra). Cambridge University Press, 1967.
[158] Yijie Han. Improved fast integer sorting in linear space. In Proceedings of the 12th ACM-
SIAM Symposium on Discrete Algorithms , pages 793–796, 2001.
[159] Yijie Han. An O.n3.log log n=logn/5=4/time algorithm for all pairs shortest path. Algo-
rithmica , 51(4):428–434, 2008.
[160] Frank Harary. Graph Theory . Addison-Wesley, 1969.
[161] Gregory C. Harfst and Edward M. Reingold. A potential-based amortized analysis of the
union-ﬁnd data structure. SIGACT News , 31(3):86–95, 2000.
[162] J. Hartmanis and R. E. Stearns. On the computational complexity of algorithms. Transac-
tions of the American Mathematical Society , 117:285–306, May 1965.1240 Bibliography
[163] Michael T. Heideman, Don H. Johnson, and C. Sidney Burrus. Gauss and the history of the
Fast Fourier Transform. IEEE ASSP Magazine , 1(4):14–21, 1984.
[164] Monika R. Henzinger and Valerie King. Fully dynamic biconnectivity and transitive clo-
sure. In Proceedings of the 36th Annual Symposium on Foundations of Computer Science ,
pages 664–672, 1995.
[165] Monika R. Henzinger and Valerie King. Randomized fully dynamic graph algorithms with
polylogarithmic time per operation. Journal of the ACM , 46(4):502–516, 1999.
[166] Monika R. Henzinger, Satish Rao, and Harold N. Gabow. Computing vertex connectivity:
New bounds from old techniques. Journal of Algorithms , 34(2):222–250, 2000.
[167] Nicholas J. Higham. Exploiting fast matrix multiplication within the level 3 BLAS. ACM
Transactions on Mathematical Software , 16(4):352–368, 1990.
[168] W. Daniel Hillis and Jr. Guy L. Steele. Data parallel algorithms. Communications of the
ACM , 29(12):1170–1183, 1986.
[169] C. A. R. Hoare. Algorithm 63 (PARTITION) and algorithm 65 (FIND). Communications
of the ACM , 4(7):321–322, 1961.
[170] C. A. R. Hoare. Quicksort. Computer Journal , 5(1):10–15, 1962.
[171] Dorit S. Hochbaum. Efﬁcient bounds for the stable set, vertex cover and set packing prob-
lems. Discrete Applied Mathematics , 6(3):243–254, 1983.
[172] Dorit S. Hochbaum, editor. Approximation Algorithms for NP-Hard Problems . PWS Pub-
lishing Company, 1997.
[173] W. Hoeffding. On the distribution of the number of successes in independent trials. Annals
of Mathematical Statistics , 27(3):713–721, 1956.
[174] Micha Hofri. Probabilistic Analysis of Algorithms . Springer, 1987.
[175] Micha Hofri. Analysis of Algorithms . Oxford University Press, 1995.
[176] John E. Hopcroft and Richard M. Karp. An n5=2algorithm for maximum matchings in
bipartite graphs. SIAM Journal on Computing , 2(4):225–231, 1973.
[177] John E. Hopcroft, Rajeev Motwani, and Jeffrey D. Ullman. Introduction to Automata The-
ory, Languages, and Computation . Addison Wesley, third edition, 2006.
[178] John E. Hopcroft and Robert E. Tarjan. Efﬁcient algorithms for graph manipulation. Com-
munications of the ACM , 16(6):372–378, 1973.
[179] John E. Hopcroft and Jeffrey D. Ullman. Set merging algorithms. SIAM Journal on Com-
puting , 2(4):294–303, 1973.
[180] John E. Hopcroft and Jeffrey D. Ullman. Introduction to Automata Theory, Languages, and
Computation . Addison-Wesley, 1979.
[181] Ellis Horowitz, Sartaj Sahni, and Sanguthevar Rajasekaran. Computer Algorithms .C o m -
puter Science Press, 1998.
[182] T. C. Hu and M. T. Shing. Computation of matrix chain products. Part I. SIAM Journal on
Computing , 11(2):362–373, 1982.
[183] T. C. Hu and M. T. Shing. Computation of matrix chain products. Part II. SIAM Journal on
Computing , 13(2):228–251, 1984.Bibliography 1241
[184] T. C. Hu and A. C. Tucker. Optimal computer search trees and variable-length alphabetic
codes. SIAM Journal on Applied Mathematics , 21(4):514–532, 1971.
[185] David A. Huffman. A method for the construction of minimum-redundancy codes. Pro-
ceedings of the IRE , 40(9):1098–1101, 1952.
[186] Steven Huss-Lederman, Elaine M. Jacobson, Jeremy R. Johnson, Anna Tsao, and Thomas
Turnbull. Implementation of Strassen’s algorithm for matrix multiplication. In Proceedings
of the 1996 ACM/IEEE Conference on Supercomputing , article 32, 1996.
[187] Oscar H. Ibarra and Chul E. Kim. Fast approximation algorithms for the knapsack and sum
of subset problems. Journal of the ACM , 22(4):463–468, 1975.
[188] E. J. Isaac and R. C. Singleton. Sorting by address calculation. Journal of the ACM ,
3(3):169–174, 1956.
[189] R. A. Jarvis. On the identiﬁcation of the convex hull of a ﬁnite set of points in the plane.
Information Processing Letters , 2(1):18–21, 1973.
[190] David S. Johnson. Approximation algorithms for combinatorial problems. Journal of Com-
puter and System Sciences , 9(3):256–278, 1974.
[191] David S. Johnson. The NP-completeness column: An ongoing guide—The tale of the sec-
ond prover. Journal of Algorithms , 13(3):502–524, 1992.
[192] Donald B. Johnson. Efﬁcient algorithms for shortest paths in sparse networks. Journal of
the ACM , 24(1):1–13, 1977.
[193] Richard Johnsonbaugh and Marcus Schaefer. Algorithms . Pearson Prentice Hall, 2004.
[194] A. Karatsuba and Yu. Ofman. Multiplication of multidigit numbers on automata. Soviet
Physics—Doklady , 7(7):595–596, 1963. Translation of an article in Doklady Akademii Nauk
SSSR , 145(2), 1962.
[195] David R. Karger, Philip N. Klein, and Robert E. Tarjan. A randomized linear-time algorithm
to ﬁnd minimum spanning trees. Journal of the ACM , 42(2):321–328, 1995.
[196] David R. Karger, Daphne Koller, and Steven J. Phillips. Finding the hidden path: Time
bounds for all-pairs shortest paths. SIAM Journal on Computing , 22(6):1199–1217, 1993.
[197] Howard Karloff. Linear Programming .B i r k h ¨ auser, 1991.
[198] N. Karmarkar. A new polynomial-time algorithm for linear programming. Combinatorica ,
4(4):373–395, 1984.
[199] Richard M. Karp. Reducibility among combinatorial problems. In Raymond E. Miller and
James W. Thatcher, editors, Complexity of Computer Computations , pages 85–103. Plenum
Press, 1972.
[200] Richard M. Karp. An introduction to randomized algorithms. Discrete Applied Mathemat-
ics, 34(1–3):165–201, 1991.
[201] Richard M. Karp and Michael O. Rabin. Efﬁcient randomized pattern-matching algorithms.
IBM Journal of Research and Development , 31(2):249–260, 1987.
[202] A. V . Karzanov. Determining the maximal ﬂow in a network by the method of preﬂows.
Soviet Mathematics Doklady , 15(2):434–437, 1974.1242 Bibliography
[203] Valerie King. A simpler minimum spanning tree veriﬁcation algorithm. Algorithmica ,
18(2):263–270, 1997.
[204] Valerie King, Satish Rao, and Robert E. Tarjan. A faster deterministic maximum ﬂow algo-
rithm. Journal of Algorithms , 17(3):447–474, 1994.
[205] Jeffrey H. Kingston. Algorithms and Data Structures: Design, Correctness, Analysis .
Addison-Wesley, second edition, 1997.
[206] D. G. Kirkpatrick and R. Seidel. The ultimate planar convex hull algorithm? SIAM Journal
on Computing , 15(2):287–299, 1986.
[207] Philip N. Klein and Neal E. Young. Approximation algorithms for NP-hard optimization
problems. In CRC Handbook on Algorithms , pages 34-1–34-19. CRC Press, 1999.
[208] Jon Kleinberg and ´Eva Tardos. Algorithm Design . Addison-Wesley, 2006.
[209] Donald E. Knuth. Fundamental Algorithms , volume 1 of The Art of Computer Program-
ming . Addison-Wesley, 1968. Third edition, 1997.
[210] Donald E. Knuth. Seminumerical Algorithms , volume 2 of The Art of Computer Program-
ming . Addison-Wesley, 1969. Third edition, 1997.
[211] Donald E. Knuth. Sorting and Searching , volume 3 of The Art of Computer Programming .
Addison-Wesley, 1973. Second edition, 1998.
[212] Donald E. Knuth. Optimum binary search trees. Acta Informatica , 1(1):14–25, 1971.
[213] Donald E. Knuth. Big omicron and big omega and big theta. SIGACT News , 8(2):18–23,
1976.
[214] Donald E. Knuth, James H. Morris, Jr., and Vaughan R. Pratt. Fast pattern matching in
strings. SIAM Journal on Computing , 6(2):323–350, 1977.
[215] J. Koml´ os. Linear veriﬁcation for spanning trees. Combinatorica , 5(1):57–65, 1985.
[216] Bernhard Korte and L´ aszl´oL o v ´ asz. Mathematical structures underlying greedy algorithms.
In F. Gecseg, editor, Fundamentals of Computation Theory , volume 117 of Lecture Notes in
Computer Science , pages 205–209. Springer, 1981.
[217] Bernhard Korte and L´ aszl´oL o v ´ asz. Structural properties of greedoids. Combinatorica ,
3(3–4):359–374, 1983.
[218] Bernhard Korte and L´ aszl´oL o v ´ asz. Greedoids—A structural framework for the greedy
algorithm. In W. Pulleybank, editor, Progress in Combinatorial Optimization , pages 221–
243. Academic Press, 1984.
[219] Bernhard Korte and L´ aszl´oL o v ´ asz. Greedoids and linear objective functions. SIAM Journal
on Algebraic and Discrete Methods , 5(2):229–238, 1984.
[220] Dexter C. Kozen. The Design and Analysis of Algorithms . Springer, 1992.
[221] David W. Krumme, George Cybenko, and K. N. Venkataraman. Gossiping in minimal time.
SIAM Journal on Computing , 21(1):111–139, 1992.
[222] Joseph B. Kruskal, Jr. On the shortest spanning subtree of a graph and the traveling salesman
problem. Proceedings of the American Mathematical Society , 7(1):48–50, 1956.
[223] Leslie Lamport. How to make a multiprocessor computer that correctly executes multipro-
cess programs. IEEE Transactions on Computers , C-28(9):690–691, 1979.Bibliography 1243
[224] Eugene L. Lawler. Combinatorial Optimization: Networks and Matroids . Holt, Rinehart,
and Winston, 1976.
[225] Eugene L. Lawler, J. K. Lenstra, A. H. G. Rinnooy Kan, and D. B. Shmoys, editors. The
Traveling Salesman Problem . John Wiley & Sons, 1985.
[226] C. Y . Lee. An algorithm for path connection and its applications. IRE Transactions on
Electronic Computers , EC-10(3):346–365, 1961.
[227] Tom Leighton. Tight bounds on the complexity of parallel sorting. IEEE Transactions on
Computers , C-34(4):344–354, 1985.
[228] Tom Leighton. Notes on better master theorems for divide-and-conquer recurrences. Class
notes. Available at http://citeseer.ist.psu.edu/252350.html, October 1996.
[229] Tom Leighton and Satish Rao. Multicommodity max-ﬂow min-cut theorems and their use
in designing approximation algorithms. Journal of the ACM , 46(6):787–832, 1999.
[230] Daan Leijen and Judd Hall. Optimize managed code for multi-core machines. MSDN
Magazine , October 2007.
[231] Debra A. Lelewer and Daniel S. Hirschberg. Data compression. ACM Computing Surveys ,
19(3):261–296, 1987.
[232] A. K. Lenstra, H. W. Lenstra, Jr., M. S. Manasse, and J. M. Pollard. The number ﬁeld sieve.
In A. K. Lenstra and H. W. Lenstra, Jr., editors, The Development of the Number Field Sieve ,
volume 1554 of Lecture Notes in Mathematics , pages 11–42. Springer, 1993.
[233] H. W. Lenstra, Jr. Factoring integers with elliptic curves. Annals of Mathematics ,
126(3):649–673, 1987.
[234] L. A. Levin. Universal sorting problems. Problemy Peredachi Informatsii , 9(3):265–266,
1973. In Russian.
[235] Anany Levitin. Introduction to the Design & Analysis of Algorithms . Addison-Wesley,
2007.
[236] Harry R. Lewis and Christos H. Papadimitriou. Elements of the Theory of Computation .
Prentice Hall, second edition, 1998.
[237] C. L. Liu. Introduction to Combinatorial Mathematics . McGraw-Hill, 1968.
[238] L´ aszl´oL o v ´ asz. On the ratio of optimal integral and fractional covers. Discrete Mathemat-
ics, 13(4):383–390, 1975.
[239] L´ aszl´oL o v ´ asz and M. D. Plummer. Matching Theory , volume 121 of Annals of Discrete
Mathematics . North Holland, 1986.
[240] Bruce M. Maggs and Serge A. Plotkin. Minimum-cost spanning tree as a path-ﬁnding
problem. Information Processing Letters , 26(6):291–293, 1988.
[241] Michael Main. Data Structures and Other Objects Using Java . Addison-Wesley, 1999.
[242] Udi Manber. Introduction to Algorithms: A Creative Approach . Addison-Wesley, 1989.
[243] Conrado Mart´ ınez and Salvador Roura. Randomized binary search trees. Journal of the
ACM , 45(2):288–323, 1998.
[244] William J. Masek and Michael S. Paterson. A faster algorithm computing string edit dis-
tances. Journal of Computer and System Sciences , 20(1):18–31, 1980.1244 Bibliography
[245] H. A. Maurer, Th. Ottmann, and H.-W. Six. Implementing dictionaries using binary trees of
very small height. Information Processing Letters , 5(1):11–14, 1976.
[246] Ernst W. Mayr, Hans J¨ urgen Pr¨ omel, and Angelika Steger, editors. Lectures on Proof Veriﬁ-
cation and Approximation Algorithms , volume 1367 of Lecture Notes in Computer Science .
Springer, 1998.
[247] C. C. McGeoch. All pairs shortest paths and the essential subgraph. Algorithmica ,
13(5):426–441, 1995.
[248] M. D. McIlroy. A killer adversary for quicksort. Software—Practice and Experience ,
29(4):341–344, 1999.
[249] Kurt Mehlhorn. Sorting and Searching , volume 1 of Data Structures and Algorithms .
Springer, 1984.
[250] Kurt Mehlhorn. Graph Algorithms and NP-Completeness , volume 2 of Data Structures and
Algorithms . Springer, 1984.
[251] Kurt Mehlhorn. Multidimensional Searching and Computational Geometry , volume 3 of
Data Structures and Algorithms . Springer, 1984.
[252] Kurt Mehlhorn and Stefan N¨ aher. Bounded ordered dictionaries in O.log log N/time and
O.n/ space. Information Processing Letters , 35(4):183–189, 1990.
[253] Kurt Mehlhorn and Stefan N¨ aher. LEDA: A Platform for Combinatorial and Geometric
Computing . Cambridge University Press, 1999.
[254] Alfred J. Menezes, Paul C. van Oorschot, and Scott A. Vanstone. Handbook of Applied
Cryptography . CRC Press, 1997.
[255] Gary L. Miller. Riemann’s hypothesis and tests for primality. Journal of Computer and
System Sciences , 13(3):300–317, 1976.
[256] John C. Mitchell. Foundations for Programming Languages . The MIT Press, 1996.
[257] Joseph S. B. Mitchell. Guillotine subdivisions approximate polygonal subdivisions: A sim-
ple polynomial-time approximation scheme for geometric TSP, k-MST, and related prob-
lems. SIAM Journal on Computing , 28(4):1298–1309, 1999.
[258] Louis Monier. Algorithmes de Factorisation D’Entiers . PhD thesis, L’Universit´ e Paris-Sud,
1980.
[259] Louis Monier. Evaluation and comparison of two efﬁcient probabilistic primality testing
algorithms. Theoretical Computer Science , 12(1):97–108, 1980.
[260] Edward F. Moore. The shortest path through a maze. In Proceedings of the International
Symposium on the Theory of Switching , pages 285–292. Harvard University Press, 1959.
[261] Rajeev Motwani, Joseph (Sefﬁ) Naor, and Prabakhar Raghavan. Randomized approxima-
tion algorithms in combinatorial optimization. In Dorit Hochbaum, editor, Approximation
Algorithms for NP-Hard Problems , chapter 11, pages 447–481. PWS Publishing Company,
1997.
[262] Rajeev Motwani and Prabhakar Raghavan. Randomized Algorithms . Cambridge University
Press, 1995.
[263] J. I. Munro and V . Raman. Fast stable in-place sorting with O.n/ data moves. Algorithmica ,
16(2):151–160, 1996.Bibliography 1245
[264] J. Nievergelt and E. M. Reingold. Binary search trees of bounded balance. SIAM Journal
on Computing , 2(1):33–43, 1973.
[265] Ivan Niven and Herbert S. Zuckerman. An Introduction to the Theory of Numbers . John
Wiley & Sons, fourth edition, 1980.
[266] Alan V . Oppenheim and Ronald W. Schafer, with John R. Buck. Discrete-Time Signal
Processing . Prentice Hall, second edition, 1998.
[267] Alan V . Oppenheim and Alan S. Willsky, with S. Hamid Nawab. Signals and Systems .
Prentice Hall, second edition, 1997.
[268] James B. Orlin. A polynomial time primal network simplex algorithm for minimum cost
ﬂows. Mathematical Programming , 78(1):109–129, 1997.
[269] Joseph O’Rourke. Computational Geometry in C . Cambridge University Press, second
edition, 1998.
[270] Christos H. Papadimitriou. Computational Complexity . Addison-Wesley, 1994.
[271] Christos H. Papadimitriou and Kenneth Steiglitz. Combinatorial Optimization: Algorithms
and Complexity . Prentice Hall, 1982.
[272] Michael S. Paterson. Progress in selection. In Proceedings of the Fifth Scandinavian Work-
shop on Algorithm Theory , pages 368–379, 1996.
[273] Mihai Pˇ atras¸cu and Mikkel Thorup. Time-space trade-offs for predecessor search. In Pro-
ceedings of the 38th Annual ACM Symposium on Theory of Computing , pages 232–240,
2006.
[274] Mihai Pˇ atras¸cu and Mikkel Thorup. Randomization does not help searching predecessors.
InProceedings of the 18th ACM-SIAM Symposium on Discrete Algorithms , pages 555–564,
2007.
[275] Pavel A. Pevzner. Computational Molecular Biology: An Algorithmic Approach .T h e M I T
Press, 2000.
[276] Steven Phillips and Jeffery Westbrook. Online load balancing and network ﬂow. In Pro-
ceedings of the 25th Annual ACM Symposium on Theory of Computing , pages 402–411,
1993.
[277] J. M. Pollard. A Monte Carlo method for factorization. BIT, 15(3):331–334, 1975.
[278] J. M. Pollard. Factoring with cubic integers. In A. K. Lenstra and H. W. Lenstra, Jr., editors,
The Development of the Number Field Sieve , volume 1554 of Lecture Notes in Mathematics ,
pages 4–10. Springer, 1993.
[279] Carl Pomerance. On the distribution of pseudoprimes. Mathematics of Computation ,
37(156):587–593, 1981.
[280] Carl Pomerance, editor. Proceedings of the AMS Symposia in Applied Mathematics: Com-
putational Number Theory and Cryptography . American Mathematical Society, 1990.
[281] William K. Pratt. Digital Image Processing . John Wiley & Sons, fourth edition, 2007.
[282] Franco P. Preparata and Michael Ian Shamos. Computational Geometry: An Introduction .
Springer, 1985.1246 Bibliography
[283] William H. Press, Saul A. Teukolsky, William T. Vetterling, and Brian P. Flannery. Numer-
ical Recipes in C++: The Art of Scientiﬁc Computing . Cambridge University Press, second
edition, 2002.
[284] William H. Press, Saul A. Teukolsky, William T. Vetterling, and Brian P. Flannery. Numer-
ical Recipes: The Art of Scientiﬁc Computing . Cambridge University Press, third edition,
2007.
[285] R. C. Prim. Shortest connection networks and some generalizations. Bell System Technical
Journal , 36(6):1389–1401, 1957.
[286] William Pugh. Skip lists: A probabilistic alternative to balanced trees. Communications of
the ACM , 33(6):668–676, 1990.
[287] Paul W. Purdom, Jr. and Cynthia A. Brown. The Analysis of Algorithms . Holt, Rinehart,
and Winston, 1985.
[288] Michael O. Rabin. Probabilistic algorithms. In J. F. Traub, editor, Algorithms and Com-
plexity: New Directions and Recent Results , pages 21–39. Academic Press, 1976.
[289] Michael O. Rabin. Probabilistic algorithm for testing primality. Journal of Number Theory ,
12(1):128–138, 1980.
[290] P. Raghavan and C. D. Thompson. Randomized rounding: A technique for provably good
algorithms and algorithmic proofs. Combinatorica , 7(4):365–374, 1987.
[291] Rajeev Raman. Recent results on the single-source shortest paths problem. SIGACT News ,
28(2):81–87, 1997.
[292] James Reinders. Intel Threading Building Blocks: Outﬁtting C++ for Multi-core Processor
Parallelism . O’Reilly Media, Inc., 2007.
[293] Edward M. Reingold, J¨ urg Nievergelt, and Narsingh Deo. Combinatorial Algorithms: The-
ory and Practice . Prentice Hall, 1977.
[294] Edward M. Reingold, Kenneth J. Urban, and David Gries. K-M-P string matching revisited.
Information Processing Letters , 64(5):217–223, 1997.
[295] Hans Riesel. Prime Numbers and Computer Methods for Factorization , volume 126 of
Progress in Mathematics .B i r k h ¨ auser, second edition, 1994.
[296] Ronald L. Rivest, Adi Shamir, and Leonard M. Adleman. A method for obtaining digital
signatures and public-key cryptosystems. Communications of the ACM , 21(2):120–126,
1978. See also U.S. Patent 4,405,829.
[297] Herbert Robbins. A remark on Stirling’s formula. American Mathematical Monthly ,
62(1):26–29, 1955.
[298] D. J. Rosenkrantz, R. E. Stearns, and P. M. Lewis. An analysis of several heuristics for the
traveling salesman problem. SIAM Journal on Computing , 6(3):563–581, 1977.
[299] Salvador Roura. An improved master theorem for divide-and-conquer recurrences. In
Proceedings of Automata, Languages and Programming, 24th International Colloquium,
ICALP’97 , volume 1256 of Lecture Notes in Computer Science , pages 449–459. Springer,
1997.
[300] Y . A. Rozanov. Probability Theory: A Concise Course . Dover, 1969.Bibliography 1247
[301] S. Sahni and T. Gonzalez. P-complete approximation problems. Journal of the ACM ,
23(3):555–565, 1976.
[302] A. Sch¨ onhage, M. Paterson, and N. Pippenger. Finding the median. Journal of Computer
and System Sciences , 13(2):184–199, 1976.
[303] Alexander Schrijver. Theory of Linear and Integer Programming . John Wiley & Sons,
1986.
[304] Alexander Schrijver. Paths and ﬂows—A historical survey. CWI Quarterly , 6(3):169–183,
1993.
[305] Robert Sedgewick. Implementing quicksort programs. Communications of the ACM ,
21(10):847–857, 1978.
[306] Robert Sedgewick. Algorithms . Addison-Wesley , second edition, 1988.
[307] Robert Sedgewick and Philippe Flajolet. An Introduction to the Analysis of Algorithms .
Addison-Wesley, 1996.
[308] Raimund Seidel. On the all-pairs-shortest-path problem in unweighted undirected graphs.
Journal of Computer and System Sciences , 51(3):400–403, 1995.
[309] Raimund Seidel and C. R. Aragon. Randomized search trees. Algorithmica , 16(4–5):464–
497, 1996.
[310] Jo˜ ao Setubal and Jo˜ ao Meidanis. Introduction to Computational Molecular Biology .P W S
Publishing Company, 1997.
[311] Clifford A. Shaffer. A Practical Introduction to Data Structures and Algorithm Analysis .
Prentice Hall, second edition, 2001.
[312] Jeffrey Shallit. Origins of the analysis of the Euclidean algorithm. Historia Mathematica ,
21(4):401–419, 1994.
[313] Michael I. Shamos and Dan Hoey. Geometric intersection problems. In Proceedings of the
17th Annual Symposium on Foundations of Computer Science , pages 208–215, 1976.
[314] M. Sharir. A strong-connectivity algorithm and its applications in data ﬂow analysis. Com-
puters and Mathematics with Applications , 7(1):67–72, 1981.
[315] David B. Shmoys. Computing near-optimal solutions to combinatorial optimization prob-
lems. In William Cook, L´ aszl´oL o v ´ asz, and Paul Seymour, editors, Combinatorial Opti-
mization , volume 20 of DIMACS Series in Discrete Mathematics and Theoretical Computer
Science . American Mathematical Society, 1995.
[316] Avi Shoshan and Uri Zwick. All pairs shortest paths in undirected graphs with integer
weights. In Proceedings of the 40th Annual Symposium on Foundations of Computer Sci-
ence, pages 605–614, 1999.
[317] Michael Sipser. Introduction to the Theory of Computation . Thomson Course Technology,
second edition, 2006.
[318] Steven S. Skiena. The Algorithm Design Manual . Springer, second edition, 1998.
[319] Daniel D. Sleator and Robert E. Tarjan. A data structure for dynamic trees. Journal of
Computer and System Sciences , 26(3):362–391, 1983.1248 Bibliography
[320] Daniel D. Sleator and Robert E. Tarjan. Self-adjusting binary search trees. Journal of the
ACM , 32(3):652–686, 1985.
[321] Joel Spencer. Ten Lectures on the Probabilistic Method , volume 64 of CBMS-NSF Regional
Conference Series in Applied Mathematics . Society for Industrial and Applied Mathematics,
1993.
[322] Daniel A. Spielman and Shang-Hua Teng. Smoothed analysis of algorithms: Why the sim-
plex algorithm usually takes polynomial time. Journal of the ACM , 51(3):385–463, 2004.
[323] Gilbert Strang. Introduction to Applied Mathematics . Wellesley-Cambridge Press, 1986.
[324] Gilbert Strang. Linear Algebra and Its Applications . Thomson Brooks/Cole, fourth edition,
2006.
[325] V olker Strassen. Gaussian elimination is not optimal. Numerische Mathematik , 14(3):354–
356, 1969.
[326] T. G. Szymanski. A special case of the maximal common subsequence problem. Technical
Report TR-170, Computer Science Laboratory, Princeton University, 1975.
[327] Robert E. Tarjan. Depth ﬁrs t search and linear graph algorithms. SIAM Journal on Com-
puting , 1(2):146–160, 1972.
[328] Robert E. Tarjan. Efﬁciency of a good but not linear set union algorithm. Journal of the
ACM , 22(2):215–225, 1975.
[329] Robert E. Tarjan. A class of algorithms which require nonlinear time to maintain disjoint
sets. Journal of Computer and System Sciences , 18(2):110–127, 1979.
[330] Robert E. Tarjan. Data Structures and Network Algorithms . Society for Industrial and
Applied Mathematics, 1983.
[331] Robert E. Tarjan. Amortized computational complexity. SIAM Journal on Algebraic and
Discrete Methods , 6(2):306–318, 1985.
[332] Robert E. Tarjan. Class notes: Disjoint set union. COS 423, Princeton University, 1999.[333] Robert E. Tarjan and Jan van Leeuwen. W orst-case analysis of set union algorithms. Jour-
nal of the ACM , 31(2):245–281, 1984.
[334] George B. Thomas, Jr., Maurice D. Weir, Joel Hass, and Frank R. Giordano. Thomas’
Calculus . Addison-Wesley, eleventh edition, 2005.
[335] Mikkel Thorup. Faster deterministic sorting and priority queues in linear space. In Pro-
ceedings of the 9th ACM-SIAM Symposium on Discrete Algorithms , pages 550–555, 1998.
[336] Mikkel Thorup. Undirected single-source shortest paths with positive integer weights in
linear time. Journal of the ACM , 46(3):362–394, 1999.
[337] Mikkel Thorup. On RAM priority queues. SIAM Journal on Computing , 30(1):86–109,
2000.
[338] Richard Tolimieri, Myoung An, and Chao Lu. Mathematics of Multidimensional Fourier
Transform Algorithms . Springer, second edition, 1997.
[339] P. van Emde Boas. Preserving order in a forest in less than logarithmic time. In Proceedings
of the 16th Annual Symposium on Foundations of Computer Science , pages 75–84, 1975.Bibliography 1249
[340] P. van Emde Boas. Preserving order in a for est in less than logarithmic time and linear
space. Information Processing Letters , 6(3):80–82, 1977.
[341] P. van Emde Boas, R. Kaas, and E. Zijlstra. Design and implementation of an efﬁcient
priority queue. Mathematical Systems Theory , 10(1):99–127, 1976.
[342] Jan van Leeuwen, editor. Handbook of Theoretical Computer Science, Volume A: Algo-
rithms and Complexity . Elsevier Science Publishers and the MIT Press, 1990.
[343] Charles Van Loan. Computational Frameworks for the Fast Fourier Transform . Society for
Industrial and Applied Mathematics, 1992.
[344] Robert J. Vanderbei. Linear Programming: Foundations and Extensions . Kluwer Academic
Publishers, 1996.
[345] Vijay V . Vazirani. Approximation Algorithms . Springer, 2001.
[346] Rakesh M. Verma. General techniques for analyzing recursive algorithms with applications.
SIAM Journal on Computing , 26(2):568–581, 1997.
[347] Hao Wang and Bill Lin. Pipelined van Emde Boas tree: Algorithms, analysis, and ap-
plications. In 26th IEEE International Conference on Computer Communications , pages
2471–2475, 2007.
[348] Antony F. Ware. Fast approximate Fourier transforms for irregularly spaced data. SIAM
Review , 40(4):838–856, 1998.
[349] Stephen Warshall. A theorem on boolean matrices. Journal of the ACM , 9(1):11–12, 1962.
[350] Michael S. Waterman. Introduction to Computational Biology, Maps, Sequences and
Genomes . Chapman & Hall, 1995.
[351] Mark Allen Weiss. Data Structures and Problem Solving Using C++ . Addison-Wesley,
second edition, 2000.
[352] Mark Allen Weiss. Data Structures and Problem Solving Using Java . Addison-Wesley,
third edition, 2006.
[353] Mark Allen Weiss. Data Structures and Algorithm Analysis in C++ . Addison-Wesley, third
edition, 2007.
[354] Mark Allen Weiss. Data Structures and Algorithm Analysis in Java . Addison-Wesley,
second edition, 2007.
[355] Hassler Whitney. On the abstract properties of linear dependence. American Journal of
Mathematics , 57(3):509–533, 1935.
[356] Herbert S. Wilf. Algorithms and Complexity . A K Peters, second edition, 2002.
[357] J. W. J. Williams. Algorithm 232 (HEAPSORT). Communications of the ACM , 7(6):347–
348, 1964.
[358] Shmuel Winograd. On the algebraic complexity of functions. In Actes du Congr` es Interna-
tional des Math´ ematiciens , volume 3, pages 283–288, 1970.
[359] Andrew C.-C. Yao. A lower bound to ﬁnding convex hulls. Journal of the ACM , 28(4):780–
787, 1981.
[360] Chee Yap. A real elementary approach to the m aster recurrence and generalizations. Un-
published manuscript. Available at http://cs.nyu.edu/yap/papers/, July 2008.1250 Bibliography
[361] Yinyu Ye. Interior Point Algorithms: Theory and Analysis . John Wiley & Sons, 1997.
[362] Daniel Zwillinger, editor. CRC Standard Mathematical Tables and Formulae . Chapman &
Hall/CRC Press, 31st edition, 2003.Index
This index uses the following conventions. Numbers are alphabetized as if spelled
out; for example, “2-3-4 tree” is indexed as if it were “two-three-four tree.” Whenan entry refers to a place other than the main text, the page number is followed bya tag: ex. for exercise, pr. for problem, ﬁg. for ﬁgure, and n. for footnote. A taggedpage number often indicates the ﬁrst page of an exercise or problem, which is notnecessarily the page on which the reference actually appears.
˛.n/ , 574
/RS(golden ratio), 59, 108 pr.
y/RS(conjugate of the golden ratio), 59
/RS.n/ (Euler’s phi function), 943
/SUB.n/-approximation algorithm, 1106, 1123
o-notation, 50–51, 64
O-notation, 45 ﬁg., 47–48, 64
O0-notation, 62 pr.
eO-notation, 62 pr.
!-notation, 51
/DEL-notation, 45 ﬁg., 48–49, 64
1/DEL-notation, 62 pr.
e/DEL-notation, 62 pr.
‚-notation, 44–47, 45 ﬁg., 64
e‚-notation, 62 pr.
fg(set), 1158
2(set member), 1158
62(not a set member), 1158
;
(empty language), 1058
(empty set), 1158
/DC2(subset), 1159
/SUB(proper subset), 1159
W(such that), 1159
\(set intersection), 1159
[(set union), 1159/NUL(set difference), 1159
jj
(ﬂow value), 710
(length of a string), 986
(set cardinality), 1161
/STX
(Cartesian product), 1162
(cross product), 1016
hi
(sequence), 1166
(standard encoding), 1057/NULn
k/SOH
(choose), 1185
kk(euclidean norm), 1222
Š(factorial), 57
de(ceiling), 54
bc(ﬂoor), 54
#p
(lower square root), 546
"p
(upper square root), 546P(sum), 1145Q(product), 1148
!(adjacency relation), 1169
;(reachability relation), 1170
^(AND), 697, 1071
:(NOT), 1071
_(OR), 697, 1071
˚(group operator), 939
˝(convolution operator), 9011252 Index
/ETX(closure operator), 1058
j(divides relation), 927
−(does-not-divide relation), 927
/DC1(equivalent modulo n), 54, 1165 ex.
6/DC1(not equivalent modulo n), 54
Œa/c141n(equivalence class modulo n), 928
Cn(addition modulo n), 940
/SOHn(multiplication modulo n), 940
.a
p/(Legendre symbol), 982 pr.
"(empty string), 986, 1058
<(preﬁx relation), 986
=(sufﬁx relation), 986
<x(above relation), 1022
//(comment symbol), 21
/GS(much-greater-than relation), 574
/FS(much-less-than relation), 783
/DC4P(polynomial-time reducibility relation),
1067, 1077 ex.
AA-tree, 338
abelian group, 940
ABOVE , 1024
above relation ( <x), 1022
absent child, 1178
absolutely convergent series, 1146
absorption laws for sets, 1160
abstract problem, 1054
acceptable pair of integers, 972
acceptance
by an algorithm, 1058
by a ﬁnite automaton, 996
accepting state, 995
accounting method, 456–459
for binary counters, 458
for dynamic tables, 465–466for stack operations, 457–458, 458 ex.
Ackermann’s function, 585
activity-selection problem, 415–422, 450
acyclic graph, 1170
relation to matroids, 448 pr.
add instruction, 23
addition
of binary integers, 22 ex.
of matrices, 1220
modulo n(C
n), 940
of polynomials, 898
additive group modulo n, 940
addressing, open, seeopen-address hash tableADD-SUBARRAY , 805 pr.
adjacency-list representation, 590
replaced by a hash table, 593 ex.
adjacency-matrix representation, 591
adjacency relation ( !), 1169
adjacent vertices, 1169
admissible edge, 749admissible network, 749–750
adversary, 190
aggregate analysis, 452–456
for binary counters, 454–455
for breadth-ﬁrst search, 597
for depth-ﬁrst search, 606
for Dijkstra’s algorithm, 661
for disjoint-set data structures, 566–567,
568 ex.
for dynamic tables, 465
for Fibonacci heaps, 518, 522 ex.
for Graham’s scan, 1036for the Knuth-Morris-Pratt algorithm, 1006
for Prim’s algorithm, 636
for rod-cutting, 367for shortest paths in a dag, 655for stack operations, 452–454
aggregate ﬂow, 863
Akra-Bazzi method for solving a recurrence,
112–113
algorithm, 5
correctness of, 6
origin of word, 42
running time of, 25
as a technology, 13
Alice, 959
A
LLOCA TE -NODE, 492
ALLOCA TE -OBJECT , 244
allocation of objects, 243–244
all-pairs shortest paths, 644, 684–707
in dynamic graphs, 707in/SI-dense graphs, 706 pr.
Floyd-Warshall algorithm for, 693–697, 706
Johnson’s algorithm for, 700–706
by matrix multiplication, 686–693, 706–707
by repeated squaring, 689–691
alphabet, 995, 1057˛.n/ , 574
amortized analysis, 451–478
accounting method of, 456–459aggregate analysis, 367, 452–456Index 1253
for bit-reversal permutation, 472 pr.
for breadth-ﬁrst search, 597
for depth-ﬁrst search, 606
for Dijkstra’s algorithm, 661for disjoint-set data structures, 566–567,
568 ex., 572 ex., 575–581, 581–582 ex.
for dynamic tables, 463–471for Fibonacci heaps, 509–512, 517–518,
520–522, 522 ex.
for the generic push-relabel algorithm, 746
for Graham’s scan, 1036
for the Knuth-Morris-Pratt algorithm, 1006
for making binary search dynamic, 473 pr.potential method of, 459–463
for restructuring red-black trees, 474 pr.
for self-organizing lists with move-to-front,
476 pr.
for shortest paths in a dag, 655
for stacks on secondary storage, 502 pr.for weight-balanced trees, 473 pr.
amortized cost
in the accounting method, 456in aggregate analysis, 452in the potential method, 459
ancestor, 1176
least common, 584 pr.
AND function (^), 697, 1071
AND gate, 1070
and, in pseudocode, 22
antiparallel edges, 711–712
antisymmetric relation, 1164
A
NY-SEGMENTS -INTERSECT , 1025
approximation
by least squares, 835–839
of summation by integrals, 1154–1156
approximation algorithm, 10, 1105–1140
for bin packing, 1134 pr.
for MAX-CNF satisﬁability, 1127 ex.
for maximum clique, 1111 ex., 1134 pr.
for maximum matching, 1135 pr.
for maximum spanning tree, 1137 pr.for maximum-weight cut, 1127 ex.
for MAX-3-CNF satisﬁability, 1123–1124,
1139
for minimum-weight vertex cover,
1124–1127, 1139
for parallel machine scheduling, 1136 pr.
randomized, 1123for set cover, 1117–1122, 1139
for subset sum, 1128–1134, 1139
for traveling-salesman problem, 1111–1117,
1139
for vertex cover, 1108–1111, 1139
for weighted set cover, 1135 pr.
for 0-1 knapsack problem, 1137 pr., 1139
approximation error, 836
approximation ratio, 1106, 1123
approximation scheme, 1107
A
PPROX -MIN-WEIGHT -VC, 1126
APPROX -SUBSET -SUM, 1131
APPROX -TSP-T OUR, 1112
APPROX -VERTEX -COVER , 1109
arbitrage, 679 pr.
arc,seeedge
argument of a function, 1166–1167
arithmetic instructions, 23
arithmetic, modular, 54, 939–946arithmetic series, 1146
arithmetic with inﬁnities, 650
arm, 485array, 21
Monge, 110 pr.
passing as a parameter, 21
articulation point, 621 pr.
assignment
multiple, 21
satisfying, 1072, 1079truth, 1072, 1079
associative laws for sets, 1160
associative operation, 939
asymptotically larger, 52
asymptotically nonnegative, 45
asymptotically positive, 45
asymptotically smaller, 52
asymptotically tight bound, 45
asymptotic efﬁciency, 43asymptotic lower bound, 48
asymptotic notation, 43–53, 62 pr.
and graph algorithms, 588
and linearity of summations, 1146
asymptotic upper bound, 47
attribute of an object, 21
augmentation of a ﬂow, 716
augmenting data structures, 339–355
augmenting path, 719–720, 763 pr.authentication, 284 pr., 960–961, 9641254 Index
automaton
ﬁnite, 995
string-matching, 996–1002
auxiliary hash function, 272auxiliary linear program, 886
average-case running time, 28, 116
AV L - I
NSERT , 333 pr.
A VL tree, 333 pr., 337
axioms, for probability, 1190
babyface, 602 ex.
back edge, 609, 613
back substitution, 817
BAD-SET-COVER -INSTANCE , 1122 ex.
BALANCE , 333 pr.
balanced search tree
AA-trees, 338
A VL trees, 333 pr., 337
B-trees, 484–504k-neighbor trees, 338
red-black trees, 308–338
scapegoat trees, 338
splay trees, 338, 482treaps, 333 pr., 338
2-3-4 trees, 489, 503 pr.
2-3 trees, 337, 504
weight-balanced trees, 338, 473 pr.
balls and bins, 133–134, 1215 pr.
base- apseudoprime, 967
base case, 65, 84
base, in DNA, 391
basic feasible solution, 866basic solution, 866
basic variable, 855
basis function, 835
Bayes’s theorem, 1194
B
ELLMAN -FORD, 651
Bellman-Ford algorithm, 651–655, 682
for all-pairs shortest paths, 684
in Johnson’s algorithm, 702–704
and objective functions, 670 ex.to solve systems of difference constraints,
668
Yen’s improvement to, 678 pr.
B
ELOW , 1024
Bernoulli trial, 1201
and balls and bins, 133–134
and streaks, 135–139best-case running time, 29 ex., 49
BFS, 595
BIASED -RANDOM , 117 ex.
biconnected component, 621 pr.
big-oh notation, 45 ﬁg., 47–48, 64
big-omega notation, 45 ﬁg., 48–49, 64
bijective function, 1167
binary character code, 428
binary counter
analyzed by accounting method, 458
analyzed by aggregate analysis, 454–455
analyzed by potential method, 461–462
bit-reversed, 472 pr.
binary entropy function, 1187
binary gcd algorithm, 981 pr.
binary heap, seeheap
binary relation, 1163
binary search, 39 ex.
with fast insertion, 473 pr.in insertion sort, 39 ex.
in multithreaded merging, 799–800
in searching B-trees, 499 ex.
B
INARY -SEARCH , 799
binary search tree, 286–307
AA-trees, 338
A VL trees, 333 pr., 337deletion from, 295–298, 299 ex.
with equal keys, 303 pr.
insertion into, 294–295
k-neighbor trees, 338
maximum key of, 291
minimum key of, 291optimal, 397–404, 413
predecessor in, 291–292
querying, 289–294randomly built, 299–303, 304 pr.
right-converting of, 314 ex.
scapegoat trees, 338
searching, 289–291
for sorting, 299 ex.
splay trees, 338successor in, 291–292
and treaps, 333 pr.
weight-balanced trees, 338see also red-black tree
binary-search-tree property, 287
in treaps, 333 pr.
vs. min-heap property, 289 ex.Index 1255
binary tree, 1177
full, 1178
number of different ones, 306 pr.
representation of, 246superimposed upon a bit vector, 533–534
see also binary search tree
binomial coefﬁcient, 1186–1187binomial distribution, 1203–1206
and balls and bins, 133
maximum value of, 1207 ex.
tails of, 1208–1215
binomial expansion, 1186
binomial heap, 527 pr.binomial tree, 527 pr.
bin packing, 1134 pr.
bipartite graph, 1172
corresponding ﬂow network of, 732
d-regular, 736 ex.
and hypergraphs, 1173 ex.
bipartite matching, 530, 732–736, 747 ex., 766
Hopcroft-Karp algorithm for, 763 pr.
birthday paradox, 130–133, 142 ex.bisection of a tree, 1181 pr.bitonic euclidean traveling-salesman problem,
405 pr.
bitonic sequence, 682 pr.bitonic tour, 405 pr.
bit operation, 927
in Euclid’s algorithm, 981 pr.
bit-reversal permutation, 472 pr., 918
B
IT-REVERSE -COPY, 918
bit-reversed binary counter, 472 pr.
BIT-REVERSED -INCREMENT , 472 pr.
bit vector, 255 ex., 532–536
black-height, 309
black vertex, 594, 603
blocking ﬂow, 765
block structure in pseudocode, 20Bob, 959
Boole’s inequality, 1195 ex.
boolean combinational circuit, 1071boolean combinational element, 1070
boolean connective, 1079
boolean formula, 1049, 1066 ex., 1079,
1086 ex.
boolean function, 1187 ex.
boolean matrix multiplication, 832 ex.Bor˙uvka’s algorithm, 641bottleneck spanning tree, 640 pr.
bottleneck traveling-salesman problem,
1117 ex.
bottom of a stack, 233
B
OTTOM -UP-CUT-ROD, 366
bottom-up method, for dynamic programming,
365
bound
asymptotically tight, 45
asymptotic lower, 48asymptotic upper, 47
on binomial coefﬁcients, 1186–1187
on binomial distributions, 1206polylogarithmic, 57
on the tails of a binomial distribution,
1208–1215
see also lower bounds
boundary condition, in a recurrence, 67, 84
boundary of a polygon, 1020 ex.bounding a summation, 1149–1156
box, nesting, 678 pr.
B
C-tree, 488
branching factor, in B-trees, 487branch instructions, 23
breadth-ﬁrst search, 594–602, 623
in maximum ﬂow, 727–730, 766and shortest paths, 597–600, 644
similarity to Dijkstra’s algorithm, 662,
663 ex.
breadth-ﬁrst tree, 594, 600
bridge, 621 pr.
B
/ETX-tree, 489 n.
B-tree, 484–504
compared with red-black trees, 484, 490
creating, 492deletion from, 499–502
full node in, 489
height of, 489–490
insertion into, 493–497
minimum degree of, 489
minimum key of, 497 ex.properties of, 488–491
searching, 491–492
splitting a node in, 493–4952-3-4 trees, 489
B-T
REE-CREATE , 492
B-T REE-DELETE , 499
B-T REE-INSERT , 4951256 Index
B-T REE-INSERT -NONFULL , 496
B-T REE-SEARCH , 492, 499 ex.
B-T REE-SPLIT-CHILD , 494
BUBBLESORT ,4 0p r .
bucket, 200
bucket sort, 200–204
BUCKET -SORT, 201
BUILD -MAX-HEAP, 157
BUILD -MAX-HEAP0, 167 pr.
BUILD -MIN-HEAP, 159
butterﬂy operation, 915
by, in pseudocode, 21
cache, 24, 449 pr.
cache hit, 449 pr.
cache miss, 449 pr.
cache obliviousness, 504
caching, off-line, 449 pr.
call
in a multithreaded computation, 776
of a subroutine, 23, 25 n.
by value, 21
call edge, 778cancellation lemma, 907
cancellation of ﬂow, 717
canonical form for task scheduling, 444capacity
of a cut, 721
of an edge, 709residual, 716, 719
of a vertex, 714 ex.
capacity constraint, 709–710
cardinality of a set ( jj), 1161
Carmichael number, 968, 975 ex.
Cartesian product (/STX), 1162
Cartesian sum, 906 ex.
cascading cut, 520
C
ASCADING -CUT, 519
Catalan numbers, 306 pr., 372
ceiling function (de), 54
in master theorem, 103–106
ceiling instruction, 23
certain event, 1190
certiﬁcate
in a cryptosystem, 964
for veriﬁcation algorithms, 1063
CHAINED -HASH-DELETE , 258
CHAINED -HASH-INSERT , 258CHAINED -HASH-SEARCH , 258
chaining, 257–260, 283 pr.
chain of a convex hull, 1038
changing a key, in a Fibonacci heap, 529 pr.changing variables, in the substitution method,
86–87
character code, 428chess-playing program, 790–791
child
in a binary tree, 1178in a multithreaded computation, 776
in a rooted tree, 1176
child list in a Fibonacci heap, 507
Chinese remainder theorem, 950–954, 983
chip multiprocessor, 772
chirp transform, 914 ex.
choose/NUL
n
k/SOH
, 1185
chord, 345 ex.
Cilk, 774, 812
Cilk++, 774, 812
ciphertext, 960
circuit
boolean combinational, 1071
depth of, 919
for fast Fourier transform, 919–920
CIRCUIT-SAT, 1072circuit satisﬁability, 1070–1077
circular, doubly linked list with a sentinel, 239
circular linked list, 236
see also linked list
class
complexity, 1059
equivalence, 1164
classiﬁcation of edges
in breadth-ﬁrst search, 621 pr.in depth-ﬁrst search, 609–610, 611 ex.
in a multithreaded dag, 778–779
clause, 1081–1082clean area, 208 pr.
clique, 1086–1089, 1105
approximation algorithm for, 1111 ex.,
1134 pr.
CLIQUE, 1087
closed interval, 348closed semiring, 707
closest pair, ﬁnding, 1039–1044, 1047
closest-point heuristic, 1117 ex.Index 1257
closure
group property, 939
of a language, 1058
operator (/ETX), 1058
transitive, seetransitive closure
cluster
in a bit vector with a superimposed tree of
constant height, 534
for parallel computing, 772
in proto van Emde Boas structures, 538
in van Emde Boas trees, 546
clustering, 272
CNF (conjunctive normal form), 1049, 1082CNF satisﬁability, 1127 ex.
coarsening leaves of recursion
in merge sort, 39 pr.when recursively spawning, 787
code, 428–429
Huffman, 428–437, 450
codeword, 429
codomain, 1166
coefﬁcient
binomial, 1186
of a polynomial, 55, 898
in slack form, 856
coefﬁcient representation, 900
and fast multiplication, 903–905
cofactor, 1224
coin changing, 446 pr.colinearity, 1016
collision, 257
resolution by chaining, 257–260
resolution by open addressing, 269–277
collision-resistant hash function, 964
coloring, 1103 pr., 1180 pr.color, of a red-black-tree node, 308
column-major order, 208 pr.
column rank, 1223
columnsort, 208 pr.
column vector, 1218
combination, 1185combinational circuit, 1071
combinational element, 1070
combine step, in divide-and-conquer, 30, 65
comment, in pseudocode ( //), 21
commodity, 862
common divisor, 929
greatest, seegreatest common divisorcommon multiple, 939 ex.
common subexpression, 915
common subsequence, 7, 391
longest, 7, 390–397, 413
commutative laws for sets, 1159
commutative operation, 940
C
OMPACTIFY -LIST, 245 ex.
compact list, 250 pr.
COMPACT -LIST-SEARCH , 250 pr.
COMPACT -LIST-SEARCH0, 251 pr.
comparable line segments, 1022
COMPARE -EXCHANGE , 208 pr.
compare-exchange operation, 208 pr.comparison sort, 191
and binary search trees, 289 ex.
randomized, 205 pr.
and selection, 222
compatible activities, 415
compatible matrices, 371, 1221
competitive analysis, 476 pr.
complement
of an event, 1190
of a graph, 1090
of a language, 1058
Schur, 820, 834
of a set, 1160
complementary slackness, 894 pr.
complete graph, 1172
complete k-ary tree, 1179
see also heap
completeness of a language, 1077 ex.
complete step, 782
completion time, 447 pr., 1136 pr.
complexity class, 1059
co-NP, 1064NP, 1049, 1064
NPC, 1050, 1069
P, 1049, 1055
complexity measure, 1059
complex numbers
inverting matrices of, 832 ex.multiplication of, 83 ex.
complex root of unity, 906
interpolation at, 912–913
component
biconnected, 621 pr.
connected, 1170strongly connected, 11701258 Index
component graph, 617
composite number, 928
witness to, 968
composition, of multithreaded computations,
784 ﬁg.
computational depth, 812
computational geometry, 1014–1047
computational problem, 5–6
computation dag, 777
computation, multithreaded, 777
COMPUTE -PREFIX -FUNCTION , 1006
COMPUTE -TRANSITION -FUNCTION , 1001
concatenation
of languages, 1058
of strings, 986
concrete problem, 1055concurrency keywords, 774, 776, 785
concurrency platform, 773
conditional branch instruction, 23
conditional independence, 1195 ex.
conditional probability, 1192, 1194
conﬁguration, 1074conjugate of the golden ratio ( y/RS), 59
conjugate transpose, 832 ex.
conjunctive normal form, 1049, 1082
connected component, 1170
identiﬁed using depth-ﬁrst search, 612 ex.
identiﬁed using disjoint-set data structures,
562–564
C
ONNECTED -COMPONENTS , 563
connected graph, 1170
connective, 1079co-NP (complexity class), 1064
conquer step, in divide-and-conquer, 30, 65
conservation of ﬂow, 709–710
consistency
of literals, 1088
sequential, 779, 812
C
ONSOLIDATE , 516
consolidating a Fibonacci-heap root list,
513–517
constraint, 851
difference, 665
equality, 670 ex., 852–853inequality, 852–853
linear, 846
nonnegativity, 851, 853
tight, 865violation of, 865
constraint graph, 666–668
contain, in a path, 1170
continuation edge, 778
continuous uniform probability distribution,
1192
contraction
of a dynamic table, 467–471
of a matroid, 442
of an undirected graph by an edge, 1172
control instructions, 23
convergence property, 650, 672–673
convergent series, 1146converting binary to decimal, 933 ex.
convex combination of points, 1015
convex function, 1199convex hull, 8, 1029–1039, 1046 pr.
convex layers, 1044 pr.
convex polygon, 1020 ex.convex set, 714 ex.
convolution (˝), 901
convolution theorem, 913
copy instruction, 23
correctness of an algorithm, 6
corresponding ﬂow network for bipartite
matching, 732
countably inﬁnite set, 1161
counter, seebinary counter
counting, 1183–1189
probabilistic, 143 pr.
counting sort, 194–197
in radix sort, 198
C
OUNTING -SORT, 195
coupon collector’s problem, 134
cover
path, 761 pr.
by a subset, 1118
vertex, 1089, 1108, 1124–1127, 1139
covertical, 1024
CREATE -NEW-RS- VEB-T REE, 557 pr.
credit, 456critical edge, 729
critical path
of a dag, 657of a multithreaded computation, 779
cross a cut, 626
cross edge, 609cross product (/STX), 1016Index 1259
cryptosystem, 958–965, 983
cubic spline, 840 pr.
currency exchange, 390 ex., 679 pr.
curve ﬁtting, 835–839cut
capacity of, 721
cascading, 520of a ﬂow network, 720–724
minimum, 721, 731 ex.
net ﬂow across, 720
of an undirected graph, 626
weight of, 1127 ex.
C
UT, 519
CUT-ROD, 363
cutting, in a Fibonacci heap, 519
cycle of a graph, 1170
hamiltonian, 1049, 1061
minimum mean-weight, 680 pr.
negative-weight, seenegative-weight cycle
and shortest paths, 646–647
cyclic group, 955
cyclic rotation, 1012 ex.cycling, of simplex algorithm, 875
dag, seedirected acyclic graph
D
AG-SHORTEST -PATHS , 655
d-ary heap, 167 pr.
in shortest-paths algorithms, 706 pr.
data-movement instructions, 23data-parallel model, 811
data structure, 9, 229–355, 481–585
AA-trees, 338
augmentation of, 339–355
A VL trees, 333 pr., 337
binary search trees, 286–307binomial heaps, 527 pr.
bit vectors, 255 ex., 532–536
B-trees, 484–504deques, 236 ex.
dictionaries, 229
direct-address tables, 254–255
for disjoint sets, 561–585
for dynamic graphs, 483
dynamic sets, 229–231dynamic trees, 482
exponential search trees, 212, 483
Fibonacci heaps, 505–530fusion trees, 212, 483hash tables, 256–261
heaps, 151–169
interval trees, 348–354
k-neighbor trees, 338
linked lists, 236–241
mergeable heap, 505
order-statistic trees, 339–345
persistent, 331 pr., 482
potential of, 459
priority queues, 162–166
proto van Emde Boas structures, 538–545
queues, 232, 234–235
radix trees, 304 pr.
red-black trees, 308–338
relaxed heaps, 530
rooted trees, 246–249scapegoat trees, 338
on secondary storage, 484–487
skip lists, 338
splay trees, 338, 482
stacks, 232–233
treaps, 333 pr., 3382-3-4 heaps, 529 pr.2-3-4 trees, 489, 503 pr.
2-3 trees, 337, 504
van Emde Boas trees, 531–560weight-balanced trees, 338
data type, 23
deadline, 444
deallocation of objects, 243–244
decision by an algorithm, 1058–1059
decision problem, 1051, 1054
and optimization problems, 1051
decision tree, 192–193
D
ECREASE -KEY, 162, 505
decreasing a key
in Fibonacci heaps, 519–522
in 2-3-4 heaps, 529 pr.
DECREMENT , 456 ex.
degeneracy, 874
degree
of a binomial-tree root, 527 pr.
maximum, of a Fibonacci heap, 509,
523–526
minimum, of a B-tree, 489
of a node, 1177
of a polynomial, 55, 898of a vertex, 11691260 Index
degree-bound, 898
DELETE , 230, 505
DELETE -LARGER -HALF, 463 ex.
deletion
from binary search trees, 295–298, 299 ex.
from a bit vector with a superimposed binary
tree, 534
from a bit vector with a superimposed tree of
constant height, 535
from B-trees, 499–502
from chained hash tables, 258
from direct-address tables, 254
from dynamic tables, 467–471from Fibonacci heaps, 522, 526 pr.
from heaps, 166 ex.
from interval trees, 349from linked lists, 238
from open-address hash tables, 271
from order-statistic trees, 343–344from proto van Emde Boas structures, 544
from queues, 234
from red-black trees, 323–330
from stacks, 232
from sweep-line statuses, 1024
from 2-3-4 heaps, 529 pr.
from van Emde Boas trees, 554–556
DeMorgan’s laws
for propositional logic, 1083
for sets, 1160, 1162 ex.
dense graph, 589
/SI-dense, 706 pr.
density
of prime numbers, 965–966
of a rod, 370 ex.
dependence
and indicator random variables, 119
linear, 1223
see also independence
depth
average, of a node in a randomly built binary
search tree, 304 pr.
of a circuit, 919
of a node in a rooted tree, 1177
of quicksort recursion tree, 178 ex.of a stack, 188 pr.
depth-determination problem, 583 pr.
depth-ﬁrst forest, 603depth-ﬁrst search, 603–612, 623in ﬁnding articulation points, bridges, and
biconnected components, 621 pr.
in ﬁnding strongly connected components,
615–621, 623
in topological sorting, 612–615
depth-ﬁrst tree, 603
deque, 236 ex.
D
EQUEUE , 235
derivative of a series, 1147
descendant, 1176
destination vertex, 644
det,seedeterminant
determinacy race, 788determinant, 1224–1225
and matrix multiplication, 832 ex.
deterministic algorithm, 123
multithreaded, 787
D
ETERMINISTIC -SEARCH , 143 pr.
DFS, 604
DFS-V ISIT, 604
DFT (discrete Fourier transform), 9, 909
diagonal matrix, 1218
LUP decomposition of, 827 ex.
diameter of a tree, 602 ex.
dictionary, 229
difference constraints, 664–670
difference equation, seerecurrence
difference of sets (/NUL), 1159
symmetric, 763 pr.
differentiation of a series, 1147
digital signature, 960
digraph, seedirected graph
DIJKSTRA , 658
Dijkstra’s algorithm, 658–664, 682
for all-pairs shortest paths, 684, 704
implemented with a Fibonacci heap, 662
implemented with a min-heap, 662
with integer edge weights, 664 ex.in Johnson’s algorithm, 702
similarity to breadth-ﬁrst search, 662,
663 ex.
similarity to Prim’s algorithm, 634, 662
D
IRECT -ADDRESS -DELETE , 254
direct addressing, 254–255, 532–536
DIRECT -ADDRESS -INSERT , 254
DIRECT -ADDRESS -SEARCH , 254
direct-address table, 254–255directed acyclic graph (dag), 1172Index 1261
and back edges, 613
and component graphs, 617
and hamiltonian paths, 1066 ex.
longest simple path in, 404 pr.for representing a multithreaded
computation, 777
single-source shortest-paths algorithm for,
655–658
topological sort of, 612–615, 623
directed graph, 1168
all-pairs shortest paths in, 684–707
constraint graph, 666
Euler tour of, 623 pr., 1048hamiltonian cycle of, 1049
and longest paths, 1048
path cover of, 761 pr.
PERT chart, 657, 657 ex.
semiconnected, 621 ex.
shortest path in, 643single-source shortest paths in, 643–683
singly connected, 612 ex.
square of, 593 ex.transitive closure of, 697transpose of, 592 ex.
universal sink in, 593 ex.
see also directed acyclic graph, graph,
network
directed segment, 1015–1017
directed version of an undirected graph, 1172
D
IRECTION , 1018
dirty area, 208 pr.
DISCHARGE , 751
discharge of an overﬂowing vertex, 751
discovered vertex, 594, 603
discovery time, in depth-ﬁrst search, 605
discrete Fourier transform, 9, 909
discrete logarithm, 955
discrete logarithm theorem, 955
discrete probability distribution, 1191
discrete random variable, 1196–1201
disjoint-set data structure, 561–585
analysis of, 575–581, 581 ex.
in connected components, 562–564
in depth determination, 583 pr.
disjoint-set-forest implementation of,
568–572
in Kruskal’s algorithm, 631
linear-time special case of, 585linked-list implementation of, 564–568
in off-line least common ancestors, 584 pr.
in off-line minimum, 582 pr.
in task scheduling, 448 pr.
disjoint-set forest, 568–572
analysis of, 575–581, 581 ex.
rank properties of, 575, 581 ex.
see also disjoint-set data structure
disjoint sets, 1161
disjunctive normal form, 1083disk, 1028 ex.
disk drive, 485–487
see also secondary storage
D
ISK-READ, 487
DISK-WRITE , 487
distance
edit, 406 pr.
euclidean, 1039
Lm, 1044 ex.
Manhattan, 225 pr., 1044 ex.
of a shortest path, 597
distributed memory, 772distribution
binomial, 1203–1206
continuous uniform, 1192
discrete, 1191
geometric, 1202–1203
of inputs, 116, 122
of prime numbers, 965probability, 1190
sparse-hulled, 1046 pr.
uniform, 1191
distributive laws for sets, 1160
divergent series, 1146
divide-and-conquer method, 30–35, 65
analysis of, 34–35
for binary search, 39 ex.
for conversion of binary to decimal, 933 ex.for fast Fourier transform, 909–912
for ﬁnding the closest pair of points,
1040–1043
for ﬁnding the convex hull, 1030
for matrix inversion, 829–831
for matrix multiplication, 76–83, 792–797
for maximum-subarray problem, 68–75
for merge sort, 30–37, 797–805
for multiplication, 920 pr.1262 Index
for multithreaded matrix multiplication,
792–797
for multithreaded merge sort, 797–805
for quicksort, 170–190relation to dynamic programming, 359
for selection, 215–224
solving recurrences for, 83–106, 112–113for Strassen’s algorithm, 79–83
divide instruction, 23
divides relation (j), 927
divide step, in divide-and-conquer, 30, 65
division method, 263, 268–269 ex.
division theorem, 928divisor, 927–928
common, 929
see also greatest common divisor
DNA, 6–7, 390–391, 406 pr.
DNF (disjunctive normal form), 1083
does-not-divide relation ( −), 927
domain, 1166
dominates relation, 1045 pr.
double hashing, 272–274, 277 ex.doubly linked list, 236
see also linked list
downto , in pseudocode, 21
d-regular graph, 736 ex.
duality, 879–886, 895 pr.
weak, 880–881, 886 ex.
dual linear program, 879
dummy key, 397
dynamic graph, 562 n.
all-pairs shortest paths algorithms for, 707data structures for, 483
minimum-spanning-tree algorithm for,
637 ex.
transitive closure of, 705 pr., 707
dynamic multithreaded algorithm, see
multithreaded algorithm
dynamic multithreading, 773
dynamic order statistics, 339–345
dynamic-programming method, 359–413
for activity selection, 421 ex.
for all-pairs shortest paths, 686–697
for bitonic euclidean traveling-salesman
problem, 405 pr.
bottom-up, 365
for breaking a string, 410 pr.compared with greedy algorithms, 381,
390 ex., 418, 423–427
for edit distance, 406 pr.
elements of, 378–390for Floyd-Warshall algorithm, 693–697
for inventory planning, 411 pr.
for longest common subsequence, 390–397for longest palindrome subsequence, 405 pr.
for longest simple path in a weighted
directed acyclic graph, 404 pr.
for matrix-chain multiplication, 370–378
and memoization, 387–389
for optimal binary search trees, 397–404optimal substructure in, 379–384
overlapping subproblems in, 384–386
for printing neatly, 405 pr.reconstructing an optimal solution in, 387
relation to divide-and-conquer, 359
for rod-cutting, 360–370for seam carving, 409 pr.
for signing free agents, 411 pr.
top-down with memoization, 365
for transitive closure, 697–699
for Viterbi algorithm, 408 pr.
for 0-1 knapsack problem, 427 ex.
dynamic set, 229–231
see also data structure
dynamic table, 463–471
analyzed by accounting method, 465–466analyzed by aggregate analysis, 465
analyzed by potential method, 466–471
load factor of, 463
dynamic tree, 482
e,5 5
EŒ/c141(expected value), 1197
early-ﬁrst form, 444
early task, 444edge, 1168
admissible, 749
antiparallel, 711–712
attributes of, 592
back, 609
bridge, 621 pr.call, 778
capacity of, 709
classiﬁcation in breadth-ﬁrst search, 621 pr.classiﬁcation in depth-ﬁrst search, 609–610Index 1263
continuation, 778
critical, 729
cross, 609
forward, 609inadmissible, 749
light, 626
negative-weight, 645–646residual, 716
return, 779
safe, 626
saturated, 739
spawn, 778
tree, 601, 603, 609weight of, 591
edge connectivity, 731 ex.
edge set, 1168
edit distance, 406 pr.
Edmonds-Karp algorithm, 727–730
elementary event, 1189elementary insertion, 465
element of a set (2), 1158
ellipsoid algorithm, 850, 897elliptic-curve factorization method, 984elseif , in pseudocode, 20 n.
else, in pseudocode, 20
empty language (;), 1058
empty set (;), 1158
empty set laws, 1159
empty stack, 233
e m p t ys t r i n g( "), 986, 1058
empty tree, 1178
encoding of problem instances, 1055–1057endpoint
of an interval, 348
of a line segment, 1015
E
NQUEUE , 235
entering a vertex, 1169
entering variable, 867
entropy function, 1187
/SI-dense graph, 706 pr.
/SI-universal hash function, 269 ex.
equality
of functions, 1166
linear, 845of sets, 1158
equality constraint, 670 ex., 852
and inequality constraints, 853
tight, 865violation of, 865
equation
and asymptotic notation, 49–50
normal, 837recurrence, seerecurrence
equivalence class, 1164
modulo n(Œa/c141
n), 928
equivalence, modular ( /DC1), 54, 1165 ex.
equivalence relation, 1164
and modular equivalence, 1165 ex.
equivalent linear programs, 852
error , in pseudocode, 22
escape problem, 760 pr.
EUCLID , 935
Euclid’s algorithm, 933–939, 981 pr., 983
euclidean distance, 1039
euclidean norm (kk), 1222
Euler’s constant, 943
Euler’s phi function, 943
Euler’s theorem, 954, 975 ex.
Euler tour, 623 pr., 1048
and hamiltonian cycles, 1048
evaluation of a polynomial, 41 pr., 900, 905 ex.
derivatives of, 922 pr.
at multiple points, 923 pr.
event, 1190event point, 1023
event-point schedule, 1023
E
XACT -SUBSET -SUM, 1129
excess ﬂow, 736
exchange property, 437
exclusion and inclusion, 1163 ex.execute a subroutine, 25 n.
expansion of a dynamic table, 464–467
expectation, seeexpected value
expected running time, 28, 117
expected value, 1197–1199
of a binomial distribution, 1204of a geometric distribution, 1202
of an indicator random variable, 118
explored vertex, 605exponential function, 55–56
exponential height, 300
exponential search tree, 212, 483
exponential series, 1147
exponentiation instruction, 24
exponentiation, modular, 956
E
XTENDED -BOTTOM -UP-CUT-ROD, 3691264 Index
EXTENDED -EUCLID , 937
EXTEND -SHORTEST -PAT HS , 688
extension of a set, 438
exterior of a polygon, 1020 ex.
external node, 1176
external path length, 1180 ex.
extracting the maximum key
from d-ary heaps, 167 pr.
from max-heaps, 163
extracting the minimum key
from Fibonacci heaps, 512–518
from 2-3-4 heaps, 529 pr.
from Young tableaus, 167 pr.
EXTRACT -MAX, 162–163
EXTRACT -MIN, 162, 505
factor, 928
twiddle, 912
factorial function ( Š), 57–58
factorization, 975–980, 984
unique, 931
failure, in a Bernoulli trial, 1201
fair coin, 1191fan-out, 1071
Farkas’s lemma, 895 pr.
farthest-pair problem, 1030
F
ASTER -ALL-PAIRS -SHORTEST -PAT HS , 691,
692 ex.
fast Fourier transform (FFT), 898–925
circuit for, 919–920
iterative implementation of, 915–918
multidimensional, 921 pr.multithreaded algorithm for, 804 ex.
recursive implementation of, 909–912
using modular arithmetic, 923 pr.
feasibility problem, 665, 894 pr.
feasible linear program, 851
feasible region, 847
feasible solution, 665, 846, 851
Fermat’s theorem, 954
FFT, seefast Fourier transform
FFTW, 924
F
IB, 775
FIB-HEAP-CHANGE -KEY, 529 pr.
FIB-HEAP-DECREASE -KEY, 519
FIB-HEAP-DELETE , 522
FIB-HEAP-EXTRACT -MIN, 513
FIB-HEAP-INSERT , 510FIB-HEAP-LINK, 516
FIB-HEAP-PRUNE , 529 pr.
FIB-HEAP-UNION , 512
Fibonacci heap, 505–530
changing a key in, 529 pr.
compared with binary heaps, 506–507
creating, 510decreasing a key in, 519–522
deletion from, 522, 526 pr.
in Dijkstra’s algorithm, 662
extracting the minimum key from, 512–518
insertion into, 510–511
in Johnson’s algorithm, 704maximum degree of, 509, 523–526
minimum key of, 511
potential function for, 509in Prim’s algorithm, 636
pruning, 529 pr.
running times of operations on, 506 ﬁg.
uniting, 511–512
Fibonacci numbers, 59–60, 108 pr., 523
computation of, 774–780, 981 pr.
FIFO (ﬁrst-in, ﬁrst-out), 232
see also queue
ﬁnal-state function, 996
ﬁnal strand, 779
F
IND-DEPTH , 583 pr.
FIND-MAX-CROSSING -SUBARRAY ,7 1
FIND-MAXIMUM -SUBARRAY ,7 2
ﬁnd path, 569
FIND-SET, 562
disjoint-set-forest implementation of, 571,
585
linked-list implementation of, 564
ﬁnished vertex, 603ﬁnishing time, in depth-ﬁrst search, 605
and strongly connected components, 618
ﬁnish time, in activity selection, 415
ﬁnite automaton, 995
for string matching, 996–1002
F
INITE -AUTOMATON -MAT CHER , 999
ﬁnite group, 940
ﬁnite sequence, 1166
ﬁnite set, 1161
ﬁrst-ﬁt heuristic, 1134 pr.
ﬁrst-in, ﬁrst-out, 232
see also queue
ﬁxed-length code, 429Index 1265
ﬂoating-point data type, 23
ﬂoor function (bc), 54
in master theorem, 103–106
ﬂoor instruction, 23ﬂow, 709–714
aggregate, 863
augmentation of, 716blocking, 765
cancellation of, 717
excess, 736
integer-valued, 733
net, across a cut, 720
value of, 710
ﬂow conservation, 709–710
ﬂow network, 709–714
corresponding to a bipartite graph, 732cut of, 720–724
with multiple sources and sinks, 712
F
LOYD -WARSHALL , 695
FLOYD -WARSHALL0, 699 ex.
Floyd-Warshall algorithm, 693–697,
699–700 ex., 706
multithreaded, 797 ex.
FORD-FULKERSON , 724
Ford-Fulkerson method, 714–731, 765
FORD-FULKERSON -METHOD , 715
forest, 1172–1173
depth-ﬁrst, 603
disjoint-set, 568–572
for, in pseudocode, 20–21
and loop invariants, 19 n.
formal power series, 108 pr.
formula satisﬁa bility, 1079–1081, 1105
forward edge, 609
forward substitution, 816–817Fourier transform, seediscrete Fourier
transform, fast Fourier transform
fractional knapsack problem, 426, 428 ex.free agent, 411 pr.
freeing of objects, 243–244
free list, 243
F
REE-OBJECT , 244
free tree, 1172–1176
frequency domain, 898
full binary tree, 1178, 1180 ex.
relation to optimal code, 430
full node, 489full rank, 1223full walk of a tree, 1114
fully parenthesized matrix-chain product, 370
fully polynomial-time approximation scheme,
1107
for subset sum, 1128–1134, 1139
function, 1166–1168
Ackermann’s, 585basis, 835
convex, 1199
ﬁnal-state, 996hash, seehash function
linear, 26, 845
objective, 664, 847, 851
potential, 459
preﬁx, 1003–1004
quadratic, 27reduction, 1067
sufﬁx, 996
transition, 995, 1001–1002, 1012 ex.
functional iteration, 58
fundamental theorem of linear programming,
892
furthest-in-future strategy, 449 pr.fusion tree, 212, 483
fuzzy sorting, 189 pr.
Gabow’s scaling algorithm for single-source
shortest paths, 679 pr.
gap character, 989 ex., 1002 ex.gap heuristic, 760 ex., 766
garbage collection, 151, 243
gate, 1070Gaussian elimination, 819, 842
gcd, seegreatest common divisor
general number-ﬁeld sieve, 984
generating function, 108 pr.
generator
of a subgroup, 944ofZ
/ETX
n, 955
GENERIC -MST, 626
GENERIC -PUSH-RELABEL , 741
generic push-relabel algorithm, 740–748
geometric distribution, 1202–1203
and balls and bins, 134
geometric series, 1147
geometry, computational, 1014–1047
GF.2/, 1227 pr.
gift wrapping, 1037, 10471266 Index
global variable, 21
Goldberg’s algorithm, seepush-relabel
algorithm
golden ratio ( /RS), 59, 108 pr.
gossiping, 478
GRAFT , 583 pr.
Graham’s scan, 1030–1036, 1047
GRAHAM -SCAN, 1031
graph, 1168–1173
adjacency-list representation of, 590
adjacency-matrix representation of, 591
algorithms for, 587–766
and asymptotic notation, 588attributes of, 588, 592
breadth-ﬁrst search of, 594–602, 623
coloring of, 1103 pr.complement of, 1090
component, 617
constraint, 666–668dense, 589
depth-ﬁrst search of, 603–612, 623
dynamic, 562 n.
/SI-dense, 706 pr.
hamiltonian, 1061
incidence matrix of, 448 pr., 593 ex.
interval, 422 ex.nonhamiltonian, 1061
shortest path in, 597
singly connected, 612 ex.sparse, 589
static, 562 n.
subproblem, 367–368
tour of, 1096
weighted, 591
see also directed acyclic graph, directed
graph, ﬂow network, undirected graph,
tree
graphic matroid, 437–438, 642
GRAPH-ISOMORPHISM, 1065 ex.
gray vertex, 594, 603
greatest common divisor (gcd), 929–930,
933 ex.
binary gcd algorithm for, 981 pr.
Euclid’s algorithm for, 933–939, 981 pr., 983with more than two arguments, 939 ex.
recursion theorem for, 934
greedoid, 450
G
REEDY , 440GREEDY -ACTIVITY -SELECTOR , 421
greedy algorithm, 414–450
for activity selection, 415–422
for coin changing, 446 pr.compared with dynamic programming, 381,
390 ex., 418, 423–427
Dijkstra’s algorithm, 658–664elements of, 423–428
for fractional knapsack problem, 426
greedy-choice property in, 424–425
for Huffman code, 428–437
Kruskal’s algorithm, 631–633
and matroids, 437–443
for minimum spanning tree, 631–638
for multithreaded scheduling, 781–783
for off-line caching, 449 pr.
optimal substructure in, 425
Prim’s algorithm, 634–636
for set cover, 1117–1122, 1139for task scheduling, 443–446, 447–448 pr.
on a weighted matroid, 439–442
for weighted set cover, 1135 pr.
greedy-choice property, 424–425
of activity selection, 417–418
of Huffman codes, 433–434
of a weighted matroid, 441
greedy scheduler, 782
G
REEDY -SET-COVER , 1119
grid, 760 pr.group, 939–946
cyclic, 955
operator (˚), 939
guessing the solution, in the substitution
method, 84–85
half 3-CNF satisﬁability, 1101 ex.
half-open interval, 348
Hall’s theorem, 735 ex.halting problem, 1048
halving lemma, 908
HAM-CYCLE, 1062
hamiltonian cycle, 1049, 1061, 1091–1096,
1105
hamiltonian graph, 1061
hamiltonian path, 1066 ex., 1101 ex.
HAM-PATH, 1066 ex.
handle, 163, 507
handshaking lemma, 1172 ex.Index 1267
harmonic number, 1147, 1153–1154
harmonic series, 1147, 1153–1154
HASH-DELETE , 277 ex.
hash function, 256, 262–269
auxiliary, 272
collision-resistant, 964
division method for, 263, 268–269 ex./SI-universal, 269 ex.
multiplication method for, 263–264
universal, 265–268
hashing, 253–285
with chaining, 257–260, 283 pr.
double, 272–274, 277 ex.k-universal, 284 pr.
in memoization, 365, 387
with open addressing, 269–277perfect, 277–282, 285
to replace adjacency lists, 593 ex.
universal, 265–268
H
ASH-INSERT , 270, 277 ex.
HASH-SEARCH , 271, 277 ex.
hash table, 256–261
dynamic, 471 ex.
secondary, 278
see also hashing
hash value, 256hat-check problem, 122 ex.
head
in a disk drive, 485of a linked list, 236
of a queue, 234
heap, 151–169
analyzed by potential method, 462 ex.
binomial, 527 pr.
building, 156–159, 166 pr.
compared with Fibonacci heaps, 506–507
d-ary, 167 pr., 706 pr.
deletion from, 166 ex.in Dijkstra’s algorithm, 662
extracting the maximum key from, 163
Fibonacci, seeFibonacci heap
as garbage-collected storage, 151
height of, 153
in Huffman’s algorithm, 433to implement a mergeable heap, 506
increasing a key in, 163–164
insertion into, 164
in Johnson’s algorithm, 704max-heap, 152
maximum key of, 163
mergeable, seemergeable heap
min-heap, 153in Prim’s algorithm, 636
as a priority queue, 162–166
relaxed, 530
running times of operations on, 506 ﬁg.
and treaps, 333 pr.
2-3-4, 529 pr.
H
EAP-DECREASE -KEY, 165 ex.
HEAP-DELETE , 166 ex.
HEAP-EXTRACT -MAX, 163
HEAP-EXTRACT -MIN, 165 ex.
HEAP-INCREASE -KEY, 164
HEAP-MAXIMUM , 163
HEAP-MINIMUM , 165 ex.
heap property, 152
maintenance of, 154–156
vs. binary-search-tr ee property, 289 ex.
heapsort, 151–169
HEAPSORT , 160
heel, 602 ex.height
of a binomial tree, 527 pr.
black-, 309of a B-tree, 489–490
of ad-ary heap, 167 pr.
of a decision tree, 193
exponential, 300
of a heap, 153
of a node in a heap, 153, 159 ex.of a node in a tree, 1177
of a red-black tree, 309
of a tree, 1177
height-balanced tree, 333 pr.
height function, in push-relabel algorithms, 738
hereditary family of subsets, 437
Hermitian matrix, 832 ex.
high endpoint of an interval, 348
high function, 537, 546
H
IRE-ASSISTANT , 115
hiring problem, 114–115, 123–124, 145
on-line, 139–141
probabilistic analysis of, 120–121
hit
cache, 449 pr.spurious, 9911268 Index
HOARE -PARTITION , 185 pr.
HOPCROFT -KARP, 764 pr.
Hopcroft-Karp bipartite matching algorithm,
763 pr.
horizontal ray, 1021 ex.
Horner’s rule, 41 pr., 900
in the Rabin-Karp algorithm, 990
HUFFMAN , 431
Huffman code, 428–437, 450
hull, convex, 8, 1029–1039, 1046 pr.
Human Genome Project, 6
hyperedge, 1172
hypergraph, 1172
and bipartite graphs, 1173 ex.
ideal parallel computer, 779
idempotency laws for sets, 1159
identity, 939
identity matrix, 1218if, in pseudocode, 20
image, 1167
image compression, 409 pr., 413
inadmissible edge, 749
incidence, 1169
incidence matrix
and difference constraints, 666of a directed graph, 448 pr., 593 ex.
of an undirected graph, 448 pr.
inclusion and exclusion, 1163 ex.incomplete step, 782
I
NCREASE -KEY, 162
increasing a key, in a max-heap, 163–164
INCREMENT , 454
incremental design method, 29
for ﬁnding the convex hull, 1030
in-degree, 1169
indentation in pseudocode, 20
independence
of events, 1192–1193, 1195 ex.
of random variables, 1197
of subproblems in dynamic programming,
383–384
independent family of subsets, 437
independent set, 1101 pr.
of tasks, 444
independent strands, 789
index function, 537, 546
index of an element of Z/ETX
n, 955indicator random variable, 118–121
in analysis of expected height of a randomly
built binary search tree, 300–303
in analysis of inserting into a treap, 333 pr.in analysis of streaks, 138–139
in analysis of the birthday paradox, 132–133
in approximation algorithm for
MAX-3-CNF satisﬁability, 1124
in bounding the right tail of the binomial
distribution, 1212–1213
in bucket sort analysis, 202–204
expected value of, 118
in hashing analysis, 259–260in hiring-problem analysis, 120–121
and linearity of expectation, 119
in quicksort analysis, 182–184, 187 pr.
in randomized-selection analysis, 217–219,
226 pr.
in universal-hashing analysis, 265–266
induced subgraph, 1171
inequality constraint, 852
and equality constraints, 853
inequality, linear, 846infeasible linear program, 851
infeasible solution, 851
inﬁnite sequence, 1166inﬁnite set, 1161
inﬁnite sum, 1145
inﬁnity, arithmetic with, 650
I
NITIALIZE -PREFLOW , 740
INITIALIZE -SIMPLEX , 871, 887
INITIALIZE -SINGLE -SOURCE , 648
initial strand, 779
injective function, 1167
inner product, 1222inorder tree walk, 287, 293 ex., 342
I
NORDER -TREE-WALK, 288
in-place sorting, 17, 148, 206 pr.input
to an algorithm, 5
to a combinational circuit, 1071
distribution of, 116, 122
to a logic gate, 1070
size of, 25
input alphabet, 995
I
NSERT , 162, 230, 463 ex., 505
insertion
into binary search trees, 294–295Index 1269
into a bit vector with a superimposed binary
tree, 534
into a bit vector with a superimposed tree of
constant height, 534
into B-trees, 493–497
into chained hash tables, 258
intod-ary heaps, 167 pr.
into direct-address tables, 254
into dynamic tables, 464–467
elementary, 465into Fibonacci heaps, 510–511
into heaps, 164
into interval trees, 349into linked lists, 237–238
into open-address hash tables, 270
into order-statistic trees, 343
into proto van Emde Boas structures, 544
into queues, 234
into red-black trees, 315–323into stacks, 232
into sweep-line statuses, 1024
into treaps, 333 pr.into 2-3-4 heaps, 529 pr.into van Emde Boas trees, 552–554
into Young tableaus, 167 pr.
insertion sort, 12, 16–20, 25–27
in bucket sort, 201–204
compared with merge sort, 14 ex.
compared with quicksort, 178 ex.
decision tree for, 192 ﬁg.
in merge sort, 39 pr.
in quicksort, 185 ex.using binary search, 39 ex.
I
NSERTION -SORT, 18, 26, 208 pr.
instance
of an abstract problem, 1051, 1054
of a problem, 5
instructions of the RAM model, 23
integer data type, 23
integer linear programming, 850, 895 pr.,
1101 ex.
integers ( Z), 1158
integer-valued ﬂow, 733
integrality theorem, 734
integral, to approximate summations,
1154–1156
integration of a series, 1147
interior of a polygon, 1020 ex.interior-point method, 850, 897
intermediate vertex, 693
internal node, 1176
internal path length, 1180 ex.interpolation by a cubic spline, 840 pr.
interpolation by a polynomial, 901, 906 ex.
at complex roots of unity, 912–913
intersection
of chords, 345 ex.
determining, for a set of line segments,
1021–1029, 1047
determining, for two line segments,
1017–1019
of languages, 1058
of sets (\), 1159
interval, 348
fuzzy sorting of, 189 pr.
I
NTERV AL -DELETE , 349
interval graph, 422 ex.
INTERV AL -INSERT , 349
INTERV AL -SEARCH , 349, 351
INTERV AL -SEARCH -EXACTLY , 354 ex.
interval tree, 348–354
interval trichotomy, 348
intractability, 1048
invalid shift, 985inventory planning, 411 pr.
inverse
of a bijective function, 1167
in a group, 940
of a matrix, 827–831, 842, 1223, 1225 ex.
multiplicative, modulo n, 949
inversion
in a self-organizing list, 476 pr.
in a sequence, 41 pr., 122 ex., 345 ex.
inverter, 1070
invertible matrix, 1223
isolated vertex, 1169
isomorphic graphs, 1171
iterated function, 63 pr.
iterated logarithm function, 58–59
I
TERA TIVE -FFT, 917
ITERA TIVE -TREE-SEARCH , 291
iter function, 577
Jarvis’s march, 1037–1038, 1047
Jensen’s inequality, 1199
JOHNSON , 7041270 Index
Johnson’s algorithm, 700–706
joining
of red-black trees, 332 pr.
of 2-3-4 trees, 503 pr.
joint probability density function, 1197
Josephus permutation, 355 pr.
Karmarkar’s algorithm, 897
Karp’s minimum mean-weight cycle algorithm,
680 pr.
k-ary tree, 1179
k-CNF, 1049
k-coloring, 1103 pr., 1180 pr.
k-combination, 1185
k-conjunctive normal form, 1049
kernel of a polygon, 1038 ex.
key, 16, 147, 162, 229
dummy, 397
interpreted as a natural number, 263median, of a B-tree node, 493
public, 959, 962
secret, 959, 962static, 277
keywords, in pseudocode, 20–22
multithreaded, 774, 776–777, 785–786
“killer adversary” for quicksort, 190Kirchhoff’s current law, 708
Kleene star (
/ETX), 1058
KMP algorithm, 1002–1013
KMP-M AT CHE R , 1005
knapsack problem
fractional, 426, 428 ex.
0-1, 425, 427 ex., 1137 pr., 1139
k-neighbor tree, 338
knot, of a spline, 840 pr.Knuth-Morris-Pratt algorithm, 1002–1013
k-permutation, 126, 1184
Kraft inequality, 1180 ex.Kruskal’s algorithm, 631–633, 642
with integer edge weights, 637 ex.
k-sorted, 207 pr.
k-string, 1184
k-subset, 1161
k-substring, 1184
kth power, 933 ex.
k-universal hashing, 284 pr.
Lagrange’s formula, 902Lagrange’s theorem, 944
Lam´e’s theorem, 936
language, 1057
completeness of, 1077 ex.
proving NP-completeness of, 1078–1079
veriﬁcation of, 1063
last-in, ﬁrst-out, 232
see also stack
late task, 444
layers
convex, 1044 pr.
maximal, 1045 pr.
LCA, 584 pr.
lcm (least common multiple), 939 ex.
LCS, 7, 390–397, 413
LCS-L
ENGTH , 394
leading submatrix, 833, 839 ex.
leaf, 1176
least common ancestor, 584 pr.
least common multiple, 939 ex.
least-squares approximation, 835–839
leaving a vertex, 1169leaving variable, 867
L
EFT, 152
left child, 1178
left-child, right-sibling representation, 246,
249 ex.
LEFT-ROTATE , 313, 353 ex.
left rotation, 312left spine, 333 pr.
left subtree, 1178
Legendre symbol .
a
p/, 982 pr.
length
of a path, 1170
of a sequence, 1166of a spine, 333 pr.
of a string, 986, 1184
level
of a function, 573
of a tree, 1177
level function, 576lexicographically less than, 304 pr.
lexicographic sorting, 304 pr.
lg (binary logarithm), 56lg
/ETX(iterated logarithm function), 58–59
lgk(exponentiation of logarithms), 56
lg lg (composition of logarithms), 56
LIFO (last-in, ﬁrst-out), 232Index 1271
see also stack
light edge, 626
linear constraint, 846
linear dependence, 1223linear equality, 845
linear equations
solving modular, 946–950solving systems of, 813–827
solving tridiagonal systems of, 840 pr.
linear function, 26, 845
linear independence, 1223
linear inequality, 846
linear-inequality feasibility problem, 894 pr.linearity of expectation, 1198
and indicator random variables, 119
linearity of summations, 1146
linear order, 1165
linear permutation, 1229 pr.
linear probing, 272linear programming, 7, 843–897
algorithms for, 850
applications of, 849duality in, 879–886ellipsoid algorithm for, 850, 897
ﬁnding an initial solution in, 886–891
fundamental theorem of, 892interior-point methods for, 850, 897
Karmarkar’s algorithm for, 897
and maximum ﬂow, 860–861
and minimum-cost circulation, 896 pr.
and minimum-cost ﬂow, 861–862
and minimum-cost multicommodity ﬂow,
864 ex.
and multicommodity ﬂow, 862–863
simplex algorithm for, 864–879, 896and single-pair shortest path, 859–860
and single-source shortest paths, 664–670,
863 ex.
slack form for, 854–857
standard form for, 850–854
see also integer linear programming, 0-1
integer programming
linear-programming relaxation, 1125
linear search, 22 ex.linear speedup, 780
line segment, 1015
comparable, 1022determining turn of, 1017determining whether any intersect,
1021–1029, 1047
determining whether two intersect,
1017–1019
link
of binomial trees, 527 pr.
of Fibonacci-heap roots, 513of trees in a disjoint-set forest, 570–571
L
INK, 571
linked list, 236–241
compact, 245 ex., 250 pr.
deletion from, 238
to implement disjoint sets, 564–568insertion into, 237–238
neighbor list, 750
searching, 237, 268 ex.
self-organizing, 476 pr.
list,seelinked list
L
IST-DELETE , 238
LIST-DELETE0, 238
LIST-INSERT , 238
LIST-INSERT0, 240
LIST-SEARCH , 237
LIST-SEARCH0, 239
literal, 1082
little-oh notation, 50–51, 64
little-omega notation, 51
Lm-distance, 1044 ex.
ln (natural logarithm), 56load factor
of a dynamic table, 463
of a hash table, 258
load instruction, 23
local variable, 21
logarithm function (log), 56–57
discrete, 955
iterated (lg
/ETX), 58–59
logical parallelism, 777logic gate, 1070
longest common subsequence, 7, 390–397, 413
longest palindrome subsequence, 405 pr.LONGEST-PATH, 1060 ex.
LONGEST-PATH-LENGTH, 1060 ex.
longest simple cycle, 1101 ex.
longest simple path, 1048
in an unweighted graph, 382
in a weighted directed acyclic graph, 404 pr.
L
OOKUP -CHAIN , 3881272 Index
loop, in pseudocode, 20
parallel, 785–787
loop invariant, 18–19
for breadth-ﬁrst search, 595for building a heap, 157
for consolidating the root list of a Fibonacci
heap, 517
for determining the rank of an element in an
order-statistic tree, 342
for Dijkstra’s algorithm, 660andforloops, 19 n.
for the generic minimum-spanning-tree
method, 625
for the generic push-relabel algorithm, 743
for Graham’s scan, 1034
for heapsort, 160 ex.
for Horner’s rule, 41 pr.
for increasing a key in a heap, 166 ex.
initialization of, 19for insertion sort, 18
maintenance of, 19
for merging, 32for modular exponentiation, 957origin of, 42
for partitioning, 171
for Prim’s algorithm, 636for the Rabin-Karp algorithm, 993
for randomly permuting an array, 127,
128 ex.
for red-black tree insertion, 318
for the relabel-to-front algorithm, 755
for searching an interval tree, 352for the simplex algorithm, 872
for string-matching automata, 998, 1000
and termination, 19
low endpoint of an interval, 348
lower bounds
on approximations, 1140
asymptotic, 48
for average sorting, 207 pr.
on binomial coefﬁcients, 1186for comparting water jugs, 206 pr.
for convex hull, 1038 ex., 1047
for disjoint-set data structures, 585
for ﬁnding the minimum, 214
for ﬁnding the predecessor, 560
for length of an optimal traveling-salesman
tour, 1112–1115for median ﬁnding, 227
for merging, 208 pr.
for minimum-weight vertex cover,
1124–1126
for multithreaded computations, 780
and potential functions, 478
for priority-queue operations, 531
and recurrences, 67
for simultaneous minimum and maximum,
215 ex.
for size of an optimal vertex cover, 1110,
1135 pr.
for sorting, 191–194, 205 pr., 211, 531for streaks, 136–138, 142 ex.
on summations, 1152, 1154
lower median, 213
lower square root/NUL
#p
/SOH
, 546
lower-triangular matrix, 1219, 1222 ex.,
1225 ex.
low function, 537, 546
LU decomposition, 806 pr., 819–822
LU-D ECOMPOSITION , 821
LUP decomposition, 806 pr., 815
computation of, 822–825
of a diagonal matrix, 827 ex.
in matrix inversion, 828
and matrix multiplication, 832 ex.
of a permutation matrix, 827 ex.
use of, 815–819
LUP-D ECOMPOSITION , 824
LUP-S OLVE , 817
main memory, 484
MAKE-HEAP, 505
MAKE-SET, 561
disjoint-set-forest implementation of, 571
linked-list implementation of, 564
makespan, 1136 pr.
MAKE-TREE, 583 pr.
Manhattan distance, 225 pr., 1044 ex.
marked node, 508, 519–520
Markov’s inequality, 1201 ex.
master method for solving a recurrence, 93–97
master theorem, 94
proof of, 97–106
matched vertex, 732
matching
bipartite, 732, 763 pr.Index 1273
maximal, 1110, 1135 pr.
maximum, 1135 pr.
and maximum ﬂow, 732–736, 747 ex.
perfect, 735 ex.of strings, 985–1013
weighted bipartite, 530
matric matroid, 437matrix, 1217–1229
addition of, 1220
adjacency, 591
conjugate transpose of, 832 ex.
determinant of, 1224–1225
diagonal, 1218Hermitian, 832 ex.
identity, 1218
incidence, 448 pr., 593 ex.
inversion of, 806 pr., 827–831, 842
lower-triangular, 1219, 1222 ex., 1225 ex.
multiplication of, seematrix multiplication
negative of, 1220
permutation, 1220, 1222 ex.
predecessor, 685product of, with a vector, 785–787, 789–790,
792 ex.
pseudoinverse of, 837
scalar multiple of, 1220subtraction of, 1221
symmetric, 1220
symmetric positive-deﬁnite, 832–835, 842
Toeplitz, 921 pr.
transpose of, 797 ex., 1217
transpose of, multithreaded, 792 ex.tridiagonal, 1219
unit lower-triangular, 1219
unit upper-triangular, 1219
upper-triangular, 1219, 1225 ex.
Vandermonde, 902, 1226 pr.
matrix-chain multiplication, 370–378
M
AT RIX -CHAIN -MULTIPLY
MAT RIX -CHAIN -ORDER , 375
matrix multiplication, 75–83, 1221
for all-pairs shortest paths, 686–693,
706–707
boolean, 832 ex.
and computing the determinant, 832 ex.
divide-and-conquer method for, 76–83
and LUP decomposition, 832 ex.
and matrix inversion, 828–831, 842multithreaded algorithm for, 792–797,
806 pr.
Pan’s method for, 82 ex.
Strassen’s algorithm for, 79–83, 111–112
MAT RIX -MULTIPLY , 371
matrix-vector multiplication, multithreaded,
785–787, 792 ex.
with race, 789–790
matroid, 437–443, 448 pr., 450, 642
for task scheduling, 443–446
MAT-VEC, 785
MAT-VEC-MAIN-LOOP, 786
MAT-VEC-WRONG , 790
MAX-CNF satisﬁability, 1127 ex.
MAX-CUT problem, 1127 ex.
MAX-FLOW -BY-SCALING , 763 pr.
max-ﬂow min-cut theorem, 723
max-heap, 152
building, 156–159d-ary, 167 pr.
deletion from, 166 ex.
extracting the maximum key from, 163
in heapsort, 159–162
increasing a key in, 163–164
insertion into, 164
maximum key of, 163as a max-priority queue, 162–166
mergeable, 250 n., 481 n., 505 n.
M
AX-HEAPIFY , 154
MAX-HEAP-INSERT , 164
building a heap with, 166 pr.
max-heap property, 152
maintenance of, 154–156
maximal element, of a partially ordered set,
1165
maximal layers, 1045 pr.
maximal matching, 1110, 1135 pr.
maximal point, 1045 pr.
maximal subset, in a matroid, 438
maximization linear program, 846
and minimization linear programs, 852
maximum, 213
in binary search trees, 291
of a binomial distribution, 1207 ex.
in a bit vector with a superimposed binary
tree, 533
in a bit vector with a superimposed tree of
constant height, 5351274 Index
ﬁnding, 214–215
in heaps, 163
in order-statistic trees, 347 ex.
in proto van Emde Boas structures, 544 ex.in red-black trees, 311
in van Emde Boas trees, 550
M
AXIMUM , 162–163, 230
maximum bipartite matching, 732–736,
747 ex., 766
Hopcroft-Karp algorithm for, 763 pr.
maximum degree, in a Fibonacci heap, 509,
523–526
maximum ﬂow, 708–766
Edmonds-Karp algorithm for, 727–730
Ford-Fulkerson method for, 714–731, 765
as a linear program, 860–861and maximum bipartite matching, 732–736,
747 ex.
push-relabel algorithms for, 736–760, 765relabel-to-front algorithm for, 748–760
scaling algorithm for, 762 pr., 765
updating, 762 pr.
maximum matching, 1135 pr.
maximum spanning tree, 1137 pr.
maximum-subarray problem, 68–75, 111
max-priority queue, 162MAX-3-CNF satisﬁability, 1123–1124, 1139
M
AYBE -MST-A, 641 pr.
MAYBE -MST-B, 641 pr.
MAYBE -MST-C, 641 pr.
mean, seeexpected value
mean weight of a cycle, 680 pr.
median, 213–227
multithreaded algorithm for, 805 ex.
of sorted lists, 223 ex.of two sorted lists, 804 ex.
weighted, 225 pr.
median key, of a B-tree node, 493median-of-3 method, 188 pr.
member of a set (2), 1158
membership
in proto van Emde Boas structures, 540–541
in Van Emde Boas trees, 550
memoization, 365, 387–389
M
EMOIZED -CUT-ROD, 365
MEMOIZED -CUT-ROD-AUX, 366
MEMOIZED -MAT RIX -CHAIN , 388
memory, 484memory hierarchy, 24
MERGE ,3 1
mergeable heap, 481, 505
binomial heaps, 527 pr.
linked-list implementation of, 250 pr.
relaxed heaps, 530
running times of operations on, 506 ﬁg.2-3-4 heaps, 529 pr.
see also Fibonacci heap
mergeable max-heap, 250 n., 481 n., 505 n.mergeable min-heap, 250 n., 481 n., 505
M
ERGE -LISTS, 1129
merge sort, 12, 30–37
compared with insertion sort, 14 ex.
multithreaded algorithm for, 797–805, 812
use of insertion sort in, 39 pr.
MERGE -SORT,3 4
MERGE -SORT0, 797
merging
ofksorted lists, 166 ex.
lower bounds for, 208 pr.
multithreaded algorithm for, 798–801of two sorted arrays, 30
M
ILLER -RABIN , 970
Miller-Rabin primality test, 968–975, 983
MIN-GAP, 354 ex.
min-heap, 153
analyzed by potential method, 462 ex.
building, 156–159d-ary, 706 pr.
in Dijkstra’s algorithm, 662
in Huffman’s algorithm, 433
in Johnson’s algorithm, 704
mergeable, 250 n., 481 n., 505
as a min-priority queue, 165 ex.in Prim’s algorithm, 636
M
IN-HEAPIFY , 156 ex.
MIN-HEAP-INSERT , 165 ex.
min-heap ordering, 507
min-heap property, 153, 507
maintenance of, 156 ex.
in treaps, 333 pr.
vs. binary-search-tr ee property, 289 ex.
minimization linear program, 846
and maximization linear programs, 852
minimum, 213
in binary search trees, 291Index 1275
in a bit vector with a superimposed binary
tree, 533
in a bit vector with a superimposed tree of
constant height, 535
in B-trees, 497 ex.
in Fibonacci heaps, 511
ﬁnding, 214–215
off-line, 582 pr.
in order-statistic trees, 347 ex.
in proto van Emde Boas structures, 541–542in red-black trees, 311
in 2-3-4 heaps, 529 pr.
in van Emde Boas trees, 550
M
INIMUM , 162, 214, 230, 505
minimum-cost circulation, 896 pr.
minimum-cost ﬂow, 861–862
minimum-cost multicommodity ﬂow, 864 ex.
minimum-cost spanning tree, seeminimum
spanning tree
minimum cut, 721, 731 ex.
minimum degree, of a B-tree, 489
minimum mean-weight cycle, 680 pr.minimum node, of a Fibonacci heap, 508minimum path cover, 761 pr.
minimum spanning tree, 624–642
in approximation algorithm for
traveling-salesman problem, 1112
Bor˙uvka’s algorithm for, 641
on dynamic graphs, 637 ex.
generic method for, 625–630
Kruskal’s algorithm for, 631–633
Prim’s algorithm for, 634–636relation to matroids, 437, 439–440
second-best, 638 pr.
minimum-weight spanning tree, seeminimum
spanning tree
minimum-weight vertex cover, 1124–1127,
1139
minor of a matrix, 1224
min-priority queue, 162
in constructing Huffman codes, 431
in Dijkstra’s algorithm, 661
in Prim’s algorithm, 634, 636
miss, 449 pr.missing child, 1178
mod, 54, 928
modifying operation, 230
modular arithmetic, 54, 923 pr., 939–946modular equivalence, 54, 1165 ex.
modular exponentiation, 956
M
ODULAR -EXPONENTIATION , 957
modular linear equations, 946–950
MODULAR -LINEAR -EQUATION -SOLVER ,
949
modulo, 54, 928
Monge array, 110 pr.
monotone sequence, 168
monotonically decreasing, 53
monotonically increasing, 53
Monty Hall problem, 1195 ex.
move-to-front heuristic, 476 pr., 478
MST-K RUSKAL , 631
MST-P RIM, 634
MST-R EDUCE , 639 pr.
much-greater-than ( /GS), 574
much-less-than (/FS), 783
multicommodity ﬂow, 862–863
minimum-cost, 864 ex.
multicore computer, 772
multidimensional fast Fourier transform,
921 pr.
multigraph, 1172
converting to equivalent undirected graph,
593 ex.
multiple, 927
of an element modulo n, 946–950
least common, 939 ex.scalar, 1220
multiple assignment, 21
multiple sources and sinks, 712
multiplication
of complex numbers, 83 ex.
divide-and-conquer method for, 920 pr.of matrices, seematrix multiplication
of a matrix chain, 370–378
matrix-vector, multithreaded, 785–787,
789–790, 792 ex.
modulo n(/SOH
n), 940
of polynomials, 899
multiplication method, 263–264
multiplicative group modulo n, 941
multiplicative inverse, modulo n, 949
multiply instruction, 23
MULTIPOP , 453
multiprocessor, 772
MULTIPUSH , 456 ex.1276 Index
multiset, 1158 n.
multithreaded algorithm, 10, 772–812
for computing Fibonacci numbers, 774–780
for fast Fourier transform, 804 ex.Floyd-Warshall algorithm, 797 ex.
for LU decomposition, 806 pr.
for LUP decomposition, 806 pr.for matrix inversion, 806 pr.
for matrix multiplication, 792–797, 806 pr.
for matrix transpose, 792 ex., 797 ex.
for matrix-vector product, 785–787,
789–790, 792 ex.
for median, 805 ex.for merge sorting, 797–805, 812
for merging, 798–801
for order statistics, 805 ex.
for partitioning, 804 ex.
for preﬁx computation, 807 pr.
for quicksort, 811 pr.for reduction, 807 pr.
for a simple stencil calculation, 809 pr.
for solving systems of linear equations,
806 pr.
Strassen’s algorithm, 795–796
multithreaded composition, 784 ﬁg.
multithreaded computation, 777multithreaded scheduling, 781–783
mutually exclusive events, 1190
mutually independent events, 1193
N(set of natural numbers), 1158
naive algorithm, for string matching, 988–990
N
AIVE -STRING -MAT CHER , 988
natural cubic spline, 840 pr.
natural numbers ( N), 1158
keys interpreted as, 263
negative of a matrix, 1220
negative-weight cycle
and difference constraints, 667
and relaxation, 677 ex.
and shortest paths, 645, 653–654, 692 ex.,
700 ex.
negative-weight edges, 645–646
neighbor, 1172
neighborhood, 735 ex.
neighbor list, 750
nested parallelism, 776, 805 pr.nesting boxes, 678 pr.net ﬂow across a cut, 720
network
admissible, 749–750
ﬂow, seeﬂow network
residual, 715–719
for sorting, 811
N
EXT-TO-TOP, 1031
NIL,2 1
node, 1176
see also vertex
nonbasic variable, 855
nondeterministic multithreaded algorithm, 787
nondeterministic polynomial time, 1064 n.
see also NP
nonhamiltonian graph, 1061
noninstance, 1056 n.noninvertible matrix, 1223
nonnegativity constraint, 851, 853
nonoverlappable string pattern, 1002 ex.
nonsaturating push, 739, 745
nonsingular matrix, 1223
nontrivial power, 933 ex.
nontrivial square root of 1, modulo n, 956
no-path property, 650, 672
normal equation, 837
norm of a vector, 1222NOT function (:), 1071
not a set member (62), 1158
not equivalent (6/DC1), 54
NOT gate, 1070
NP (complexity class), 1049, 1064, 1066 ex.,
1105
NPC (complexity class), 1050, 1069
NP-complete, 1050, 1069
NP-completeness, 9–10, 1048–1105
of the circuit-satisﬁability problem,
1070–1077
of the clique problem, 1086–1089, 1105of determining whether a boolean formula is
a tautology, 1086 ex.
of the formula-satisﬁability problem,
1079–1081, 1105
of the graph-coloring problem, 1103 pr.
of the half 3-CNF satisﬁability problem,
1101 ex.
of the hamiltonian-cycle problem,
1091–1096, 1105
of the hamiltonian-path problem, 1101 ex.Index 1277
of the independent-set problem, 1101 pr.
of integer linear programming, 1101 ex.
of the longest-simple-cycle problem,
1101 ex.
proving, of a language, 1078–1079
of scheduling with proﬁts and deadlines,
1104 pr.
of the set-covering problem, 1122 ex.
of the set-partition problem, 1101 ex.
of the subgraph-isomorphism problem,
1100 ex.
of the subset-sum problem, 1097–1100
of the 3-CNF-satisﬁability problem,
1082–1085, 1105
of the traveling-salesman problem,
1096–1097
of the vertex-cover problem, 1089–1091,
1105
of 0-1 integer programming, 1100 ex.
NP-hard, 1069
n-set, 1161
n-tuple, 1162
null event, 1190null tree, 1178
null vector, 1224
number-ﬁeld sieve, 984numerical stability, 813, 815, 842
n-vector, 1218
o-notation, 50–51, 64
O-notation, 45 ﬁg., 47–48, 64
O
0-notation, 62 pr.
eO-notation, 62 pr.
object, 21
allocation and freeing of, 243–244
array implementation of, 241–246
passing as parameter, 21
objective function, 664, 847, 851
objective value, 847, 851
oblivious compare-exchange algorithm, 208 pr.
occurrence of a pattern, 985
OFF-LINE-MINIMUM , 583 pr.
off-line problem
caching, 449 pr.least common ancestors, 584 pr.
minimum, 582 pr.
Omega-notation, 45 ﬁg., 48–49, 64
1-approximation algorithm, 1107one-pass method, 585
one-to-one correspondence, 1167
one-to-one function, 1167
on-line convex-hull problem, 1039 ex.
on-line hiring problem, 139–141
O
N-LINE-MAXIMUM , 140
on-line multithreaded scheduler, 781
ON-SEGMENT , 1018
onto function, 1167
open-address hash table, 269–277
with double hashing, 272–274, 277 ex.
with linear probing, 272
with quadratic probing, 272, 283 pr.
open interval, 348
OpenMP, 774
optimal binary search tree, 397–404, 413
OPTIMAL -BST, 402
optimal objective value, 851
optimal solution, 851optimal subset, of a matroid, 439
optimal substructure
of activity selection, 416
of binary search trees, 399–400
in dynamic programming, 379–384
of the fractional knapsack problem, 426
in greedy algorithms, 425of Huffman codes, 435
of longest common subsequences, 392–393
of matrix-chain multiplication, 373of rod-cutting, 362
of shortest paths, 644–645, 687, 693–694
of unweighted shortest paths, 382
of weighted matroids, 442
of the 0-1 knapsack problem, 426
optimal vertex cover, 1108optimization problem, 359, 1050, 1054
approximation algorithms for, 10,
1106–1140
and decision problems, 1051
OR function (_), 697, 1071
order
of a group, 945
linear, 1165
partial, 1165total, 1165
ordered pair, 1161
ordered tree, 1177
order of growth, 281278 Index
order statistics, 213–227
dynamic, 339–345
multithreaded algorithm for, 805 ex.
order-statistic tree, 339–345
querying, 347 ex.
OR gate, 1070
origin, 1015
or, in pseudocode, 22
orthonormal, 842
OS-K EY-RANK, 344 ex.
OS-R ANK, 342
OS-S ELECT , 341
out-degree, 1169outer product, 1222
output
of an algorithm, 5of a combinational circuit, 1071
of a logic gate, 1070
overdetermined system of linear equations, 814
overﬂow
of a queue, 235
of a stack, 233
overﬂowing vertex, 736
discharge of, 751
overlapping intervals, 348
ﬁnding all, 354 ex.
point of maximum overlap, 354 pr.
overlapping rectangles, 354 ex.
overlapping subproblems, 384–386
overlapping-sufﬁx lemma, 987
P (complexity class), 1049, 1055, 1059,
1061 ex., 1105
package wrapping, 1037, 1047
page on a disk, 486, 499 ex., 502 pr.
pair, ordered, 1161
pairwise disjoint sets, 1161
pairwise independence, 1193
pairwise relatively prime, 931
palindrome, 405 pr.
Pan’s method for matrix multiplication, 82 ex.parallel algorithm, 10, 772
see also multithreaded algorithm
parallel computer, 772
ideal, 779
parallel for , in pseudocode, 785–786
parallelism
logical, 777of a multithreaded computation, 780
nested, 776
of a randomized multithreaded algorithm,
811 pr.
parallel loop, 785–787, 805 pr.
parallel-machine-scheduling problem, 1136 pr.
parallel preﬁx, 807 pr.parallel random-access machine, 811
parallel slackness, 781
rule of thumb, 783
parallel, strands being logically in, 778
parameter, 21
costs of passing, 107 pr.
parent
in a breadth-ﬁrst tree, 594
in a multithreaded computation, 776in a rooted tree, 1176
P
ARENT , 152
parenthesis structure of depth-ﬁrst search, 606
parenthesis theorem, 606
parenthesization of a matrix-chain product, 370
parse tree, 1082
partially ordered set, 1165partial order, 1165
P
ARTITION , 171
PARTITION0, 186 pr.
partition function, 361 n.
partitioning, 171–173
around median of 3 elements, 185 ex.Hoare’s method for, 185 pr.
multithreaded algorithm for, 804 ex.
randomized, 179
partition of a set, 1161, 1164
Pascal’s triangle, 1188 ex.
path, 1170
augmenting, 719–720, 763 pr.
critical, 657
ﬁnd, 569hamiltonian, 1066 ex.
longest, 382, 1048
shortest, seeshortest paths
simple, 1170
weight of, 643
PATH, 1051, 1058path compression, 569
path cover, 761 pr.
path length, of a tree, 304 pr., 1180 ex.
path-relaxation property, 650, 673Index 1279
pattern, in string matching, 985
nonoverlappable, 1002 ex.
pattern matching, seestring matching
penalty, 444perfect hashing, 277–282, 285
perfect linear speedup, 780
perfect matching, 735 ex.
permutation, 1167
bit-reversal, 472 pr., 918
Josephus, 355 pr.k-permutation, 126, 1184
linear, 1229 pr.
in place, 126random, 124–128
of a set, 1184
uniform random, 116, 125
permutation matrix, 1220, 1222 ex., 1226 ex.
LUP decomposition of, 827 ex.
P
ERMUTE -BY-CYCLIC , 129 ex.
PERMUTE -BY-SORTING , 125
PERMUTE -WITH-ALL, 129 ex.
PERMUTE -WITHOUT -IDENTITY , 128 ex.
persistent data structure, 331 pr., 482
PERSISTENT -TREE-INSERT , 331 pr.
PERT chart, 657, 657 ex.
P-F IB, 776
phase, of the relabel-t o-front algorithm, 758
phi function ( /RS.n/ ), 943
PISANO -DELETE , 526 pr.
pivot
in linear programming, 867, 869–870,
878 ex.
in LU decomposition, 821
in quicksort, 171
PIVOT , 869
platter, 485
P-M AT RIX -MULTIPLY -RECURSIVE , 794
P-M ERGE , 800
P-M ERGE -SORT, 803
pointer, 21
array implementation of, 241–246
trailing, 295
point-value representation, 901
polar angle, 1020 ex.Pollard’s rho heuristic, 976–980, 980 ex., 984
P
OLLARD -RHO, 976
polygon, 1020 ex.
kernel of, 1038 ex.star-shaped, 1038 ex.
polylogarithmically bounded, 57
polynomial, 55, 898
addition of, 898
asymptotic behavior of, 61 pr.
coefﬁcient representation of, 900
derivatives of, 922 pr.evaluation of, 41 pr., 900, 905 ex., 923 pr.
interpolation by, 901, 906 ex.
multiplication of, 899, 903–905, 920 pr.point-value representation of, 901
polynomial-growth condition, 113
polynomially bounded, 55
polynomially related, 1056
polynomial-time acceptance, 1058
polynomial-time algorithm, 927, 1048polynomial-time approximation scheme, 1107
for maximum clique, 1134 pr.
polynomial-time computability, 1056polynomial-time decision, 1059
polynomial-time reducibility ( /DC4
P), 1067,
1077 ex.
polynomial-time solvability, 1055
polynomial-time veriﬁcation, 1061–1066
POP, 233, 452
pop from a run-time stack, 188 pr.positional tree, 1178
positive-deﬁnite matrix, 1225
post-ofﬁce location problem, 225 pr.postorder tree walk, 287
potential function, 459
for lower bounds, 478
potential method, 459–463
for binary counters, 461–462
for disjoint-set data structures, 575–581,
582 ex.
for dynamic tables, 466–471
for Fibonacci heaps, 509–512, 517–518,
520–522
for the generic push-relabel algorithm, 746
for min-heaps, 462 ex.for restructuring red-black trees, 474 pr.
for self-organizing lists with move-to-front,
476 pr.
for stack operations, 460–461
potential, of a data structure, 459
power
of an element, modulo n, 954–9581280 Index
kth, 933 ex.
nontrivial, 933 ex.
power series, 108 pr.
power set, 1161Prfg(probability distribution), 1190
PRAM, 811
predecessor
in binary search trees, 291–292
in a bit vector with a superimposed binary
tree, 534
in a bit vector with a superimposed tree of
constant height, 535
in breadth-ﬁrst trees, 594in B-trees, 497 ex.
in linked lists, 236
in order-statistic trees, 347 ex.
in proto van Emde Boas structures, 544 ex.
in red-black trees, 311
in shortest-paths trees, 647in Van Emde Boas trees, 551–552
P
REDECESSOR , 230
predecessor matrix, 685predecessor subgraph
in all-pairs shortest paths, 685
in breadth-ﬁrst search, 600
in depth-ﬁrst search, 603in single-source shortest paths, 647
predecessor-subgraph property, 650, 676
preemption, 447 pr.preﬁx
of a sequence, 392
of a string ( <), 986
preﬁx code, 429
preﬁx computation, 807 pr.
preﬁx function, 1003–1004
preﬁx-function iteration lemma, 1007
preﬂow, 736, 765
preimage of a matrix, 1228 pr.preorder, total, 1165
preorder tree walk, 287
presorting, 1043Prim’s algorithm, 634–636, 642
with an adjacency matrix, 637 ex.
in approximation algorithm for
traveling-salesman problem, 1112
implemented with a Fibonacci heap, 636
implemented with a min-heap, 636with integer edge weights, 637 ex.similarity to Dijkstra’s algorithm, 634, 662
for sparse graphs, 638 pr.
primality testing, 965–975, 983
Miller-Rabin test, 968–975, 983pseudoprimality testing, 966–968
primal linear program, 880
primary clustering, 272primary memory, 484
prime distribution function, 965
prime number, 928
density of, 965–966
prime number theorem, 965
primitive root of Z
/ETX
n, 955
principal root of unity, 907
principle of inclusion and exclusion, 1163 ex.
PRINT -ALL-PAIRS -SHORTEST -PATH, 685
PRINT -CUT-ROD-SOLUTION , 369
PRINT -INTERSECTING -SEGMENTS , 1028 ex.
PRINT -LCS, 395
PRINT -OPTIMAL -PARENS , 377
PRINT -PAT H, 601
PRINT -SET, 572 ex.
priority queue, 162–166
in constructing Huffman codes, 431
in Dijkstra’s algorithm, 661
heap implementation of, 162–166lower bounds for, 531
max-priority queue, 162
min-priority queue, 162, 165 ex.
with monotone extractions, 168
in Prim’s algorithm, 634, 636
proto van Emde Boas structure
implementation of, 538–545
van Emde Boas tree implementation of,
531–560
see also binary search tree, binomial heap,
Fibonacci heap
probabilistically checkable proof, 1105, 1140probabilistic analysis, 115–116, 130–142
of approximation algorithm for
MAX-3-CNF satisﬁability, 1124
and average inputs, 28
of average node depth in a randomly built
binary search tree, 304 pr.
of balls and bins, 133–134
of birthday paradox, 130–133
of bucket sort, 201–204, 204 ex.
of collisions, 261 ex., 282 ex.Index 1281
of convex hull over a sparse-hulled
distribution, 1046 pr.
of ﬁle comparison, 995 ex.
of fuzzy sorting of intervals, 189 pr.of hashing with chaining, 258–260
of height of a randomly built binary search
tree, 299–303
of hiring problem, 120–121, 139–141
of insertion into a binary search tree with
equal keys, 303 pr.
of longest-probe bound for hashing, 282 pr.
of lower bound for sorting, 205 pr.
of Miller-Rabin primality test, 971–975and multithreaded algorithms, 811 pr.
of on-line hiring problem, 139–141
of open-address hashing, 274–276, 277 ex.of partitioning, 179 ex., 185 ex., 187–188 pr.
of perfect hashing, 279–282
of Pollard’s rho heuristic, 977–980
of probabilistic counting, 143 pr.
of quicksort, 181–184, 187–188 pr., 303 ex.
of Rabin-Karp algorithm, 994and randomized algorithms, 123–124of randomized selection, 217–219, 226 pr.
of searching a compact list, 250 pr.
of slot-size bound for chaining, 283 pr.of sorting points by distance from origin,
204 ex.
of streaks, 135–139
of universal hashing, 265–268
probabilistic counting, 143 pr.
probability, 1189–1196probability density function, 1196
probability distribution, 1190
probability distribution function, 204 ex.probe sequence, 270
probing, 270, 282 pr.
see also linear probing, quadratic probing,
double hashing
problem
abstract, 1054
computational, 5–6
concrete, 1055
decision, 1051, 1054intractable, 1048
optimization, 359, 1050, 1054
solution to, 6, 1054–1055
tractable, 1048procedure, 6, 16–17
product .Q/, 1148
Cartesian, 1162
cross, 1016
inner, 1222
of matrices, 1221, 1226 ex.
outer, 1222of polynomials, 899
rule of, 1184
scalar ﬂow, 714 ex.
professional wrestler, 602 ex.
program counter, 1073
programming, seedynamic programming,
linear programming
proper ancestor, 1176
proper descendant, 1176proper subgroup, 944
proper subset (/SUB), 1159
proto van Emde Boas structure, 538–545
cluster in, 538
compared with van Emde Boas trees, 547
deletion from, 544
insertion into, 544
maximum in, 544 ex.
membership in, 540–541
minimum in, 541–542predecessor in, 544 ex.
successor in, 543–544
summary in, 540
P
ROTO -VEB-I NSERT , 544
PROTO -VEB-M EMBER , 541
PROTO -VEB-M INIMUM , 542
proto-vEB structure, seeproto van Emde Boas
structure
PROTO -VEB-S UCCESSOR , 543
prune-and-search method, 1030
pruning a Fibonacci heap, 529 pr.
P-S CAN-1, 808 pr.
P-S CAN-2, 808 pr.
P-S CAN-3, 809 pr.
P-S CAN-DOWN , 809 pr.
P-S CAN-UP, 809 pr.
pseudocode, 16, 20–22
pseudoinverse, 837
pseudoprime, 966–968
PSEUDOPRIME , 967
pseudorandom-number generator, 117
P-S QUARE -MAT RIX -MULTIPLY , 7931282 Index
P-T RANSPOSE , 792 ex.
public key, 959, 962
public-key cryptosystem, 958–965, 983
PUSH
push-relabel operation, 739
stack operation, 233, 452
push onto a run-time stack, 188 pr.
push operation (in push-relabel algorithms),
738–739
nonsaturating, 739, 745saturating, 739, 745
push-relabel algorithm, 736–760, 765
basic operations in, 738–740by discharging an overﬂowing vertex of
maximum height, 760 ex.
to ﬁnd a maximum bipartite matching,
747 ex.
gap heuristic for, 760 ex., 766
generic algorithm, 740–748with a queue of overﬂowing vertices, 759 ex.
relabel-to-front algorithm, 748–760
quadratic function, 27
quadratic probing, 272, 283 pr.
quadratic residue, 982 pr.
quantile, 223 ex.
query, 230
queue, 232, 234–235
in breadth-ﬁrst search, 595implemented by stacks, 236 ex.
linked-list implementation of, 240 ex.
priority, seepriority queue
in push-relabel algorithms, 759 ex.
quicksort, 170–190
analysis of, 174–185average-case analysis of, 181–184
compared with insertion sort, 178 ex.
compared with radix sort, 199
with equal element values, 186 pr.
good worst-case implementation of, 223 ex.
“killer adversary” for, 190with median-of-3 method, 188 pr.
multithreaded algorithm for, 811 pr.
randomized version of, 179–180, 187 pr.stack depth of, 188 pr.
tail-recursive version of, 188 pr.
use of insertion sort in, 185 ex.worst-case analysis of, 180–181Q
UICKSORT , 171
QUICKSORT0, 186 pr.
quotient, 928
R(set of real numbers), 1158
Rabin-Karp algorithm, 990–995, 1013
RABIN -KARP-MAT CHE R , 993
race, 787–790
RACE-EXAMPLE , 788
radix sort, 197–200
compared with quicksort, 199
RADIX -SORT, 198
radix tree, 304 pr.RAM, 23–24
R
ANDOM , 117
random-access machine, 23–24
parallel, 811
randomized algorithm, 116–117, 122–130
and average inputs, 28comparison sort, 205 pr.
for fuzzy sorting of intervals, 189 pr.
for hiring problem, 123–124
for insertion into a bi nary search tree with
equal keys, 303 pr.
for MAX-3-CNF satisﬁability, 1123–1124,
1139
Miller-Rabin primality test, 968–975, 983
multithreaded, 811 pr.
for partitioning, 179, 185 ex., 187–188 pr.
for permuting an array, 124–128
Pollard’s rho heuristic, 976–980, 980 ex.,
984
and probabilistic analysis, 123–124
quicksort, 179–180, 185 ex., 187–188 pr.
randomized rounding, 1139
for searching a compact list, 250 pr.
for selection, 215–220
universal hashing, 265–268worst-case performance of, 180 ex.
R
ANDOMIZED -HIRE-ASSISTANT , 124
RANDOMIZED -PARTITION , 179
RANDOMIZED -QUICKSORT , 179, 303 ex.
relation to randomly built binary search
trees, 304 pr.
randomized rounding, 1139
RANDOMIZED -SELECT , 216
RANDOMIZE -IN-PLACE , 126Index 1283
randomly built binary search tree, 299–303,
304 pr.
random-number generator, 117
random permutation, 124–128
uniform, 116, 125
RANDOM -SAMPLE , 130 ex.
random sampling, 129 ex., 179
RANDOM -SEARCH , 143 pr.
random variable, 1196–1201
indicator, seeindicator random variable
range, 1167
of a matrix, 1228 pr.
rank
column, 1223
full, 1223
of a matrix, 1223, 1226 ex.of a node in a disjoint-set forest, 569, 575,
581 ex.
of a number in an ordered set, 300, 339in order-statistic trees, 341–343, 344–345 ex.
row, 1223
rate of growth, 28
ray, 1021 ex.
RB-D
ELETE , 324
RB-D ELETE -FIXUP , 326
RB-E NUMERATE , 348 ex.
RB-I NSERT , 315
RB-I NSERT -FIXUP , 316
RB-J OIN, 332 pr.
RB-T RANSPLANT , 323
reachability in a graph ( ;), 1170
real numbers ( R), 1158
reconstructing an optimal solution, in dynamic
programming, 387
record, 147rectangle, 354 ex.
recurrence, 34, 65–67, 83–113
solution by Akra-Bazzi method, 112–113
solution by master method, 93–97
solution by recursion-tree method, 88–93
solution by substitution method, 83–88
recurrence equation, seerecurrence
recursion, 30
recursion tree, 37, 88–93
in proof of master theorem, 98–100
and the substitution method, 91–92
R
ECURSIVE -ACTIVITY -SELECTOR , 419
recursive case, 65RECURSIVE -FFT,911
RECURSIVE -MAT RIX -CHAIN , 385
red-black tree, 308–338
augmentation of, 346–347compared with B-trees, 484, 490
deletion from, 323–330
in determining whether any line segments
intersect, 1024
for enumerating keys in a range, 348 ex.
height of, 309
insertion into, 315–323
joining of, 332 pr.
maximum key of, 311minimum key of, 311
predecessor in, 311
properties of, 308–312relaxed, 311 ex.
restructuring, 474 pr.
rotation in, 312–314searching in, 311
successor in, 311
see also interval tree, order-statistic tree
R
EDUCE , 807 pr.
reduced-space van Emde Boas tree, 557 pr.
reducibility, 1067–1068
reduction algorithm, 1052, 1067
reduction function, 1067
reduction, of an array, 807 pr.
reﬂexive relation, 1163
reﬂexivity of asymptotic notation, 51
region, feasible, 847
regularity condition, 95rejection
by an algorithm, 1058
by a ﬁnite automaton, 996
R
ELABEL , 740
relabeled vertex, 740
relabel operation, in push-relabel algorithms,
740, 745
RELABEL -TO-FRONT , 755
relabel-to-front algorithm, 748–760
phase of, 758
relation, 1163–1166
relatively prime, 931
RELAX , 649
relaxation
of an edge, 648–650
linear programming, 11251284 Index
relaxed heap, 530
relaxed red-black tree, 311 ex.
release time, 447 pr.
remainder, 54, 928remainder instruction, 23
repeated squaring
for all-pairs shortest paths, 689–691for raising a number to a power, 956
repeat , in pseudocode, 20
repetition factor, of a string, 1012 pr.
R
EPETITION -MAT CHER , 1013 pr.
representative of a set, 561
RESET , 459 ex.
residual capacity, 716, 719
residual edge, 716
residual network, 715–719residue, 54, 928, 982 pr.
respecting a set of edges, 626
return edge, 779
return , in pseudocode, 22
return instruction, 23
reweighting
in all-pairs shortest paths, 700–702
in single-source shortest paths, 679 pr.
rho heuristic, 976–980, 980 ex., 984
/SUB.n/-approximation algorithm, 1106, 1123
R
IGHT , 152
right child, 1178
right-conversion, 314 ex.
right horizontal ray, 1021 ex.
RIGHT -ROTATE , 313
right rotation, 312right spine, 333 pr.
right subtree, 1178
rod-cutting, 360–370, 390 ex.
root
of a tree, 1176
of unity, 906–907
ofZ
/ETX
n, 955
rooted tree, 1176
representation of, 246–249
root list, of a Fibonacci heap, 509
rotation
cyclic, 1012 ex.in a red-black tree, 312–314
rotational sweep, 1030–1038
rounding, 1126
randomized, 1139row-major order, 394
row rank, 1223
row vector, 1218
RSA public-key cryptosystem, 958–965, 983
RS-vEB tree, 557 pr.
rule of product, 1184
rule of sum, 1183running time, 25
average-case, 28, 116
best-case, 29 ex., 49
expected, 28, 117
of a graph algorithm, 588
and multithreaded computation, 779–780order of growth, 28
rate of growth, 28
worst-case, 27, 49
sabermetrics, 412 n.
safe edge, 626
S
AME -COMPONENT , 563
sample space, 1189
sampling, 129 ex., 179
SAT, 1079
satellite data, 147, 229
satisﬁability, 1072, 1079–1081, 1105,
1123–1124, 1127 ex., 1139
satisﬁable formula, 1049, 1079
satisfying assignment, 1072, 1079
saturated edge, 739saturating push, 739, 745
scalar ﬂow product, 714 ex.
scalar multiple, 1220scaling
in maximum ﬂow, 762 pr., 765
in single-source shortest paths, 679 pr.
scan, 807 pr.
S
CAN, 807 pr.
scapegoat tree, 338schedule, 444, 1136 pr.
event-point, 1023
scheduler, for multithreaded computations,
777, 781–783, 812
centralized, 782
greedy, 782
work-stealing algorithm for, 812
scheduling, 443–446, 447 pr., 450, 1104 pr.,
1136 pr.
Schur complement, 820, 834Index 1285
Schur complement lemma, 834
SCRAMBLE -SEARCH , 143 pr.
seam carving, 409 pr., 413
SEARCH , 230
searching, 22 ex.
binary search, 39 ex., 799–800
in binary search trees, 289–291in B-trees, 491–492
in chained hash tables, 258
in compact lists, 250 pr.
in direct-address tables, 254
for an exact interval, 354 ex.
in interval trees, 350–353linear search, 22 ex.
in linked lists, 237
in open-address hash tables, 270–271in proto van Emde Boas structures, 540–541
in red-black trees, 311
in an unsorted array, 143 pr.
in Van Emde Boas trees, 550
search tree, seebalanced search tree, binary
search tree, B-tree, exponential searchtree, interval tree, optimal binary search
tree, order-statistic tree, red-black tree,
splay tree, 2-3 tree, 2-3-4 tree
secondary clustering, 272secondary hash table, 278
secondary storage
search tree for, 484–504stacks on, 502 pr.
second-best minimum spanning tree, 638 pr.
secret key, 959, 962
segment, seedirected segment, line segment
S
EGMENTS -INTERSECT , 1018
SELECT , 220
selection, 213
of activities, 415–422, 450
and comparison sorts, 222
in expected linear time, 215–220
multithreaded, 805 ex.
in order-statistic trees, 340–341in worst-case linear time, 220–224
selection sort, 29 ex.
selector vertex, 1093
self-loop, 1168
self-organizing list, 476 pr., 478
semiconnected graph, 621 ex.sentinel, 31, 238–240, 309sequence (hi)
bitonic, 682 pr.
ﬁnite, 1166
inﬁnite, 1166
inversion in, 41 pr., 122 ex., 345 ex.
probe, 270
sequential consistency, 779, 812
serial algorithm versus parallel algorithm, 772
serialization, of a multithreaded algorithm,
774, 776
series, 108 pr., 1146–1148
strands being logically in, 778
set (fg), 1158–1163
cardinality (jj), 1161
convex, 714 ex.
difference (/NUL), 1159
independent, 1101 pr.
intersection (\), 1159
member (2), 1158
not a member (62), 1158
union ([), 1159
set-covering problem, 1117–1122, 1139
weighted, 1135 pr.
set-partition problem, 1101 ex.
shadow of a point, 1038 ex.
shared memory, 772
Shell’s sort, 42
shift, in string matching, 985
shift instruction, 24short-circuiting operator, 22
SHORTEST-PATH, 1050
shortest paths, 7, 643–707
all-pairs, 644, 684–707
Bellman-Ford algorithm for, 651–655
with bitonic paths, 682 pr.
and breadth-ﬁrst search, 597–600, 644
convergence property of, 650, 672–673
and difference constraints, 664–670Dijkstra’s algorithm for, 658–664
in a directed acyclic graph, 655–658
in/SI-dense graphs, 706 pr.
estimate of, 648
Floyd-Warshall algorithm for, 693–697,
700 ex., 706
Gabow’s scaling algorithm for, 679 pr.
Johnson’s algorithm for, 700–706
as a linear program, 859–860
and longest paths, 10481286 Index
by matrix multiplication, 686–693, 706–707
and negative-weight cycles, 645, 653–654,
692 ex., 700 ex.
with negative-weight edges, 645–646no-path property of, 650, 672
optimal substructure of, 644–645, 687,
693–694
path-relaxation property of, 650, 673
predecessor-subgraph property of, 650, 676
problem variants, 644and relaxation, 648–650
by repeated squaring, 689–691
single-destination, 644single-pair, 381, 644
single-source, 643–683
tree of, 647–648, 673–676
triangle inequality of, 650, 671
in an unweighted graph, 381, 597
upper-bound property of, 650, 671–672
in a weighted graph, 643
sibling, 1176
side of a polygon, 1020 ex.
signature, 960simple cycle, 1170
simple graph, 1170
simple path, 1170
longest, 382, 1048
simple polygon, 1020 ex.
simple stencil calculation, 809 pr.
simple uniform hashing, 259
simplex, 848
S
IMPLEX , 871
simplex algorithm, 848, 864–879, 896–897
single-destination shortest paths, 644
single-pair shortest path, 381, 644
as a linear program, 859–860
single-source shortest paths, 643–683
Bellman-Ford algorithm for, 651–655with bitonic paths, 682 pr.
and difference constraints, 664–670
Dijkstra’s algorithm for, 658–664
in a directed acyclic graph, 655–658
in/SI-dense graphs, 706 pr.
Gabow’s scaling algorithm for, 679 pr.
as a linear program, 863 ex.
and longest paths, 1048
singleton, 1161
singly connected graph, 612 ex.singly linked list, 236
see also linked list
singular matrix, 1223
singular value decomposition, 842
sink vertex, 593 ex., 709, 712
size
of an algorithm’s input, 25, 926–927,
1055–1057
of a binomial tree, 527 pr.
of a boolean combinational circuit, 1072of a clique, 1086
of a set, 1161
of a subtree in a Fibonacci heap, 524of a vertex cover, 1089, 1108
skip list, 338
slack, 855slack form, 846, 854–857
uniqueness of, 876
slackness
complementary, 894 pr.
parallel, 781
slack variable, 855slot
of a direct-access table, 254
of a hash table, 256
S
LOW -ALL-PAIRS -SHORTEST -PATHS , 689
smoothed analysis, 897
?Socrates, 790
solution
to an abstract problem, 1054
basic, 866
to a computational problem, 6
to a concrete problem, 1055
feasible, 665, 846, 851
infeasible, 851optimal, 851
to a system of linear equations, 814
sorted linked list, 236
see also linked list
sorting, 5, 16–20, 30–37, 147–212, 797–805
bubblesort, 40 pr.
bucket sort, 200–204
columnsort, 208 pr.
comparison sort, 191counting sort, 194–197
fuzzy, 189 pr.
heapsort, 151–169insertion sort, 12, 16–20Index 1287
k-sorting, 207 pr.
lexicographic, 304 pr.
in linear time, 194–204, 206 pr.
lower bounds for, 191–194, 211, 531merge sort, 12, 30–37, 797–805
by oblivious compare-exchange algorithms,
208 pr.
in place, 17, 148, 206 pr.
of points by polar angle, 1020 ex.
probabilistic lower bound for, 205 pr.quicksort, 170–190
radix sort, 197–200
selection sort, 29 ex.Shell’s sort, 42
stable, 196
table of running times, 149
topological, 8, 612–615, 623
using a binary search tree, 299 ex.
with variable-length items, 206 pr.
0-1 sorting lemma, 208 pr.
sorting network, 811
source vertex, 594, 644, 709, 712
span law, 780spanning tree, 439, 624
bottleneck, 640 pr.
maximum, 1137 pr.veriﬁcation of, 642
see also minimum spanning tree
span, of a multithreaded computation, 779
sparse graph, 589
all-pairs shortest paths for, 700–705
and Prim’s algorithm, 638 pr.
sparse-hulled distribution, 1046 pr.
spawn , in pseudocode, 776–777
spawn edge, 778speedup, 780
of a randomized multithreaded algorithm,
811 pr.
spindle, 485
spine
of a string-matching automaton, 997 ﬁg.
of a treap, 333 pr.
splay tree, 338, 482
spline, 840 pr.splitting
of B-tree nodes, 493–495
of 2-3-4 trees, 503 pr.
splitting summations, 1152–1154spurious hit, 991
square matrix, 1218
S
QUARE -MAT RIX -MULTIPLY , 75, 689
SQUARE -MAT RIX -MULTIPLY -RECURSIVE ,
77
square of a directed graph, 593 ex.
square root, modulo a prime, 982 pr.squaring, repeated
for all-pairs shortest paths, 689–691
for raising a number to a power, 956
stability
numerical, 813, 815, 842
of sorting algorithms, 196, 200 ex.
stack, 232–233
in Graham’s scan, 1030
implemented by queues, 236 ex.linked-list implementation of, 240 ex.
operations analyzed by accounting method,
457–458
operations analyzed by aggregate analysis,
452–454
operations analyzed by potential method,
460–461
for procedure execution, 188 pr.
on secondary storage, 502 pr.
S
TACK -EMPTY , 233
standard deviation, 1200
standard encoding ( hi), 1057
standard form, 846, 850–854star-shaped polygon, 1038 ex.
start state, 995
start time, 415
state of a ﬁnite automaton, 995
static graph, 562 n.
static set of keys, 277
static threading, 773
stencil, 809 pr.
stencil calculation, 809 pr.Stirling’s approximation, 57
storage management, 151, 243–244, 245 ex.,
261 ex.
store instruction, 23
straddle, 1017
strand, 777
ﬁnal, 779
independent, 789
initial, 779
logically in parallel, 7781288 Index
logically in series, 778
Strassen’s algorithm, 79–83, 111–112
multithreaded, 795–796
streaks, 135–139strictly decreasing, 53
strictly increasing, 53
string, 985, 1184
string matching, 985–1013
based on repetition factors, 1012 pr.
by ﬁnite automata, 995–1002with gap characters, 989 ex., 1002 ex.
Knuth-Morris-Pratt algorithm for,
1002–1013
naive algorithm for, 988–990
Rabin-Karp algorithm for, 990–995, 1013
string-matching automaton, 996–1002,
1002 ex.
strongly connected component, 1170
decomposition into, 615–621, 623
S
TRONGLY -CONNECTED -COMPONENTS , 617
strongly connected graph, 1170
subgraph, 1171
predecessor, seepredecessor subgraph
subgraph-isomorphism problem, 1100 ex.
subgroup, 943–946
subpath, 1170
subproblem graph, 367–368
subroutine
calling, 21, 23, 25 n.executing, 25 n.
subsequence, 391
subset (/DC2), 1159, 1161
hereditary family of, 437
independent family of, 437
SUBSET-SUM, 1097
subset-sum problem
approximation algorithm for, 1128–1134,
1139
NP-completeness of, 1097–1100
with unary target, 1101 ex.
substitution method, 83–88
and recursion trees, 91–92
substring, 1184
subtract instruction, 23
subtraction of matrices, 1221
subtree, 1176
maintaining sizes of, in order-statistic trees,
343–344success, in a Bernoulli trial, 1201
successor
in binary search trees, 291–292
in a bit vector with a superimposed binary
tree, 533
in a bit vector with a superimposed tree of
constant height, 535
ﬁnding ith, of a node in an order-statistic
tree, 344 ex.
in linked lists, 236in order-statistic trees, 347 ex.
in proto van Emde Boas structures, 543–544
in red-black trees, 311
in Van Emde Boas trees, 550–551
S
UCCESSOR , 230
such that (W), 1159
sufﬁx (=), 986
sufﬁx function, 996
sufﬁx-function inequality, 999sufﬁx-function recursion lemma, 1000
sum.P/, 1145
Cartesian, 906 ex.
inﬁnite, 1145
of matrices, 1220
of polynomials, 898
rule of, 1183telescoping, 1148
S
UM-ARRAYS , 805 pr.
SUM-ARRAYS0, 805 pr.
summary
in a bit vector with a superimposed tree of
constant height, 534
in proto van Emde Boas structures, 540
in van Emde Boas trees, 546
summation, 1145–1157
in asymptotic notation, 49–50, 1146
bounding, 1149–1156
formulas and properties of, 1145–1149linearity of, 1146
summation lemma, 908
supercomputer, 772superpolynomial time, 1048
supersink, 712
supersource, 712
surjection, 1167
SVD, 842
sweeping, 1021–1029, 1045 pr.
rotational, 1030–1038Index 1289
sweep line, 1022
sweep-line status, 1023–1024
symbol table, 253, 262, 265
symmetric difference, 763 pr.symmetric matrix, 1220, 1222 ex., 1226 ex.
symmetric positive-deﬁnite matrix, 832–835,
842
symmetric relation, 1163
symmetry of ‚-notation, 52
sync , in pseudocode, 776–777
system of difference constraints, 664–670
system of linear equations, 806 pr., 813–827,
840 pr.
T
ABLE -DELETE , 468
TABLE -INSERT , 464
tail
of a binomial distribution, 1208–1215
of a linked list, 236of a queue, 234
tail recursion, 188 pr., 419
T
AIL-RECURSIVE -QUICKSORT , 188 pr.
target, 1097Tarjan’s off-line least-common-ancestors
algorithm, 584 pr.
task, 443Task Parallel Library, 774
task scheduling, 443–446, 448 pr., 450
tautology, 1066 ex., 1086 ex.
Taylor series, 306 pr.
telescoping series, 1148
telescoping sum, 1148testing
of primality, 965–975, 983
of pseudoprimality, 966–968
text, in string matching, 985
then clause, 20 n.
Theta-notation, 44–47, 64thread, 773
Threading Building Blocks, 774
3-CNF, 1082
3-CNF-SAT, 1082
3-CNF satisﬁability, 1082–1085, 1105
approximation algorithm for, 1123–1124,
1139
and 2-CNF satisﬁability, 1049
3-COLOR, 1103 pr.3-conjunctive normal form, 1082tight constraint, 865
time, seerunning time
time domain, 898
time-memory trade-off, 365timestamp, 603, 611 ex.
Toeplitz matrix, 921 pr.
to, in pseudocode, 20
T
OP, 1031
top-down method, for dynamic programming,
365
top of a stack, 232
topological sort, 8, 612–615, 623
in computing single-source shortest paths in
a dag, 655
TOPOLOGICAL -SORT, 613
total order, 1165
total path length, 304 pr.
total preorder, 1165
total relation, 1165tour
bitonic, 405 pr.
Euler, 623 pr., 1048
of a graph, 1096
track, 486
tractability, 1048
trailing pointer, 295
transition function, 995, 1001–1002, 1012 ex.
transitive closure, 697–699
and boolean matrix multiplication, 832 ex.of dynamic graphs, 705 pr., 707
T
RANSITIVE -CLOSURE , 698
transitive relation, 1163transitivity of asymptotic notation, 51
T
RANSPLANT , 296, 323
transpose
conjugate, 832 ex.
of a directed graph, 592 ex.
of a matrix, 1217of a matrix, multithreaded, 792 ex.
transpose symmetry of asymptotic notation, 52
traveling-salesman problem
approximation algorithm for, 1111–1117,
1139
bitonic euclidean, 405 pr.bottleneck, 1117 ex.
NP-completeness of, 1096–1097
with the triangle inequality, 1112–1115
without the triangle inequality, 1115–11161290 Index
traversal of a tree, 287, 293 ex., 342, 1114
treap, 333 pr., 338
TREAP -INSERT , 333 pr.
tree, 1173–1180
AA-trees, 338
A VL, 333 pr., 337
binary, seebinary tree
binomial, 527 pr.
bisection of, 1181 pr.
breadth-ﬁrst, 594, 600
B-trees, 484–504
decision, 192–193
depth-ﬁrst, 603diameter of, 602 ex.
dynamic, 482
free, 1172–1176full walk of, 1114
fusion, 212, 483
heap, 151–169height-balanced, 333 pr.
height of, 1177
interval, 348–354
k-neighbor, 338
minimum spanning, seeminimum spanning
tree
optimal binary search, 397–404, 413order-statistic, 339–345
parse, 1082
recursion, 37, 88–93red-black, seered-black tree
rooted, 246–249, 1176
scapegoat, 338
search, seesearch tree
shortest-paths, 647–648, 673–676
spanning, seeminimum spanning tree,
spanning tree
splay, 338, 482
treap, 333 pr., 3382-3, 337, 504
2-3-4, 489, 503 pr.
van Emde Boas, 531–560walk of, 287, 293 ex., 342, 1114
weight-balanced trees, 338
T
REE-DELETE , 298, 299 ex., 323–324
tree edge, 601, 603, 609
TREE-INSERT , 294, 315
TREE-MAXIMUM , 291
TREE-MINIMUM , 291TREE-PREDECESSOR , 292
TREE-SEARCH , 290
TREE-SUCCESSOR , 292
tree walk, 287, 293 ex., 342, 1114trial, Bernoulli, 1201
trial division, 966
triangle inequality, 1112
for shortest paths, 650, 671
triangular matrix, 1219, 1222 ex., 1225 ex.
trichotomy, interval, 348
trichotomy property of real numbers, 52
tridiagonal linear systems, 840 pr.
tridiagonal matrix, 1219trie (radix tree), 304 pr.
y-fast, 558 pr.
T
RIM, 1130
trimming a list, 1130
trivial divisor, 928
truth assignment, 1072, 1079
truth table, 1070
TSP, 1096
tuple, 1162twiddle factor, 9122-CNF-SAT, 1086 ex.
2-CNF satisﬁability, 1086 ex.
and 3-CNF satisﬁability, 1049
two-pass method, 571
2-3-4 heap, 529 pr.
2-3-4 tree, 489
joining, 503 pr.
splitting, 503 pr.
2-3 tree, 337, 504
unary, 1056
unbounded linear program, 851
unconditional branch instruction, 23
uncountable set, 1161
underdetermined system of linear equations,
814
underﬂow
of a queue, 234of a stack, 233
undirected graph, 1168
articulation point of, 621 pr.
biconnected component of, 621 pr.
bridge of, 621 pr.
clique in, 1086coloring of, 1103 pr., 1180 pr.Index 1291
computing a minimum spanning tree in,
624–642
converting to, from a multigraph, 593 ex.
d-regular, 736 ex.
grid, 760 pr.
hamiltonian, 1061
independent set of, 1101 pr.
matching of, 732
nonhamiltonian, 1061
vertex cover of, 1089, 1108see also graph
undirected version of a directed graph, 1172
uniform hashing, 271uniform probability distribution, 1191–1192
uniform random permutation, 116, 125
union
of dynamic sets, seeuniting
of languages, 1058
of sets ([), 1159
U
NION , 505, 562
disjoint-set-forest implementation of, 571
linked-list implementation of, 565–567,
568 ex.
union by rank, 569
unique factorization of integers, 931
unit ( 1), 928
uniting
of Fibonacci heaps, 511–512
of heaps, 506
of linked lists, 241 ex.
of 2-3-4 heaps, 529 pr.
unit lower-triangular matrix, 1219
unit-time task, 443
unit upper-triangular matrix, 1219
unit vector, 1218universal collection of hash functions, 265
universal hashing, 265–268
universal sink, 593 ex.universe, 1160
of keys in van Emde Boas trees, 532
universe size, 532unmatched vertex, 732
unsorted linked list, 236
see also linked list
until , in pseudocode, 20
unweighted longest simple paths, 382
unweighted shortest paths, 381
upper bound, 47upper-bound property, 650, 671–672
upper median, 213
upper square root/NUL
"p
/SOH
, 546
upper-triangular matrix, 1219, 1225 ex.
valid shift, 985
value
of a ﬂow, 710
of a function, 1166
objective, 847, 851
value over replacement player, 411 pr.
Vandermonde matrix, 902, 1226 pr.
van Emde Boas tree, 531–560
cluster in, 546
compared with proto van Emde Boas
structures, 547
deletion from, 554–556
insertion into, 552–554
maximum in, 550membership in, 550
minimum in, 550
predecessor in, 551–552
with reduced space, 557 pr.
successor in, 550–551
summary in, 546
VarŒ/c141(variance), 1199
variable
basic, 855
entering, 867
leaving, 867
nonbasic, 855
in pseudocode, 21
random, 1196–1201
slack, 855
see also indicator random variable
variable-length code, 429
variance, 1199
of a binomial distribution, 1205of a geometric distribution, 1203
VEB-E MPTY -TREE-INSERT , 553
vEB tree, seevan Emde Boas tree
VEB-T REE-DELETE , 554
VEB-T REE-INSERT , 553
VEB-T REE-MAXIMUM , 550
VEB-T REE-MEMBER , 550
VEB-T REE-MINIMUM , 550
VEB-T REE-PREDECESSOR , 552
VEB-T REE-SUCCESSOR , 5511292 Index
vector, 1218, 1222–1224
convolution of, 901
cross product of, 1016
orthonormal, 842in the plane, 1015
Venn diagram, 1160
veriﬁcation, 1061–1066
of spanning trees, 642
veriﬁcation algorithm, 1063
vertex
articulation point, 621 pr.
attributes of, 592
capacity of, 714 ex.in a graph, 1168
intermediate, 693
isolated, 1169
overﬂowing, 736
of a polygon, 1020 ex.
relabeled, 740selector, 1093
vertex cover, 1089, 1108, 1124–1127, 1139
VERTEX-COVER, 1090vertex-cover problem
approximation algorithm for, 1108–1111,
1139
NP-completeness of, 1089–1091, 1105
vertex set, 1168
violation, of an equality constraint, 865
virtual memory, 24
Viterbi algorithm, 408 pr.
VORP, 411 pr.
walk of a tree, 287, 293 ex., 342, 1114
weak duality, 880–881, 886 ex., 895 pr.
weight
of a cut, 1127 ex.
of an edge, 591
mean, 680 pr.
of a path, 643
weight-balanced tree, 338, 473 pr.
weighted bipartite matching, 530
weighted matroid, 439–442
weighted median, 225 pr.
weighted set-covering problem, 1135 pr.weighted-union heuristic, 566
weighted vertex cover, 1124–1127, 1139
weight function
for a graph, 591in a weighted matroid, 439
while , in pseudocode, 20
white-path theorem, 608
white vertex, 594, 603widget, 1092
wire, 1071
W
ITNESS , 969
witness, to the compositeness of a number, 968
work law, 780
work, of a multithreaded computation, 779
work-stealing scheduling algorithm, 812
worst-case running time, 27, 49
Yen’s improvement to the Bellman-Ford
algorithm, 678 pr.
y-fast trie, 558 pr.
Young tableau, 167 pr.
Z(set of integers), 1158
Zn(equivalence classes modulo n), 928
Z/ETX
n(elements of multiplicative group
modulo n), 941
ZC
n(nonzero elements of Zn), 967
zero matrix, 1218
zero of a polynomial modulo a prime, 950 ex.
0-1 integer programming, 1100 ex., 1125
0-1 knapsack problem, 425, 427 ex., 1137 pr.,
1139
0-1 sorting lemma, 208 pr.
zonk, 1195 ex.